{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PSENet_trial_run.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kavyajeetbora/PSENet.pytorch/blob/master/PSENet_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UByxwp0F3QUp",
        "colab_type": "text"
      },
      "source": [
        "## Installing softwares and other dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GSEHnmCxK_o9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, Dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y99kqKlHoYkz",
        "colab_type": "code",
        "outputId": "e66cfd79-3f56-403a-a469-b1709335653d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "!pip install pyclipper"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyclipper\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e1/40/57a0d54a1c696d58253c88a95677e50ab2b305a15af0ac64b70db4320562/pyclipper-1.1.0.post3-cp36-cp36m-manylinux1_x86_64.whl (131kB)\n",
            "\r\u001b[K     |██▌                             | 10kB 28.1MB/s eta 0:00:01\r\u001b[K     |█████                           | 20kB 24.6MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 30kB 29.5MB/s eta 0:00:01\r\u001b[K     |██████████                      | 40kB 33.6MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 51kB 25.7MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 61kB 28.5MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 71kB 23.1MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 81kB 24.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 92kB 26.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 102kB 24.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 112kB 24.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 122kB 24.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 133kB 24.6MB/s \n",
            "\u001b[?25hInstalling collected packages: pyclipper\n",
            "Successfully installed pyclipper-1.1.0.post3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ncoynOlfnIW1",
        "colab_type": "code",
        "outputId": "498622a8-d7b6-4a07-ff8c-7cfaab176c95",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hhJ-6yyXcnRx",
        "colab_type": "code",
        "outputId": "b80e2e86-7650-437f-b1d2-0c46b0ffd759",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "import shutil, os\n",
        "os.chdir('/content')\n",
        "directory = '/content/cloned-repo'\n",
        "if os.path.exists(directory):\n",
        "  shutil.rmtree(directory)\n",
        "\n",
        "!git clone https://github.com/kavyajeetbora/PSENet.pytorch.git /content/cloned-repo\n",
        "print(\"Cloned the repository\")\n",
        "os.chdir('/content/cloned-repo')\n",
        "!ls"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into '/content/cloned-repo'...\n",
            "remote: Enumerating objects: 12, done.\u001b[K\n",
            "remote: Counting objects:   8% (1/12)\u001b[K\rremote: Counting objects:  16% (2/12)\u001b[K\rremote: Counting objects:  25% (3/12)\u001b[K\rremote: Counting objects:  33% (4/12)\u001b[K\rremote: Counting objects:  41% (5/12)\u001b[K\rremote: Counting objects:  50% (6/12)\u001b[K\rremote: Counting objects:  58% (7/12)\u001b[K\rremote: Counting objects:  66% (8/12)\u001b[K\rremote: Counting objects:  75% (9/12)\u001b[K\rremote: Counting objects:  83% (10/12)\u001b[K\rremote: Counting objects:  91% (11/12)\u001b[K\rremote: Counting objects: 100% (12/12)\u001b[K\rremote: Counting objects: 100% (12/12), done.\u001b[K\n",
            "remote: Compressing objects: 100% (12/12), done.\u001b[K\n",
            "remote: Total 575 (delta 3), reused 0 (delta 0), pack-reused 563\u001b[K\n",
            "Receiving objects: 100% (575/575), 21.66 MiB | 2.73 MiB/s, done.\n",
            "Resolving deltas: 100% (300/300), done.\n",
            "Cloned the repository\n",
            "cal_recall  install_dependencies.sh  PSENet.ipynb\t     train.py\n",
            "config.py   LICENSE\t\t     PSENet_predict.ipynb    utils\n",
            "dataset     models\t\t     PSENet_training.ipynb\n",
            "eval.py     predict.py\t\t     PSENet_trial_run.ipynb\n",
            "imgs\t    pse\t\t\t     README.md\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PrOmfo6_3X2I",
        "colab_type": "text"
      },
      "source": [
        "## Extracting the data and setting up the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dGLMxTujlm0w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## unzipping the files\n",
        "from zipfile import ZipFile\n",
        "\n",
        "def unzip_files(file,output_dir):\n",
        "  with ZipFile(file, 'r') as zipObj:\n",
        "    # Extract all the contents of zip file in current directory\n",
        "    zipObj.extractall(output_dir)\n",
        "  print('Extracted to',output_dir)\n",
        "\n",
        "def make_directory(directory):\n",
        "  if os.path.isdir(directory):\n",
        "    shutil.rmtree(directory)\n",
        "  \n",
        "  os.mkdir(directory)\n",
        "  print('Created a new directory')\n",
        "\n",
        "training_data_zip = '/content/drive/My Drive/Colab Notebooks/padh.ai.notebooks/15. Object Detection/Scene Text Detection Dataset/English and Hindi MLT 2019.zip'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FqJ6xb5u2dgg",
        "colab_type": "code",
        "outputId": "1b59d55e-ae1a-4a82-fbcb-99feaae5e938",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# make directories\n",
        "make_directory('Training Set')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Created a new directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FFSaSXr-m-K4",
        "colab_type": "code",
        "outputId": "3caec78a-532f-4693-9fae-067cd7ad2a60",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "%%time\n",
        "unzip_files(training_data_zip,'Training Set')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracted to Training Set\n",
            "CPU times: user 2.35 s, sys: 942 ms, total: 3.29 s\n",
            "Wall time: 7.85 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YRSeyluknVH4",
        "colab_type": "code",
        "outputId": "b5d3be3b-dcea-43bb-acce-2cd0e38b54f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(len(os.listdir('Training Set/Images')))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MOp4JsZ6nXvn",
        "colab_type": "code",
        "outputId": "fdda0f2a-5669-4780-e17a-2eb655a01992",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(len(os.listdir('Training Set/Annotations')))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mhvg89mXn8zG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from dataset.data_utils import *\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader, Dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4JeHTz3G6Lyj",
        "colab_type": "code",
        "outputId": "5fd5fbba-6a9c-4475-b91f-8dd377d75e89",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_data = MyDataset('Training Set',transform=transforms.ToTensor())\n",
        "len(train_data)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1938"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "odLKMn-5SBAz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device(\"cuda:0\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9a9PsyF3dqo",
        "colab_type": "text"
      },
      "source": [
        "## Training "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9x221Ya0RBGO",
        "outputId": "20797a0d-cf4f-45c6-b5f9-020d4406121a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "!chmod +x install_dependencies.sh # make shell script executable\n",
        "!./install_dependencies.sh # run the shell script"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyclipper in /usr/local/lib/python3.6/dist-packages (1.1.0.post3)\n",
            "Collecting Polygon3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/73/a0/d07a4f3e80ed7020a33f3111db217f54ac44a485ff45da3c21ce49f65041/Polygon3-3.0.8.tar.gz (71kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 5.9MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: Polygon3\n",
            "  Building wheel for Polygon3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for Polygon3: filename=Polygon3-3.0.8-cp36-cp36m-linux_x86_64.whl size=101493 sha256=78cbc473c386b1b80ef2762715f40d39f776aea684a89ef5ede33bc97ba066a3\n",
            "  Stored in directory: /root/.cache/pip/wheels/95/32/f1/5525b233996d9d99cbce2f0a8da60d137ddddc555d3e8b0e2a\n",
            "Successfully built Polygon3\n",
            "Installing collected packages: Polygon3\n",
            "Successfully installed Polygon3-3.0.8\n",
            "Collecting colorlog\n",
            "  Downloading https://files.pythonhosted.org/packages/68/4d/892728b0c14547224f0ac40884e722a3d00cb54e7a146aea0b3186806c9e/colorlog-4.0.2-py2.py3-none-any.whl\n",
            "Installing collected packages: colorlog\n",
            "Successfully installed colorlog-4.0.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xD4GuTKC3wXY",
        "colab_type": "code",
        "outputId": "b6ccea4a-d773-4d43-b5f1-e6c07ea03559",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!python3 train.py"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "make: Entering directory '/content/cloned-repo/pse'\n",
            "make: 'pse.so' is up to date.\n",
            "make: Leaving directory '/content/cloned-repo/pse'\n",
            "2019-12-06 03:46:43 \u001b[32mINFO     \u001b[0m utils.py: logger init finished\u001b[0m\n",
            "2019-12-06 03:46:43 \u001b[32mINFO     \u001b[0m train.py: {'Lambda': 0.7,\n",
            " 'OHEM_ratio': 3,\n",
            " 'backbone': 'resnet50',\n",
            " 'checkpoint': '',\n",
            " 'data_shape': 640,\n",
            " 'display_input_images': False,\n",
            " 'display_interval': 10,\n",
            " 'display_output_images': False,\n",
            " 'end_lr': 1e-06,\n",
            " 'epochs': 300,\n",
            " 'gpu_id': '0',\n",
            " 'lr': 0.0001,\n",
            " 'lr_decay_step': [100, 200],\n",
            " 'lr_gamma': 0.1,\n",
            " 'm': 0.5,\n",
            " 'n': 6,\n",
            " 'output_dir': '/content/drive/My Drive/PSENet_2',\n",
            " 'pretrained': False,\n",
            " 'pretrained_path': '/content/drive/My Drive/PSENet_2/PSENet_resnet50.pth',\n",
            " 'restart_training': False,\n",
            " 'scale': 1,\n",
            " 'seed': 2,\n",
            " 'show_images_interval': 50,\n",
            " 'start_epoch': 0,\n",
            " 'testroot': 'Test Set',\n",
            " 'train_batch_size': 4,\n",
            " 'trainroot': 'Training Set',\n",
            " 'warm_up_epoch': 6,\n",
            " 'warm_up_lr': 1e-05,\n",
            " 'weight_decay': 0.0005,\n",
            " 'workers': 0}\u001b[0m\n",
            "2019-12-06 03:46:44 \u001b[32mINFO     \u001b[0m train.py: train with gpu 0 and pytorch 1.3.1\u001b[0m\n",
            "2019-12-06 03:46:55 \u001b[32mINFO     \u001b[0m train.py: train dataset has 1938 samples,484 in dataloader\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:100: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule.See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "2019-12-06 03:47:11 \u001b[32mINFO     \u001b[0m train.py: [0/300], [0/484], step: 0, 2.442 samples/sec, batch_loss: 0.3130, batch_loss_c: 0.3139, batch_loss_s: 0.3107, time:16.3791, lr:0.0001\u001b[0m\n",
            "tcmalloc: large alloc 1170505728 bytes == 0xd727e000 @  0x7feeb13d01e7 0x7feea59b4f71 0x7feea5a1855d 0x7feea5a1be28 0x7feea5a1c3e5 0x7feea5ab2fc2 0x50ac25 0x50d390 0x508245 0x50a080 0x50aa7d 0x50d390 0x509455 0x595311 0x5a067e 0x557d28 0x50c88b 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x509455 0x595311 0x5a522c 0x557f7e 0x4b5eff 0x50c810\n",
            "2019-12-06 03:47:27 \u001b[32mINFO     \u001b[0m train.py: [0/300], [10/484], step: 10, 2.581 samples/sec, batch_loss: 0.0982, batch_loss_c: 0.0954, batch_loss_s: 0.1047, time:15.4980, lr:0.0001\u001b[0m\n",
            "2019-12-06 03:47:39 \u001b[32mINFO     \u001b[0m train.py: [0/300], [20/484], step: 20, 3.340 samples/sec, batch_loss: 0.0508, batch_loss_c: 0.0381, batch_loss_s: 0.0804, time:11.9767, lr:0.0001\u001b[0m\n",
            "tcmalloc: large alloc 1343692800 bytes == 0xe3bf8000 @  0x7feeb13d01e7 0x7feea59b4f71 0x7feea5a1855d 0x7feea5a1be28 0x7feea5a1c3e5 0x7feea5ab2fc2 0x50ac25 0x50d390 0x508245 0x50a080 0x50aa7d 0x50d390 0x509455 0x595311 0x5a067e 0x557d28 0x50c88b 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x509455 0x595311 0x5a522c 0x557f7e 0x4b5eff 0x50c810\n",
            "2019-12-06 03:47:56 \u001b[32mINFO     \u001b[0m train.py: [0/300], [30/484], step: 30, 2.267 samples/sec, batch_loss: 0.1117, batch_loss_c: 0.0975, batch_loss_s: 0.1447, time:17.6418, lr:0.0001\u001b[0m\n",
            "2019-12-06 03:48:11 \u001b[32mINFO     \u001b[0m train.py: [0/300], [40/484], step: 40, 2.655 samples/sec, batch_loss: 0.0863, batch_loss_c: 0.0891, batch_loss_s: 0.0797, time:15.0652, lr:0.0001\u001b[0m\n",
            "tcmalloc: large alloc 1725898752 bytes == 0xe3b7e000 @  0x7feeb13d01e7 0x7feea59b4f71 0x7feea5a1855d 0x7feea5a1be28 0x7feea5a1c3e5 0x7feea5ab2fc2 0x50ac25 0x50d390 0x508245 0x50a080 0x50aa7d 0x50d390 0x509455 0x595311 0x5a067e 0x557d28 0x50c88b 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x509455 0x595311 0x5a522c 0x557f7e 0x4b5eff 0x50c810\n",
            "tcmalloc: large alloc 1871200256 bytes == 0xed5f8000 @  0x7feeb13d01e7 0x7feea59b4f71 0x7feea5a1855d 0x7feea5a1be28 0x7feea5a1c3e5 0x7feea5ab2fc2 0x50ac25 0x50d390 0x508245 0x50a080 0x50aa7d 0x50d390 0x509455 0x595311 0x5a067e 0x557d28 0x50c88b 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x509455 0x595311 0x5a522c 0x557f7e 0x4b5eff 0x50c810\n",
            "2019-12-06 03:48:33 \u001b[32mINFO     \u001b[0m train.py: [0/300], [50/484], step: 50, 1.824 samples/sec, batch_loss: 0.0884, batch_loss_c: 0.0811, batch_loss_s: 0.1056, time:21.9266, lr:0.0001\u001b[0m\n",
            "2019-12-06 03:48:50 \u001b[32mINFO     \u001b[0m train.py: [0/300], [60/484], step: 60, 2.424 samples/sec, batch_loss: 0.5600, batch_loss_c: 0.5606, batch_loss_s: 0.5587, time:16.4994, lr:0.0001\u001b[0m\n",
            "2019-12-06 03:49:02 \u001b[32mINFO     \u001b[0m train.py: [0/300], [70/484], step: 70, 3.235 samples/sec, batch_loss: 0.0606, batch_loss_c: 0.0466, batch_loss_s: 0.0933, time:12.3631, lr:0.0001\u001b[0m\n",
            "2019-12-06 03:49:27 \u001b[32mINFO     \u001b[0m train.py: [0/300], [80/484], step: 80, 1.616 samples/sec, batch_loss: 0.0448, batch_loss_c: 0.0349, batch_loss_s: 0.0677, time:24.7590, lr:0.0001\u001b[0m\n",
            "tcmalloc: large alloc 2633637888 bytes == 0x7feb2305e000 @  0x7feeb13d01e7 0x7feea59b4f71 0x7feea5a1855d 0x7feea5a1be28 0x7feea5a1c3e5 0x7feea5ab2fc2 0x50ac25 0x50d390 0x508245 0x50a080 0x50aa7d 0x50d390 0x509455 0x595311 0x5a067e 0x557d28 0x50c88b 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x509455 0x595311 0x5a522c 0x557f7e 0x4b5eff 0x50c810\n",
            "2019-12-06 03:49:46 \u001b[32mINFO     \u001b[0m train.py: [0/300], [90/484], step: 90, 2.073 samples/sec, batch_loss: 0.0708, batch_loss_c: 0.0595, batch_loss_s: 0.0972, time:19.2992, lr:0.0001\u001b[0m\n",
            "tcmalloc: large alloc 2018959360 bytes == 0x7feb461f4000 @  0x7feeb13d01e7 0x7feea59b4f71 0x7feea5a1855d 0x7feea5a1be28 0x7feea5a1c3e5 0x7feea5ab2fc2 0x50ac25 0x50d390 0x508245 0x50a080 0x50aa7d 0x50d390 0x509455 0x595311 0x5a067e 0x557d28 0x50c88b 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x509455 0x595311 0x5a522c 0x557f7e 0x4b5eff 0x50c810\n",
            "2019-12-06 03:50:16 \u001b[32mINFO     \u001b[0m train.py: [0/300], [100/484], step: 100, 1.344 samples/sec, batch_loss: 0.0989, batch_loss_c: 0.0977, batch_loss_s: 0.1015, time:29.7568, lr:0.0001\u001b[0m\n",
            "2019-12-06 03:50:32 \u001b[32mINFO     \u001b[0m train.py: [0/300], [110/484], step: 110, 2.503 samples/sec, batch_loss: 0.0623, batch_loss_c: 0.0558, batch_loss_s: 0.0775, time:15.9826, lr:0.0001\u001b[0m\n",
            "tcmalloc: large alloc 3371098112 bytes == 0x7fea5a170000 @  0x7feeb13d01e7 0x7feea59b4f71 0x7feea5a1855d 0x7feea5a1be28 0x7feea5a1c3e5 0x7feea5ab2fc2 0x50ac25 0x50d390 0x508245 0x50a080 0x50aa7d 0x50d390 0x509455 0x595311 0x5a067e 0x557d28 0x50c88b 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x509455 0x595311 0x5a522c 0x557f7e 0x4b5eff 0x50c810\n",
            "2019-12-06 03:51:04 \u001b[32mINFO     \u001b[0m train.py: [0/300], [120/484], step: 120, 1.232 samples/sec, batch_loss: 0.0509, batch_loss_c: 0.0418, batch_loss_s: 0.0720, time:32.4567, lr:0.0001\u001b[0m\n",
            "tcmalloc: large alloc 2632581120 bytes == 0x7fea5a170000 @  0x7feeb13d01e7 0x7feea59b4f71 0x7feea5a1855d 0x7feea5a1be28 0x7feea5a1c3e5 0x7feea5ab2fc2 0x50ac25 0x50d390 0x508245 0x50a080 0x50aa7d 0x50d390 0x509455 0x595311 0x5a067e 0x557d28 0x50c88b 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x509455 0x595311 0x5a522c 0x557f7e 0x4b5eff 0x50c810\n",
            "2019-12-06 03:51:29 \u001b[32mINFO     \u001b[0m train.py: [0/300], [130/484], step: 130, 1.607 samples/sec, batch_loss: 0.1629, batch_loss_c: 0.1949, batch_loss_s: 0.0884, time:24.8841, lr:0.0001\u001b[0m\n",
            "2019-12-06 03:51:42 \u001b[32mINFO     \u001b[0m train.py: [0/300], [140/484], step: 140, 3.108 samples/sec, batch_loss: 0.2995, batch_loss_c: 0.2937, batch_loss_s: 0.3130, time:12.8686, lr:0.0001\u001b[0m\n",
            "2019-12-06 03:51:55 \u001b[32mINFO     \u001b[0m train.py: [0/300], [150/484], step: 150, 3.186 samples/sec, batch_loss: 0.0730, batch_loss_c: 0.0658, batch_loss_s: 0.0899, time:12.5535, lr:0.0001\u001b[0m\n",
            "2019-12-06 03:52:20 \u001b[32mINFO     \u001b[0m train.py: [0/300], [160/484], step: 160, 1.590 samples/sec, batch_loss: 0.1352, batch_loss_c: 0.1302, batch_loss_s: 0.1467, time:25.1519, lr:0.0001\u001b[0m\n",
            "2019-12-06 03:52:34 \u001b[32mINFO     \u001b[0m train.py: [0/300], [170/484], step: 170, 2.849 samples/sec, batch_loss: 0.0579, batch_loss_c: 0.0529, batch_loss_s: 0.0698, time:14.0410, lr:0.0001\u001b[0m\n",
            "2019-12-06 03:52:50 \u001b[32mINFO     \u001b[0m train.py: [0/300], [180/484], step: 180, 2.416 samples/sec, batch_loss: 0.2056, batch_loss_c: 0.1894, batch_loss_s: 0.2433, time:16.5560, lr:0.0001\u001b[0m\n",
            "2019-12-06 03:53:07 \u001b[32mINFO     \u001b[0m train.py: [0/300], [190/484], step: 190, 2.484 samples/sec, batch_loss: 0.3476, batch_loss_c: 0.3470, batch_loss_s: 0.3490, time:16.1028, lr:0.0001\u001b[0m\n",
            "tcmalloc: large alloc 3381018624 bytes == 0x7fea5a170000 @  0x7feeb13d01e7 0x7feea59b4f71 0x7feea5a1855d 0x7feea5a1be28 0x7feea5a1c3e5 0x7feea5ab2fc2 0x50ac25 0x50d390 0x508245 0x50a080 0x50aa7d 0x50d390 0x509455 0x595311 0x5a067e 0x557d28 0x50c88b 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x509455 0x595311 0x5a522c 0x557f7e 0x4b5eff 0x50c810\n",
            "2019-12-06 03:53:41 \u001b[32mINFO     \u001b[0m train.py: [0/300], [200/484], step: 200, 1.171 samples/sec, batch_loss: 0.1899, batch_loss_c: 0.1746, batch_loss_s: 0.2258, time:34.1598, lr:0.0001\u001b[0m\n",
            "2019-12-06 03:53:58 \u001b[32mINFO     \u001b[0m train.py: [0/300], [210/484], step: 210, 2.333 samples/sec, batch_loss: 0.0847, batch_loss_c: 0.0741, batch_loss_s: 0.1093, time:17.1471, lr:0.0001\u001b[0m\n",
            "2019-12-06 03:54:15 \u001b[32mINFO     \u001b[0m train.py: [0/300], [220/484], step: 220, 2.337 samples/sec, batch_loss: 0.2818, batch_loss_c: 0.2793, batch_loss_s: 0.2878, time:17.1148, lr:0.0001\u001b[0m\n",
            "2019-12-06 03:54:27 \u001b[32mINFO     \u001b[0m train.py: [0/300], [230/484], step: 230, 3.247 samples/sec, batch_loss: 0.0594, batch_loss_c: 0.0534, batch_loss_s: 0.0734, time:12.3209, lr:0.0001\u001b[0m\n",
            "2019-12-06 03:54:43 \u001b[32mINFO     \u001b[0m train.py: [0/300], [240/484], step: 240, 2.571 samples/sec, batch_loss: 0.2705, batch_loss_c: 0.2490, batch_loss_s: 0.3206, time:15.5553, lr:0.0001\u001b[0m\n",
            "2019-12-06 03:54:55 \u001b[32mINFO     \u001b[0m train.py: [0/300], [250/484], step: 250, 3.250 samples/sec, batch_loss: 0.1639, batch_loss_c: 0.1779, batch_loss_s: 0.1312, time:12.3058, lr:0.0001\u001b[0m\n",
            "2019-12-06 03:55:19 \u001b[32mINFO     \u001b[0m train.py: [0/300], [260/484], step: 260, 1.697 samples/sec, batch_loss: 0.0881, batch_loss_c: 0.0850, batch_loss_s: 0.0954, time:23.5664, lr:0.0001\u001b[0m\n",
            "2019-12-06 03:55:37 \u001b[32mINFO     \u001b[0m train.py: [0/300], [270/484], step: 270, 2.188 samples/sec, batch_loss: 0.1380, batch_loss_c: 0.1340, batch_loss_s: 0.1473, time:18.2823, lr:0.0001\u001b[0m\n",
            "2019-12-06 03:56:05 \u001b[32mINFO     \u001b[0m train.py: [0/300], [280/484], step: 280, 1.455 samples/sec, batch_loss: 0.5482, batch_loss_c: 0.5366, batch_loss_s: 0.5752, time:27.4875, lr:0.0001\u001b[0m\n",
            "2019-12-06 03:56:23 \u001b[32mINFO     \u001b[0m train.py: [0/300], [290/484], step: 290, 2.166 samples/sec, batch_loss: 0.1583, batch_loss_c: 0.1678, batch_loss_s: 0.1360, time:18.4642, lr:0.0001\u001b[0m\n",
            "2019-12-06 03:56:39 \u001b[32mINFO     \u001b[0m train.py: [0/300], [300/484], step: 300, 2.569 samples/sec, batch_loss: 0.0742, batch_loss_c: 0.0676, batch_loss_s: 0.0895, time:15.5718, lr:0.0001\u001b[0m\n",
            "2019-12-06 03:56:55 \u001b[32mINFO     \u001b[0m train.py: [0/300], [310/484], step: 310, 2.447 samples/sec, batch_loss: 0.2765, batch_loss_c: 0.2601, batch_loss_s: 0.3150, time:16.3477, lr:0.0001\u001b[0m\n",
            "2019-12-06 03:57:12 \u001b[32mINFO     \u001b[0m train.py: [0/300], [320/484], step: 320, 2.355 samples/sec, batch_loss: 0.3025, batch_loss_c: 0.2968, batch_loss_s: 0.3156, time:16.9879, lr:0.0001\u001b[0m\n",
            "2019-12-06 03:57:30 \u001b[32mINFO     \u001b[0m train.py: [0/300], [330/484], step: 330, 2.179 samples/sec, batch_loss: 0.0635, batch_loss_c: 0.0600, batch_loss_s: 0.0718, time:18.3571, lr:0.0001\u001b[0m\n",
            "tcmalloc: large alloc 4142145536 bytes == 0x7fea5a170000 @  0x7feeb13d01e7 0x7feea59b4f71 0x7feea5a1855d 0x7feea5a1be28 0x7feea5a1c3e5 0x7feea5ab2fc2 0x50ac25 0x50d390 0x508245 0x50a080 0x50aa7d 0x50d390 0x509455 0x595311 0x5a067e 0x557d28 0x50c88b 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x509455 0x595311 0x5a522c 0x557f7e 0x4b5eff 0x50c810\n",
            "2019-12-06 03:58:16 \u001b[32mINFO     \u001b[0m train.py: [0/300], [340/484], step: 340, 0.883 samples/sec, batch_loss: 0.0602, batch_loss_c: 0.0534, batch_loss_s: 0.0759, time:45.2813, lr:0.0001\u001b[0m\n",
            "2019-12-06 03:58:31 \u001b[32mINFO     \u001b[0m train.py: [0/300], [350/484], step: 350, 2.579 samples/sec, batch_loss: 0.1356, batch_loss_c: 0.1464, batch_loss_s: 0.1105, time:15.5091, lr:0.0001\u001b[0m\n",
            "2019-12-06 03:59:04 \u001b[32mINFO     \u001b[0m train.py: [0/300], [360/484], step: 360, 1.221 samples/sec, batch_loss: 0.2361, batch_loss_c: 0.2021, batch_loss_s: 0.3154, time:32.7632, lr:0.0001\u001b[0m\n",
            "2019-12-06 03:59:20 \u001b[32mINFO     \u001b[0m train.py: [0/300], [370/484], step: 370, 2.426 samples/sec, batch_loss: 0.2957, batch_loss_c: 0.2922, batch_loss_s: 0.3038, time:16.4908, lr:0.0001\u001b[0m\n",
            "2019-12-06 03:59:37 \u001b[32mINFO     \u001b[0m train.py: [0/300], [380/484], step: 380, 2.350 samples/sec, batch_loss: 0.1158, batch_loss_c: 0.1205, batch_loss_s: 0.1048, time:17.0235, lr:0.0001\u001b[0m\n",
            "2019-12-06 03:59:52 \u001b[32mINFO     \u001b[0m train.py: [0/300], [390/484], step: 390, 2.819 samples/sec, batch_loss: 0.3109, batch_loss_c: 0.3122, batch_loss_s: 0.3079, time:14.1871, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:00:13 \u001b[32mINFO     \u001b[0m train.py: [0/300], [400/484], step: 400, 1.876 samples/sec, batch_loss: 0.3279, batch_loss_c: 0.3269, batch_loss_s: 0.3303, time:21.3170, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:00:28 \u001b[32mINFO     \u001b[0m train.py: [0/300], [410/484], step: 410, 2.588 samples/sec, batch_loss: 0.0789, batch_loss_c: 0.0594, batch_loss_s: 0.1242, time:15.4532, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:00:42 \u001b[32mINFO     \u001b[0m train.py: [0/300], [420/484], step: 420, 3.023 samples/sec, batch_loss: 0.1307, batch_loss_c: 0.1275, batch_loss_s: 0.1381, time:13.2317, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:00:58 \u001b[32mINFO     \u001b[0m train.py: [0/300], [430/484], step: 430, 2.420 samples/sec, batch_loss: 0.1117, batch_loss_c: 0.0973, batch_loss_s: 0.1454, time:16.5277, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:01:11 \u001b[32mINFO     \u001b[0m train.py: [0/300], [440/484], step: 440, 2.995 samples/sec, batch_loss: 0.1816, batch_loss_c: 0.2164, batch_loss_s: 0.1005, time:13.3542, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:01:26 \u001b[32mINFO     \u001b[0m train.py: [0/300], [450/484], step: 450, 2.660 samples/sec, batch_loss: 0.0666, batch_loss_c: 0.0559, batch_loss_s: 0.0917, time:15.0357, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:01:48 \u001b[32mINFO     \u001b[0m train.py: [0/300], [460/484], step: 460, 1.835 samples/sec, batch_loss: 0.3492, batch_loss_c: 0.3498, batch_loss_s: 0.3479, time:21.7978, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:02:05 \u001b[32mINFO     \u001b[0m train.py: [0/300], [470/484], step: 470, 2.406 samples/sec, batch_loss: 0.1101, batch_loss_c: 0.1096, batch_loss_s: 0.1112, time:16.6227, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:02:33 \u001b[32mINFO     \u001b[0m train.py: [0/300], [480/484], step: 480, 1.443 samples/sec, batch_loss: 0.5355, batch_loss_c: 0.5267, batch_loss_s: 0.5562, time:27.7242, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:02:36 \u001b[32mINFO     \u001b[0m train.py: [0/300], train_loss: 0.1537, time: 941.0128, lr: 0.0001\u001b[0m\n",
            "2019-12-06 04:02:37 \u001b[32mINFO     \u001b[0m train.py: [1/300], [0/484], step: 484, 38.505 samples/sec, batch_loss: 0.0439, batch_loss_c: 0.0393, batch_loss_s: 0.0544, time:1.0388, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:02:54 \u001b[32mINFO     \u001b[0m train.py: [1/300], [10/484], step: 494, 2.453 samples/sec, batch_loss: 0.1691, batch_loss_c: 0.2098, batch_loss_s: 0.0740, time:16.3090, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:03:06 \u001b[32mINFO     \u001b[0m train.py: [1/300], [20/484], step: 504, 3.346 samples/sec, batch_loss: 0.0762, batch_loss_c: 0.0720, batch_loss_s: 0.0861, time:11.9539, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:03:34 \u001b[32mINFO     \u001b[0m train.py: [1/300], [30/484], step: 514, 1.385 samples/sec, batch_loss: 0.1464, batch_loss_c: 0.1558, batch_loss_s: 0.1244, time:28.8791, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:03:54 \u001b[32mINFO     \u001b[0m train.py: [1/300], [40/484], step: 524, 2.088 samples/sec, batch_loss: 0.0876, batch_loss_c: 0.0792, batch_loss_s: 0.1072, time:19.1565, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:04:24 \u001b[32mINFO     \u001b[0m train.py: [1/300], [50/484], step: 534, 1.296 samples/sec, batch_loss: 0.2405, batch_loss_c: 0.2414, batch_loss_s: 0.2386, time:30.8553, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:04:40 \u001b[32mINFO     \u001b[0m train.py: [1/300], [60/484], step: 544, 2.540 samples/sec, batch_loss: 0.3885, batch_loss_c: 0.3887, batch_loss_s: 0.3882, time:15.7475, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:04:54 \u001b[32mINFO     \u001b[0m train.py: [1/300], [70/484], step: 554, 2.847 samples/sec, batch_loss: 0.0483, batch_loss_c: 0.0424, batch_loss_s: 0.0621, time:14.0518, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:05:08 \u001b[32mINFO     \u001b[0m train.py: [1/300], [80/484], step: 564, 2.994 samples/sec, batch_loss: 0.0951, batch_loss_c: 0.0909, batch_loss_s: 0.1049, time:13.3613, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:05:23 \u001b[32mINFO     \u001b[0m train.py: [1/300], [90/484], step: 574, 2.569 samples/sec, batch_loss: 0.0665, batch_loss_c: 0.0666, batch_loss_s: 0.0663, time:15.5717, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:05:46 \u001b[32mINFO     \u001b[0m train.py: [1/300], [100/484], step: 584, 1.744 samples/sec, batch_loss: 0.2905, batch_loss_c: 0.2859, batch_loss_s: 0.3013, time:22.9417, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:05:59 \u001b[32mINFO     \u001b[0m train.py: [1/300], [110/484], step: 594, 3.058 samples/sec, batch_loss: 0.0607, batch_loss_c: 0.0560, batch_loss_s: 0.0715, time:13.0804, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:06:12 \u001b[32mINFO     \u001b[0m train.py: [1/300], [120/484], step: 604, 3.067 samples/sec, batch_loss: 0.1095, batch_loss_c: 0.0770, batch_loss_s: 0.1852, time:13.0434, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:06:29 \u001b[32mINFO     \u001b[0m train.py: [1/300], [130/484], step: 614, 2.436 samples/sec, batch_loss: 0.0634, batch_loss_c: 0.0538, batch_loss_s: 0.0861, time:16.4170, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:07:01 \u001b[32mINFO     \u001b[0m train.py: [1/300], [140/484], step: 624, 1.225 samples/sec, batch_loss: 0.1410, batch_loss_c: 0.1396, batch_loss_s: 0.1445, time:32.6534, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:07:16 \u001b[32mINFO     \u001b[0m train.py: [1/300], [150/484], step: 634, 2.689 samples/sec, batch_loss: 0.1547, batch_loss_c: 0.1479, batch_loss_s: 0.1704, time:14.8735, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:07:31 \u001b[32mINFO     \u001b[0m train.py: [1/300], [160/484], step: 644, 2.764 samples/sec, batch_loss: 0.0685, batch_loss_c: 0.0460, batch_loss_s: 0.1209, time:14.4710, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:07:46 \u001b[32mINFO     \u001b[0m train.py: [1/300], [170/484], step: 654, 2.557 samples/sec, batch_loss: 0.0508, batch_loss_c: 0.0406, batch_loss_s: 0.0747, time:15.6435, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:08:03 \u001b[32mINFO     \u001b[0m train.py: [1/300], [180/484], step: 664, 2.348 samples/sec, batch_loss: 0.4143, batch_loss_c: 0.4017, batch_loss_s: 0.4438, time:17.0342, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:08:20 \u001b[32mINFO     \u001b[0m train.py: [1/300], [190/484], step: 674, 2.438 samples/sec, batch_loss: 0.1171, batch_loss_c: 0.1057, batch_loss_s: 0.1436, time:16.4060, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:08:59 \u001b[32mINFO     \u001b[0m train.py: [1/300], [200/484], step: 684, 1.010 samples/sec, batch_loss: 0.0618, batch_loss_c: 0.0478, batch_loss_s: 0.0945, time:39.6133, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:09:13 \u001b[32mINFO     \u001b[0m train.py: [1/300], [210/484], step: 694, 2.910 samples/sec, batch_loss: 0.0822, batch_loss_c: 0.0766, batch_loss_s: 0.0954, time:13.7459, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:09:35 \u001b[32mINFO     \u001b[0m train.py: [1/300], [220/484], step: 704, 1.855 samples/sec, batch_loss: 0.2106, batch_loss_c: 0.1471, batch_loss_s: 0.3586, time:21.5677, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:09:58 \u001b[32mINFO     \u001b[0m train.py: [1/300], [230/484], step: 714, 1.717 samples/sec, batch_loss: 0.2955, batch_loss_c: 0.2924, batch_loss_s: 0.3026, time:23.2905, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:10:12 \u001b[32mINFO     \u001b[0m train.py: [1/300], [240/484], step: 724, 2.932 samples/sec, batch_loss: 0.0735, batch_loss_c: 0.0664, batch_loss_s: 0.0903, time:13.6434, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:10:31 \u001b[32mINFO     \u001b[0m train.py: [1/300], [250/484], step: 734, 2.023 samples/sec, batch_loss: 0.0934, batch_loss_c: 0.0821, batch_loss_s: 0.1198, time:19.7704, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:10:55 \u001b[32mINFO     \u001b[0m train.py: [1/300], [260/484], step: 744, 1.732 samples/sec, batch_loss: 0.2981, batch_loss_c: 0.2827, batch_loss_s: 0.3340, time:23.0881, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:11:15 \u001b[32mINFO     \u001b[0m train.py: [1/300], [270/484], step: 754, 1.942 samples/sec, batch_loss: 0.0805, batch_loss_c: 0.0873, batch_loss_s: 0.0644, time:20.5955, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:11:31 \u001b[32mINFO     \u001b[0m train.py: [1/300], [280/484], step: 764, 2.514 samples/sec, batch_loss: 0.0693, batch_loss_c: 0.0653, batch_loss_s: 0.0785, time:15.9099, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:11:47 \u001b[32mINFO     \u001b[0m train.py: [1/300], [290/484], step: 774, 2.504 samples/sec, batch_loss: 0.3721, batch_loss_c: 0.3626, batch_loss_s: 0.3943, time:15.9732, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:11:58 \u001b[32mINFO     \u001b[0m train.py: [1/300], [300/484], step: 784, 3.492 samples/sec, batch_loss: 0.0885, batch_loss_c: 0.0696, batch_loss_s: 0.1329, time:11.4562, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:12:17 \u001b[32mINFO     \u001b[0m train.py: [1/300], [310/484], step: 794, 2.214 samples/sec, batch_loss: 0.0719, batch_loss_c: 0.0709, batch_loss_s: 0.0741, time:18.0675, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:12:30 \u001b[32mINFO     \u001b[0m train.py: [1/300], [320/484], step: 804, 2.923 samples/sec, batch_loss: 0.0668, batch_loss_c: 0.0508, batch_loss_s: 0.1041, time:13.6847, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:12:47 \u001b[32mINFO     \u001b[0m train.py: [1/300], [330/484], step: 814, 2.319 samples/sec, batch_loss: 0.2680, batch_loss_c: 0.2505, batch_loss_s: 0.3088, time:17.2495, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:13:04 \u001b[32mINFO     \u001b[0m train.py: [1/300], [340/484], step: 824, 2.375 samples/sec, batch_loss: 0.1137, batch_loss_c: 0.0841, batch_loss_s: 0.1826, time:16.8414, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:13:17 \u001b[32mINFO     \u001b[0m train.py: [1/300], [350/484], step: 834, 3.070 samples/sec, batch_loss: 0.3856, batch_loss_c: 0.3990, batch_loss_s: 0.3542, time:13.0282, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:13:30 \u001b[32mINFO     \u001b[0m train.py: [1/300], [360/484], step: 844, 3.052 samples/sec, batch_loss: 0.1054, batch_loss_c: 0.1010, batch_loss_s: 0.1157, time:13.1072, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:13:44 \u001b[32mINFO     \u001b[0m train.py: [1/300], [370/484], step: 854, 2.869 samples/sec, batch_loss: 0.0933, batch_loss_c: 0.0904, batch_loss_s: 0.1000, time:13.9405, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:13:57 \u001b[32mINFO     \u001b[0m train.py: [1/300], [380/484], step: 864, 3.067 samples/sec, batch_loss: 0.0757, batch_loss_c: 0.0665, batch_loss_s: 0.0971, time:13.0421, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:14:11 \u001b[32mINFO     \u001b[0m train.py: [1/300], [390/484], step: 874, 2.994 samples/sec, batch_loss: 0.1284, batch_loss_c: 0.1292, batch_loss_s: 0.1266, time:13.3612, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:14:25 \u001b[32mINFO     \u001b[0m train.py: [1/300], [400/484], step: 884, 2.723 samples/sec, batch_loss: 0.0669, batch_loss_c: 0.0662, batch_loss_s: 0.0686, time:14.6890, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:14:51 \u001b[32mINFO     \u001b[0m train.py: [1/300], [410/484], step: 894, 1.565 samples/sec, batch_loss: 0.5500, batch_loss_c: 0.5436, batch_loss_s: 0.5648, time:25.5571, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:15:10 \u001b[32mINFO     \u001b[0m train.py: [1/300], [420/484], step: 904, 2.155 samples/sec, batch_loss: 0.0508, batch_loss_c: 0.0430, batch_loss_s: 0.0689, time:18.5618, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:15:25 \u001b[32mINFO     \u001b[0m train.py: [1/300], [430/484], step: 914, 2.583 samples/sec, batch_loss: 0.3452, batch_loss_c: 0.3385, batch_loss_s: 0.3606, time:15.4872, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:15:41 \u001b[32mINFO     \u001b[0m train.py: [1/300], [440/484], step: 924, 2.439 samples/sec, batch_loss: 0.2651, batch_loss_c: 0.3043, batch_loss_s: 0.1737, time:16.3999, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:16:00 \u001b[32mINFO     \u001b[0m train.py: [1/300], [450/484], step: 934, 2.107 samples/sec, batch_loss: 0.0640, batch_loss_c: 0.0508, batch_loss_s: 0.0947, time:18.9831, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:16:22 \u001b[32mINFO     \u001b[0m train.py: [1/300], [460/484], step: 944, 1.865 samples/sec, batch_loss: 0.3202, batch_loss_c: 0.3056, batch_loss_s: 0.3545, time:21.4483, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:16:43 \u001b[32mINFO     \u001b[0m train.py: [1/300], [470/484], step: 954, 1.892 samples/sec, batch_loss: 0.1203, batch_loss_c: 0.1037, batch_loss_s: 0.1591, time:21.1469, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:17:01 \u001b[32mINFO     \u001b[0m train.py: [1/300], [480/484], step: 964, 2.229 samples/sec, batch_loss: 0.2265, batch_loss_c: 0.2255, batch_loss_s: 0.2290, time:17.9430, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:17:14 \u001b[32mINFO     \u001b[0m train.py: [1/300], train_loss: 0.1589, time: 877.4938, lr: 0.0001\u001b[0m\n",
            "2019-12-06 04:17:15 \u001b[32mINFO     \u001b[0m train.py: [2/300], [0/484], step: 968, 32.762 samples/sec, batch_loss: 0.0832, batch_loss_c: 0.0752, batch_loss_s: 0.1019, time:1.2209, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:17:27 \u001b[32mINFO     \u001b[0m train.py: [2/300], [10/484], step: 978, 3.346 samples/sec, batch_loss: 0.0889, batch_loss_c: 0.0827, batch_loss_s: 0.1033, time:11.9538, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:17:59 \u001b[32mINFO     \u001b[0m train.py: [2/300], [20/484], step: 988, 1.250 samples/sec, batch_loss: 0.1470, batch_loss_c: 0.1470, batch_loss_s: 0.1470, time:31.9954, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:18:14 \u001b[32mINFO     \u001b[0m train.py: [2/300], [30/484], step: 998, 2.691 samples/sec, batch_loss: 0.0660, batch_loss_c: 0.0623, batch_loss_s: 0.0747, time:14.8654, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:18:29 \u001b[32mINFO     \u001b[0m train.py: [2/300], [40/484], step: 1008, 2.809 samples/sec, batch_loss: 0.0573, batch_loss_c: 0.0534, batch_loss_s: 0.0665, time:14.2381, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:18:44 \u001b[32mINFO     \u001b[0m train.py: [2/300], [50/484], step: 1018, 2.559 samples/sec, batch_loss: 0.1227, batch_loss_c: 0.1223, batch_loss_s: 0.1238, time:15.6313, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:19:07 \u001b[32mINFO     \u001b[0m train.py: [2/300], [60/484], step: 1028, 1.787 samples/sec, batch_loss: 0.0870, batch_loss_c: 0.0716, batch_loss_s: 0.1231, time:22.3848, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:19:25 \u001b[32mINFO     \u001b[0m train.py: [2/300], [70/484], step: 1038, 2.217 samples/sec, batch_loss: 0.2922, batch_loss_c: 0.2836, batch_loss_s: 0.3122, time:18.0416, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:19:46 \u001b[32mINFO     \u001b[0m train.py: [2/300], [80/484], step: 1048, 1.882 samples/sec, batch_loss: 0.0674, batch_loss_c: 0.0606, batch_loss_s: 0.0834, time:21.2591, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:20:03 \u001b[32mINFO     \u001b[0m train.py: [2/300], [90/484], step: 1058, 2.289 samples/sec, batch_loss: 0.1731, batch_loss_c: 0.1623, batch_loss_s: 0.1981, time:17.4715, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:20:23 \u001b[32mINFO     \u001b[0m train.py: [2/300], [100/484], step: 1068, 2.078 samples/sec, batch_loss: 0.0802, batch_loss_c: 0.0755, batch_loss_s: 0.0913, time:19.2522, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:20:44 \u001b[32mINFO     \u001b[0m train.py: [2/300], [110/484], step: 1078, 1.860 samples/sec, batch_loss: 0.0940, batch_loss_c: 0.0718, batch_loss_s: 0.1456, time:21.4997, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:20:56 \u001b[32mINFO     \u001b[0m train.py: [2/300], [120/484], step: 1088, 3.323 samples/sec, batch_loss: 0.2241, batch_loss_c: 0.1817, batch_loss_s: 0.3229, time:12.0376, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:21:28 \u001b[32mINFO     \u001b[0m train.py: [2/300], [130/484], step: 1098, 1.247 samples/sec, batch_loss: 0.0769, batch_loss_c: 0.0688, batch_loss_s: 0.0958, time:32.0867, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:21:49 \u001b[32mINFO     \u001b[0m train.py: [2/300], [140/484], step: 1108, 1.888 samples/sec, batch_loss: 0.0566, batch_loss_c: 0.0519, batch_loss_s: 0.0676, time:21.1902, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:22:10 \u001b[32mINFO     \u001b[0m train.py: [2/300], [150/484], step: 1118, 1.936 samples/sec, batch_loss: 0.1665, batch_loss_c: 0.1590, batch_loss_s: 0.1841, time:20.6629, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:22:33 \u001b[32mINFO     \u001b[0m train.py: [2/300], [160/484], step: 1128, 1.712 samples/sec, batch_loss: 0.0807, batch_loss_c: 0.0826, batch_loss_s: 0.0763, time:23.3588, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:22:46 \u001b[32mINFO     \u001b[0m train.py: [2/300], [170/484], step: 1138, 3.136 samples/sec, batch_loss: 0.1583, batch_loss_c: 0.1571, batch_loss_s: 0.1612, time:12.7547, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:23:03 \u001b[32mINFO     \u001b[0m train.py: [2/300], [180/484], step: 1148, 2.419 samples/sec, batch_loss: 0.0622, batch_loss_c: 0.0611, batch_loss_s: 0.0648, time:16.5330, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:23:22 \u001b[32mINFO     \u001b[0m train.py: [2/300], [190/484], step: 1158, 2.024 samples/sec, batch_loss: 0.3284, batch_loss_c: 0.2955, batch_loss_s: 0.4052, time:19.7631, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:23:44 \u001b[32mINFO     \u001b[0m train.py: [2/300], [200/484], step: 1168, 1.872 samples/sec, batch_loss: 0.1101, batch_loss_c: 0.1057, batch_loss_s: 0.1204, time:21.3725, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:23:58 \u001b[32mINFO     \u001b[0m train.py: [2/300], [210/484], step: 1178, 2.770 samples/sec, batch_loss: 0.0901, batch_loss_c: 0.0858, batch_loss_s: 0.1000, time:14.4429, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:24:22 \u001b[32mINFO     \u001b[0m train.py: [2/300], [220/484], step: 1188, 1.695 samples/sec, batch_loss: 0.4448, batch_loss_c: 0.4194, batch_loss_s: 0.5040, time:23.5961, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:24:41 \u001b[32mINFO     \u001b[0m train.py: [2/300], [230/484], step: 1198, 2.095 samples/sec, batch_loss: 0.2255, batch_loss_c: 0.2369, batch_loss_s: 0.1991, time:19.0886, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:24:57 \u001b[32mINFO     \u001b[0m train.py: [2/300], [240/484], step: 1208, 2.460 samples/sec, batch_loss: 0.0737, batch_loss_c: 0.0629, batch_loss_s: 0.0991, time:16.2607, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:25:13 \u001b[32mINFO     \u001b[0m train.py: [2/300], [250/484], step: 1218, 2.585 samples/sec, batch_loss: 0.2034, batch_loss_c: 0.2184, batch_loss_s: 0.1684, time:15.4716, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:25:30 \u001b[32mINFO     \u001b[0m train.py: [2/300], [260/484], step: 1228, 2.318 samples/sec, batch_loss: 0.3285, batch_loss_c: 0.3278, batch_loss_s: 0.3300, time:17.2581, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:25:43 \u001b[32mINFO     \u001b[0m train.py: [2/300], [270/484], step: 1238, 3.000 samples/sec, batch_loss: 0.3179, batch_loss_c: 0.3147, batch_loss_s: 0.3254, time:13.3342, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:25:59 \u001b[32mINFO     \u001b[0m train.py: [2/300], [280/484], step: 1248, 2.547 samples/sec, batch_loss: 0.1729, batch_loss_c: 0.1689, batch_loss_s: 0.1822, time:15.7026, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:26:36 \u001b[32mINFO     \u001b[0m train.py: [2/300], [290/484], step: 1258, 1.067 samples/sec, batch_loss: 0.1131, batch_loss_c: 0.1043, batch_loss_s: 0.1335, time:37.4861, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:26:59 \u001b[32mINFO     \u001b[0m train.py: [2/300], [300/484], step: 1268, 1.790 samples/sec, batch_loss: 0.1238, batch_loss_c: 0.1371, batch_loss_s: 0.0929, time:22.3441, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:27:15 \u001b[32mINFO     \u001b[0m train.py: [2/300], [310/484], step: 1278, 2.496 samples/sec, batch_loss: 0.0743, batch_loss_c: 0.0682, batch_loss_s: 0.0884, time:16.0238, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:27:47 \u001b[32mINFO     \u001b[0m train.py: [2/300], [320/484], step: 1288, 1.255 samples/sec, batch_loss: 0.0544, batch_loss_c: 0.0474, batch_loss_s: 0.0709, time:31.8627, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:28:00 \u001b[32mINFO     \u001b[0m train.py: [2/300], [330/484], step: 1298, 2.931 samples/sec, batch_loss: 0.0918, batch_loss_c: 0.0848, batch_loss_s: 0.1083, time:13.6483, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:28:16 \u001b[32mINFO     \u001b[0m train.py: [2/300], [340/484], step: 1308, 2.509 samples/sec, batch_loss: 0.3344, batch_loss_c: 0.3299, batch_loss_s: 0.3450, time:15.9397, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:28:32 \u001b[32mINFO     \u001b[0m train.py: [2/300], [350/484], step: 1318, 2.564 samples/sec, batch_loss: 0.0995, batch_loss_c: 0.0864, batch_loss_s: 0.1299, time:15.5981, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:28:50 \u001b[32mINFO     \u001b[0m train.py: [2/300], [360/484], step: 1328, 2.242 samples/sec, batch_loss: 0.1134, batch_loss_c: 0.0985, batch_loss_s: 0.1481, time:17.8436, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:29:14 \u001b[32mINFO     \u001b[0m train.py: [2/300], [370/484], step: 1338, 1.633 samples/sec, batch_loss: 0.1086, batch_loss_c: 0.1095, batch_loss_s: 0.1064, time:24.4941, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:29:32 \u001b[32mINFO     \u001b[0m train.py: [2/300], [380/484], step: 1348, 2.272 samples/sec, batch_loss: 0.0536, batch_loss_c: 0.0441, batch_loss_s: 0.0759, time:17.6066, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:29:46 \u001b[32mINFO     \u001b[0m train.py: [2/300], [390/484], step: 1358, 2.899 samples/sec, batch_loss: 0.2813, batch_loss_c: 0.2752, batch_loss_s: 0.2955, time:13.7967, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:30:06 \u001b[32mINFO     \u001b[0m train.py: [2/300], [400/484], step: 1368, 2.012 samples/sec, batch_loss: 0.3247, batch_loss_c: 0.3197, batch_loss_s: 0.3365, time:19.8773, lr:0.0001\u001b[0m\n",
            "tcmalloc: large alloc 3552092160 bytes == 0x7fea5a170000 @  0x7feeb13d01e7 0x7feea59b4f71 0x7feea5a1855d 0x7feea5a1be28 0x7feea5a1c3e5 0x7feea5ab2fc2 0x50ac25 0x50d390 0x508245 0x50a080 0x50aa7d 0x50d390 0x509455 0x595311 0x5a067e 0x557d28 0x50c88b 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x509455 0x595311 0x5a522c 0x557f7e 0x4b5eff 0x50c810\n",
            "2019-12-06 04:30:45 \u001b[32mINFO     \u001b[0m train.py: [2/300], [410/484], step: 1378, 1.020 samples/sec, batch_loss: 0.1273, batch_loss_c: 0.1247, batch_loss_s: 0.1334, time:39.2237, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:31:00 \u001b[32mINFO     \u001b[0m train.py: [2/300], [420/484], step: 1388, 2.585 samples/sec, batch_loss: 0.0740, batch_loss_c: 0.0705, batch_loss_s: 0.0821, time:15.4729, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:31:12 \u001b[32mINFO     \u001b[0m train.py: [2/300], [430/484], step: 1398, 3.471 samples/sec, batch_loss: 0.1974, batch_loss_c: 0.2174, batch_loss_s: 0.1505, time:11.5253, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:31:25 \u001b[32mINFO     \u001b[0m train.py: [2/300], [440/484], step: 1408, 2.999 samples/sec, batch_loss: 0.1048, batch_loss_c: 0.1057, batch_loss_s: 0.1028, time:13.3390, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:31:51 \u001b[32mINFO     \u001b[0m train.py: [2/300], [450/484], step: 1418, 1.562 samples/sec, batch_loss: 0.1349, batch_loss_c: 0.1437, batch_loss_s: 0.1144, time:25.6037, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:32:03 \u001b[32mINFO     \u001b[0m train.py: [2/300], [460/484], step: 1428, 3.176 samples/sec, batch_loss: 0.3026, batch_loss_c: 0.2966, batch_loss_s: 0.3168, time:12.5933, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:32:24 \u001b[32mINFO     \u001b[0m train.py: [2/300], [470/484], step: 1438, 1.959 samples/sec, batch_loss: 0.3735, batch_loss_c: 0.3565, batch_loss_s: 0.4131, time:20.4216, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:33:05 \u001b[32mINFO     \u001b[0m train.py: [2/300], [480/484], step: 1448, 0.968 samples/sec, batch_loss: 0.4999, batch_loss_c: 0.4840, batch_loss_s: 0.5369, time:41.3356, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:33:10 \u001b[32mINFO     \u001b[0m train.py: [2/300], train_loss: 0.1590, time: 955.4588, lr: 0.0001\u001b[0m\n",
            "2019-12-06 04:33:11 \u001b[32mINFO     \u001b[0m train.py: [3/300], [0/484], step: 1452, 43.464 samples/sec, batch_loss: 0.2971, batch_loss_c: 0.2913, batch_loss_s: 0.3104, time:0.9203, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:33:29 \u001b[32mINFO     \u001b[0m train.py: [3/300], [10/484], step: 1462, 2.264 samples/sec, batch_loss: 0.4395, batch_loss_c: 0.3922, batch_loss_s: 0.5498, time:17.6653, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:33:46 \u001b[32mINFO     \u001b[0m train.py: [3/300], [20/484], step: 1472, 2.388 samples/sec, batch_loss: 0.1236, batch_loss_c: 0.1171, batch_loss_s: 0.1386, time:16.7511, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:34:02 \u001b[32mINFO     \u001b[0m train.py: [3/300], [30/484], step: 1482, 2.367 samples/sec, batch_loss: 0.0637, batch_loss_c: 0.0609, batch_loss_s: 0.0702, time:16.8979, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:34:16 \u001b[32mINFO     \u001b[0m train.py: [3/300], [40/484], step: 1492, 2.882 samples/sec, batch_loss: 0.0800, batch_loss_c: 0.0758, batch_loss_s: 0.0899, time:13.8792, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:34:46 \u001b[32mINFO     \u001b[0m train.py: [3/300], [50/484], step: 1502, 1.337 samples/sec, batch_loss: 0.1429, batch_loss_c: 0.1510, batch_loss_s: 0.1241, time:29.9262, lr:0.0001\u001b[0m\n",
            "tcmalloc: large alloc 4128727040 bytes == 0x7fea5a170000 @  0x7feeb13d01e7 0x7feea59b4f71 0x7feea5a1855d 0x7feea5a1be28 0x7feea5a1c3e5 0x7feea5ab2fc2 0x50ac25 0x50d390 0x508245 0x50a080 0x50aa7d 0x50d390 0x509455 0x595311 0x5a067e 0x557d28 0x50c88b 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x509455 0x595311 0x5a522c 0x557f7e 0x4b5eff 0x50c810\n",
            "2019-12-06 04:35:16 \u001b[32mINFO     \u001b[0m train.py: [3/300], [60/484], step: 1512, 1.340 samples/sec, batch_loss: 0.1253, batch_loss_c: 0.1261, batch_loss_s: 0.1237, time:29.8521, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:35:30 \u001b[32mINFO     \u001b[0m train.py: [3/300], [70/484], step: 1522, 2.860 samples/sec, batch_loss: 0.0641, batch_loss_c: 0.0601, batch_loss_s: 0.0733, time:13.9843, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:35:49 \u001b[32mINFO     \u001b[0m train.py: [3/300], [80/484], step: 1532, 2.100 samples/sec, batch_loss: 0.2834, batch_loss_c: 0.2777, batch_loss_s: 0.2965, time:19.0497, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:36:09 \u001b[32mINFO     \u001b[0m train.py: [3/300], [90/484], step: 1542, 1.978 samples/sec, batch_loss: 0.2920, batch_loss_c: 0.2890, batch_loss_s: 0.2989, time:20.2234, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:36:36 \u001b[32mINFO     \u001b[0m train.py: [3/300], [100/484], step: 1552, 1.479 samples/sec, batch_loss: 0.0484, batch_loss_c: 0.0418, batch_loss_s: 0.0640, time:27.0477, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:36:58 \u001b[32mINFO     \u001b[0m train.py: [3/300], [110/484], step: 1562, 1.839 samples/sec, batch_loss: 0.0545, batch_loss_c: 0.0515, batch_loss_s: 0.0615, time:21.7481, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:37:14 \u001b[32mINFO     \u001b[0m train.py: [3/300], [120/484], step: 1572, 2.575 samples/sec, batch_loss: 0.0510, batch_loss_c: 0.0473, batch_loss_s: 0.0597, time:15.5336, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:37:35 \u001b[32mINFO     \u001b[0m train.py: [3/300], [130/484], step: 1582, 1.863 samples/sec, batch_loss: 0.1220, batch_loss_c: 0.1159, batch_loss_s: 0.1364, time:21.4707, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:37:48 \u001b[32mINFO     \u001b[0m train.py: [3/300], [140/484], step: 1592, 3.034 samples/sec, batch_loss: 0.3110, batch_loss_c: 0.3032, batch_loss_s: 0.3291, time:13.1820, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:38:23 \u001b[32mINFO     \u001b[0m train.py: [3/300], [150/484], step: 1602, 1.155 samples/sec, batch_loss: 0.0618, batch_loss_c: 0.0616, batch_loss_s: 0.0624, time:34.6267, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:38:37 \u001b[32mINFO     \u001b[0m train.py: [3/300], [160/484], step: 1612, 2.865 samples/sec, batch_loss: 0.1278, batch_loss_c: 0.1222, batch_loss_s: 0.1407, time:13.9640, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:38:53 \u001b[32mINFO     \u001b[0m train.py: [3/300], [170/484], step: 1622, 2.468 samples/sec, batch_loss: 0.2477, batch_loss_c: 0.2315, batch_loss_s: 0.2854, time:16.2056, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:39:14 \u001b[32mINFO     \u001b[0m train.py: [3/300], [180/484], step: 1632, 1.962 samples/sec, batch_loss: 0.0804, batch_loss_c: 0.0750, batch_loss_s: 0.0931, time:20.3924, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:39:30 \u001b[32mINFO     \u001b[0m train.py: [3/300], [190/484], step: 1642, 2.447 samples/sec, batch_loss: 0.0701, batch_loss_c: 0.0660, batch_loss_s: 0.0797, time:16.3489, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:39:54 \u001b[32mINFO     \u001b[0m train.py: [3/300], [200/484], step: 1652, 1.660 samples/sec, batch_loss: 0.0995, batch_loss_c: 0.0865, batch_loss_s: 0.1300, time:24.1036, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:40:07 \u001b[32mINFO     \u001b[0m train.py: [3/300], [210/484], step: 1662, 2.972 samples/sec, batch_loss: 0.0723, batch_loss_c: 0.0683, batch_loss_s: 0.0816, time:13.4605, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:40:27 \u001b[32mINFO     \u001b[0m train.py: [3/300], [220/484], step: 1672, 1.994 samples/sec, batch_loss: 0.3152, batch_loss_c: 0.3177, batch_loss_s: 0.3095, time:20.0630, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:40:41 \u001b[32mINFO     \u001b[0m train.py: [3/300], [230/484], step: 1682, 3.014 samples/sec, batch_loss: 0.3279, batch_loss_c: 0.3290, batch_loss_s: 0.3253, time:13.2722, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:41:00 \u001b[32mINFO     \u001b[0m train.py: [3/300], [240/484], step: 1692, 2.095 samples/sec, batch_loss: 0.1006, batch_loss_c: 0.0935, batch_loss_s: 0.1172, time:19.0925, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:41:30 \u001b[32mINFO     \u001b[0m train.py: [3/300], [250/484], step: 1702, 1.348 samples/sec, batch_loss: 0.0859, batch_loss_c: 0.0799, batch_loss_s: 0.1001, time:29.6729, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:41:47 \u001b[32mINFO     \u001b[0m train.py: [3/300], [260/484], step: 1712, 2.293 samples/sec, batch_loss: 0.0798, batch_loss_c: 0.0828, batch_loss_s: 0.0728, time:17.4453, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:42:17 \u001b[32mINFO     \u001b[0m train.py: [3/300], [270/484], step: 1722, 1.333 samples/sec, batch_loss: 0.1254, batch_loss_c: 0.1288, batch_loss_s: 0.1175, time:29.9998, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:42:30 \u001b[32mINFO     \u001b[0m train.py: [3/300], [280/484], step: 1732, 3.042 samples/sec, batch_loss: 0.0573, batch_loss_c: 0.0501, batch_loss_s: 0.0743, time:13.1493, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:42:44 \u001b[32mINFO     \u001b[0m train.py: [3/300], [290/484], step: 1742, 2.893 samples/sec, batch_loss: 0.0506, batch_loss_c: 0.0439, batch_loss_s: 0.0662, time:13.8249, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:43:08 \u001b[32mINFO     \u001b[0m train.py: [3/300], [300/484], step: 1752, 1.659 samples/sec, batch_loss: 0.2971, batch_loss_c: 0.2922, batch_loss_s: 0.3086, time:24.1108, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:43:33 \u001b[32mINFO     \u001b[0m train.py: [3/300], [310/484], step: 1762, 1.636 samples/sec, batch_loss: 0.3105, batch_loss_c: 0.3082, batch_loss_s: 0.3159, time:24.4497, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:44:16 \u001b[32mINFO     \u001b[0m train.py: [3/300], [320/484], step: 1772, 0.924 samples/sec, batch_loss: 0.5212, batch_loss_c: 0.5175, batch_loss_s: 0.5299, time:43.3119, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:44:29 \u001b[32mINFO     \u001b[0m train.py: [3/300], [330/484], step: 1782, 2.932 samples/sec, batch_loss: 0.1065, batch_loss_c: 0.0991, batch_loss_s: 0.1237, time:13.6425, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:45:14 \u001b[32mINFO     \u001b[0m train.py: [3/300], [340/484], step: 1792, 0.900 samples/sec, batch_loss: 0.0704, batch_loss_c: 0.0574, batch_loss_s: 0.1006, time:44.4303, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:45:27 \u001b[32mINFO     \u001b[0m train.py: [3/300], [350/484], step: 1802, 3.025 samples/sec, batch_loss: 0.1175, batch_loss_c: 0.1138, batch_loss_s: 0.1260, time:13.2223, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:45:44 \u001b[32mINFO     \u001b[0m train.py: [3/300], [360/484], step: 1812, 2.399 samples/sec, batch_loss: 0.1981, batch_loss_c: 0.1877, batch_loss_s: 0.2222, time:16.6751, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:46:03 \u001b[32mINFO     \u001b[0m train.py: [3/300], [370/484], step: 1822, 2.108 samples/sec, batch_loss: 0.0653, batch_loss_c: 0.0554, batch_loss_s: 0.0883, time:18.9746, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:46:27 \u001b[32mINFO     \u001b[0m train.py: [3/300], [380/484], step: 1832, 1.643 samples/sec, batch_loss: 0.0418, batch_loss_c: 0.0354, batch_loss_s: 0.0568, time:24.3451, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:46:45 \u001b[32mINFO     \u001b[0m train.py: [3/300], [390/484], step: 1842, 2.283 samples/sec, batch_loss: 0.2776, batch_loss_c: 0.2741, batch_loss_s: 0.2856, time:17.5213, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:47:00 \u001b[32mINFO     \u001b[0m train.py: [3/300], [400/484], step: 1852, 2.632 samples/sec, batch_loss: 0.2060, batch_loss_c: 0.2159, batch_loss_s: 0.1827, time:15.2002, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:47:24 \u001b[32mINFO     \u001b[0m train.py: [3/300], [410/484], step: 1862, 1.650 samples/sec, batch_loss: 0.5138, batch_loss_c: 0.5124, batch_loss_s: 0.5172, time:24.2389, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:47:50 \u001b[32mINFO     \u001b[0m train.py: [3/300], [420/484], step: 1872, 1.527 samples/sec, batch_loss: 0.3396, batch_loss_c: 0.3376, batch_loss_s: 0.3442, time:26.2011, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:48:04 \u001b[32mINFO     \u001b[0m train.py: [3/300], [430/484], step: 1882, 2.945 samples/sec, batch_loss: 0.3199, batch_loss_c: 0.3039, batch_loss_s: 0.3571, time:13.5844, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:48:25 \u001b[32mINFO     \u001b[0m train.py: [3/300], [440/484], step: 1892, 1.915 samples/sec, batch_loss: 0.3603, batch_loss_c: 0.3526, batch_loss_s: 0.3784, time:20.8902, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:48:39 \u001b[32mINFO     \u001b[0m train.py: [3/300], [450/484], step: 1902, 2.846 samples/sec, batch_loss: 0.0747, batch_loss_c: 0.0775, batch_loss_s: 0.0680, time:14.0566, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:49:01 \u001b[32mINFO     \u001b[0m train.py: [3/300], [460/484], step: 1912, 1.768 samples/sec, batch_loss: 0.3839, batch_loss_c: 0.3973, batch_loss_s: 0.3527, time:22.6273, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:49:28 \u001b[32mINFO     \u001b[0m train.py: [3/300], [470/484], step: 1922, 1.495 samples/sec, batch_loss: 0.3760, batch_loss_c: 0.3690, batch_loss_s: 0.3923, time:26.7553, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:49:47 \u001b[32mINFO     \u001b[0m train.py: [3/300], [480/484], step: 1932, 2.170 samples/sec, batch_loss: 0.0659, batch_loss_c: 0.0628, batch_loss_s: 0.0732, time:18.4323, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:49:50 \u001b[32mINFO     \u001b[0m train.py: [3/300], train_loss: 0.1545, time: 1000.1329, lr: 0.0001\u001b[0m\n",
            "2019-12-06 04:49:52 \u001b[32mINFO     \u001b[0m train.py: [4/300], [0/484], step: 1936, 30.673 samples/sec, batch_loss: 0.3483, batch_loss_c: 0.3379, batch_loss_s: 0.3726, time:1.3041, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:50:17 \u001b[32mINFO     \u001b[0m train.py: [4/300], [10/484], step: 1946, 1.584 samples/sec, batch_loss: 0.0594, batch_loss_c: 0.0528, batch_loss_s: 0.0748, time:25.2511, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:50:50 \u001b[32mINFO     \u001b[0m train.py: [4/300], [20/484], step: 1956, 1.219 samples/sec, batch_loss: 0.0559, batch_loss_c: 0.0483, batch_loss_s: 0.0737, time:32.8049, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:51:07 \u001b[32mINFO     \u001b[0m train.py: [4/300], [30/484], step: 1966, 2.380 samples/sec, batch_loss: 0.2029, batch_loss_c: 0.1944, batch_loss_s: 0.2229, time:16.8072, lr:0.0001\u001b[0m\n",
            "tcmalloc: large alloc 5409136640 bytes == 0x7fea5a170000 @  0x7feeb13d01e7 0x7feea59b4f71 0x7feea5a1855d 0x7feea5a1be28 0x7feea5a1c3e5 0x7feea5ab2fc2 0x50ac25 0x50d390 0x508245 0x50a080 0x50aa7d 0x50d390 0x509455 0x595311 0x5a067e 0x557d28 0x50c88b 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x509455 0x595311 0x5a522c 0x557f7e 0x4b5eff 0x50c810\n",
            "2019-12-06 04:51:49 \u001b[32mINFO     \u001b[0m train.py: [4/300], [40/484], step: 1976, 0.947 samples/sec, batch_loss: 0.5543, batch_loss_c: 0.5543, batch_loss_s: 0.5541, time:42.2533, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:52:11 \u001b[32mINFO     \u001b[0m train.py: [4/300], [50/484], step: 1986, 1.817 samples/sec, batch_loss: 0.3253, batch_loss_c: 0.3870, batch_loss_s: 0.1815, time:22.0177, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:52:28 \u001b[32mINFO     \u001b[0m train.py: [4/300], [60/484], step: 1996, 2.363 samples/sec, batch_loss: 0.1438, batch_loss_c: 0.1416, batch_loss_s: 0.1490, time:16.9287, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:52:49 \u001b[32mINFO     \u001b[0m train.py: [4/300], [70/484], step: 2006, 1.897 samples/sec, batch_loss: 0.0530, batch_loss_c: 0.0412, batch_loss_s: 0.0805, time:21.0888, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:53:05 \u001b[32mINFO     \u001b[0m train.py: [4/300], [80/484], step: 2016, 2.588 samples/sec, batch_loss: 0.1299, batch_loss_c: 0.1166, batch_loss_s: 0.1611, time:15.4587, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:53:33 \u001b[32mINFO     \u001b[0m train.py: [4/300], [90/484], step: 2026, 1.429 samples/sec, batch_loss: 0.0528, batch_loss_c: 0.0409, batch_loss_s: 0.0807, time:28.0007, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:53:52 \u001b[32mINFO     \u001b[0m train.py: [4/300], [100/484], step: 2036, 2.066 samples/sec, batch_loss: 0.4593, batch_loss_c: 0.4354, batch_loss_s: 0.5150, time:19.3641, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:54:08 \u001b[32mINFO     \u001b[0m train.py: [4/300], [110/484], step: 2046, 2.455 samples/sec, batch_loss: 0.0681, batch_loss_c: 0.0535, batch_loss_s: 0.1023, time:16.2926, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:54:29 \u001b[32mINFO     \u001b[0m train.py: [4/300], [120/484], step: 2056, 1.896 samples/sec, batch_loss: 0.0576, batch_loss_c: 0.0502, batch_loss_s: 0.0749, time:21.0973, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:54:54 \u001b[32mINFO     \u001b[0m train.py: [4/300], [130/484], step: 2066, 1.646 samples/sec, batch_loss: 0.0925, batch_loss_c: 0.0966, batch_loss_s: 0.0830, time:24.3044, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:55:21 \u001b[32mINFO     \u001b[0m train.py: [4/300], [140/484], step: 2076, 1.463 samples/sec, batch_loss: 0.0567, batch_loss_c: 0.0477, batch_loss_s: 0.0778, time:27.3442, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:55:36 \u001b[32mINFO     \u001b[0m train.py: [4/300], [150/484], step: 2086, 2.732 samples/sec, batch_loss: 0.0732, batch_loss_c: 0.0591, batch_loss_s: 0.1061, time:14.6392, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:56:07 \u001b[32mINFO     \u001b[0m train.py: [4/300], [160/484], step: 2096, 1.260 samples/sec, batch_loss: 0.3679, batch_loss_c: 0.3454, batch_loss_s: 0.4201, time:31.7581, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:56:19 \u001b[32mINFO     \u001b[0m train.py: [4/300], [170/484], step: 2106, 3.360 samples/sec, batch_loss: 0.2901, batch_loss_c: 0.2845, batch_loss_s: 0.3032, time:11.9035, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:56:31 \u001b[32mINFO     \u001b[0m train.py: [4/300], [180/484], step: 2116, 3.460 samples/sec, batch_loss: 0.4183, batch_loss_c: 0.4439, batch_loss_s: 0.3585, time:11.5590, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:56:55 \u001b[32mINFO     \u001b[0m train.py: [4/300], [190/484], step: 2126, 1.647 samples/sec, batch_loss: 0.4091, batch_loss_c: 0.4092, batch_loss_s: 0.4087, time:24.2924, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:57:08 \u001b[32mINFO     \u001b[0m train.py: [4/300], [200/484], step: 2136, 3.053 samples/sec, batch_loss: 0.3186, batch_loss_c: 0.3125, batch_loss_s: 0.3328, time:13.1028, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:57:23 \u001b[32mINFO     \u001b[0m train.py: [4/300], [210/484], step: 2146, 2.748 samples/sec, batch_loss: 0.1008, batch_loss_c: 0.0924, batch_loss_s: 0.1205, time:14.5549, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:57:40 \u001b[32mINFO     \u001b[0m train.py: [4/300], [220/484], step: 2156, 2.322 samples/sec, batch_loss: 0.0804, batch_loss_c: 0.0699, batch_loss_s: 0.1049, time:17.2249, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:57:53 \u001b[32mINFO     \u001b[0m train.py: [4/300], [230/484], step: 2166, 3.088 samples/sec, batch_loss: 0.2919, batch_loss_c: 0.2906, batch_loss_s: 0.2949, time:12.9529, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:58:13 \u001b[32mINFO     \u001b[0m train.py: [4/300], [240/484], step: 2176, 2.031 samples/sec, batch_loss: 0.0933, batch_loss_c: 0.0912, batch_loss_s: 0.0983, time:19.6966, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:58:30 \u001b[32mINFO     \u001b[0m train.py: [4/300], [250/484], step: 2186, 2.334 samples/sec, batch_loss: 0.0901, batch_loss_c: 0.0839, batch_loss_s: 0.1044, time:17.1393, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:59:09 \u001b[32mINFO     \u001b[0m train.py: [4/300], [260/484], step: 2196, 1.012 samples/sec, batch_loss: 0.0574, batch_loss_c: 0.0488, batch_loss_s: 0.0775, time:39.5104, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:59:35 \u001b[32mINFO     \u001b[0m train.py: [4/300], [270/484], step: 2206, 1.580 samples/sec, batch_loss: 0.0640, batch_loss_c: 0.0599, batch_loss_s: 0.0734, time:25.3107, lr:0.0001\u001b[0m\n",
            "2019-12-06 04:59:47 \u001b[32mINFO     \u001b[0m train.py: [4/300], [280/484], step: 2216, 3.153 samples/sec, batch_loss: 0.0526, batch_loss_c: 0.0461, batch_loss_s: 0.0677, time:12.6857, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:00:05 \u001b[32mINFO     \u001b[0m train.py: [4/300], [290/484], step: 2226, 2.257 samples/sec, batch_loss: 0.0738, batch_loss_c: 0.0652, batch_loss_s: 0.0939, time:17.7218, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:00:17 \u001b[32mINFO     \u001b[0m train.py: [4/300], [300/484], step: 2236, 3.470 samples/sec, batch_loss: 0.0515, batch_loss_c: 0.0468, batch_loss_s: 0.0623, time:11.5281, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:00:33 \u001b[32mINFO     \u001b[0m train.py: [4/300], [310/484], step: 2246, 2.469 samples/sec, batch_loss: 0.1238, batch_loss_c: 0.1070, batch_loss_s: 0.1632, time:16.1977, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:00:52 \u001b[32mINFO     \u001b[0m train.py: [4/300], [320/484], step: 2256, 2.089 samples/sec, batch_loss: 0.1678, batch_loss_c: 0.1643, batch_loss_s: 0.1761, time:19.1446, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:01:12 \u001b[32mINFO     \u001b[0m train.py: [4/300], [330/484], step: 2266, 1.984 samples/sec, batch_loss: 0.1330, batch_loss_c: 0.1168, batch_loss_s: 0.1709, time:20.1583, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:01:25 \u001b[32mINFO     \u001b[0m train.py: [4/300], [340/484], step: 2276, 3.036 samples/sec, batch_loss: 0.0740, batch_loss_c: 0.0635, batch_loss_s: 0.0984, time:13.1768, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:01:43 \u001b[32mINFO     \u001b[0m train.py: [4/300], [350/484], step: 2286, 2.221 samples/sec, batch_loss: 0.3170, batch_loss_c: 0.3155, batch_loss_s: 0.3204, time:18.0118, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:02:00 \u001b[32mINFO     \u001b[0m train.py: [4/300], [360/484], step: 2296, 2.391 samples/sec, batch_loss: 0.1574, batch_loss_c: 0.1499, batch_loss_s: 0.1751, time:16.7325, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:02:29 \u001b[32mINFO     \u001b[0m train.py: [4/300], [370/484], step: 2306, 1.386 samples/sec, batch_loss: 0.2797, batch_loss_c: 0.2746, batch_loss_s: 0.2916, time:28.8525, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:02:49 \u001b[32mINFO     \u001b[0m train.py: [4/300], [380/484], step: 2316, 1.963 samples/sec, batch_loss: 0.0536, batch_loss_c: 0.0468, batch_loss_s: 0.0694, time:20.3727, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:03:11 \u001b[32mINFO     \u001b[0m train.py: [4/300], [390/484], step: 2326, 1.845 samples/sec, batch_loss: 0.0956, batch_loss_c: 0.0666, batch_loss_s: 0.1631, time:21.6770, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:03:47 \u001b[32mINFO     \u001b[0m train.py: [4/300], [400/484], step: 2336, 1.121 samples/sec, batch_loss: 0.0680, batch_loss_c: 0.0671, batch_loss_s: 0.0701, time:35.6931, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:04:08 \u001b[32mINFO     \u001b[0m train.py: [4/300], [410/484], step: 2346, 1.897 samples/sec, batch_loss: 0.0671, batch_loss_c: 0.0585, batch_loss_s: 0.0870, time:21.0836, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:04:22 \u001b[32mINFO     \u001b[0m train.py: [4/300], [420/484], step: 2356, 2.756 samples/sec, batch_loss: 0.3232, batch_loss_c: 0.3224, batch_loss_s: 0.3252, time:14.5116, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:04:54 \u001b[32mINFO     \u001b[0m train.py: [4/300], [430/484], step: 2366, 1.243 samples/sec, batch_loss: 0.0605, batch_loss_c: 0.0555, batch_loss_s: 0.0722, time:32.1700, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:05:26 \u001b[32mINFO     \u001b[0m train.py: [4/300], [440/484], step: 2376, 1.264 samples/sec, batch_loss: 0.0924, batch_loss_c: 0.0819, batch_loss_s: 0.1167, time:31.6435, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:05:41 \u001b[32mINFO     \u001b[0m train.py: [4/300], [450/484], step: 2386, 2.612 samples/sec, batch_loss: 0.1639, batch_loss_c: 0.1472, batch_loss_s: 0.2029, time:15.3139, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:05:56 \u001b[32mINFO     \u001b[0m train.py: [4/300], [460/484], step: 2396, 2.784 samples/sec, batch_loss: 0.0800, batch_loss_c: 0.0789, batch_loss_s: 0.0826, time:14.3690, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:06:12 \u001b[32mINFO     \u001b[0m train.py: [4/300], [470/484], step: 2406, 2.396 samples/sec, batch_loss: 0.0998, batch_loss_c: 0.1057, batch_loss_s: 0.0860, time:16.6965, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:06:40 \u001b[32mINFO     \u001b[0m train.py: [4/300], [480/484], step: 2416, 1.477 samples/sec, batch_loss: 0.0856, batch_loss_c: 0.0753, batch_loss_s: 0.1096, time:27.0728, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:06:43 \u001b[32mINFO     \u001b[0m train.py: [4/300], train_loss: 0.1616, time: 1012.7002, lr: 0.0001\u001b[0m\n",
            "2019-12-06 05:06:45 \u001b[32mINFO     \u001b[0m train.py: [5/300], [0/484], step: 2420, 33.563 samples/sec, batch_loss: 0.2827, batch_loss_c: 0.2679, batch_loss_s: 0.3171, time:1.1918, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:07:01 \u001b[32mINFO     \u001b[0m train.py: [5/300], [10/484], step: 2430, 2.520 samples/sec, batch_loss: 0.2782, batch_loss_c: 0.2643, batch_loss_s: 0.3106, time:15.8706, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:07:27 \u001b[32mINFO     \u001b[0m train.py: [5/300], [20/484], step: 2440, 1.551 samples/sec, batch_loss: 0.1017, batch_loss_c: 0.1003, batch_loss_s: 0.1051, time:25.7840, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:07:45 \u001b[32mINFO     \u001b[0m train.py: [5/300], [30/484], step: 2450, 2.230 samples/sec, batch_loss: 0.5304, batch_loss_c: 0.5296, batch_loss_s: 0.5321, time:17.9344, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:07:59 \u001b[32mINFO     \u001b[0m train.py: [5/300], [40/484], step: 2460, 2.800 samples/sec, batch_loss: 0.2994, batch_loss_c: 0.2926, batch_loss_s: 0.3154, time:14.2854, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:08:24 \u001b[32mINFO     \u001b[0m train.py: [5/300], [50/484], step: 2470, 1.576 samples/sec, batch_loss: 0.1326, batch_loss_c: 0.1451, batch_loss_s: 0.1035, time:25.3841, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:08:38 \u001b[32mINFO     \u001b[0m train.py: [5/300], [60/484], step: 2480, 2.993 samples/sec, batch_loss: 0.1648, batch_loss_c: 0.1672, batch_loss_s: 0.1592, time:13.3646, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:08:56 \u001b[32mINFO     \u001b[0m train.py: [5/300], [70/484], step: 2490, 2.162 samples/sec, batch_loss: 0.1168, batch_loss_c: 0.1131, batch_loss_s: 0.1255, time:18.5019, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:09:16 \u001b[32mINFO     \u001b[0m train.py: [5/300], [80/484], step: 2500, 2.019 samples/sec, batch_loss: 0.2002, batch_loss_c: 0.2222, batch_loss_s: 0.1488, time:19.8096, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:09:32 \u001b[32mINFO     \u001b[0m train.py: [5/300], [90/484], step: 2510, 2.573 samples/sec, batch_loss: 0.0783, batch_loss_c: 0.0745, batch_loss_s: 0.0870, time:15.5443, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:09:51 \u001b[32mINFO     \u001b[0m train.py: [5/300], [100/484], step: 2520, 2.112 samples/sec, batch_loss: 0.3265, batch_loss_c: 0.3309, batch_loss_s: 0.3163, time:18.9406, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:10:15 \u001b[32mINFO     \u001b[0m train.py: [5/300], [110/484], step: 2530, 1.641 samples/sec, batch_loss: 0.0684, batch_loss_c: 0.0631, batch_loss_s: 0.0808, time:24.3746, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:10:32 \u001b[32mINFO     \u001b[0m train.py: [5/300], [120/484], step: 2540, 2.386 samples/sec, batch_loss: 0.0883, batch_loss_c: 0.0826, batch_loss_s: 0.1015, time:16.7652, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:10:58 \u001b[32mINFO     \u001b[0m train.py: [5/300], [130/484], step: 2550, 1.534 samples/sec, batch_loss: 0.3150, batch_loss_c: 0.3142, batch_loss_s: 0.3170, time:26.0698, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:11:15 \u001b[32mINFO     \u001b[0m train.py: [5/300], [140/484], step: 2560, 2.299 samples/sec, batch_loss: 0.1461, batch_loss_c: 0.1215, batch_loss_s: 0.2037, time:17.3989, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:11:28 \u001b[32mINFO     \u001b[0m train.py: [5/300], [150/484], step: 2570, 3.039 samples/sec, batch_loss: 0.0690, batch_loss_c: 0.0612, batch_loss_s: 0.0872, time:13.1641, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:11:44 \u001b[32mINFO     \u001b[0m train.py: [5/300], [160/484], step: 2580, 2.528 samples/sec, batch_loss: 0.0516, batch_loss_c: 0.0476, batch_loss_s: 0.0610, time:15.8211, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:12:02 \u001b[32mINFO     \u001b[0m train.py: [5/300], [170/484], step: 2590, 2.276 samples/sec, batch_loss: 0.0593, batch_loss_c: 0.0508, batch_loss_s: 0.0791, time:17.5761, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:12:24 \u001b[32mINFO     \u001b[0m train.py: [5/300], [180/484], step: 2600, 1.824 samples/sec, batch_loss: 0.1239, batch_loss_c: 0.1379, batch_loss_s: 0.0913, time:21.9330, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:12:50 \u001b[32mINFO     \u001b[0m train.py: [5/300], [190/484], step: 2610, 1.503 samples/sec, batch_loss: 0.0491, batch_loss_c: 0.0363, batch_loss_s: 0.0791, time:26.6120, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:13:09 \u001b[32mINFO     \u001b[0m train.py: [5/300], [200/484], step: 2620, 2.161 samples/sec, batch_loss: 0.0568, batch_loss_c: 0.0455, batch_loss_s: 0.0832, time:18.5092, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:13:43 \u001b[32mINFO     \u001b[0m train.py: [5/300], [210/484], step: 2630, 1.167 samples/sec, batch_loss: 0.0504, batch_loss_c: 0.0457, batch_loss_s: 0.0615, time:34.2643, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:14:03 \u001b[32mINFO     \u001b[0m train.py: [5/300], [220/484], step: 2640, 1.967 samples/sec, batch_loss: 0.1275, batch_loss_c: 0.1415, batch_loss_s: 0.0947, time:20.3385, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:14:35 \u001b[32mINFO     \u001b[0m train.py: [5/300], [230/484], step: 2650, 1.280 samples/sec, batch_loss: 0.0942, batch_loss_c: 0.0889, batch_loss_s: 0.1066, time:31.2448, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:15:19 \u001b[32mINFO     \u001b[0m train.py: [5/300], [240/484], step: 2660, 0.911 samples/sec, batch_loss: 0.3192, batch_loss_c: 0.3249, batch_loss_s: 0.3056, time:43.9291, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:15:38 \u001b[32mINFO     \u001b[0m train.py: [5/300], [250/484], step: 2670, 2.053 samples/sec, batch_loss: 0.1034, batch_loss_c: 0.0919, batch_loss_s: 0.1302, time:19.4883, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:16:11 \u001b[32mINFO     \u001b[0m train.py: [5/300], [260/484], step: 2680, 1.205 samples/sec, batch_loss: 0.1501, batch_loss_c: 0.1669, batch_loss_s: 0.1108, time:33.1867, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:16:26 \u001b[32mINFO     \u001b[0m train.py: [5/300], [270/484], step: 2690, 2.720 samples/sec, batch_loss: 0.1595, batch_loss_c: 0.1576, batch_loss_s: 0.1639, time:14.7054, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:17:03 \u001b[32mINFO     \u001b[0m train.py: [5/300], [280/484], step: 2700, 1.089 samples/sec, batch_loss: 0.0740, batch_loss_c: 0.0704, batch_loss_s: 0.0824, time:36.7389, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:17:19 \u001b[32mINFO     \u001b[0m train.py: [5/300], [290/484], step: 2710, 2.418 samples/sec, batch_loss: 0.0562, batch_loss_c: 0.0516, batch_loss_s: 0.0670, time:16.5394, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:17:44 \u001b[32mINFO     \u001b[0m train.py: [5/300], [300/484], step: 2720, 1.621 samples/sec, batch_loss: 0.3968, batch_loss_c: 0.3686, batch_loss_s: 0.4626, time:24.6730, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:18:00 \u001b[32mINFO     \u001b[0m train.py: [5/300], [310/484], step: 2730, 2.415 samples/sec, batch_loss: 0.0657, batch_loss_c: 0.0565, batch_loss_s: 0.0871, time:16.5600, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:18:14 \u001b[32mINFO     \u001b[0m train.py: [5/300], [320/484], step: 2740, 3.063 samples/sec, batch_loss: 0.0777, batch_loss_c: 0.0777, batch_loss_s: 0.0779, time:13.0604, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:18:28 \u001b[32mINFO     \u001b[0m train.py: [5/300], [330/484], step: 2750, 2.852 samples/sec, batch_loss: 0.0902, batch_loss_c: 0.0922, batch_loss_s: 0.0857, time:14.0231, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:18:50 \u001b[32mINFO     \u001b[0m train.py: [5/300], [340/484], step: 2760, 1.803 samples/sec, batch_loss: 0.1340, batch_loss_c: 0.1315, batch_loss_s: 0.1396, time:22.1801, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:19:03 \u001b[32mINFO     \u001b[0m train.py: [5/300], [350/484], step: 2770, 3.063 samples/sec, batch_loss: 0.4295, batch_loss_c: 0.4017, batch_loss_s: 0.4945, time:13.0586, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:19:16 \u001b[32mINFO     \u001b[0m train.py: [5/300], [360/484], step: 2780, 2.983 samples/sec, batch_loss: 0.1144, batch_loss_c: 0.1067, batch_loss_s: 0.1324, time:13.4090, lr:0.0001\u001b[0m\n",
            "tcmalloc: large alloc 5415993344 bytes == 0x7fea5a170000 @  0x7feeb13d01e7 0x7feea59b4f71 0x7feea5a1855d 0x7feea5a1be28 0x7feea5a1c3e5 0x7feea5ab2fc2 0x50ac25 0x50d390 0x508245 0x50a080 0x50aa7d 0x50d390 0x509455 0x595311 0x5a067e 0x557d28 0x50c88b 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x509455 0x595311 0x5a522c 0x557f7e 0x4b5eff 0x50c810\n",
            "2019-12-06 05:20:02 \u001b[32mINFO     \u001b[0m train.py: [5/300], [370/484], step: 2790, 0.865 samples/sec, batch_loss: 0.0983, batch_loss_c: 0.0909, batch_loss_s: 0.1157, time:46.2175, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:20:17 \u001b[32mINFO     \u001b[0m train.py: [5/300], [380/484], step: 2800, 2.710 samples/sec, batch_loss: 0.0557, batch_loss_c: 0.0551, batch_loss_s: 0.0570, time:14.7592, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:20:34 \u001b[32mINFO     \u001b[0m train.py: [5/300], [390/484], step: 2810, 2.357 samples/sec, batch_loss: 0.0808, batch_loss_c: 0.0670, batch_loss_s: 0.1129, time:16.9675, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:20:48 \u001b[32mINFO     \u001b[0m train.py: [5/300], [400/484], step: 2820, 2.866 samples/sec, batch_loss: 0.0832, batch_loss_c: 0.0769, batch_loss_s: 0.0980, time:13.9573, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:21:02 \u001b[32mINFO     \u001b[0m train.py: [5/300], [410/484], step: 2830, 2.905 samples/sec, batch_loss: 0.4124, batch_loss_c: 0.3836, batch_loss_s: 0.4795, time:13.7693, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:21:24 \u001b[32mINFO     \u001b[0m train.py: [5/300], [420/484], step: 2840, 1.804 samples/sec, batch_loss: 0.2360, batch_loss_c: 0.2752, batch_loss_s: 0.1444, time:22.1787, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:21:51 \u001b[32mINFO     \u001b[0m train.py: [5/300], [430/484], step: 2850, 1.464 samples/sec, batch_loss: 0.5526, batch_loss_c: 0.5499, batch_loss_s: 0.5587, time:27.3192, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:22:13 \u001b[32mINFO     \u001b[0m train.py: [5/300], [440/484], step: 2860, 1.856 samples/sec, batch_loss: 0.0772, batch_loss_c: 0.0631, batch_loss_s: 0.1101, time:21.5490, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:22:29 \u001b[32mINFO     \u001b[0m train.py: [5/300], [450/484], step: 2870, 2.497 samples/sec, batch_loss: 0.0877, batch_loss_c: 0.0834, batch_loss_s: 0.0977, time:16.0223, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:22:44 \u001b[32mINFO     \u001b[0m train.py: [5/300], [460/484], step: 2880, 2.655 samples/sec, batch_loss: 0.2065, batch_loss_c: 0.1866, batch_loss_s: 0.2530, time:15.0632, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:22:59 \u001b[32mINFO     \u001b[0m train.py: [5/300], [470/484], step: 2890, 2.626 samples/sec, batch_loss: 0.1113, batch_loss_c: 0.0963, batch_loss_s: 0.1461, time:15.2322, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:23:21 \u001b[32mINFO     \u001b[0m train.py: [5/300], [480/484], step: 2900, 1.850 samples/sec, batch_loss: 0.0613, batch_loss_c: 0.0512, batch_loss_s: 0.0848, time:21.6183, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:23:25 \u001b[32mINFO     \u001b[0m train.py: [5/300], train_loss: 0.1551, time: 1000.6673, lr: 0.0001\u001b[0m\n",
            "2019-12-06 05:23:26 \u001b[32mINFO     \u001b[0m train.py: [6/300], [0/484], step: 2904, 39.352 samples/sec, batch_loss: 0.0688, batch_loss_c: 0.0588, batch_loss_s: 0.0921, time:1.0165, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:23:56 \u001b[32mINFO     \u001b[0m train.py: [6/300], [10/484], step: 2914, 1.348 samples/sec, batch_loss: 0.2851, batch_loss_c: 0.2739, batch_loss_s: 0.3114, time:29.6692, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:24:10 \u001b[32mINFO     \u001b[0m train.py: [6/300], [20/484], step: 2924, 2.809 samples/sec, batch_loss: 0.0883, batch_loss_c: 0.0825, batch_loss_s: 0.1020, time:14.2412, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:24:29 \u001b[32mINFO     \u001b[0m train.py: [6/300], [30/484], step: 2934, 2.200 samples/sec, batch_loss: 0.0894, batch_loss_c: 0.0832, batch_loss_s: 0.1038, time:18.1829, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:24:45 \u001b[32mINFO     \u001b[0m train.py: [6/300], [40/484], step: 2944, 2.476 samples/sec, batch_loss: 0.1011, batch_loss_c: 0.0905, batch_loss_s: 0.1256, time:16.1561, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:25:05 \u001b[32mINFO     \u001b[0m train.py: [6/300], [50/484], step: 2954, 1.946 samples/sec, batch_loss: 0.3015, batch_loss_c: 0.2973, batch_loss_s: 0.3111, time:20.5548, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:25:20 \u001b[32mINFO     \u001b[0m train.py: [6/300], [60/484], step: 2964, 2.783 samples/sec, batch_loss: 0.0949, batch_loss_c: 0.0902, batch_loss_s: 0.1060, time:14.3727, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:25:33 \u001b[32mINFO     \u001b[0m train.py: [6/300], [70/484], step: 2974, 2.998 samples/sec, batch_loss: 0.1120, batch_loss_c: 0.1059, batch_loss_s: 0.1262, time:13.3407, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:25:48 \u001b[32mINFO     \u001b[0m train.py: [6/300], [80/484], step: 2984, 2.717 samples/sec, batch_loss: 0.3051, batch_loss_c: 0.2952, batch_loss_s: 0.3283, time:14.7196, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:26:09 \u001b[32mINFO     \u001b[0m train.py: [6/300], [90/484], step: 2994, 1.877 samples/sec, batch_loss: 0.3154, batch_loss_c: 0.3127, batch_loss_s: 0.3215, time:21.3057, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:26:22 \u001b[32mINFO     \u001b[0m train.py: [6/300], [100/484], step: 3004, 3.003 samples/sec, batch_loss: 0.0549, batch_loss_c: 0.0468, batch_loss_s: 0.0738, time:13.3193, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:26:42 \u001b[32mINFO     \u001b[0m train.py: [6/300], [110/484], step: 3014, 2.066 samples/sec, batch_loss: 0.1539, batch_loss_c: 0.1618, batch_loss_s: 0.1355, time:19.3585, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:27:00 \u001b[32mINFO     \u001b[0m train.py: [6/300], [120/484], step: 3024, 2.160 samples/sec, batch_loss: 0.3398, batch_loss_c: 0.3369, batch_loss_s: 0.3466, time:18.5205, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:27:20 \u001b[32mINFO     \u001b[0m train.py: [6/300], [130/484], step: 3034, 2.018 samples/sec, batch_loss: 0.3104, batch_loss_c: 0.3046, batch_loss_s: 0.3240, time:19.8209, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:27:36 \u001b[32mINFO     \u001b[0m train.py: [6/300], [140/484], step: 3044, 2.477 samples/sec, batch_loss: 0.0848, batch_loss_c: 0.0817, batch_loss_s: 0.0922, time:16.1516, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:27:47 \u001b[32mINFO     \u001b[0m train.py: [6/300], [150/484], step: 3054, 3.548 samples/sec, batch_loss: 0.0606, batch_loss_c: 0.0593, batch_loss_s: 0.0635, time:11.2729, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:28:12 \u001b[32mINFO     \u001b[0m train.py: [6/300], [160/484], step: 3064, 1.616 samples/sec, batch_loss: 0.0783, batch_loss_c: 0.0697, batch_loss_s: 0.0984, time:24.7466, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:28:27 \u001b[32mINFO     \u001b[0m train.py: [6/300], [170/484], step: 3074, 2.637 samples/sec, batch_loss: 0.0576, batch_loss_c: 0.0475, batch_loss_s: 0.0812, time:15.1697, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:28:41 \u001b[32mINFO     \u001b[0m train.py: [6/300], [180/484], step: 3084, 2.856 samples/sec, batch_loss: 0.0630, batch_loss_c: 0.0513, batch_loss_s: 0.0904, time:14.0043, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:29:08 \u001b[32mINFO     \u001b[0m train.py: [6/300], [190/484], step: 3094, 1.507 samples/sec, batch_loss: 0.1107, batch_loss_c: 0.1046, batch_loss_s: 0.1249, time:26.5375, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:29:19 \u001b[32mINFO     \u001b[0m train.py: [6/300], [200/484], step: 3104, 3.652 samples/sec, batch_loss: 0.0497, batch_loss_c: 0.0439, batch_loss_s: 0.0634, time:10.9523, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:29:38 \u001b[32mINFO     \u001b[0m train.py: [6/300], [210/484], step: 3114, 2.136 samples/sec, batch_loss: 0.2979, batch_loss_c: 0.2902, batch_loss_s: 0.3158, time:18.7276, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:29:51 \u001b[32mINFO     \u001b[0m train.py: [6/300], [220/484], step: 3124, 3.007 samples/sec, batch_loss: 0.1387, batch_loss_c: 0.1495, batch_loss_s: 0.1136, time:13.3009, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:30:28 \u001b[32mINFO     \u001b[0m train.py: [6/300], [230/484], step: 3134, 1.091 samples/sec, batch_loss: 0.0785, batch_loss_c: 0.0679, batch_loss_s: 0.1033, time:36.6503, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:30:43 \u001b[32mINFO     \u001b[0m train.py: [6/300], [240/484], step: 3144, 2.598 samples/sec, batch_loss: 0.0939, batch_loss_c: 0.0894, batch_loss_s: 0.1043, time:15.3988, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:30:57 \u001b[32mINFO     \u001b[0m train.py: [6/300], [250/484], step: 3154, 2.833 samples/sec, batch_loss: 0.2889, batch_loss_c: 0.2842, batch_loss_s: 0.3000, time:14.1176, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:31:15 \u001b[32mINFO     \u001b[0m train.py: [6/300], [260/484], step: 3164, 2.230 samples/sec, batch_loss: 0.0625, batch_loss_c: 0.0604, batch_loss_s: 0.0671, time:17.9362, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:31:35 \u001b[32mINFO     \u001b[0m train.py: [6/300], [270/484], step: 3174, 2.018 samples/sec, batch_loss: 0.1457, batch_loss_c: 0.1589, batch_loss_s: 0.1150, time:19.8263, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:31:51 \u001b[32mINFO     \u001b[0m train.py: [6/300], [280/484], step: 3184, 2.519 samples/sec, batch_loss: 0.1797, batch_loss_c: 0.1731, batch_loss_s: 0.1950, time:15.8806, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:32:08 \u001b[32mINFO     \u001b[0m train.py: [6/300], [290/484], step: 3194, 2.359 samples/sec, batch_loss: 0.1786, batch_loss_c: 0.1711, batch_loss_s: 0.1961, time:16.9560, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:32:23 \u001b[32mINFO     \u001b[0m train.py: [6/300], [300/484], step: 3204, 2.600 samples/sec, batch_loss: 0.3047, batch_loss_c: 0.2975, batch_loss_s: 0.3214, time:15.3825, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:32:40 \u001b[32mINFO     \u001b[0m train.py: [6/300], [310/484], step: 3214, 2.329 samples/sec, batch_loss: 0.4764, batch_loss_c: 0.4727, batch_loss_s: 0.4850, time:17.1735, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:33:05 \u001b[32mINFO     \u001b[0m train.py: [6/300], [320/484], step: 3224, 1.641 samples/sec, batch_loss: 0.0961, batch_loss_c: 0.0954, batch_loss_s: 0.0979, time:24.3681, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:33:23 \u001b[32mINFO     \u001b[0m train.py: [6/300], [330/484], step: 3234, 2.209 samples/sec, batch_loss: 0.1115, batch_loss_c: 0.0992, batch_loss_s: 0.1401, time:18.1084, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:33:36 \u001b[32mINFO     \u001b[0m train.py: [6/300], [340/484], step: 3244, 3.061 samples/sec, batch_loss: 0.0778, batch_loss_c: 0.0756, batch_loss_s: 0.0828, time:13.0687, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:33:52 \u001b[32mINFO     \u001b[0m train.py: [6/300], [350/484], step: 3254, 2.456 samples/sec, batch_loss: 0.1596, batch_loss_c: 0.1766, batch_loss_s: 0.1199, time:16.2849, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:34:13 \u001b[32mINFO     \u001b[0m train.py: [6/300], [360/484], step: 3264, 1.937 samples/sec, batch_loss: 0.0885, batch_loss_c: 0.0800, batch_loss_s: 0.1082, time:20.6502, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:34:39 \u001b[32mINFO     \u001b[0m train.py: [6/300], [370/484], step: 3274, 1.543 samples/sec, batch_loss: 0.0758, batch_loss_c: 0.0683, batch_loss_s: 0.0934, time:25.9262, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:34:58 \u001b[32mINFO     \u001b[0m train.py: [6/300], [380/484], step: 3284, 2.060 samples/sec, batch_loss: 0.0638, batch_loss_c: 0.0561, batch_loss_s: 0.0819, time:19.4162, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:35:15 \u001b[32mINFO     \u001b[0m train.py: [6/300], [390/484], step: 3294, 2.358 samples/sec, batch_loss: 0.0621, batch_loss_c: 0.0508, batch_loss_s: 0.0885, time:16.9661, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:35:44 \u001b[32mINFO     \u001b[0m train.py: [6/300], [400/484], step: 3304, 1.366 samples/sec, batch_loss: 0.1252, batch_loss_c: 0.1257, batch_loss_s: 0.1240, time:29.2768, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:36:07 \u001b[32mINFO     \u001b[0m train.py: [6/300], [410/484], step: 3314, 1.762 samples/sec, batch_loss: 0.0995, batch_loss_c: 0.0907, batch_loss_s: 0.1201, time:22.6954, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:36:26 \u001b[32mINFO     \u001b[0m train.py: [6/300], [420/484], step: 3324, 2.120 samples/sec, batch_loss: 0.0617, batch_loss_c: 0.0515, batch_loss_s: 0.0855, time:18.8712, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:36:46 \u001b[32mINFO     \u001b[0m train.py: [6/300], [430/484], step: 3334, 1.956 samples/sec, batch_loss: 0.1098, batch_loss_c: 0.1014, batch_loss_s: 0.1295, time:20.4495, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:37:14 \u001b[32mINFO     \u001b[0m train.py: [6/300], [440/484], step: 3344, 1.444 samples/sec, batch_loss: 0.1295, batch_loss_c: 0.1274, batch_loss_s: 0.1345, time:27.6923, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:37:40 \u001b[32mINFO     \u001b[0m train.py: [6/300], [450/484], step: 3354, 1.510 samples/sec, batch_loss: 0.2356, batch_loss_c: 0.2493, batch_loss_s: 0.2037, time:26.4867, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:38:00 \u001b[32mINFO     \u001b[0m train.py: [6/300], [460/484], step: 3364, 2.069 samples/sec, batch_loss: 0.0748, batch_loss_c: 0.0706, batch_loss_s: 0.0846, time:19.3320, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:38:18 \u001b[32mINFO     \u001b[0m train.py: [6/300], [470/484], step: 3374, 2.256 samples/sec, batch_loss: 0.0839, batch_loss_c: 0.0823, batch_loss_s: 0.0876, time:17.7336, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:38:42 \u001b[32mINFO     \u001b[0m train.py: [6/300], [480/484], step: 3384, 1.643 samples/sec, batch_loss: 0.1059, batch_loss_c: 0.0957, batch_loss_s: 0.1296, time:24.3509, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:38:50 \u001b[32mINFO     \u001b[0m train.py: [6/300], train_loss: 0.1524, time: 925.2767, lr: 0.0001\u001b[0m\n",
            "2019-12-06 05:38:56 \u001b[32mINFO     \u001b[0m train.py: [7/300], [0/484], step: 3388, 7.283 samples/sec, batch_loss: 0.3097, batch_loss_c: 0.3068, batch_loss_s: 0.3163, time:5.4922, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:39:23 \u001b[32mINFO     \u001b[0m train.py: [7/300], [10/484], step: 3398, 1.483 samples/sec, batch_loss: 0.1034, batch_loss_c: 0.1144, batch_loss_s: 0.0778, time:26.9670, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:39:38 \u001b[32mINFO     \u001b[0m train.py: [7/300], [20/484], step: 3408, 2.702 samples/sec, batch_loss: 0.0959, batch_loss_c: 0.0829, batch_loss_s: 0.1260, time:14.8020, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:39:58 \u001b[32mINFO     \u001b[0m train.py: [7/300], [30/484], step: 3418, 2.008 samples/sec, batch_loss: 0.0493, batch_loss_c: 0.0405, batch_loss_s: 0.0700, time:19.9217, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:40:15 \u001b[32mINFO     \u001b[0m train.py: [7/300], [40/484], step: 3428, 2.300 samples/sec, batch_loss: 0.1135, batch_loss_c: 0.1029, batch_loss_s: 0.1381, time:17.3891, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:40:38 \u001b[32mINFO     \u001b[0m train.py: [7/300], [50/484], step: 3438, 1.749 samples/sec, batch_loss: 0.3596, batch_loss_c: 0.3598, batch_loss_s: 0.3590, time:22.8674, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:40:54 \u001b[32mINFO     \u001b[0m train.py: [7/300], [60/484], step: 3448, 2.498 samples/sec, batch_loss: 0.1158, batch_loss_c: 0.0998, batch_loss_s: 0.1532, time:16.0144, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:41:16 \u001b[32mINFO     \u001b[0m train.py: [7/300], [70/484], step: 3458, 1.846 samples/sec, batch_loss: 0.2078, batch_loss_c: 0.1775, batch_loss_s: 0.2783, time:21.6676, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:41:50 \u001b[32mINFO     \u001b[0m train.py: [7/300], [80/484], step: 3468, 1.193 samples/sec, batch_loss: 0.0718, batch_loss_c: 0.0664, batch_loss_s: 0.0845, time:33.5407, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:42:11 \u001b[32mINFO     \u001b[0m train.py: [7/300], [90/484], step: 3478, 1.901 samples/sec, batch_loss: 0.3096, batch_loss_c: 0.3028, batch_loss_s: 0.3255, time:21.0431, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:42:33 \u001b[32mINFO     \u001b[0m train.py: [7/300], [100/484], step: 3488, 1.792 samples/sec, batch_loss: 0.2001, batch_loss_c: 0.1975, batch_loss_s: 0.2062, time:22.3191, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:42:51 \u001b[32mINFO     \u001b[0m train.py: [7/300], [110/484], step: 3498, 2.232 samples/sec, batch_loss: 0.1705, batch_loss_c: 0.1685, batch_loss_s: 0.1751, time:17.9202, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:43:04 \u001b[32mINFO     \u001b[0m train.py: [7/300], [120/484], step: 3508, 3.110 samples/sec, batch_loss: 0.0924, batch_loss_c: 0.0817, batch_loss_s: 0.1173, time:12.8600, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:43:24 \u001b[32mINFO     \u001b[0m train.py: [7/300], [130/484], step: 3518, 1.923 samples/sec, batch_loss: 0.5241, batch_loss_c: 0.5209, batch_loss_s: 0.5316, time:20.8034, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:43:43 \u001b[32mINFO     \u001b[0m train.py: [7/300], [140/484], step: 3528, 2.219 samples/sec, batch_loss: 0.3375, batch_loss_c: 0.3141, batch_loss_s: 0.3921, time:18.0282, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:44:07 \u001b[32mINFO     \u001b[0m train.py: [7/300], [150/484], step: 3538, 1.609 samples/sec, batch_loss: 0.2781, batch_loss_c: 0.2577, batch_loss_s: 0.3258, time:24.8586, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:44:23 \u001b[32mINFO     \u001b[0m train.py: [7/300], [160/484], step: 3548, 2.586 samples/sec, batch_loss: 0.3574, batch_loss_c: 0.3522, batch_loss_s: 0.3695, time:15.4663, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:44:58 \u001b[32mINFO     \u001b[0m train.py: [7/300], [170/484], step: 3558, 1.146 samples/sec, batch_loss: 0.2051, batch_loss_c: 0.2520, batch_loss_s: 0.0957, time:34.8933, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:45:25 \u001b[32mINFO     \u001b[0m train.py: [7/300], [180/484], step: 3568, 1.488 samples/sec, batch_loss: 0.5637, batch_loss_c: 0.4972, batch_loss_s: 0.7191, time:26.8887, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:45:44 \u001b[32mINFO     \u001b[0m train.py: [7/300], [190/484], step: 3578, 2.048 samples/sec, batch_loss: 0.2032, batch_loss_c: 0.2358, batch_loss_s: 0.1271, time:19.5304, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:46:15 \u001b[32mINFO     \u001b[0m train.py: [7/300], [200/484], step: 3588, 1.277 samples/sec, batch_loss: 0.2863, batch_loss_c: 0.2798, batch_loss_s: 0.3014, time:31.3299, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:46:28 \u001b[32mINFO     \u001b[0m train.py: [7/300], [210/484], step: 3598, 3.168 samples/sec, batch_loss: 0.0737, batch_loss_c: 0.0695, batch_loss_s: 0.0835, time:12.6244, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:46:43 \u001b[32mINFO     \u001b[0m train.py: [7/300], [220/484], step: 3608, 2.746 samples/sec, batch_loss: 0.1871, batch_loss_c: 0.1972, batch_loss_s: 0.1636, time:14.5672, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:46:59 \u001b[32mINFO     \u001b[0m train.py: [7/300], [230/484], step: 3618, 2.388 samples/sec, batch_loss: 0.0565, batch_loss_c: 0.0474, batch_loss_s: 0.0777, time:16.7482, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:47:23 \u001b[32mINFO     \u001b[0m train.py: [7/300], [240/484], step: 3628, 1.664 samples/sec, batch_loss: 0.2885, batch_loss_c: 0.2781, batch_loss_s: 0.3127, time:24.0351, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:47:37 \u001b[32mINFO     \u001b[0m train.py: [7/300], [250/484], step: 3638, 2.915 samples/sec, batch_loss: 0.1104, batch_loss_c: 0.1043, batch_loss_s: 0.1247, time:13.7208, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:47:52 \u001b[32mINFO     \u001b[0m train.py: [7/300], [260/484], step: 3648, 2.643 samples/sec, batch_loss: 0.0790, batch_loss_c: 0.0730, batch_loss_s: 0.0931, time:15.1350, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:48:19 \u001b[32mINFO     \u001b[0m train.py: [7/300], [270/484], step: 3658, 1.503 samples/sec, batch_loss: 0.1682, batch_loss_c: 0.1664, batch_loss_s: 0.1725, time:26.6219, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:48:43 \u001b[32mINFO     \u001b[0m train.py: [7/300], [280/484], step: 3668, 1.675 samples/sec, batch_loss: 0.0622, batch_loss_c: 0.0480, batch_loss_s: 0.0955, time:23.8780, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:48:58 \u001b[32mINFO     \u001b[0m train.py: [7/300], [290/484], step: 3678, 2.677 samples/sec, batch_loss: 0.3113, batch_loss_c: 0.3021, batch_loss_s: 0.3330, time:14.9408, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:49:14 \u001b[32mINFO     \u001b[0m train.py: [7/300], [300/484], step: 3688, 2.474 samples/sec, batch_loss: 0.0857, batch_loss_c: 0.0790, batch_loss_s: 0.1013, time:16.1698, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:49:32 \u001b[32mINFO     \u001b[0m train.py: [7/300], [310/484], step: 3698, 2.167 samples/sec, batch_loss: 0.0502, batch_loss_c: 0.0440, batch_loss_s: 0.0649, time:18.4556, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:49:56 \u001b[32mINFO     \u001b[0m train.py: [7/300], [320/484], step: 3708, 1.660 samples/sec, batch_loss: 0.0504, batch_loss_c: 0.0454, batch_loss_s: 0.0620, time:24.0959, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:50:34 \u001b[32mINFO     \u001b[0m train.py: [7/300], [330/484], step: 3718, 1.076 samples/sec, batch_loss: 0.0416, batch_loss_c: 0.0365, batch_loss_s: 0.0533, time:37.1702, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:50:52 \u001b[32mINFO     \u001b[0m train.py: [7/300], [340/484], step: 3728, 2.220 samples/sec, batch_loss: 0.0861, batch_loss_c: 0.0783, batch_loss_s: 0.1042, time:18.0154, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:51:10 \u001b[32mINFO     \u001b[0m train.py: [7/300], [350/484], step: 3738, 2.218 samples/sec, batch_loss: 0.0466, batch_loss_c: 0.0402, batch_loss_s: 0.0616, time:18.0359, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:51:27 \u001b[32mINFO     \u001b[0m train.py: [7/300], [360/484], step: 3748, 2.291 samples/sec, batch_loss: 0.1277, batch_loss_c: 0.1119, batch_loss_s: 0.1645, time:17.4584, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:51:47 \u001b[32mINFO     \u001b[0m train.py: [7/300], [370/484], step: 3758, 2.001 samples/sec, batch_loss: 0.3035, batch_loss_c: 0.2970, batch_loss_s: 0.3185, time:19.9946, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:52:04 \u001b[32mINFO     \u001b[0m train.py: [7/300], [380/484], step: 3768, 2.444 samples/sec, batch_loss: 0.0943, batch_loss_c: 0.0876, batch_loss_s: 0.1098, time:16.3684, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:52:17 \u001b[32mINFO     \u001b[0m train.py: [7/300], [390/484], step: 3778, 3.067 samples/sec, batch_loss: 0.3527, batch_loss_c: 0.3549, batch_loss_s: 0.3478, time:13.0404, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:52:33 \u001b[32mINFO     \u001b[0m train.py: [7/300], [400/484], step: 3788, 2.446 samples/sec, batch_loss: 0.1666, batch_loss_c: 0.1578, batch_loss_s: 0.1871, time:16.3531, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:52:49 \u001b[32mINFO     \u001b[0m train.py: [7/300], [410/484], step: 3798, 2.475 samples/sec, batch_loss: 0.0760, batch_loss_c: 0.0693, batch_loss_s: 0.0918, time:16.1587, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:53:06 \u001b[32mINFO     \u001b[0m train.py: [7/300], [420/484], step: 3808, 2.413 samples/sec, batch_loss: 0.2121, batch_loss_c: 0.1843, batch_loss_s: 0.2769, time:16.5760, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:53:19 \u001b[32mINFO     \u001b[0m train.py: [7/300], [430/484], step: 3818, 2.915 samples/sec, batch_loss: 0.0549, batch_loss_c: 0.0448, batch_loss_s: 0.0786, time:13.7208, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:53:34 \u001b[32mINFO     \u001b[0m train.py: [7/300], [440/484], step: 3828, 2.725 samples/sec, batch_loss: 0.0885, batch_loss_c: 0.0770, batch_loss_s: 0.1154, time:14.6767, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:53:57 \u001b[32mINFO     \u001b[0m train.py: [7/300], [450/484], step: 3838, 1.735 samples/sec, batch_loss: 0.0809, batch_loss_c: 0.0776, batch_loss_s: 0.0887, time:23.0577, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:54:27 \u001b[32mINFO     \u001b[0m train.py: [7/300], [460/484], step: 3848, 1.339 samples/sec, batch_loss: 0.3003, batch_loss_c: 0.2948, batch_loss_s: 0.3132, time:29.8718, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:54:47 \u001b[32mINFO     \u001b[0m train.py: [7/300], [470/484], step: 3858, 1.976 samples/sec, batch_loss: 0.1389, batch_loss_c: 0.1523, batch_loss_s: 0.1076, time:20.2413, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:55:16 \u001b[32mINFO     \u001b[0m train.py: [7/300], [480/484], step: 3868, 1.369 samples/sec, batch_loss: 0.1274, batch_loss_c: 0.0923, batch_loss_s: 0.2094, time:29.2183, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:55:21 \u001b[32mINFO     \u001b[0m train.py: [7/300], train_loss: 0.1552, time: 990.0099, lr: 0.0001\u001b[0m\n",
            "2019-12-06 05:55:22 \u001b[32mINFO     \u001b[0m train.py: [8/300], [0/484], step: 3872, 39.020 samples/sec, batch_loss: 0.1724, batch_loss_c: 0.1921, batch_loss_s: 0.1263, time:1.0251, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:55:40 \u001b[32mINFO     \u001b[0m train.py: [8/300], [10/484], step: 3882, 2.314 samples/sec, batch_loss: 0.0498, batch_loss_c: 0.0464, batch_loss_s: 0.0578, time:17.2831, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:55:54 \u001b[32mINFO     \u001b[0m train.py: [8/300], [20/484], step: 3892, 2.848 samples/sec, batch_loss: 0.1681, batch_loss_c: 0.1725, batch_loss_s: 0.1578, time:14.0453, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:56:08 \u001b[32mINFO     \u001b[0m train.py: [8/300], [30/484], step: 3902, 2.838 samples/sec, batch_loss: 0.1459, batch_loss_c: 0.1519, batch_loss_s: 0.1318, time:14.0947, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:56:31 \u001b[32mINFO     \u001b[0m train.py: [8/300], [40/484], step: 3912, 1.703 samples/sec, batch_loss: 0.2329, batch_loss_c: 0.2298, batch_loss_s: 0.2401, time:23.4881, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:56:50 \u001b[32mINFO     \u001b[0m train.py: [8/300], [50/484], step: 3922, 2.127 samples/sec, batch_loss: 0.0760, batch_loss_c: 0.0703, batch_loss_s: 0.0894, time:18.8015, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:57:05 \u001b[32mINFO     \u001b[0m train.py: [8/300], [60/484], step: 3932, 2.716 samples/sec, batch_loss: 0.0524, batch_loss_c: 0.0483, batch_loss_s: 0.0619, time:14.7267, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:57:20 \u001b[32mINFO     \u001b[0m train.py: [8/300], [70/484], step: 3942, 2.644 samples/sec, batch_loss: 0.3133, batch_loss_c: 0.3083, batch_loss_s: 0.3249, time:15.1277, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:57:36 \u001b[32mINFO     \u001b[0m train.py: [8/300], [80/484], step: 3952, 2.473 samples/sec, batch_loss: 0.0476, batch_loss_c: 0.0422, batch_loss_s: 0.0600, time:16.1757, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:57:51 \u001b[32mINFO     \u001b[0m train.py: [8/300], [90/484], step: 3962, 2.792 samples/sec, batch_loss: 0.2118, batch_loss_c: 0.1993, batch_loss_s: 0.2408, time:14.3249, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:58:04 \u001b[32mINFO     \u001b[0m train.py: [8/300], [100/484], step: 3972, 2.938 samples/sec, batch_loss: 0.3874, batch_loss_c: 0.3890, batch_loss_s: 0.3836, time:13.6130, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:58:29 \u001b[32mINFO     \u001b[0m train.py: [8/300], [110/484], step: 3982, 1.622 samples/sec, batch_loss: 0.0647, batch_loss_c: 0.0569, batch_loss_s: 0.0827, time:24.6685, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:58:48 \u001b[32mINFO     \u001b[0m train.py: [8/300], [120/484], step: 3992, 2.065 samples/sec, batch_loss: 0.4530, batch_loss_c: 0.4285, batch_loss_s: 0.5100, time:19.3672, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:59:10 \u001b[32mINFO     \u001b[0m train.py: [8/300], [130/484], step: 4002, 1.849 samples/sec, batch_loss: 0.0760, batch_loss_c: 0.0776, batch_loss_s: 0.0722, time:21.6369, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:59:40 \u001b[32mINFO     \u001b[0m train.py: [8/300], [140/484], step: 4012, 1.344 samples/sec, batch_loss: 0.1447, batch_loss_c: 0.1452, batch_loss_s: 0.1435, time:29.7709, lr:0.0001\u001b[0m\n",
            "2019-12-06 05:59:52 \u001b[32mINFO     \u001b[0m train.py: [8/300], [150/484], step: 4022, 3.097 samples/sec, batch_loss: 0.0883, batch_loss_c: 0.0858, batch_loss_s: 0.0941, time:12.9150, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:00:08 \u001b[32mINFO     \u001b[0m train.py: [8/300], [160/484], step: 4032, 2.617 samples/sec, batch_loss: 0.3125, batch_loss_c: 0.3053, batch_loss_s: 0.3293, time:15.2841, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:00:31 \u001b[32mINFO     \u001b[0m train.py: [8/300], [170/484], step: 4042, 1.720 samples/sec, batch_loss: 0.0447, batch_loss_c: 0.0392, batch_loss_s: 0.0575, time:23.2506, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:01:08 \u001b[32mINFO     \u001b[0m train.py: [8/300], [180/484], step: 4052, 1.095 samples/sec, batch_loss: 0.0710, batch_loss_c: 0.0589, batch_loss_s: 0.0993, time:36.5299, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:01:27 \u001b[32mINFO     \u001b[0m train.py: [8/300], [190/484], step: 4062, 2.025 samples/sec, batch_loss: 0.0759, batch_loss_c: 0.0764, batch_loss_s: 0.0749, time:19.7483, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:01:53 \u001b[32mINFO     \u001b[0m train.py: [8/300], [200/484], step: 4072, 1.584 samples/sec, batch_loss: 0.1179, batch_loss_c: 0.1291, batch_loss_s: 0.0916, time:25.2521, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:02:08 \u001b[32mINFO     \u001b[0m train.py: [8/300], [210/484], step: 4082, 2.644 samples/sec, batch_loss: 0.0617, batch_loss_c: 0.0577, batch_loss_s: 0.0710, time:15.1274, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:02:24 \u001b[32mINFO     \u001b[0m train.py: [8/300], [220/484], step: 4092, 2.423 samples/sec, batch_loss: 0.0786, batch_loss_c: 0.0764, batch_loss_s: 0.0837, time:16.5093, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:03:01 \u001b[32mINFO     \u001b[0m train.py: [8/300], [230/484], step: 4102, 1.079 samples/sec, batch_loss: 0.1255, batch_loss_c: 0.0953, batch_loss_s: 0.1958, time:37.0819, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:03:26 \u001b[32mINFO     \u001b[0m train.py: [8/300], [240/484], step: 4112, 1.634 samples/sec, batch_loss: 0.0830, batch_loss_c: 0.0696, batch_loss_s: 0.1141, time:24.4846, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:03:42 \u001b[32mINFO     \u001b[0m train.py: [8/300], [250/484], step: 4122, 2.497 samples/sec, batch_loss: 0.0776, batch_loss_c: 0.0762, batch_loss_s: 0.0808, time:16.0213, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:04:10 \u001b[32mINFO     \u001b[0m train.py: [8/300], [260/484], step: 4132, 1.417 samples/sec, batch_loss: 0.1710, batch_loss_c: 0.1886, batch_loss_s: 0.1299, time:28.2314, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:04:24 \u001b[32mINFO     \u001b[0m train.py: [8/300], [270/484], step: 4142, 2.829 samples/sec, batch_loss: 0.0646, batch_loss_c: 0.0554, batch_loss_s: 0.0859, time:14.1373, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:04:40 \u001b[32mINFO     \u001b[0m train.py: [8/300], [280/484], step: 4152, 2.579 samples/sec, batch_loss: 0.3060, batch_loss_c: 0.3036, batch_loss_s: 0.3115, time:15.5098, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:05:00 \u001b[32mINFO     \u001b[0m train.py: [8/300], [290/484], step: 4162, 1.938 samples/sec, batch_loss: 0.0570, batch_loss_c: 0.0497, batch_loss_s: 0.0740, time:20.6378, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:05:17 \u001b[32mINFO     \u001b[0m train.py: [8/300], [300/484], step: 4172, 2.416 samples/sec, batch_loss: 0.1621, batch_loss_c: 0.1742, batch_loss_s: 0.1340, time:16.5584, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:05:32 \u001b[32mINFO     \u001b[0m train.py: [8/300], [310/484], step: 4182, 2.720 samples/sec, batch_loss: 0.0484, batch_loss_c: 0.0397, batch_loss_s: 0.0687, time:14.7039, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:05:50 \u001b[32mINFO     \u001b[0m train.py: [8/300], [320/484], step: 4192, 2.174 samples/sec, batch_loss: 0.1286, batch_loss_c: 0.1463, batch_loss_s: 0.0874, time:18.3960, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:06:06 \u001b[32mINFO     \u001b[0m train.py: [8/300], [330/484], step: 4202, 2.448 samples/sec, batch_loss: 0.1771, batch_loss_c: 0.1706, batch_loss_s: 0.1922, time:16.3408, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:06:35 \u001b[32mINFO     \u001b[0m train.py: [8/300], [340/484], step: 4212, 1.416 samples/sec, batch_loss: 0.0586, batch_loss_c: 0.0549, batch_loss_s: 0.0671, time:28.2519, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:06:52 \u001b[32mINFO     \u001b[0m train.py: [8/300], [350/484], step: 4222, 2.268 samples/sec, batch_loss: 0.0964, batch_loss_c: 0.0896, batch_loss_s: 0.1123, time:17.6348, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:07:05 \u001b[32mINFO     \u001b[0m train.py: [8/300], [360/484], step: 4232, 3.061 samples/sec, batch_loss: 0.3004, batch_loss_c: 0.3035, batch_loss_s: 0.2932, time:13.0660, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:07:48 \u001b[32mINFO     \u001b[0m train.py: [8/300], [370/484], step: 4242, 0.933 samples/sec, batch_loss: 0.5372, batch_loss_c: 0.5374, batch_loss_s: 0.5365, time:42.8747, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:08:06 \u001b[32mINFO     \u001b[0m train.py: [8/300], [380/484], step: 4252, 2.296 samples/sec, batch_loss: 0.1372, batch_loss_c: 0.1353, batch_loss_s: 0.1416, time:17.4216, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:08:25 \u001b[32mINFO     \u001b[0m train.py: [8/300], [390/484], step: 4262, 2.074 samples/sec, batch_loss: 0.1257, batch_loss_c: 0.1309, batch_loss_s: 0.1136, time:19.2882, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:08:38 \u001b[32mINFO     \u001b[0m train.py: [8/300], [400/484], step: 4272, 2.948 samples/sec, batch_loss: 0.0752, batch_loss_c: 0.0737, batch_loss_s: 0.0785, time:13.5704, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:08:53 \u001b[32mINFO     \u001b[0m train.py: [8/300], [410/484], step: 4282, 2.833 samples/sec, batch_loss: 0.0662, batch_loss_c: 0.0624, batch_loss_s: 0.0749, time:14.1172, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:09:07 \u001b[32mINFO     \u001b[0m train.py: [8/300], [420/484], step: 4292, 2.708 samples/sec, batch_loss: 0.1022, batch_loss_c: 0.0997, batch_loss_s: 0.1078, time:14.7691, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:09:33 \u001b[32mINFO     \u001b[0m train.py: [8/300], [430/484], step: 4302, 1.563 samples/sec, batch_loss: 0.3036, batch_loss_c: 0.2948, batch_loss_s: 0.3241, time:25.5866, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:09:49 \u001b[32mINFO     \u001b[0m train.py: [8/300], [440/484], step: 4312, 2.512 samples/sec, batch_loss: 0.0695, batch_loss_c: 0.0622, batch_loss_s: 0.0866, time:15.9257, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:10:05 \u001b[32mINFO     \u001b[0m train.py: [8/300], [450/484], step: 4322, 2.545 samples/sec, batch_loss: 0.2225, batch_loss_c: 0.1955, batch_loss_s: 0.2853, time:15.7141, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:10:22 \u001b[32mINFO     \u001b[0m train.py: [8/300], [460/484], step: 4332, 2.322 samples/sec, batch_loss: 0.0809, batch_loss_c: 0.0713, batch_loss_s: 0.1032, time:17.2240, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:10:48 \u001b[32mINFO     \u001b[0m train.py: [8/300], [470/484], step: 4342, 1.505 samples/sec, batch_loss: 0.0691, batch_loss_c: 0.0648, batch_loss_s: 0.0791, time:26.5785, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:11:06 \u001b[32mINFO     \u001b[0m train.py: [8/300], [480/484], step: 4352, 2.285 samples/sec, batch_loss: 0.1032, batch_loss_c: 0.1026, batch_loss_s: 0.1044, time:17.5022, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:11:10 \u001b[32mINFO     \u001b[0m train.py: [8/300], train_loss: 0.1586, time: 948.8377, lr: 0.0001\u001b[0m\n",
            "2019-12-06 06:11:12 \u001b[32mINFO     \u001b[0m train.py: [9/300], [0/484], step: 4356, 41.761 samples/sec, batch_loss: 0.3814, batch_loss_c: 0.3846, batch_loss_s: 0.3742, time:0.9578, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:11:28 \u001b[32mINFO     \u001b[0m train.py: [9/300], [10/484], step: 4366, 2.420 samples/sec, batch_loss: 0.0529, batch_loss_c: 0.0511, batch_loss_s: 0.0573, time:16.5279, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:11:42 \u001b[32mINFO     \u001b[0m train.py: [9/300], [20/484], step: 4376, 3.006 samples/sec, batch_loss: 0.2113, batch_loss_c: 0.1836, batch_loss_s: 0.2759, time:13.3077, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:12:10 \u001b[32mINFO     \u001b[0m train.py: [9/300], [30/484], step: 4386, 1.393 samples/sec, batch_loss: 0.0883, batch_loss_c: 0.0844, batch_loss_s: 0.0975, time:28.7216, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:12:38 \u001b[32mINFO     \u001b[0m train.py: [9/300], [40/484], step: 4396, 1.428 samples/sec, batch_loss: 0.1070, batch_loss_c: 0.1060, batch_loss_s: 0.1093, time:28.0040, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:12:59 \u001b[32mINFO     \u001b[0m train.py: [9/300], [50/484], step: 4406, 1.895 samples/sec, batch_loss: 0.2861, batch_loss_c: 0.2769, batch_loss_s: 0.3076, time:21.1103, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:13:22 \u001b[32mINFO     \u001b[0m train.py: [9/300], [60/484], step: 4416, 1.795 samples/sec, batch_loss: 0.0695, batch_loss_c: 0.0689, batch_loss_s: 0.0708, time:22.2875, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:13:40 \u001b[32mINFO     \u001b[0m train.py: [9/300], [70/484], step: 4426, 2.144 samples/sec, batch_loss: 0.3968, batch_loss_c: 0.3818, batch_loss_s: 0.4320, time:18.6525, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:13:56 \u001b[32mINFO     \u001b[0m train.py: [9/300], [80/484], step: 4436, 2.542 samples/sec, batch_loss: 0.0653, batch_loss_c: 0.0558, batch_loss_s: 0.0875, time:15.7333, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:14:28 \u001b[32mINFO     \u001b[0m train.py: [9/300], [90/484], step: 4446, 1.258 samples/sec, batch_loss: 0.0694, batch_loss_c: 0.0619, batch_loss_s: 0.0868, time:31.7919, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:14:42 \u001b[32mINFO     \u001b[0m train.py: [9/300], [100/484], step: 4456, 2.815 samples/sec, batch_loss: 0.1117, batch_loss_c: 0.1237, batch_loss_s: 0.0838, time:14.2082, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:15:18 \u001b[32mINFO     \u001b[0m train.py: [9/300], [110/484], step: 4466, 1.110 samples/sec, batch_loss: 0.0915, batch_loss_c: 0.0941, batch_loss_s: 0.0855, time:36.0212, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:15:42 \u001b[32mINFO     \u001b[0m train.py: [9/300], [120/484], step: 4476, 1.685 samples/sec, batch_loss: 0.0552, batch_loss_c: 0.0503, batch_loss_s: 0.0668, time:23.7379, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:16:01 \u001b[32mINFO     \u001b[0m train.py: [9/300], [130/484], step: 4486, 2.134 samples/sec, batch_loss: 0.0679, batch_loss_c: 0.0649, batch_loss_s: 0.0748, time:18.7434, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:16:14 \u001b[32mINFO     \u001b[0m train.py: [9/300], [140/484], step: 4496, 2.912 samples/sec, batch_loss: 0.0344, batch_loss_c: 0.0274, batch_loss_s: 0.0507, time:13.7355, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:16:31 \u001b[32mINFO     \u001b[0m train.py: [9/300], [150/484], step: 4506, 2.324 samples/sec, batch_loss: 0.0898, batch_loss_c: 0.0866, batch_loss_s: 0.0971, time:17.2104, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:16:48 \u001b[32mINFO     \u001b[0m train.py: [9/300], [160/484], step: 4516, 2.438 samples/sec, batch_loss: 0.2900, batch_loss_c: 0.2787, batch_loss_s: 0.3164, time:16.4046, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:17:01 \u001b[32mINFO     \u001b[0m train.py: [9/300], [170/484], step: 4526, 3.156 samples/sec, batch_loss: 0.3091, batch_loss_c: 0.3059, batch_loss_s: 0.3165, time:12.6744, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:17:15 \u001b[32mINFO     \u001b[0m train.py: [9/300], [180/484], step: 4536, 2.704 samples/sec, batch_loss: 0.0440, batch_loss_c: 0.0377, batch_loss_s: 0.0587, time:14.7926, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:17:29 \u001b[32mINFO     \u001b[0m train.py: [9/300], [190/484], step: 4546, 2.842 samples/sec, batch_loss: 0.3034, batch_loss_c: 0.3005, batch_loss_s: 0.3101, time:14.0747, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:17:46 \u001b[32mINFO     \u001b[0m train.py: [9/300], [200/484], step: 4556, 2.385 samples/sec, batch_loss: 0.0871, batch_loss_c: 0.0832, batch_loss_s: 0.0960, time:16.7719, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:18:04 \u001b[32mINFO     \u001b[0m train.py: [9/300], [210/484], step: 4566, 2.253 samples/sec, batch_loss: 0.0856, batch_loss_c: 0.0797, batch_loss_s: 0.0994, time:17.7573, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:18:16 \u001b[32mINFO     \u001b[0m train.py: [9/300], [220/484], step: 4576, 3.200 samples/sec, batch_loss: 0.3438, batch_loss_c: 0.3437, batch_loss_s: 0.3442, time:12.5018, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:18:43 \u001b[32mINFO     \u001b[0m train.py: [9/300], [230/484], step: 4586, 1.482 samples/sec, batch_loss: 0.4048, batch_loss_c: 0.4040, batch_loss_s: 0.4066, time:26.9986, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:18:56 \u001b[32mINFO     \u001b[0m train.py: [9/300], [240/484], step: 4596, 3.229 samples/sec, batch_loss: 0.0892, batch_loss_c: 0.0754, batch_loss_s: 0.1215, time:12.3876, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:19:21 \u001b[32mINFO     \u001b[0m train.py: [9/300], [250/484], step: 4606, 1.593 samples/sec, batch_loss: 0.0574, batch_loss_c: 0.0516, batch_loss_s: 0.0711, time:25.1142, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:19:38 \u001b[32mINFO     \u001b[0m train.py: [9/300], [260/484], step: 4616, 2.331 samples/sec, batch_loss: 0.3163, batch_loss_c: 0.3062, batch_loss_s: 0.3397, time:17.1572, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:19:50 \u001b[32mINFO     \u001b[0m train.py: [9/300], [270/484], step: 4626, 3.384 samples/sec, batch_loss: 0.0718, batch_loss_c: 0.0650, batch_loss_s: 0.0876, time:11.8206, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:20:13 \u001b[32mINFO     \u001b[0m train.py: [9/300], [280/484], step: 4636, 1.731 samples/sec, batch_loss: 0.3311, batch_loss_c: 0.3257, batch_loss_s: 0.3437, time:23.1138, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:20:26 \u001b[32mINFO     \u001b[0m train.py: [9/300], [290/484], step: 4646, 3.022 samples/sec, batch_loss: 0.1668, batch_loss_c: 0.1681, batch_loss_s: 0.1636, time:13.2375, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:20:47 \u001b[32mINFO     \u001b[0m train.py: [9/300], [300/484], step: 4656, 1.913 samples/sec, batch_loss: 0.3822, batch_loss_c: 0.3912, batch_loss_s: 0.3610, time:20.9136, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:21:03 \u001b[32mINFO     \u001b[0m train.py: [9/300], [310/484], step: 4666, 2.542 samples/sec, batch_loss: 0.4081, batch_loss_c: 0.4117, batch_loss_s: 0.3997, time:15.7327, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:21:18 \u001b[32mINFO     \u001b[0m train.py: [9/300], [320/484], step: 4676, 2.705 samples/sec, batch_loss: 0.1815, batch_loss_c: 0.1810, batch_loss_s: 0.1828, time:14.7853, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:21:40 \u001b[32mINFO     \u001b[0m train.py: [9/300], [330/484], step: 4686, 1.767 samples/sec, batch_loss: 0.0943, batch_loss_c: 0.0808, batch_loss_s: 0.1257, time:22.6341, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:21:58 \u001b[32mINFO     \u001b[0m train.py: [9/300], [340/484], step: 4696, 2.219 samples/sec, batch_loss: 0.5292, batch_loss_c: 0.5254, batch_loss_s: 0.5379, time:18.0261, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:22:18 \u001b[32mINFO     \u001b[0m train.py: [9/300], [350/484], step: 4706, 2.047 samples/sec, batch_loss: 0.0982, batch_loss_c: 0.1093, batch_loss_s: 0.0723, time:19.5438, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:22:31 \u001b[32mINFO     \u001b[0m train.py: [9/300], [360/484], step: 4716, 3.093 samples/sec, batch_loss: 0.0696, batch_loss_c: 0.0602, batch_loss_s: 0.0915, time:12.9317, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:22:47 \u001b[32mINFO     \u001b[0m train.py: [9/300], [370/484], step: 4726, 2.469 samples/sec, batch_loss: 0.1797, batch_loss_c: 0.1493, batch_loss_s: 0.2507, time:16.2009, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:23:00 \u001b[32mINFO     \u001b[0m train.py: [9/300], [380/484], step: 4736, 3.091 samples/sec, batch_loss: 0.1035, batch_loss_c: 0.0988, batch_loss_s: 0.1146, time:12.9422, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:23:15 \u001b[32mINFO     \u001b[0m train.py: [9/300], [390/484], step: 4746, 2.739 samples/sec, batch_loss: 0.1074, batch_loss_c: 0.1022, batch_loss_s: 0.1196, time:14.6045, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:23:32 \u001b[32mINFO     \u001b[0m train.py: [9/300], [400/484], step: 4756, 2.315 samples/sec, batch_loss: 0.1327, batch_loss_c: 0.1275, batch_loss_s: 0.1446, time:17.2785, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:23:48 \u001b[32mINFO     \u001b[0m train.py: [9/300], [410/484], step: 4766, 2.444 samples/sec, batch_loss: 0.0706, batch_loss_c: 0.0623, batch_loss_s: 0.0899, time:16.3694, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:24:03 \u001b[32mINFO     \u001b[0m train.py: [9/300], [420/484], step: 4776, 2.627 samples/sec, batch_loss: 0.1031, batch_loss_c: 0.0984, batch_loss_s: 0.1141, time:15.2255, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:24:34 \u001b[32mINFO     \u001b[0m train.py: [9/300], [430/484], step: 4786, 1.308 samples/sec, batch_loss: 0.0566, batch_loss_c: 0.0485, batch_loss_s: 0.0755, time:30.5855, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:24:56 \u001b[32mINFO     \u001b[0m train.py: [9/300], [440/484], step: 4796, 1.857 samples/sec, batch_loss: 0.1784, batch_loss_c: 0.1760, batch_loss_s: 0.1838, time:21.5404, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:25:31 \u001b[32mINFO     \u001b[0m train.py: [9/300], [450/484], step: 4806, 1.135 samples/sec, batch_loss: 0.0826, batch_loss_c: 0.0777, batch_loss_s: 0.0941, time:35.2450, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:25:53 \u001b[32mINFO     \u001b[0m train.py: [9/300], [460/484], step: 4816, 1.808 samples/sec, batch_loss: 0.1820, batch_loss_c: 0.1758, batch_loss_s: 0.1964, time:22.1253, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:26:06 \u001b[32mINFO     \u001b[0m train.py: [9/300], [470/484], step: 4826, 2.963 samples/sec, batch_loss: 0.0753, batch_loss_c: 0.0675, batch_loss_s: 0.0933, time:13.5010, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:26:33 \u001b[32mINFO     \u001b[0m train.py: [9/300], [480/484], step: 4836, 1.483 samples/sec, batch_loss: 0.1178, batch_loss_c: 0.1202, batch_loss_s: 0.1120, time:26.9659, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:26:42 \u001b[32mINFO     \u001b[0m train.py: [9/300], train_loss: 0.1588, time: 930.8546, lr: 0.0001\u001b[0m\n",
            "2019-12-06 06:26:44 \u001b[32mINFO     \u001b[0m train.py: [10/300], [0/484], step: 4840, 22.020 samples/sec, batch_loss: 0.3138, batch_loss_c: 0.3095, batch_loss_s: 0.3239, time:1.8166, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:26:56 \u001b[32mINFO     \u001b[0m train.py: [10/300], [10/484], step: 4850, 3.330 samples/sec, batch_loss: 0.0512, batch_loss_c: 0.0370, batch_loss_s: 0.0845, time:12.0111, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:27:18 \u001b[32mINFO     \u001b[0m train.py: [10/300], [20/484], step: 4860, 1.843 samples/sec, batch_loss: 0.2774, batch_loss_c: 0.2715, batch_loss_s: 0.2912, time:21.6987, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:27:44 \u001b[32mINFO     \u001b[0m train.py: [10/300], [30/484], step: 4870, 1.503 samples/sec, batch_loss: 0.2928, batch_loss_c: 0.2847, batch_loss_s: 0.3116, time:26.6098, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:28:06 \u001b[32mINFO     \u001b[0m train.py: [10/300], [40/484], step: 4880, 1.855 samples/sec, batch_loss: 0.0904, batch_loss_c: 0.0836, batch_loss_s: 0.1062, time:21.5585, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:28:30 \u001b[32mINFO     \u001b[0m train.py: [10/300], [50/484], step: 4890, 1.661 samples/sec, batch_loss: 0.0540, batch_loss_c: 0.0472, batch_loss_s: 0.0697, time:24.0812, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:28:43 \u001b[32mINFO     \u001b[0m train.py: [10/300], [60/484], step: 4900, 3.045 samples/sec, batch_loss: 0.2950, batch_loss_c: 0.2923, batch_loss_s: 0.3013, time:13.1349, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:29:00 \u001b[32mINFO     \u001b[0m train.py: [10/300], [70/484], step: 4910, 2.416 samples/sec, batch_loss: 0.4018, batch_loss_c: 0.3829, batch_loss_s: 0.4460, time:16.5550, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:29:27 \u001b[32mINFO     \u001b[0m train.py: [10/300], [80/484], step: 4920, 1.447 samples/sec, batch_loss: 0.0618, batch_loss_c: 0.0534, batch_loss_s: 0.0812, time:27.6365, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:29:47 \u001b[32mINFO     \u001b[0m train.py: [10/300], [90/484], step: 4930, 1.969 samples/sec, batch_loss: 0.1019, batch_loss_c: 0.0928, batch_loss_s: 0.1232, time:20.3115, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:30:03 \u001b[32mINFO     \u001b[0m train.py: [10/300], [100/484], step: 4940, 2.541 samples/sec, batch_loss: 0.0535, batch_loss_c: 0.0478, batch_loss_s: 0.0668, time:15.7416, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:30:19 \u001b[32mINFO     \u001b[0m train.py: [10/300], [110/484], step: 4950, 2.536 samples/sec, batch_loss: 0.3699, batch_loss_c: 0.3797, batch_loss_s: 0.3471, time:15.7733, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:30:37 \u001b[32mINFO     \u001b[0m train.py: [10/300], [120/484], step: 4960, 2.258 samples/sec, batch_loss: 0.0695, batch_loss_c: 0.0617, batch_loss_s: 0.0879, time:17.7133, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:30:57 \u001b[32mINFO     \u001b[0m train.py: [10/300], [130/484], step: 4970, 1.957 samples/sec, batch_loss: 0.1039, batch_loss_c: 0.0943, batch_loss_s: 0.1262, time:20.4438, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:31:16 \u001b[32mINFO     \u001b[0m train.py: [10/300], [140/484], step: 4980, 2.077 samples/sec, batch_loss: 0.2884, batch_loss_c: 0.2770, batch_loss_s: 0.3148, time:19.2561, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:31:31 \u001b[32mINFO     \u001b[0m train.py: [10/300], [150/484], step: 4990, 2.744 samples/sec, batch_loss: 0.2833, batch_loss_c: 0.2768, batch_loss_s: 0.2983, time:14.5785, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:31:47 \u001b[32mINFO     \u001b[0m train.py: [10/300], [160/484], step: 5000, 2.497 samples/sec, batch_loss: 0.0521, batch_loss_c: 0.0472, batch_loss_s: 0.0635, time:16.0193, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:32:06 \u001b[32mINFO     \u001b[0m train.py: [10/300], [170/484], step: 5010, 2.067 samples/sec, batch_loss: 0.0980, batch_loss_c: 0.0854, batch_loss_s: 0.1273, time:19.3509, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:32:23 \u001b[32mINFO     \u001b[0m train.py: [10/300], [180/484], step: 5020, 2.413 samples/sec, batch_loss: 0.1169, batch_loss_c: 0.1092, batch_loss_s: 0.1351, time:16.5761, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:32:50 \u001b[32mINFO     \u001b[0m train.py: [10/300], [190/484], step: 5030, 1.452 samples/sec, batch_loss: 0.0743, batch_loss_c: 0.0703, batch_loss_s: 0.0836, time:27.5536, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:33:07 \u001b[32mINFO     \u001b[0m train.py: [10/300], [200/484], step: 5040, 2.455 samples/sec, batch_loss: 0.3822, batch_loss_c: 0.3784, batch_loss_s: 0.3910, time:16.2911, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:33:29 \u001b[32mINFO     \u001b[0m train.py: [10/300], [210/484], step: 5050, 1.775 samples/sec, batch_loss: 0.1910, batch_loss_c: 0.1804, batch_loss_s: 0.2158, time:22.5351, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:34:24 \u001b[32mINFO     \u001b[0m train.py: [10/300], [220/484], step: 5060, 0.735 samples/sec, batch_loss: 0.0506, batch_loss_c: 0.0443, batch_loss_s: 0.0652, time:54.4404, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:34:44 \u001b[32mINFO     \u001b[0m train.py: [10/300], [230/484], step: 5070, 1.996 samples/sec, batch_loss: 0.3147, batch_loss_c: 0.3103, batch_loss_s: 0.3250, time:20.0361, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:34:59 \u001b[32mINFO     \u001b[0m train.py: [10/300], [240/484], step: 5080, 2.632 samples/sec, batch_loss: 0.0842, batch_loss_c: 0.0758, batch_loss_s: 0.1040, time:15.1997, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:35:29 \u001b[32mINFO     \u001b[0m train.py: [10/300], [250/484], step: 5090, 1.349 samples/sec, batch_loss: 0.3142, batch_loss_c: 0.3166, batch_loss_s: 0.3086, time:29.6582, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:35:49 \u001b[32mINFO     \u001b[0m train.py: [10/300], [260/484], step: 5100, 1.946 samples/sec, batch_loss: 0.3152, batch_loss_c: 0.3071, batch_loss_s: 0.3343, time:20.5517, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:36:04 \u001b[32mINFO     \u001b[0m train.py: [10/300], [270/484], step: 5110, 2.619 samples/sec, batch_loss: 0.1135, batch_loss_c: 0.1086, batch_loss_s: 0.1249, time:15.2740, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:36:22 \u001b[32mINFO     \u001b[0m train.py: [10/300], [280/484], step: 5120, 2.300 samples/sec, batch_loss: 0.0664, batch_loss_c: 0.0631, batch_loss_s: 0.0742, time:17.3939, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:36:53 \u001b[32mINFO     \u001b[0m train.py: [10/300], [290/484], step: 5130, 1.284 samples/sec, batch_loss: 0.1036, batch_loss_c: 0.1057, batch_loss_s: 0.0985, time:31.1540, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:37:06 \u001b[32mINFO     \u001b[0m train.py: [10/300], [300/484], step: 5140, 3.082 samples/sec, batch_loss: 0.1341, batch_loss_c: 0.1365, batch_loss_s: 0.1286, time:12.9790, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:37:17 \u001b[32mINFO     \u001b[0m train.py: [10/300], [310/484], step: 5150, 3.542 samples/sec, batch_loss: 0.0570, batch_loss_c: 0.0441, batch_loss_s: 0.0871, time:11.2940, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:37:33 \u001b[32mINFO     \u001b[0m train.py: [10/300], [320/484], step: 5160, 2.528 samples/sec, batch_loss: 0.0723, batch_loss_c: 0.0637, batch_loss_s: 0.0925, time:15.8246, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:37:46 \u001b[32mINFO     \u001b[0m train.py: [10/300], [330/484], step: 5170, 3.131 samples/sec, batch_loss: 0.0789, batch_loss_c: 0.0653, batch_loss_s: 0.1108, time:12.7744, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:38:12 \u001b[32mINFO     \u001b[0m train.py: [10/300], [340/484], step: 5180, 1.517 samples/sec, batch_loss: 0.1505, batch_loss_c: 0.1159, batch_loss_s: 0.2314, time:26.3726, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:38:31 \u001b[32mINFO     \u001b[0m train.py: [10/300], [350/484], step: 5190, 2.111 samples/sec, batch_loss: 0.2971, batch_loss_c: 0.2927, batch_loss_s: 0.3074, time:18.9474, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:38:54 \u001b[32mINFO     \u001b[0m train.py: [10/300], [360/484], step: 5200, 1.734 samples/sec, batch_loss: 0.1215, batch_loss_c: 0.1416, batch_loss_s: 0.0745, time:23.0639, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:39:08 \u001b[32mINFO     \u001b[0m train.py: [10/300], [370/484], step: 5210, 2.925 samples/sec, batch_loss: 0.0420, batch_loss_c: 0.0347, batch_loss_s: 0.0592, time:13.6746, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:39:27 \u001b[32mINFO     \u001b[0m train.py: [10/300], [380/484], step: 5220, 2.097 samples/sec, batch_loss: 0.1476, batch_loss_c: 0.1280, batch_loss_s: 0.1932, time:19.0774, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:39:42 \u001b[32mINFO     \u001b[0m train.py: [10/300], [390/484], step: 5230, 2.741 samples/sec, batch_loss: 0.0646, batch_loss_c: 0.0523, batch_loss_s: 0.0935, time:14.5938, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:39:58 \u001b[32mINFO     \u001b[0m train.py: [10/300], [400/484], step: 5240, 2.474 samples/sec, batch_loss: 0.0645, batch_loss_c: 0.0530, batch_loss_s: 0.0912, time:16.1656, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:40:11 \u001b[32mINFO     \u001b[0m train.py: [10/300], [410/484], step: 5250, 3.002 samples/sec, batch_loss: 0.0633, batch_loss_c: 0.0553, batch_loss_s: 0.0820, time:13.3256, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:40:25 \u001b[32mINFO     \u001b[0m train.py: [10/300], [420/484], step: 5260, 2.797 samples/sec, batch_loss: 0.0487, batch_loss_c: 0.0409, batch_loss_s: 0.0669, time:14.3003, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:40:46 \u001b[32mINFO     \u001b[0m train.py: [10/300], [430/484], step: 5270, 1.956 samples/sec, batch_loss: 0.0504, batch_loss_c: 0.0434, batch_loss_s: 0.0666, time:20.4476, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:41:30 \u001b[32mINFO     \u001b[0m train.py: [10/300], [440/484], step: 5280, 0.916 samples/sec, batch_loss: 0.3188, batch_loss_c: 0.3118, batch_loss_s: 0.3352, time:43.6708, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:41:43 \u001b[32mINFO     \u001b[0m train.py: [10/300], [450/484], step: 5290, 2.869 samples/sec, batch_loss: 0.3287, batch_loss_c: 0.3333, batch_loss_s: 0.3181, time:13.9432, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:41:57 \u001b[32mINFO     \u001b[0m train.py: [10/300], [460/484], step: 5300, 3.023 samples/sec, batch_loss: 0.0516, batch_loss_c: 0.0473, batch_loss_s: 0.0617, time:13.2331, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:42:10 \u001b[32mINFO     \u001b[0m train.py: [10/300], [470/484], step: 5310, 3.036 samples/sec, batch_loss: 0.0713, batch_loss_c: 0.0752, batch_loss_s: 0.0623, time:13.1761, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:42:24 \u001b[32mINFO     \u001b[0m train.py: [10/300], [480/484], step: 5320, 2.841 samples/sec, batch_loss: 0.2970, batch_loss_c: 0.3431, batch_loss_s: 0.1894, time:14.0786, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:42:28 \u001b[32mINFO     \u001b[0m train.py: [10/300], train_loss: 0.1514, time: 946.3254, lr: 0.0001\u001b[0m\n",
            "2019-12-06 06:42:30 \u001b[32mINFO     \u001b[0m train.py: [11/300], [0/484], step: 5324, 30.344 samples/sec, batch_loss: 0.1486, batch_loss_c: 0.1386, batch_loss_s: 0.1719, time:1.3182, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:42:48 \u001b[32mINFO     \u001b[0m train.py: [11/300], [10/484], step: 5334, 2.264 samples/sec, batch_loss: 0.0927, batch_loss_c: 0.0870, batch_loss_s: 0.1061, time:17.6661, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:43:06 \u001b[32mINFO     \u001b[0m train.py: [11/300], [20/484], step: 5344, 2.179 samples/sec, batch_loss: 0.1217, batch_loss_c: 0.1083, batch_loss_s: 0.1528, time:18.3596, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:43:28 \u001b[32mINFO     \u001b[0m train.py: [11/300], [30/484], step: 5354, 1.851 samples/sec, batch_loss: 0.0677, batch_loss_c: 0.0590, batch_loss_s: 0.0879, time:21.6104, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:43:51 \u001b[32mINFO     \u001b[0m train.py: [11/300], [40/484], step: 5364, 1.702 samples/sec, batch_loss: 0.2926, batch_loss_c: 0.2860, batch_loss_s: 0.3082, time:23.5071, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:44:18 \u001b[32mINFO     \u001b[0m train.py: [11/300], [50/484], step: 5374, 1.504 samples/sec, batch_loss: 0.1206, batch_loss_c: 0.1232, batch_loss_s: 0.1146, time:26.5886, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:44:39 \u001b[32mINFO     \u001b[0m train.py: [11/300], [60/484], step: 5384, 1.888 samples/sec, batch_loss: 0.1396, batch_loss_c: 0.1377, batch_loss_s: 0.1441, time:21.1819, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:44:53 \u001b[32mINFO     \u001b[0m train.py: [11/300], [70/484], step: 5394, 2.797 samples/sec, batch_loss: 0.0608, batch_loss_c: 0.0571, batch_loss_s: 0.0695, time:14.3016, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:45:11 \u001b[32mINFO     \u001b[0m train.py: [11/300], [80/484], step: 5404, 2.238 samples/sec, batch_loss: 0.3083, batch_loss_c: 0.2989, batch_loss_s: 0.3303, time:17.8713, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:45:33 \u001b[32mINFO     \u001b[0m train.py: [11/300], [90/484], step: 5414, 1.846 samples/sec, batch_loss: 0.1264, batch_loss_c: 0.1153, batch_loss_s: 0.1522, time:21.6742, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:45:51 \u001b[32mINFO     \u001b[0m train.py: [11/300], [100/484], step: 5424, 2.211 samples/sec, batch_loss: 0.0690, batch_loss_c: 0.0657, batch_loss_s: 0.0766, time:18.0902, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:46:19 \u001b[32mINFO     \u001b[0m train.py: [11/300], [110/484], step: 5434, 1.427 samples/sec, batch_loss: 0.1009, batch_loss_c: 0.1104, batch_loss_s: 0.0789, time:28.0392, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:46:32 \u001b[32mINFO     \u001b[0m train.py: [11/300], [120/484], step: 5444, 3.185 samples/sec, batch_loss: 0.3378, batch_loss_c: 0.3359, batch_loss_s: 0.3422, time:12.5584, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:46:54 \u001b[32mINFO     \u001b[0m train.py: [11/300], [130/484], step: 5454, 1.783 samples/sec, batch_loss: 0.0703, batch_loss_c: 0.0665, batch_loss_s: 0.0791, time:22.4354, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:47:11 \u001b[32mINFO     \u001b[0m train.py: [11/300], [140/484], step: 5464, 2.405 samples/sec, batch_loss: 0.2078, batch_loss_c: 0.2068, batch_loss_s: 0.2103, time:16.6345, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:47:46 \u001b[32mINFO     \u001b[0m train.py: [11/300], [150/484], step: 5474, 1.135 samples/sec, batch_loss: 0.0615, batch_loss_c: 0.0530, batch_loss_s: 0.0813, time:35.2390, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:48:00 \u001b[32mINFO     \u001b[0m train.py: [11/300], [160/484], step: 5484, 2.868 samples/sec, batch_loss: 0.0794, batch_loss_c: 0.0780, batch_loss_s: 0.0825, time:13.9455, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:48:19 \u001b[32mINFO     \u001b[0m train.py: [11/300], [170/484], step: 5494, 2.146 samples/sec, batch_loss: 0.0619, batch_loss_c: 0.0480, batch_loss_s: 0.0944, time:18.6390, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:48:43 \u001b[32mINFO     \u001b[0m train.py: [11/300], [180/484], step: 5504, 1.649 samples/sec, batch_loss: 0.0720, batch_loss_c: 0.0613, batch_loss_s: 0.0969, time:24.2576, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:49:04 \u001b[32mINFO     \u001b[0m train.py: [11/300], [190/484], step: 5514, 1.886 samples/sec, batch_loss: 0.0858, batch_loss_c: 0.0861, batch_loss_s: 0.0851, time:21.2060, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:49:26 \u001b[32mINFO     \u001b[0m train.py: [11/300], [200/484], step: 5524, 1.828 samples/sec, batch_loss: 0.2918, batch_loss_c: 0.2908, batch_loss_s: 0.2940, time:21.8844, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:49:51 \u001b[32mINFO     \u001b[0m train.py: [11/300], [210/484], step: 5534, 1.595 samples/sec, batch_loss: 0.1118, batch_loss_c: 0.1130, batch_loss_s: 0.1091, time:25.0844, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:50:14 \u001b[32mINFO     \u001b[0m train.py: [11/300], [220/484], step: 5544, 1.767 samples/sec, batch_loss: 0.0579, batch_loss_c: 0.0546, batch_loss_s: 0.0656, time:22.6426, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:50:29 \u001b[32mINFO     \u001b[0m train.py: [11/300], [230/484], step: 5554, 2.652 samples/sec, batch_loss: 0.0768, batch_loss_c: 0.0723, batch_loss_s: 0.0871, time:15.0856, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:50:47 \u001b[32mINFO     \u001b[0m train.py: [11/300], [240/484], step: 5564, 2.143 samples/sec, batch_loss: 0.3128, batch_loss_c: 0.3143, batch_loss_s: 0.3093, time:18.6674, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:51:15 \u001b[32mINFO     \u001b[0m train.py: [11/300], [250/484], step: 5574, 1.462 samples/sec, batch_loss: 0.0911, batch_loss_c: 0.0899, batch_loss_s: 0.0940, time:27.3650, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:51:35 \u001b[32mINFO     \u001b[0m train.py: [11/300], [260/484], step: 5584, 1.931 samples/sec, batch_loss: 0.1763, batch_loss_c: 0.1422, batch_loss_s: 0.2557, time:20.7164, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:51:51 \u001b[32mINFO     \u001b[0m train.py: [11/300], [270/484], step: 5594, 2.542 samples/sec, batch_loss: 0.2988, batch_loss_c: 0.2940, batch_loss_s: 0.3102, time:15.7336, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:52:11 \u001b[32mINFO     \u001b[0m train.py: [11/300], [280/484], step: 5604, 2.033 samples/sec, batch_loss: 0.0664, batch_loss_c: 0.0592, batch_loss_s: 0.0833, time:19.6779, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:52:40 \u001b[32mINFO     \u001b[0m train.py: [11/300], [290/484], step: 5614, 1.352 samples/sec, batch_loss: 0.1855, batch_loss_c: 0.1694, batch_loss_s: 0.2229, time:29.5886, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:52:58 \u001b[32mINFO     \u001b[0m train.py: [11/300], [300/484], step: 5624, 2.315 samples/sec, batch_loss: 0.0563, batch_loss_c: 0.0494, batch_loss_s: 0.0723, time:17.2814, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:53:23 \u001b[32mINFO     \u001b[0m train.py: [11/300], [310/484], step: 5634, 1.577 samples/sec, batch_loss: 0.1064, batch_loss_c: 0.0993, batch_loss_s: 0.1230, time:25.3667, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:54:10 \u001b[32mINFO     \u001b[0m train.py: [11/300], [320/484], step: 5644, 0.845 samples/sec, batch_loss: 0.0788, batch_loss_c: 0.0753, batch_loss_s: 0.0867, time:47.3632, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:54:23 \u001b[32mINFO     \u001b[0m train.py: [11/300], [330/484], step: 5654, 3.265 samples/sec, batch_loss: 0.1163, batch_loss_c: 0.1095, batch_loss_s: 0.1323, time:12.2512, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:54:39 \u001b[32mINFO     \u001b[0m train.py: [11/300], [340/484], step: 5664, 2.468 samples/sec, batch_loss: 0.0710, batch_loss_c: 0.0631, batch_loss_s: 0.0895, time:16.2103, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:54:55 \u001b[32mINFO     \u001b[0m train.py: [11/300], [350/484], step: 5674, 2.512 samples/sec, batch_loss: 0.2906, batch_loss_c: 0.2855, batch_loss_s: 0.3025, time:15.9211, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:55:14 \u001b[32mINFO     \u001b[0m train.py: [11/300], [360/484], step: 5684, 2.102 samples/sec, batch_loss: 0.3302, batch_loss_c: 0.3262, batch_loss_s: 0.3395, time:19.0254, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:55:44 \u001b[32mINFO     \u001b[0m train.py: [11/300], [370/484], step: 5694, 1.312 samples/sec, batch_loss: 0.0586, batch_loss_c: 0.0562, batch_loss_s: 0.0641, time:30.4861, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:56:09 \u001b[32mINFO     \u001b[0m train.py: [11/300], [380/484], step: 5704, 1.613 samples/sec, batch_loss: 0.6868, batch_loss_c: 0.7067, batch_loss_s: 0.6402, time:24.7950, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:56:23 \u001b[32mINFO     \u001b[0m train.py: [11/300], [390/484], step: 5714, 2.909 samples/sec, batch_loss: 0.1119, batch_loss_c: 0.1156, batch_loss_s: 0.1035, time:13.7521, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:56:36 \u001b[32mINFO     \u001b[0m train.py: [11/300], [400/484], step: 5724, 3.174 samples/sec, batch_loss: 0.0710, batch_loss_c: 0.0656, batch_loss_s: 0.0837, time:12.6009, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:56:54 \u001b[32mINFO     \u001b[0m train.py: [11/300], [410/484], step: 5734, 2.168 samples/sec, batch_loss: 0.3213, batch_loss_c: 0.2963, batch_loss_s: 0.3796, time:18.4524, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:57:10 \u001b[32mINFO     \u001b[0m train.py: [11/300], [420/484], step: 5744, 2.574 samples/sec, batch_loss: 0.0711, batch_loss_c: 0.0575, batch_loss_s: 0.1028, time:15.5378, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:57:29 \u001b[32mINFO     \u001b[0m train.py: [11/300], [430/484], step: 5754, 2.020 samples/sec, batch_loss: 0.0684, batch_loss_c: 0.0554, batch_loss_s: 0.0986, time:19.7972, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:57:41 \u001b[32mINFO     \u001b[0m train.py: [11/300], [440/484], step: 5764, 3.347 samples/sec, batch_loss: 0.2233, batch_loss_c: 0.2560, batch_loss_s: 0.1471, time:11.9499, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:57:55 \u001b[32mINFO     \u001b[0m train.py: [11/300], [450/484], step: 5774, 2.991 samples/sec, batch_loss: 0.0678, batch_loss_c: 0.0673, batch_loss_s: 0.0690, time:13.3715, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:58:12 \u001b[32mINFO     \u001b[0m train.py: [11/300], [460/484], step: 5784, 2.314 samples/sec, batch_loss: 0.0834, batch_loss_c: 0.0748, batch_loss_s: 0.1034, time:17.2843, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:58:27 \u001b[32mINFO     \u001b[0m train.py: [11/300], [470/484], step: 5794, 2.695 samples/sec, batch_loss: 0.2585, batch_loss_c: 0.2807, batch_loss_s: 0.2067, time:14.8417, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:58:40 \u001b[32mINFO     \u001b[0m train.py: [11/300], [480/484], step: 5804, 3.076 samples/sec, batch_loss: 0.0590, batch_loss_c: 0.0474, batch_loss_s: 0.0860, time:13.0046, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:58:47 \u001b[32mINFO     \u001b[0m train.py: [11/300], train_loss: 0.1569, time: 978.2256, lr: 0.0001\u001b[0m\n",
            "2019-12-06 06:58:52 \u001b[32mINFO     \u001b[0m train.py: [12/300], [0/484], step: 5808, 9.152 samples/sec, batch_loss: 0.3306, batch_loss_c: 0.3246, batch_loss_s: 0.3445, time:4.3707, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:59:05 \u001b[32mINFO     \u001b[0m train.py: [12/300], [10/484], step: 5818, 2.952 samples/sec, batch_loss: 0.1430, batch_loss_c: 0.1296, batch_loss_s: 0.1744, time:13.5517, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:59:27 \u001b[32mINFO     \u001b[0m train.py: [12/300], [20/484], step: 5828, 1.856 samples/sec, batch_loss: 0.0663, batch_loss_c: 0.0540, batch_loss_s: 0.0951, time:21.5513, lr:0.0001\u001b[0m\n",
            "2019-12-06 06:59:46 \u001b[32mINFO     \u001b[0m train.py: [12/300], [30/484], step: 5838, 2.112 samples/sec, batch_loss: 0.0812, batch_loss_c: 0.0809, batch_loss_s: 0.0820, time:18.9369, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:00:10 \u001b[32mINFO     \u001b[0m train.py: [12/300], [40/484], step: 5848, 1.638 samples/sec, batch_loss: 0.4463, batch_loss_c: 0.4338, batch_loss_s: 0.4753, time:24.4135, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:00:47 \u001b[32mINFO     \u001b[0m train.py: [12/300], [50/484], step: 5858, 1.087 samples/sec, batch_loss: 0.0737, batch_loss_c: 0.0562, batch_loss_s: 0.1145, time:36.7931, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:01:08 \u001b[32mINFO     \u001b[0m train.py: [12/300], [60/484], step: 5868, 1.949 samples/sec, batch_loss: 0.1085, batch_loss_c: 0.1114, batch_loss_s: 0.1017, time:20.5269, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:01:33 \u001b[32mINFO     \u001b[0m train.py: [12/300], [70/484], step: 5878, 1.563 samples/sec, batch_loss: 0.0680, batch_loss_c: 0.0714, batch_loss_s: 0.0601, time:25.5944, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:01:53 \u001b[32mINFO     \u001b[0m train.py: [12/300], [80/484], step: 5888, 2.061 samples/sec, batch_loss: 0.2940, batch_loss_c: 0.2883, batch_loss_s: 0.3072, time:19.4068, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:02:07 \u001b[32mINFO     \u001b[0m train.py: [12/300], [90/484], step: 5898, 2.902 samples/sec, batch_loss: 0.0706, batch_loss_c: 0.0657, batch_loss_s: 0.0820, time:13.7825, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:02:19 \u001b[32mINFO     \u001b[0m train.py: [12/300], [100/484], step: 5908, 3.100 samples/sec, batch_loss: 0.0788, batch_loss_c: 0.0631, batch_loss_s: 0.1155, time:12.9020, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:02:35 \u001b[32mINFO     \u001b[0m train.py: [12/300], [110/484], step: 5918, 2.600 samples/sec, batch_loss: 0.0561, batch_loss_c: 0.0460, batch_loss_s: 0.0796, time:15.3858, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:02:53 \u001b[32mINFO     \u001b[0m train.py: [12/300], [120/484], step: 5928, 2.189 samples/sec, batch_loss: 0.1768, batch_loss_c: 0.1929, batch_loss_s: 0.1393, time:18.2728, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:03:26 \u001b[32mINFO     \u001b[0m train.py: [12/300], [130/484], step: 5938, 1.217 samples/sec, batch_loss: 0.0465, batch_loss_c: 0.0416, batch_loss_s: 0.0581, time:32.8659, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:03:48 \u001b[32mINFO     \u001b[0m train.py: [12/300], [140/484], step: 5948, 1.849 samples/sec, batch_loss: 0.1513, batch_loss_c: 0.1230, batch_loss_s: 0.2173, time:21.6321, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:04:15 \u001b[32mINFO     \u001b[0m train.py: [12/300], [150/484], step: 5958, 1.455 samples/sec, batch_loss: 0.0598, batch_loss_c: 0.0524, batch_loss_s: 0.0769, time:27.4973, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:04:30 \u001b[32mINFO     \u001b[0m train.py: [12/300], [160/484], step: 5968, 2.618 samples/sec, batch_loss: 0.3104, batch_loss_c: 0.2968, batch_loss_s: 0.3421, time:15.2790, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:04:48 \u001b[32mINFO     \u001b[0m train.py: [12/300], [170/484], step: 5978, 2.312 samples/sec, batch_loss: 0.1325, batch_loss_c: 0.1251, batch_loss_s: 0.1497, time:17.3000, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:05:05 \u001b[32mINFO     \u001b[0m train.py: [12/300], [180/484], step: 5988, 2.335 samples/sec, batch_loss: 0.0457, batch_loss_c: 0.0344, batch_loss_s: 0.0720, time:17.1305, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:05:18 \u001b[32mINFO     \u001b[0m train.py: [12/300], [190/484], step: 5998, 3.057 samples/sec, batch_loss: 0.2523, batch_loss_c: 0.2342, batch_loss_s: 0.2946, time:13.0833, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:05:33 \u001b[32mINFO     \u001b[0m train.py: [12/300], [200/484], step: 6008, 2.571 samples/sec, batch_loss: 0.1155, batch_loss_c: 0.1187, batch_loss_s: 0.1083, time:15.5588, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:05:54 \u001b[32mINFO     \u001b[0m train.py: [12/300], [210/484], step: 6018, 1.945 samples/sec, batch_loss: 0.0916, batch_loss_c: 0.0922, batch_loss_s: 0.0903, time:20.5612, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:06:08 \u001b[32mINFO     \u001b[0m train.py: [12/300], [220/484], step: 6028, 2.778 samples/sec, batch_loss: 0.0922, batch_loss_c: 0.0856, batch_loss_s: 0.1075, time:14.3970, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:06:41 \u001b[32mINFO     \u001b[0m train.py: [12/300], [230/484], step: 6038, 1.231 samples/sec, batch_loss: 0.0969, batch_loss_c: 0.0943, batch_loss_s: 0.1030, time:32.4828, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:07:00 \u001b[32mINFO     \u001b[0m train.py: [12/300], [240/484], step: 6048, 2.121 samples/sec, batch_loss: 0.2949, batch_loss_c: 0.2924, batch_loss_s: 0.3005, time:18.8552, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:07:15 \u001b[32mINFO     \u001b[0m train.py: [12/300], [250/484], step: 6058, 2.635 samples/sec, batch_loss: 0.0584, batch_loss_c: 0.0551, batch_loss_s: 0.0661, time:15.1787, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:07:37 \u001b[32mINFO     \u001b[0m train.py: [12/300], [260/484], step: 6068, 1.813 samples/sec, batch_loss: 0.0895, batch_loss_c: 0.0847, batch_loss_s: 0.1005, time:22.0631, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:07:54 \u001b[32mINFO     \u001b[0m train.py: [12/300], [270/484], step: 6078, 2.356 samples/sec, batch_loss: 0.1369, batch_loss_c: 0.1322, batch_loss_s: 0.1479, time:16.9766, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:08:10 \u001b[32mINFO     \u001b[0m train.py: [12/300], [280/484], step: 6088, 2.542 samples/sec, batch_loss: 0.0994, batch_loss_c: 0.1029, batch_loss_s: 0.0912, time:15.7379, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:08:36 \u001b[32mINFO     \u001b[0m train.py: [12/300], [290/484], step: 6098, 1.497 samples/sec, batch_loss: 0.2847, batch_loss_c: 0.2814, batch_loss_s: 0.2925, time:26.7134, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:09:05 \u001b[32mINFO     \u001b[0m train.py: [12/300], [300/484], step: 6108, 1.415 samples/sec, batch_loss: 0.1974, batch_loss_c: 0.1747, batch_loss_s: 0.2503, time:28.2619, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:09:30 \u001b[32mINFO     \u001b[0m train.py: [12/300], [310/484], step: 6118, 1.609 samples/sec, batch_loss: 0.1362, batch_loss_c: 0.1495, batch_loss_s: 0.1051, time:24.8638, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:09:52 \u001b[32mINFO     \u001b[0m train.py: [12/300], [320/484], step: 6128, 1.760 samples/sec, batch_loss: 0.5221, batch_loss_c: 0.5185, batch_loss_s: 0.5304, time:22.7308, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:10:10 \u001b[32mINFO     \u001b[0m train.py: [12/300], [330/484], step: 6138, 2.271 samples/sec, batch_loss: 0.1498, batch_loss_c: 0.1740, batch_loss_s: 0.0935, time:17.6162, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:10:38 \u001b[32mINFO     \u001b[0m train.py: [12/300], [340/484], step: 6148, 1.446 samples/sec, batch_loss: 0.0673, batch_loss_c: 0.0597, batch_loss_s: 0.0849, time:27.6561, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:11:20 \u001b[32mINFO     \u001b[0m train.py: [12/300], [350/484], step: 6158, 0.933 samples/sec, batch_loss: 0.1166, batch_loss_c: 0.0974, batch_loss_s: 0.1613, time:42.8543, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:11:44 \u001b[32mINFO     \u001b[0m train.py: [12/300], [360/484], step: 6168, 1.727 samples/sec, batch_loss: 0.3915, batch_loss_c: 0.3866, batch_loss_s: 0.4028, time:23.1665, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:12:05 \u001b[32mINFO     \u001b[0m train.py: [12/300], [370/484], step: 6178, 1.905 samples/sec, batch_loss: 0.2972, batch_loss_c: 0.2912, batch_loss_s: 0.3113, time:20.9950, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:12:19 \u001b[32mINFO     \u001b[0m train.py: [12/300], [380/484], step: 6188, 2.856 samples/sec, batch_loss: 0.1079, batch_loss_c: 0.1041, batch_loss_s: 0.1168, time:14.0047, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:12:32 \u001b[32mINFO     \u001b[0m train.py: [12/300], [390/484], step: 6198, 2.935 samples/sec, batch_loss: 0.0751, batch_loss_c: 0.0732, batch_loss_s: 0.0794, time:13.6283, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:12:47 \u001b[32mINFO     \u001b[0m train.py: [12/300], [400/484], step: 6208, 2.626 samples/sec, batch_loss: 0.1452, batch_loss_c: 0.1539, batch_loss_s: 0.1250, time:15.2301, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:13:05 \u001b[32mINFO     \u001b[0m train.py: [12/300], [410/484], step: 6218, 2.244 samples/sec, batch_loss: 0.1448, batch_loss_c: 0.1641, batch_loss_s: 0.0999, time:17.8248, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:13:26 \u001b[32mINFO     \u001b[0m train.py: [12/300], [420/484], step: 6228, 1.896 samples/sec, batch_loss: 0.0480, batch_loss_c: 0.0378, batch_loss_s: 0.0718, time:21.1003, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:13:39 \u001b[32mINFO     \u001b[0m train.py: [12/300], [430/484], step: 6238, 3.042 samples/sec, batch_loss: 0.1967, batch_loss_c: 0.1783, batch_loss_s: 0.2396, time:13.1512, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:14:06 \u001b[32mINFO     \u001b[0m train.py: [12/300], [440/484], step: 6248, 1.489 samples/sec, batch_loss: 0.0781, batch_loss_c: 0.0667, batch_loss_s: 0.1046, time:26.8685, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:14:41 \u001b[32mINFO     \u001b[0m train.py: [12/300], [450/484], step: 6258, 1.145 samples/sec, batch_loss: 0.0746, batch_loss_c: 0.0738, batch_loss_s: 0.0764, time:34.9413, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:14:57 \u001b[32mINFO     \u001b[0m train.py: [12/300], [460/484], step: 6268, 2.549 samples/sec, batch_loss: 0.0536, batch_loss_c: 0.0469, batch_loss_s: 0.0693, time:15.6946, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:15:13 \u001b[32mINFO     \u001b[0m train.py: [12/300], [470/484], step: 6278, 2.484 samples/sec, batch_loss: 0.0412, batch_loss_c: 0.0355, batch_loss_s: 0.0546, time:16.1043, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:15:25 \u001b[32mINFO     \u001b[0m train.py: [12/300], [480/484], step: 6288, 3.267 samples/sec, batch_loss: 0.1202, batch_loss_c: 0.1170, batch_loss_s: 0.1275, time:12.2426, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:15:29 \u001b[32mINFO     \u001b[0m train.py: [12/300], train_loss: 0.1557, time: 1001.5104, lr: 0.0001\u001b[0m\n",
            "2019-12-06 07:15:31 \u001b[32mINFO     \u001b[0m train.py: [13/300], [0/484], step: 6292, 34.967 samples/sec, batch_loss: 0.0629, batch_loss_c: 0.0502, batch_loss_s: 0.0926, time:1.1439, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:16:01 \u001b[32mINFO     \u001b[0m train.py: [13/300], [10/484], step: 6302, 1.325 samples/sec, batch_loss: 0.5614, batch_loss_c: 0.5568, batch_loss_s: 0.5719, time:30.1834, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:16:27 \u001b[32mINFO     \u001b[0m train.py: [13/300], [20/484], step: 6312, 1.547 samples/sec, batch_loss: 0.0983, batch_loss_c: 0.0935, batch_loss_s: 0.1096, time:25.8598, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:16:48 \u001b[32mINFO     \u001b[0m train.py: [13/300], [30/484], step: 6322, 1.895 samples/sec, batch_loss: 0.1668, batch_loss_c: 0.1655, batch_loss_s: 0.1696, time:21.1030, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:17:04 \u001b[32mINFO     \u001b[0m train.py: [13/300], [40/484], step: 6332, 2.442 samples/sec, batch_loss: 0.1483, batch_loss_c: 0.1530, batch_loss_s: 0.1372, time:16.3808, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:17:20 \u001b[32mINFO     \u001b[0m train.py: [13/300], [50/484], step: 6342, 2.490 samples/sec, batch_loss: 0.0777, batch_loss_c: 0.0703, batch_loss_s: 0.0951, time:16.0612, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:17:40 \u001b[32mINFO     \u001b[0m train.py: [13/300], [60/484], step: 6352, 1.984 samples/sec, batch_loss: 0.0546, batch_loss_c: 0.0501, batch_loss_s: 0.0651, time:20.1598, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:18:03 \u001b[32mINFO     \u001b[0m train.py: [13/300], [70/484], step: 6362, 1.739 samples/sec, batch_loss: 0.6307, batch_loss_c: 0.5908, batch_loss_s: 0.7238, time:22.9966, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:18:19 \u001b[32mINFO     \u001b[0m train.py: [13/300], [80/484], step: 6372, 2.529 samples/sec, batch_loss: 0.2935, batch_loss_c: 0.2910, batch_loss_s: 0.2994, time:15.8195, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:18:37 \u001b[32mINFO     \u001b[0m train.py: [13/300], [90/484], step: 6382, 2.241 samples/sec, batch_loss: 0.0474, batch_loss_c: 0.0418, batch_loss_s: 0.0604, time:17.8531, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:18:57 \u001b[32mINFO     \u001b[0m train.py: [13/300], [100/484], step: 6392, 2.018 samples/sec, batch_loss: 0.0531, batch_loss_c: 0.0452, batch_loss_s: 0.0716, time:19.8182, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:19:18 \u001b[32mINFO     \u001b[0m train.py: [13/300], [110/484], step: 6402, 1.882 samples/sec, batch_loss: 0.1023, batch_loss_c: 0.1014, batch_loss_s: 0.1045, time:21.2540, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:19:35 \u001b[32mINFO     \u001b[0m train.py: [13/300], [120/484], step: 6412, 2.446 samples/sec, batch_loss: 0.0580, batch_loss_c: 0.0523, batch_loss_s: 0.0712, time:16.3549, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:19:56 \u001b[32mINFO     \u001b[0m train.py: [13/300], [130/484], step: 6422, 1.868 samples/sec, batch_loss: 0.3083, batch_loss_c: 0.3046, batch_loss_s: 0.3169, time:21.4169, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:20:10 \u001b[32mINFO     \u001b[0m train.py: [13/300], [140/484], step: 6432, 2.758 samples/sec, batch_loss: 0.1676, batch_loss_c: 0.1821, batch_loss_s: 0.1338, time:14.5010, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:20:25 \u001b[32mINFO     \u001b[0m train.py: [13/300], [150/484], step: 6442, 2.694 samples/sec, batch_loss: 0.3250, batch_loss_c: 0.3193, batch_loss_s: 0.3383, time:14.8462, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:20:45 \u001b[32mINFO     \u001b[0m train.py: [13/300], [160/484], step: 6452, 2.017 samples/sec, batch_loss: 0.0762, batch_loss_c: 0.0674, batch_loss_s: 0.0966, time:19.8274, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:21:08 \u001b[32mINFO     \u001b[0m train.py: [13/300], [170/484], step: 6462, 1.749 samples/sec, batch_loss: 0.1496, batch_loss_c: 0.1414, batch_loss_s: 0.1686, time:22.8637, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:21:27 \u001b[32mINFO     \u001b[0m train.py: [13/300], [180/484], step: 6472, 2.155 samples/sec, batch_loss: 0.3430, batch_loss_c: 0.3467, batch_loss_s: 0.3343, time:18.5578, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:21:46 \u001b[32mINFO     \u001b[0m train.py: [13/300], [190/484], step: 6482, 2.096 samples/sec, batch_loss: 0.0551, batch_loss_c: 0.0429, batch_loss_s: 0.0834, time:19.0847, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:22:14 \u001b[32mINFO     \u001b[0m train.py: [13/300], [200/484], step: 6492, 1.419 samples/sec, batch_loss: 0.3242, batch_loss_c: 0.3128, batch_loss_s: 0.3510, time:28.1965, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:22:40 \u001b[32mINFO     \u001b[0m train.py: [13/300], [210/484], step: 6502, 1.511 samples/sec, batch_loss: 0.0653, batch_loss_c: 0.0543, batch_loss_s: 0.0910, time:26.4715, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:23:07 \u001b[32mINFO     \u001b[0m train.py: [13/300], [220/484], step: 6512, 1.496 samples/sec, batch_loss: 0.0654, batch_loss_c: 0.0612, batch_loss_s: 0.0750, time:26.7322, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:23:29 \u001b[32mINFO     \u001b[0m train.py: [13/300], [230/484], step: 6522, 1.812 samples/sec, batch_loss: 0.0925, batch_loss_c: 0.0855, batch_loss_s: 0.1086, time:22.0714, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:23:45 \u001b[32mINFO     \u001b[0m train.py: [13/300], [240/484], step: 6532, 2.587 samples/sec, batch_loss: 0.0710, batch_loss_c: 0.0613, batch_loss_s: 0.0936, time:15.4631, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:24:01 \u001b[32mINFO     \u001b[0m train.py: [13/300], [250/484], step: 6542, 2.410 samples/sec, batch_loss: 0.0835, batch_loss_c: 0.0876, batch_loss_s: 0.0740, time:16.5995, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:24:14 \u001b[32mINFO     \u001b[0m train.py: [13/300], [260/484], step: 6552, 3.167 samples/sec, batch_loss: 0.3240, batch_loss_c: 0.3188, batch_loss_s: 0.3359, time:12.6286, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:24:34 \u001b[32mINFO     \u001b[0m train.py: [13/300], [270/484], step: 6562, 1.939 samples/sec, batch_loss: 0.0681, batch_loss_c: 0.0619, batch_loss_s: 0.0826, time:20.6320, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:24:59 \u001b[32mINFO     \u001b[0m train.py: [13/300], [280/484], step: 6572, 1.604 samples/sec, batch_loss: 0.0672, batch_loss_c: 0.0582, batch_loss_s: 0.0884, time:24.9389, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:25:19 \u001b[32mINFO     \u001b[0m train.py: [13/300], [290/484], step: 6582, 2.047 samples/sec, batch_loss: 0.1330, batch_loss_c: 0.1228, batch_loss_s: 0.1569, time:19.5453, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:25:47 \u001b[32mINFO     \u001b[0m train.py: [13/300], [300/484], step: 6592, 1.429 samples/sec, batch_loss: 0.2651, batch_loss_c: 0.2709, batch_loss_s: 0.2514, time:27.9926, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:26:17 \u001b[32mINFO     \u001b[0m train.py: [13/300], [310/484], step: 6602, 1.312 samples/sec, batch_loss: 0.0512, batch_loss_c: 0.0425, batch_loss_s: 0.0717, time:30.4930, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:26:36 \u001b[32mINFO     \u001b[0m train.py: [13/300], [320/484], step: 6612, 2.177 samples/sec, batch_loss: 0.1777, batch_loss_c: 0.2153, batch_loss_s: 0.0901, time:18.3766, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:26:59 \u001b[32mINFO     \u001b[0m train.py: [13/300], [330/484], step: 6622, 1.697 samples/sec, batch_loss: 0.4431, batch_loss_c: 0.4249, batch_loss_s: 0.4854, time:23.5766, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:27:25 \u001b[32mINFO     \u001b[0m train.py: [13/300], [340/484], step: 6632, 1.579 samples/sec, batch_loss: 0.0707, batch_loss_c: 0.0688, batch_loss_s: 0.0751, time:25.3319, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:27:52 \u001b[32mINFO     \u001b[0m train.py: [13/300], [350/484], step: 6642, 1.464 samples/sec, batch_loss: 0.0755, batch_loss_c: 0.0664, batch_loss_s: 0.0965, time:27.3242, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:28:27 \u001b[32mINFO     \u001b[0m train.py: [13/300], [360/484], step: 6652, 1.159 samples/sec, batch_loss: 0.4987, batch_loss_c: 0.4733, batch_loss_s: 0.5581, time:34.5028, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:28:41 \u001b[32mINFO     \u001b[0m train.py: [13/300], [370/484], step: 6662, 2.793 samples/sec, batch_loss: 0.0847, batch_loss_c: 0.0402, batch_loss_s: 0.1885, time:14.3233, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:28:58 \u001b[32mINFO     \u001b[0m train.py: [13/300], [380/484], step: 6672, 2.279 samples/sec, batch_loss: 0.0710, batch_loss_c: 0.0694, batch_loss_s: 0.0748, time:17.5544, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:29:25 \u001b[32mINFO     \u001b[0m train.py: [13/300], [390/484], step: 6682, 1.509 samples/sec, batch_loss: 0.2986, batch_loss_c: 0.2812, batch_loss_s: 0.3392, time:26.5115, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:29:43 \u001b[32mINFO     \u001b[0m train.py: [13/300], [400/484], step: 6692, 2.244 samples/sec, batch_loss: 0.0531, batch_loss_c: 0.0447, batch_loss_s: 0.0729, time:17.8225, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:29:56 \u001b[32mINFO     \u001b[0m train.py: [13/300], [410/484], step: 6702, 3.116 samples/sec, batch_loss: 0.0450, batch_loss_c: 0.0392, batch_loss_s: 0.0587, time:12.8366, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:30:18 \u001b[32mINFO     \u001b[0m train.py: [13/300], [420/484], step: 6712, 1.824 samples/sec, batch_loss: 0.0676, batch_loss_c: 0.0588, batch_loss_s: 0.0881, time:21.9286, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:30:45 \u001b[32mINFO     \u001b[0m train.py: [13/300], [430/484], step: 6722, 1.444 samples/sec, batch_loss: 0.0735, batch_loss_c: 0.0681, batch_loss_s: 0.0861, time:27.6981, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:31:05 \u001b[32mINFO     \u001b[0m train.py: [13/300], [440/484], step: 6732, 2.055 samples/sec, batch_loss: 0.3256, batch_loss_c: 0.3191, batch_loss_s: 0.3410, time:19.4649, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:31:24 \u001b[32mINFO     \u001b[0m train.py: [13/300], [450/484], step: 6742, 2.117 samples/sec, batch_loss: 0.1242, batch_loss_c: 0.1179, batch_loss_s: 0.1389, time:18.8917, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:31:38 \u001b[32mINFO     \u001b[0m train.py: [13/300], [460/484], step: 6752, 2.775 samples/sec, batch_loss: 0.1000, batch_loss_c: 0.0882, batch_loss_s: 0.1273, time:14.4146, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:31:57 \u001b[32mINFO     \u001b[0m train.py: [13/300], [470/484], step: 6762, 2.100 samples/sec, batch_loss: 0.0737, batch_loss_c: 0.0659, batch_loss_s: 0.0919, time:19.0504, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:32:17 \u001b[32mINFO     \u001b[0m train.py: [13/300], [480/484], step: 6772, 1.964 samples/sec, batch_loss: 0.0559, batch_loss_c: 0.0430, batch_loss_s: 0.0860, time:20.3666, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:32:21 \u001b[32mINFO     \u001b[0m train.py: [13/300], train_loss: 0.1588, time: 1011.6516, lr: 0.0001\u001b[0m\n",
            "2019-12-06 07:32:25 \u001b[32mINFO     \u001b[0m train.py: [14/300], [0/484], step: 6776, 11.208 samples/sec, batch_loss: 0.3376, batch_loss_c: 0.3329, batch_loss_s: 0.3487, time:3.5687, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:32:47 \u001b[32mINFO     \u001b[0m train.py: [14/300], [10/484], step: 6786, 1.818 samples/sec, batch_loss: 0.3013, batch_loss_c: 0.2914, batch_loss_s: 0.3245, time:21.9974, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:33:12 \u001b[32mINFO     \u001b[0m train.py: [14/300], [20/484], step: 6796, 1.598 samples/sec, batch_loss: 0.4633, batch_loss_c: 0.4612, batch_loss_s: 0.4683, time:25.0305, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:33:25 \u001b[32mINFO     \u001b[0m train.py: [14/300], [30/484], step: 6806, 3.059 samples/sec, batch_loss: 0.0558, batch_loss_c: 0.0471, batch_loss_s: 0.0761, time:13.0742, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:33:49 \u001b[32mINFO     \u001b[0m train.py: [14/300], [40/484], step: 6816, 1.695 samples/sec, batch_loss: 0.0891, batch_loss_c: 0.0947, batch_loss_s: 0.0760, time:23.6051, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:34:19 \u001b[32mINFO     \u001b[0m train.py: [14/300], [50/484], step: 6826, 1.344 samples/sec, batch_loss: 0.0547, batch_loss_c: 0.0487, batch_loss_s: 0.0688, time:29.7651, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:34:55 \u001b[32mINFO     \u001b[0m train.py: [14/300], [60/484], step: 6836, 1.096 samples/sec, batch_loss: 0.0492, batch_loss_c: 0.0437, batch_loss_s: 0.0621, time:36.4922, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:35:23 \u001b[32mINFO     \u001b[0m train.py: [14/300], [70/484], step: 6846, 1.424 samples/sec, batch_loss: 0.0949, batch_loss_c: 0.0718, batch_loss_s: 0.1488, time:28.0811, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:35:44 \u001b[32mINFO     \u001b[0m train.py: [14/300], [80/484], step: 6856, 1.902 samples/sec, batch_loss: 0.2935, batch_loss_c: 0.2857, batch_loss_s: 0.3118, time:21.0250, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:36:15 \u001b[32mINFO     \u001b[0m train.py: [14/300], [90/484], step: 6866, 1.296 samples/sec, batch_loss: 0.0576, batch_loss_c: 0.0522, batch_loss_s: 0.0702, time:30.8618, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:36:40 \u001b[32mINFO     \u001b[0m train.py: [14/300], [100/484], step: 6876, 1.628 samples/sec, batch_loss: 0.0928, batch_loss_c: 0.0933, batch_loss_s: 0.0916, time:24.5684, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:36:55 \u001b[32mINFO     \u001b[0m train.py: [14/300], [110/484], step: 6886, 2.541 samples/sec, batch_loss: 0.1493, batch_loss_c: 0.0967, batch_loss_s: 0.2720, time:15.7433, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:37:22 \u001b[32mINFO     \u001b[0m train.py: [14/300], [120/484], step: 6896, 1.489 samples/sec, batch_loss: 0.0620, batch_loss_c: 0.0538, batch_loss_s: 0.0809, time:26.8569, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:37:40 \u001b[32mINFO     \u001b[0m train.py: [14/300], [130/484], step: 6906, 2.260 samples/sec, batch_loss: 0.0825, batch_loss_c: 0.0732, batch_loss_s: 0.1041, time:17.6998, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:37:54 \u001b[32mINFO     \u001b[0m train.py: [14/300], [140/484], step: 6916, 2.972 samples/sec, batch_loss: 0.3000, batch_loss_c: 0.2952, batch_loss_s: 0.3112, time:13.4602, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:38:07 \u001b[32mINFO     \u001b[0m train.py: [14/300], [150/484], step: 6926, 3.009 samples/sec, batch_loss: 0.0849, batch_loss_c: 0.0781, batch_loss_s: 0.1005, time:13.2954, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:38:21 \u001b[32mINFO     \u001b[0m train.py: [14/300], [160/484], step: 6936, 2.886 samples/sec, batch_loss: 0.1148, batch_loss_c: 0.1007, batch_loss_s: 0.1476, time:13.8577, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:38:40 \u001b[32mINFO     \u001b[0m train.py: [14/300], [170/484], step: 6946, 2.106 samples/sec, batch_loss: 0.0744, batch_loss_c: 0.0675, batch_loss_s: 0.0907, time:18.9948, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:39:23 \u001b[32mINFO     \u001b[0m train.py: [14/300], [180/484], step: 6956, 0.918 samples/sec, batch_loss: 0.0856, batch_loss_c: 0.0815, batch_loss_s: 0.0952, time:43.5822, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:39:46 \u001b[32mINFO     \u001b[0m train.py: [14/300], [190/484], step: 6966, 1.765 samples/sec, batch_loss: 0.3138, batch_loss_c: 0.2990, batch_loss_s: 0.3485, time:22.6673, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:40:01 \u001b[32mINFO     \u001b[0m train.py: [14/300], [200/484], step: 6976, 2.673 samples/sec, batch_loss: 0.1779, batch_loss_c: 0.2157, batch_loss_s: 0.0897, time:14.9668, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:40:20 \u001b[32mINFO     \u001b[0m train.py: [14/300], [210/484], step: 6986, 2.061 samples/sec, batch_loss: 0.1798, batch_loss_c: 0.1581, batch_loss_s: 0.2305, time:19.4048, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:40:35 \u001b[32mINFO     \u001b[0m train.py: [14/300], [220/484], step: 6996, 2.674 samples/sec, batch_loss: 0.1178, batch_loss_c: 0.1280, batch_loss_s: 0.0940, time:14.9578, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:40:50 \u001b[32mINFO     \u001b[0m train.py: [14/300], [230/484], step: 7006, 2.781 samples/sec, batch_loss: 0.1073, batch_loss_c: 0.0994, batch_loss_s: 0.1259, time:14.3826, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:41:14 \u001b[32mINFO     \u001b[0m train.py: [14/300], [240/484], step: 7016, 1.612 samples/sec, batch_loss: 0.0459, batch_loss_c: 0.0412, batch_loss_s: 0.0569, time:24.8189, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:41:29 \u001b[32mINFO     \u001b[0m train.py: [14/300], [250/484], step: 7026, 2.787 samples/sec, batch_loss: 0.0930, batch_loss_c: 0.0969, batch_loss_s: 0.0837, time:14.3506, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:41:49 \u001b[32mINFO     \u001b[0m train.py: [14/300], [260/484], step: 7036, 2.005 samples/sec, batch_loss: 0.0496, batch_loss_c: 0.0438, batch_loss_s: 0.0633, time:19.9472, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:42:16 \u001b[32mINFO     \u001b[0m train.py: [14/300], [270/484], step: 7046, 1.449 samples/sec, batch_loss: 0.0583, batch_loss_c: 0.0471, batch_loss_s: 0.0844, time:27.6068, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:42:32 \u001b[32mINFO     \u001b[0m train.py: [14/300], [280/484], step: 7056, 2.532 samples/sec, batch_loss: 0.0841, batch_loss_c: 0.0732, batch_loss_s: 0.1096, time:15.7997, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:42:47 \u001b[32mINFO     \u001b[0m train.py: [14/300], [290/484], step: 7066, 2.695 samples/sec, batch_loss: 0.0958, batch_loss_c: 0.0898, batch_loss_s: 0.1097, time:14.8397, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:43:06 \u001b[32mINFO     \u001b[0m train.py: [14/300], [300/484], step: 7076, 2.141 samples/sec, batch_loss: 0.0855, batch_loss_c: 0.0875, batch_loss_s: 0.0808, time:18.6805, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:43:28 \u001b[32mINFO     \u001b[0m train.py: [14/300], [310/484], step: 7086, 1.768 samples/sec, batch_loss: 0.0808, batch_loss_c: 0.0745, batch_loss_s: 0.0953, time:22.6183, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:43:50 \u001b[32mINFO     \u001b[0m train.py: [14/300], [320/484], step: 7096, 1.881 samples/sec, batch_loss: 0.1488, batch_loss_c: 0.1090, batch_loss_s: 0.2416, time:21.2705, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:44:08 \u001b[32mINFO     \u001b[0m train.py: [14/300], [330/484], step: 7106, 2.229 samples/sec, batch_loss: 0.2820, batch_loss_c: 0.2770, batch_loss_s: 0.2937, time:17.9460, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:44:30 \u001b[32mINFO     \u001b[0m train.py: [14/300], [340/484], step: 7116, 1.773 samples/sec, batch_loss: 0.0955, batch_loss_c: 0.0920, batch_loss_s: 0.1039, time:22.5640, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:44:48 \u001b[32mINFO     \u001b[0m train.py: [14/300], [350/484], step: 7126, 2.246 samples/sec, batch_loss: 0.0735, batch_loss_c: 0.0639, batch_loss_s: 0.0959, time:17.8055, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:45:05 \u001b[32mINFO     \u001b[0m train.py: [14/300], [360/484], step: 7136, 2.308 samples/sec, batch_loss: 0.1229, batch_loss_c: 0.1172, batch_loss_s: 0.1364, time:17.3321, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:45:21 \u001b[32mINFO     \u001b[0m train.py: [14/300], [370/484], step: 7146, 2.537 samples/sec, batch_loss: 0.2962, batch_loss_c: 0.2927, batch_loss_s: 0.3044, time:15.7692, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:45:41 \u001b[32mINFO     \u001b[0m train.py: [14/300], [380/484], step: 7156, 2.026 samples/sec, batch_loss: 0.1778, batch_loss_c: 0.1747, batch_loss_s: 0.1852, time:19.7441, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:45:55 \u001b[32mINFO     \u001b[0m train.py: [14/300], [390/484], step: 7166, 2.828 samples/sec, batch_loss: 0.1674, batch_loss_c: 0.1831, batch_loss_s: 0.1307, time:14.1447, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:46:15 \u001b[32mINFO     \u001b[0m train.py: [14/300], [400/484], step: 7176, 1.993 samples/sec, batch_loss: 0.0829, batch_loss_c: 0.0794, batch_loss_s: 0.0911, time:20.0672, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:46:28 \u001b[32mINFO     \u001b[0m train.py: [14/300], [410/484], step: 7186, 3.005 samples/sec, batch_loss: 0.0611, batch_loss_c: 0.0522, batch_loss_s: 0.0820, time:13.3120, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:46:50 \u001b[32mINFO     \u001b[0m train.py: [14/300], [420/484], step: 7196, 1.838 samples/sec, batch_loss: 0.3491, batch_loss_c: 0.3283, batch_loss_s: 0.3979, time:21.7617, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:47:08 \u001b[32mINFO     \u001b[0m train.py: [14/300], [430/484], step: 7206, 2.245 samples/sec, batch_loss: 0.1757, batch_loss_c: 0.2129, batch_loss_s: 0.0889, time:17.8149, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:47:31 \u001b[32mINFO     \u001b[0m train.py: [14/300], [440/484], step: 7216, 1.746 samples/sec, batch_loss: 0.1262, batch_loss_c: 0.1160, batch_loss_s: 0.1500, time:22.9119, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:47:54 \u001b[32mINFO     \u001b[0m train.py: [14/300], [450/484], step: 7226, 1.691 samples/sec, batch_loss: 0.0651, batch_loss_c: 0.0554, batch_loss_s: 0.0876, time:23.6507, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:48:09 \u001b[32mINFO     \u001b[0m train.py: [14/300], [460/484], step: 7236, 2.754 samples/sec, batch_loss: 0.2131, batch_loss_c: 0.1978, batch_loss_s: 0.2488, time:14.5259, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:48:27 \u001b[32mINFO     \u001b[0m train.py: [14/300], [470/484], step: 7246, 2.271 samples/sec, batch_loss: 0.0794, batch_loss_c: 0.0645, batch_loss_s: 0.1142, time:17.6097, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:48:55 \u001b[32mINFO     \u001b[0m train.py: [14/300], [480/484], step: 7256, 1.396 samples/sec, batch_loss: 0.0716, batch_loss_c: 0.0634, batch_loss_s: 0.0906, time:28.6584, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:49:10 \u001b[32mINFO     \u001b[0m train.py: [14/300], train_loss: 0.1514, time: 1008.6471, lr: 0.0001\u001b[0m\n",
            "2019-12-06 07:49:15 \u001b[32mINFO     \u001b[0m train.py: [15/300], [0/484], step: 7260, 8.580 samples/sec, batch_loss: 0.0808, batch_loss_c: 0.0681, batch_loss_s: 0.1103, time:4.6620, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:49:30 \u001b[32mINFO     \u001b[0m train.py: [15/300], [10/484], step: 7270, 2.734 samples/sec, batch_loss: 0.0468, batch_loss_c: 0.0399, batch_loss_s: 0.0630, time:14.6318, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:49:43 \u001b[32mINFO     \u001b[0m train.py: [15/300], [20/484], step: 7280, 3.107 samples/sec, batch_loss: 0.0559, batch_loss_c: 0.0491, batch_loss_s: 0.0717, time:12.8728, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:50:11 \u001b[32mINFO     \u001b[0m train.py: [15/300], [30/484], step: 7290, 1.434 samples/sec, batch_loss: 0.0670, batch_loss_c: 0.0619, batch_loss_s: 0.0790, time:27.8995, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:50:29 \u001b[32mINFO     \u001b[0m train.py: [15/300], [40/484], step: 7300, 2.181 samples/sec, batch_loss: 0.0659, batch_loss_c: 0.0619, batch_loss_s: 0.0754, time:18.3367, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:50:58 \u001b[32mINFO     \u001b[0m train.py: [15/300], [50/484], step: 7310, 1.394 samples/sec, batch_loss: 0.1346, batch_loss_c: 0.1017, batch_loss_s: 0.2116, time:28.6994, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:51:18 \u001b[32mINFO     \u001b[0m train.py: [15/300], [60/484], step: 7320, 1.996 samples/sec, batch_loss: 0.4404, batch_loss_c: 0.4344, batch_loss_s: 0.4545, time:20.0397, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:51:32 \u001b[32mINFO     \u001b[0m train.py: [15/300], [70/484], step: 7330, 2.731 samples/sec, batch_loss: 0.0600, batch_loss_c: 0.0497, batch_loss_s: 0.0842, time:14.6459, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:51:53 \u001b[32mINFO     \u001b[0m train.py: [15/300], [80/484], step: 7340, 1.929 samples/sec, batch_loss: 0.2228, batch_loss_c: 0.2680, batch_loss_s: 0.1173, time:20.7401, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:52:14 \u001b[32mINFO     \u001b[0m train.py: [15/300], [90/484], step: 7350, 1.888 samples/sec, batch_loss: 0.0812, batch_loss_c: 0.0689, batch_loss_s: 0.1101, time:21.1853, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:52:33 \u001b[32mINFO     \u001b[0m train.py: [15/300], [100/484], step: 7360, 2.119 samples/sec, batch_loss: 0.1695, batch_loss_c: 0.1679, batch_loss_s: 0.1731, time:18.8807, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:52:46 \u001b[32mINFO     \u001b[0m train.py: [15/300], [110/484], step: 7370, 3.223 samples/sec, batch_loss: 0.0837, batch_loss_c: 0.0698, batch_loss_s: 0.1163, time:12.4112, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:53:01 \u001b[32mINFO     \u001b[0m train.py: [15/300], [120/484], step: 7380, 2.649 samples/sec, batch_loss: 0.2154, batch_loss_c: 0.1881, batch_loss_s: 0.2792, time:15.0983, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:53:36 \u001b[32mINFO     \u001b[0m train.py: [15/300], [130/484], step: 7390, 1.152 samples/sec, batch_loss: 0.1023, batch_loss_c: 0.0971, batch_loss_s: 0.1146, time:34.7131, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:53:53 \u001b[32mINFO     \u001b[0m train.py: [15/300], [140/484], step: 7400, 2.233 samples/sec, batch_loss: 0.0774, batch_loss_c: 0.0736, batch_loss_s: 0.0863, time:17.9170, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:54:09 \u001b[32mINFO     \u001b[0m train.py: [15/300], [150/484], step: 7410, 2.599 samples/sec, batch_loss: 0.3321, batch_loss_c: 0.3175, batch_loss_s: 0.3663, time:15.3908, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:54:24 \u001b[32mINFO     \u001b[0m train.py: [15/300], [160/484], step: 7420, 2.636 samples/sec, batch_loss: 0.0678, batch_loss_c: 0.0539, batch_loss_s: 0.1004, time:15.1742, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:54:41 \u001b[32mINFO     \u001b[0m train.py: [15/300], [170/484], step: 7430, 2.421 samples/sec, batch_loss: 0.0868, batch_loss_c: 0.0804, batch_loss_s: 0.1018, time:16.5202, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:55:09 \u001b[32mINFO     \u001b[0m train.py: [15/300], [180/484], step: 7440, 1.394 samples/sec, batch_loss: 0.1492, batch_loss_c: 0.1349, batch_loss_s: 0.1826, time:28.7011, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:55:22 \u001b[32mINFO     \u001b[0m train.py: [15/300], [190/484], step: 7450, 3.027 samples/sec, batch_loss: 0.1459, batch_loss_c: 0.1325, batch_loss_s: 0.1772, time:13.2139, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:55:35 \u001b[32mINFO     \u001b[0m train.py: [15/300], [200/484], step: 7460, 3.158 samples/sec, batch_loss: 0.0652, batch_loss_c: 0.0552, batch_loss_s: 0.0883, time:12.6673, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:55:52 \u001b[32mINFO     \u001b[0m train.py: [15/300], [210/484], step: 7470, 2.340 samples/sec, batch_loss: 0.3041, batch_loss_c: 0.2977, batch_loss_s: 0.3193, time:17.0955, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:56:11 \u001b[32mINFO     \u001b[0m train.py: [15/300], [220/484], step: 7480, 2.178 samples/sec, batch_loss: 0.0802, batch_loss_c: 0.0764, batch_loss_s: 0.0891, time:18.3637, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:56:28 \u001b[32mINFO     \u001b[0m train.py: [15/300], [230/484], step: 7490, 2.320 samples/sec, batch_loss: 0.0644, batch_loss_c: 0.0622, batch_loss_s: 0.0697, time:17.2392, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:56:41 \u001b[32mINFO     \u001b[0m train.py: [15/300], [240/484], step: 7500, 3.087 samples/sec, batch_loss: 0.0592, batch_loss_c: 0.0554, batch_loss_s: 0.0682, time:12.9579, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:57:05 \u001b[32mINFO     \u001b[0m train.py: [15/300], [250/484], step: 7510, 1.647 samples/sec, batch_loss: 0.0782, batch_loss_c: 0.0702, batch_loss_s: 0.0970, time:24.2804, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:57:35 \u001b[32mINFO     \u001b[0m train.py: [15/300], [260/484], step: 7520, 1.345 samples/sec, batch_loss: 0.0716, batch_loss_c: 0.0564, batch_loss_s: 0.1069, time:29.7384, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:57:50 \u001b[32mINFO     \u001b[0m train.py: [15/300], [270/484], step: 7530, 2.570 samples/sec, batch_loss: 0.0875, batch_loss_c: 0.0809, batch_loss_s: 0.1029, time:15.5646, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:58:08 \u001b[32mINFO     \u001b[0m train.py: [15/300], [280/484], step: 7540, 2.230 samples/sec, batch_loss: 0.2885, batch_loss_c: 0.2857, batch_loss_s: 0.2950, time:17.9366, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:58:22 \u001b[32mINFO     \u001b[0m train.py: [15/300], [290/484], step: 7550, 3.025 samples/sec, batch_loss: 0.3011, batch_loss_c: 0.2972, batch_loss_s: 0.3103, time:13.2231, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:58:36 \u001b[32mINFO     \u001b[0m train.py: [15/300], [300/484], step: 7560, 2.691 samples/sec, batch_loss: 0.0397, batch_loss_c: 0.0327, batch_loss_s: 0.0563, time:14.8659, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:58:53 \u001b[32mINFO     \u001b[0m train.py: [15/300], [310/484], step: 7570, 2.414 samples/sec, batch_loss: 0.0846, batch_loss_c: 0.0739, batch_loss_s: 0.1095, time:16.5696, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:59:05 \u001b[32mINFO     \u001b[0m train.py: [15/300], [320/484], step: 7580, 3.197 samples/sec, batch_loss: 0.0938, batch_loss_c: 0.0875, batch_loss_s: 0.1084, time:12.5105, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:59:26 \u001b[32mINFO     \u001b[0m train.py: [15/300], [330/484], step: 7590, 1.931 samples/sec, batch_loss: 0.3163, batch_loss_c: 0.3126, batch_loss_s: 0.3248, time:20.7100, lr:0.0001\u001b[0m\n",
            "2019-12-06 07:59:49 \u001b[32mINFO     \u001b[0m train.py: [15/300], [340/484], step: 7600, 1.762 samples/sec, batch_loss: 0.0790, batch_loss_c: 0.0759, batch_loss_s: 0.0862, time:22.6995, lr:0.0001\u001b[0m\n",
            "tcmalloc: large alloc 5657370624 bytes == 0x7fea5a170000 @  0x7feeb13d01e7 0x7feea59b4f71 0x7feea5a1855d 0x7feea5a1be28 0x7feea5a1c3e5 0x7feea5ab2fc2 0x50ac25 0x50d390 0x508245 0x50a080 0x50aa7d 0x50d390 0x509455 0x595311 0x5a067e 0x557d28 0x50c88b 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x509455 0x595311 0x5a522c 0x557f7e 0x4b5eff 0x50c810\n",
            "2019-12-06 08:00:25 \u001b[32mINFO     \u001b[0m train.py: [15/300], [350/484], step: 7610, 1.105 samples/sec, batch_loss: 0.1017, batch_loss_c: 0.1054, batch_loss_s: 0.0931, time:36.1948, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:00:43 \u001b[32mINFO     \u001b[0m train.py: [15/300], [360/484], step: 7620, 2.290 samples/sec, batch_loss: 0.1268, batch_loss_c: 0.1142, batch_loss_s: 0.1561, time:17.4637, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:01:11 \u001b[32mINFO     \u001b[0m train.py: [15/300], [370/484], step: 7630, 1.407 samples/sec, batch_loss: 0.2905, batch_loss_c: 0.2884, batch_loss_s: 0.2953, time:28.4384, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:01:23 \u001b[32mINFO     \u001b[0m train.py: [15/300], [380/484], step: 7640, 3.296 samples/sec, batch_loss: 0.0497, batch_loss_c: 0.0434, batch_loss_s: 0.0643, time:12.1376, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:01:40 \u001b[32mINFO     \u001b[0m train.py: [15/300], [390/484], step: 7650, 2.309 samples/sec, batch_loss: 0.5086, batch_loss_c: 0.4924, batch_loss_s: 0.5463, time:17.3228, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:01:52 \u001b[32mINFO     \u001b[0m train.py: [15/300], [400/484], step: 7660, 3.427 samples/sec, batch_loss: 0.0920, batch_loss_c: 0.0896, batch_loss_s: 0.0978, time:11.6714, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:02:05 \u001b[32mINFO     \u001b[0m train.py: [15/300], [410/484], step: 7670, 3.069 samples/sec, batch_loss: 0.1014, batch_loss_c: 0.0997, batch_loss_s: 0.1054, time:13.0351, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:02:23 \u001b[32mINFO     \u001b[0m train.py: [15/300], [420/484], step: 7680, 2.286 samples/sec, batch_loss: 0.3514, batch_loss_c: 0.3404, batch_loss_s: 0.3771, time:17.4981, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:02:41 \u001b[32mINFO     \u001b[0m train.py: [15/300], [430/484], step: 7690, 2.147 samples/sec, batch_loss: 0.5683, batch_loss_c: 0.5729, batch_loss_s: 0.5574, time:18.6333, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:02:59 \u001b[32mINFO     \u001b[0m train.py: [15/300], [440/484], step: 7700, 2.323 samples/sec, batch_loss: 0.0826, batch_loss_c: 0.0718, batch_loss_s: 0.1079, time:17.2195, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:03:23 \u001b[32mINFO     \u001b[0m train.py: [15/300], [450/484], step: 7710, 1.638 samples/sec, batch_loss: 0.0803, batch_loss_c: 0.0666, batch_loss_s: 0.1123, time:24.4154, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:03:40 \u001b[32mINFO     \u001b[0m train.py: [15/300], [460/484], step: 7720, 2.390 samples/sec, batch_loss: 0.0566, batch_loss_c: 0.0514, batch_loss_s: 0.0685, time:16.7367, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:04:08 \u001b[32mINFO     \u001b[0m train.py: [15/300], [470/484], step: 7730, 1.434 samples/sec, batch_loss: 0.2575, batch_loss_c: 0.2373, batch_loss_s: 0.3046, time:27.9000, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:04:33 \u001b[32mINFO     \u001b[0m train.py: [15/300], [480/484], step: 7740, 1.587 samples/sec, batch_loss: 0.1374, batch_loss_c: 0.1479, batch_loss_s: 0.1128, time:25.2081, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:04:43 \u001b[32mINFO     \u001b[0m train.py: [15/300], train_loss: 0.1539, time: 932.4782, lr: 0.0001\u001b[0m\n",
            "2019-12-06 08:04:45 \u001b[32mINFO     \u001b[0m train.py: [16/300], [0/484], step: 7744, 32.906 samples/sec, batch_loss: 0.1417, batch_loss_c: 0.1290, batch_loss_s: 0.1716, time:1.2156, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:04:58 \u001b[32mINFO     \u001b[0m train.py: [16/300], [10/484], step: 7754, 2.967 samples/sec, batch_loss: 0.0708, batch_loss_c: 0.0608, batch_loss_s: 0.0943, time:13.4799, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:05:16 \u001b[32mINFO     \u001b[0m train.py: [16/300], [20/484], step: 7764, 2.299 samples/sec, batch_loss: 0.1029, batch_loss_c: 0.0909, batch_loss_s: 0.1309, time:17.3981, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:05:31 \u001b[32mINFO     \u001b[0m train.py: [16/300], [30/484], step: 7774, 2.631 samples/sec, batch_loss: 0.1114, batch_loss_c: 0.1126, batch_loss_s: 0.1086, time:15.2061, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:06:04 \u001b[32mINFO     \u001b[0m train.py: [16/300], [40/484], step: 7784, 1.199 samples/sec, batch_loss: 0.1079, batch_loss_c: 0.1004, batch_loss_s: 0.1255, time:33.3525, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:06:31 \u001b[32mINFO     \u001b[0m train.py: [16/300], [50/484], step: 7794, 1.506 samples/sec, batch_loss: 0.0708, batch_loss_c: 0.0639, batch_loss_s: 0.0870, time:26.5625, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:06:44 \u001b[32mINFO     \u001b[0m train.py: [16/300], [60/484], step: 7804, 3.164 samples/sec, batch_loss: 0.0983, batch_loss_c: 0.0871, batch_loss_s: 0.1245, time:12.6424, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:06:58 \u001b[32mINFO     \u001b[0m train.py: [16/300], [70/484], step: 7814, 2.855 samples/sec, batch_loss: 0.1930, batch_loss_c: 0.2240, batch_loss_s: 0.1207, time:14.0082, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:07:13 \u001b[32mINFO     \u001b[0m train.py: [16/300], [80/484], step: 7824, 2.681 samples/sec, batch_loss: 0.3084, batch_loss_c: 0.3056, batch_loss_s: 0.3149, time:14.9208, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:07:45 \u001b[32mINFO     \u001b[0m train.py: [16/300], [90/484], step: 7834, 1.245 samples/sec, batch_loss: 0.0770, batch_loss_c: 0.0692, batch_loss_s: 0.0952, time:32.1202, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:07:59 \u001b[32mINFO     \u001b[0m train.py: [16/300], [100/484], step: 7844, 2.840 samples/sec, batch_loss: 0.0570, batch_loss_c: 0.0488, batch_loss_s: 0.0762, time:14.0831, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:08:20 \u001b[32mINFO     \u001b[0m train.py: [16/300], [110/484], step: 7854, 1.856 samples/sec, batch_loss: 0.0706, batch_loss_c: 0.0676, batch_loss_s: 0.0776, time:21.5571, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:08:34 \u001b[32mINFO     \u001b[0m train.py: [16/300], [120/484], step: 7864, 2.957 samples/sec, batch_loss: 0.2966, batch_loss_c: 0.2947, batch_loss_s: 0.3012, time:13.5286, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:08:49 \u001b[32mINFO     \u001b[0m train.py: [16/300], [130/484], step: 7874, 2.585 samples/sec, batch_loss: 0.0641, batch_loss_c: 0.0568, batch_loss_s: 0.0810, time:15.4725, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:09:13 \u001b[32mINFO     \u001b[0m train.py: [16/300], [140/484], step: 7884, 1.715 samples/sec, batch_loss: 0.0551, batch_loss_c: 0.0414, batch_loss_s: 0.0872, time:23.3285, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:09:44 \u001b[32mINFO     \u001b[0m train.py: [16/300], [150/484], step: 7894, 1.292 samples/sec, batch_loss: 0.5286, batch_loss_c: 0.5249, batch_loss_s: 0.5372, time:30.9547, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:10:23 \u001b[32mINFO     \u001b[0m train.py: [16/300], [160/484], step: 7904, 1.004 samples/sec, batch_loss: 0.0543, batch_loss_c: 0.0472, batch_loss_s: 0.0706, time:39.8422, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:10:41 \u001b[32mINFO     \u001b[0m train.py: [16/300], [170/484], step: 7914, 2.318 samples/sec, batch_loss: 0.1455, batch_loss_c: 0.1510, batch_loss_s: 0.1327, time:17.2559, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:10:54 \u001b[32mINFO     \u001b[0m train.py: [16/300], [180/484], step: 7924, 3.082 samples/sec, batch_loss: 0.0490, batch_loss_c: 0.0441, batch_loss_s: 0.0605, time:12.9775, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:11:10 \u001b[32mINFO     \u001b[0m train.py: [16/300], [190/484], step: 7934, 2.419 samples/sec, batch_loss: 0.0688, batch_loss_c: 0.0638, batch_loss_s: 0.0807, time:16.5388, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:11:24 \u001b[32mINFO     \u001b[0m train.py: [16/300], [200/484], step: 7944, 2.957 samples/sec, batch_loss: 0.0465, batch_loss_c: 0.0408, batch_loss_s: 0.0600, time:13.5262, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:11:39 \u001b[32mINFO     \u001b[0m train.py: [16/300], [210/484], step: 7954, 2.668 samples/sec, batch_loss: 0.0874, batch_loss_c: 0.0872, batch_loss_s: 0.0877, time:14.9931, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:11:56 \u001b[32mINFO     \u001b[0m train.py: [16/300], [220/484], step: 7964, 2.342 samples/sec, batch_loss: 0.2881, batch_loss_c: 0.2851, batch_loss_s: 0.2952, time:17.0801, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:12:46 \u001b[32mINFO     \u001b[0m train.py: [16/300], [230/484], step: 7974, 0.793 samples/sec, batch_loss: 0.0725, batch_loss_c: 0.0691, batch_loss_s: 0.0805, time:50.4428, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:13:12 \u001b[32mINFO     \u001b[0m train.py: [16/300], [240/484], step: 7984, 1.532 samples/sec, batch_loss: 0.0612, batch_loss_c: 0.0497, batch_loss_s: 0.0879, time:26.1164, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:13:45 \u001b[32mINFO     \u001b[0m train.py: [16/300], [250/484], step: 7994, 1.217 samples/sec, batch_loss: 0.0918, batch_loss_c: 0.0842, batch_loss_s: 0.1094, time:32.8559, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:14:03 \u001b[32mINFO     \u001b[0m train.py: [16/300], [260/484], step: 8004, 2.211 samples/sec, batch_loss: 0.0524, batch_loss_c: 0.0435, batch_loss_s: 0.0732, time:18.0889, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:14:24 \u001b[32mINFO     \u001b[0m train.py: [16/300], [270/484], step: 8014, 1.898 samples/sec, batch_loss: 0.2918, batch_loss_c: 0.2862, batch_loss_s: 0.3049, time:21.0707, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:14:42 \u001b[32mINFO     \u001b[0m train.py: [16/300], [280/484], step: 8024, 2.298 samples/sec, batch_loss: 0.1129, batch_loss_c: 0.1053, batch_loss_s: 0.1307, time:17.4071, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:15:05 \u001b[32mINFO     \u001b[0m train.py: [16/300], [290/484], step: 8034, 1.759 samples/sec, batch_loss: 0.2893, batch_loss_c: 0.2882, batch_loss_s: 0.2918, time:22.7338, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:15:21 \u001b[32mINFO     \u001b[0m train.py: [16/300], [300/484], step: 8044, 2.466 samples/sec, batch_loss: 0.2405, batch_loss_c: 0.2770, batch_loss_s: 0.1553, time:16.2220, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:15:38 \u001b[32mINFO     \u001b[0m train.py: [16/300], [310/484], step: 8054, 2.344 samples/sec, batch_loss: 0.3018, batch_loss_c: 0.2864, batch_loss_s: 0.3377, time:17.0658, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:15:57 \u001b[32mINFO     \u001b[0m train.py: [16/300], [320/484], step: 8064, 2.059 samples/sec, batch_loss: 0.3302, batch_loss_c: 0.3238, batch_loss_s: 0.3452, time:19.4287, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:16:21 \u001b[32mINFO     \u001b[0m train.py: [16/300], [330/484], step: 8074, 1.677 samples/sec, batch_loss: 0.0672, batch_loss_c: 0.0610, batch_loss_s: 0.0818, time:23.8503, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:16:36 \u001b[32mINFO     \u001b[0m train.py: [16/300], [340/484], step: 8084, 2.602 samples/sec, batch_loss: 0.0759, batch_loss_c: 0.0661, batch_loss_s: 0.0989, time:15.3751, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:16:56 \u001b[32mINFO     \u001b[0m train.py: [16/300], [350/484], step: 8094, 2.080 samples/sec, batch_loss: 0.2919, batch_loss_c: 0.2897, batch_loss_s: 0.2971, time:19.2336, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:17:10 \u001b[32mINFO     \u001b[0m train.py: [16/300], [360/484], step: 8104, 2.852 samples/sec, batch_loss: 0.0569, batch_loss_c: 0.0545, batch_loss_s: 0.0623, time:14.0270, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:17:23 \u001b[32mINFO     \u001b[0m train.py: [16/300], [370/484], step: 8114, 3.016 samples/sec, batch_loss: 0.0617, batch_loss_c: 0.0505, batch_loss_s: 0.0877, time:13.2625, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:17:38 \u001b[32mINFO     \u001b[0m train.py: [16/300], [380/484], step: 8124, 2.608 samples/sec, batch_loss: 0.3135, batch_loss_c: 0.3013, batch_loss_s: 0.3422, time:15.3367, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:17:56 \u001b[32mINFO     \u001b[0m train.py: [16/300], [390/484], step: 8134, 2.307 samples/sec, batch_loss: 0.1052, batch_loss_c: 0.0967, batch_loss_s: 0.1252, time:17.3419, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:18:12 \u001b[32mINFO     \u001b[0m train.py: [16/300], [400/484], step: 8144, 2.456 samples/sec, batch_loss: 0.1237, batch_loss_c: 0.1275, batch_loss_s: 0.1147, time:16.2890, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:18:32 \u001b[32mINFO     \u001b[0m train.py: [16/300], [410/484], step: 8154, 2.008 samples/sec, batch_loss: 0.1253, batch_loss_c: 0.1305, batch_loss_s: 0.1133, time:19.9178, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:18:47 \u001b[32mINFO     \u001b[0m train.py: [16/300], [420/484], step: 8164, 2.663 samples/sec, batch_loss: 0.1394, batch_loss_c: 0.1235, batch_loss_s: 0.1765, time:15.0234, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:19:03 \u001b[32mINFO     \u001b[0m train.py: [16/300], [430/484], step: 8174, 2.412 samples/sec, batch_loss: 0.1312, batch_loss_c: 0.1342, batch_loss_s: 0.1242, time:16.5859, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:19:21 \u001b[32mINFO     \u001b[0m train.py: [16/300], [440/484], step: 8184, 2.273 samples/sec, batch_loss: 0.1485, batch_loss_c: 0.1455, batch_loss_s: 0.1555, time:17.5949, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:19:36 \u001b[32mINFO     \u001b[0m train.py: [16/300], [450/484], step: 8194, 2.734 samples/sec, batch_loss: 0.0409, batch_loss_c: 0.0349, batch_loss_s: 0.0549, time:14.6281, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:19:51 \u001b[32mINFO     \u001b[0m train.py: [16/300], [460/484], step: 8204, 2.623 samples/sec, batch_loss: 0.1009, batch_loss_c: 0.0971, batch_loss_s: 0.1098, time:15.2488, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:20:05 \u001b[32mINFO     \u001b[0m train.py: [16/300], [470/484], step: 8214, 2.822 samples/sec, batch_loss: 0.0577, batch_loss_c: 0.0500, batch_loss_s: 0.0758, time:14.1763, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:20:20 \u001b[32mINFO     \u001b[0m train.py: [16/300], [480/484], step: 8224, 2.671 samples/sec, batch_loss: 0.0786, batch_loss_c: 0.0671, batch_loss_s: 0.1053, time:14.9783, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:20:30 \u001b[32mINFO     \u001b[0m train.py: [16/300], train_loss: 0.1474, time: 945.9282, lr: 0.0001\u001b[0m\n",
            "2019-12-06 08:20:32 \u001b[32mINFO     \u001b[0m train.py: [17/300], [0/484], step: 8228, 24.487 samples/sec, batch_loss: 0.0922, batch_loss_c: 0.0799, batch_loss_s: 0.1209, time:1.6335, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:20:51 \u001b[32mINFO     \u001b[0m train.py: [17/300], [10/484], step: 8238, 2.071 samples/sec, batch_loss: 0.0680, batch_loss_c: 0.0666, batch_loss_s: 0.0713, time:19.3157, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:21:04 \u001b[32mINFO     \u001b[0m train.py: [17/300], [20/484], step: 8248, 3.141 samples/sec, batch_loss: 0.0812, batch_loss_c: 0.0684, batch_loss_s: 0.1110, time:12.7365, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:21:44 \u001b[32mINFO     \u001b[0m train.py: [17/300], [30/484], step: 8258, 0.987 samples/sec, batch_loss: 0.0785, batch_loss_c: 0.0680, batch_loss_s: 0.1031, time:40.5462, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:22:02 \u001b[32mINFO     \u001b[0m train.py: [17/300], [40/484], step: 8268, 2.307 samples/sec, batch_loss: 0.1614, batch_loss_c: 0.1601, batch_loss_s: 0.1644, time:17.3410, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:22:21 \u001b[32mINFO     \u001b[0m train.py: [17/300], [50/484], step: 8278, 2.127 samples/sec, batch_loss: 0.1493, batch_loss_c: 0.1466, batch_loss_s: 0.1556, time:18.8021, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:22:42 \u001b[32mINFO     \u001b[0m train.py: [17/300], [60/484], step: 8288, 1.843 samples/sec, batch_loss: 0.2812, batch_loss_c: 0.2669, batch_loss_s: 0.3147, time:21.7065, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:23:04 \u001b[32mINFO     \u001b[0m train.py: [17/300], [70/484], step: 8298, 1.878 samples/sec, batch_loss: 0.0895, batch_loss_c: 0.0778, batch_loss_s: 0.1166, time:21.3030, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:23:28 \u001b[32mINFO     \u001b[0m train.py: [17/300], [80/484], step: 8308, 1.621 samples/sec, batch_loss: 0.1757, batch_loss_c: 0.1746, batch_loss_s: 0.1781, time:24.6780, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:23:55 \u001b[32mINFO     \u001b[0m train.py: [17/300], [90/484], step: 8318, 1.488 samples/sec, batch_loss: 0.2929, batch_loss_c: 0.2859, batch_loss_s: 0.3094, time:26.8890, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:24:09 \u001b[32mINFO     \u001b[0m train.py: [17/300], [100/484], step: 8328, 2.956 samples/sec, batch_loss: 0.0529, batch_loss_c: 0.0494, batch_loss_s: 0.0610, time:13.5305, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:24:25 \u001b[32mINFO     \u001b[0m train.py: [17/300], [110/484], step: 8338, 2.427 samples/sec, batch_loss: 0.1300, batch_loss_c: 0.1362, batch_loss_s: 0.1154, time:16.4823, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:24:44 \u001b[32mINFO     \u001b[0m train.py: [17/300], [120/484], step: 8348, 2.172 samples/sec, batch_loss: 0.1645, batch_loss_c: 0.1886, batch_loss_s: 0.1081, time:18.4156, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:25:03 \u001b[32mINFO     \u001b[0m train.py: [17/300], [130/484], step: 8358, 2.106 samples/sec, batch_loss: 0.0610, batch_loss_c: 0.0576, batch_loss_s: 0.0689, time:18.9965, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:25:17 \u001b[32mINFO     \u001b[0m train.py: [17/300], [140/484], step: 8368, 2.740 samples/sec, batch_loss: 0.0628, batch_loss_c: 0.0594, batch_loss_s: 0.0706, time:14.5977, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:25:33 \u001b[32mINFO     \u001b[0m train.py: [17/300], [150/484], step: 8378, 2.538 samples/sec, batch_loss: 0.0893, batch_loss_c: 0.0788, batch_loss_s: 0.1137, time:15.7583, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:25:46 \u001b[32mINFO     \u001b[0m train.py: [17/300], [160/484], step: 8388, 2.980 samples/sec, batch_loss: 0.3351, batch_loss_c: 0.3284, batch_loss_s: 0.3507, time:13.4251, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:26:17 \u001b[32mINFO     \u001b[0m train.py: [17/300], [170/484], step: 8398, 1.320 samples/sec, batch_loss: 0.0990, batch_loss_c: 0.0903, batch_loss_s: 0.1193, time:30.3024, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:26:34 \u001b[32mINFO     \u001b[0m train.py: [17/300], [180/484], step: 8408, 2.345 samples/sec, batch_loss: 0.0964, batch_loss_c: 0.0930, batch_loss_s: 0.1043, time:17.0550, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:27:07 \u001b[32mINFO     \u001b[0m train.py: [17/300], [190/484], step: 8418, 1.213 samples/sec, batch_loss: 0.0916, batch_loss_c: 0.0864, batch_loss_s: 0.1038, time:32.9896, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:27:20 \u001b[32mINFO     \u001b[0m train.py: [17/300], [200/484], step: 8428, 3.016 samples/sec, batch_loss: 0.0782, batch_loss_c: 0.0668, batch_loss_s: 0.1050, time:13.2619, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:27:40 \u001b[32mINFO     \u001b[0m train.py: [17/300], [210/484], step: 8438, 1.955 samples/sec, batch_loss: 0.0814, batch_loss_c: 0.0623, batch_loss_s: 0.1259, time:20.4618, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:27:54 \u001b[32mINFO     \u001b[0m train.py: [17/300], [220/484], step: 8448, 2.892 samples/sec, batch_loss: 0.3179, batch_loss_c: 0.3136, batch_loss_s: 0.3280, time:13.8293, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:28:10 \u001b[32mINFO     \u001b[0m train.py: [17/300], [230/484], step: 8458, 2.560 samples/sec, batch_loss: 0.0811, batch_loss_c: 0.0770, batch_loss_s: 0.0905, time:15.6264, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:28:44 \u001b[32mINFO     \u001b[0m train.py: [17/300], [240/484], step: 8468, 1.168 samples/sec, batch_loss: 0.0560, batch_loss_c: 0.0510, batch_loss_s: 0.0676, time:34.2544, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:29:04 \u001b[32mINFO     \u001b[0m train.py: [17/300], [250/484], step: 8478, 2.012 samples/sec, batch_loss: 0.2849, batch_loss_c: 0.2809, batch_loss_s: 0.2941, time:19.8779, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:29:22 \u001b[32mINFO     \u001b[0m train.py: [17/300], [260/484], step: 8488, 2.265 samples/sec, batch_loss: 0.2395, batch_loss_c: 0.2191, batch_loss_s: 0.2869, time:17.6628, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:29:37 \u001b[32mINFO     \u001b[0m train.py: [17/300], [270/484], step: 8498, 2.604 samples/sec, batch_loss: 0.0804, batch_loss_c: 0.0730, batch_loss_s: 0.0975, time:15.3582, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:29:52 \u001b[32mINFO     \u001b[0m train.py: [17/300], [280/484], step: 8508, 2.614 samples/sec, batch_loss: 0.0596, batch_loss_c: 0.0448, batch_loss_s: 0.0942, time:15.3030, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:30:12 \u001b[32mINFO     \u001b[0m train.py: [17/300], [290/484], step: 8518, 2.058 samples/sec, batch_loss: 0.2907, batch_loss_c: 0.2845, batch_loss_s: 0.3050, time:19.4342, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:30:34 \u001b[32mINFO     \u001b[0m train.py: [17/300], [300/484], step: 8528, 1.834 samples/sec, batch_loss: 0.0372, batch_loss_c: 0.0309, batch_loss_s: 0.0521, time:21.8065, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:31:00 \u001b[32mINFO     \u001b[0m train.py: [17/300], [310/484], step: 8538, 1.507 samples/sec, batch_loss: 0.3129, batch_loss_c: 0.3092, batch_loss_s: 0.3215, time:26.5439, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:31:15 \u001b[32mINFO     \u001b[0m train.py: [17/300], [320/484], step: 8548, 2.780 samples/sec, batch_loss: 0.0716, batch_loss_c: 0.0620, batch_loss_s: 0.0938, time:14.3886, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:31:29 \u001b[32mINFO     \u001b[0m train.py: [17/300], [330/484], step: 8558, 2.803 samples/sec, batch_loss: 0.0761, batch_loss_c: 0.0645, batch_loss_s: 0.1030, time:14.2715, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:32:00 \u001b[32mINFO     \u001b[0m train.py: [17/300], [340/484], step: 8568, 1.274 samples/sec, batch_loss: 0.0682, batch_loss_c: 0.0597, batch_loss_s: 0.0880, time:31.4079, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:32:28 \u001b[32mINFO     \u001b[0m train.py: [17/300], [350/484], step: 8578, 1.435 samples/sec, batch_loss: 0.0632, batch_loss_c: 0.0603, batch_loss_s: 0.0698, time:27.8739, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:32:41 \u001b[32mINFO     \u001b[0m train.py: [17/300], [360/484], step: 8588, 3.218 samples/sec, batch_loss: 0.0923, batch_loss_c: 0.0915, batch_loss_s: 0.0942, time:12.4291, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:33:08 \u001b[32mINFO     \u001b[0m train.py: [17/300], [370/484], step: 8598, 1.457 samples/sec, batch_loss: 0.1366, batch_loss_c: 0.1224, batch_loss_s: 0.1698, time:27.4445, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:33:21 \u001b[32mINFO     \u001b[0m train.py: [17/300], [380/484], step: 8608, 2.970 samples/sec, batch_loss: 0.1476, batch_loss_c: 0.1361, batch_loss_s: 0.1742, time:13.4666, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:33:41 \u001b[32mINFO     \u001b[0m train.py: [17/300], [390/484], step: 8618, 2.069 samples/sec, batch_loss: 0.2886, batch_loss_c: 0.2828, batch_loss_s: 0.3021, time:19.3362, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:33:55 \u001b[32mINFO     \u001b[0m train.py: [17/300], [400/484], step: 8628, 2.841 samples/sec, batch_loss: 0.0994, batch_loss_c: 0.0988, batch_loss_s: 0.1010, time:14.0778, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:34:12 \u001b[32mINFO     \u001b[0m train.py: [17/300], [410/484], step: 8638, 2.390 samples/sec, batch_loss: 0.0704, batch_loss_c: 0.0566, batch_loss_s: 0.1024, time:16.7398, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:34:30 \u001b[32mINFO     \u001b[0m train.py: [17/300], [420/484], step: 8648, 2.183 samples/sec, batch_loss: 0.1167, batch_loss_c: 0.1107, batch_loss_s: 0.1308, time:18.3214, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:34:56 \u001b[32mINFO     \u001b[0m train.py: [17/300], [430/484], step: 8658, 1.513 samples/sec, batch_loss: 0.1187, batch_loss_c: 0.1179, batch_loss_s: 0.1204, time:26.4316, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:35:15 \u001b[32mINFO     \u001b[0m train.py: [17/300], [440/484], step: 8668, 2.162 samples/sec, batch_loss: 0.0668, batch_loss_c: 0.0657, batch_loss_s: 0.0692, time:18.4982, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:35:34 \u001b[32mINFO     \u001b[0m train.py: [17/300], [450/484], step: 8678, 2.132 samples/sec, batch_loss: 0.0642, batch_loss_c: 0.0514, batch_loss_s: 0.0940, time:18.7622, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:35:49 \u001b[32mINFO     \u001b[0m train.py: [17/300], [460/484], step: 8688, 2.616 samples/sec, batch_loss: 0.3049, batch_loss_c: 0.3027, batch_loss_s: 0.3099, time:15.2905, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:36:07 \u001b[32mINFO     \u001b[0m train.py: [17/300], [470/484], step: 8698, 2.200 samples/sec, batch_loss: 0.1473, batch_loss_c: 0.1345, batch_loss_s: 0.1773, time:18.1859, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:36:32 \u001b[32mINFO     \u001b[0m train.py: [17/300], [480/484], step: 8708, 1.631 samples/sec, batch_loss: 0.3971, batch_loss_c: 0.3933, batch_loss_s: 0.4058, time:24.5294, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:36:36 \u001b[32mINFO     \u001b[0m train.py: [17/300], train_loss: 0.1511, time: 966.2588, lr: 0.0001\u001b[0m\n",
            "2019-12-06 08:36:39 \u001b[32mINFO     \u001b[0m train.py: [18/300], [0/484], step: 8712, 24.164 samples/sec, batch_loss: 0.0982, batch_loss_c: 0.0886, batch_loss_s: 0.1204, time:1.6553, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:36:54 \u001b[32mINFO     \u001b[0m train.py: [18/300], [10/484], step: 8722, 2.653 samples/sec, batch_loss: 0.1016, batch_loss_c: 0.0983, batch_loss_s: 0.1093, time:15.0759, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:37:13 \u001b[32mINFO     \u001b[0m train.py: [18/300], [20/484], step: 8732, 2.058 samples/sec, batch_loss: 0.3007, batch_loss_c: 0.2952, batch_loss_s: 0.3136, time:19.4326, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:37:31 \u001b[32mINFO     \u001b[0m train.py: [18/300], [30/484], step: 8742, 2.298 samples/sec, batch_loss: 0.0964, batch_loss_c: 0.0896, batch_loss_s: 0.1124, time:17.4064, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:37:46 \u001b[32mINFO     \u001b[0m train.py: [18/300], [40/484], step: 8752, 2.533 samples/sec, batch_loss: 0.0644, batch_loss_c: 0.0546, batch_loss_s: 0.0874, time:15.7894, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:38:07 \u001b[32mINFO     \u001b[0m train.py: [18/300], [50/484], step: 8762, 1.899 samples/sec, batch_loss: 0.3350, batch_loss_c: 0.3268, batch_loss_s: 0.3542, time:21.0612, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:38:23 \u001b[32mINFO     \u001b[0m train.py: [18/300], [60/484], step: 8772, 2.637 samples/sec, batch_loss: 0.1154, batch_loss_c: 0.1018, batch_loss_s: 0.1471, time:15.1691, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:38:42 \u001b[32mINFO     \u001b[0m train.py: [18/300], [70/484], step: 8782, 2.094 samples/sec, batch_loss: 0.2041, batch_loss_c: 0.1930, batch_loss_s: 0.2299, time:19.1015, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:38:55 \u001b[32mINFO     \u001b[0m train.py: [18/300], [80/484], step: 8792, 3.071 samples/sec, batch_loss: 0.0969, batch_loss_c: 0.1016, batch_loss_s: 0.0861, time:13.0251, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:39:20 \u001b[32mINFO     \u001b[0m train.py: [18/300], [90/484], step: 8802, 1.581 samples/sec, batch_loss: 0.1054, batch_loss_c: 0.1051, batch_loss_s: 0.1061, time:25.2930, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:39:33 \u001b[32mINFO     \u001b[0m train.py: [18/300], [100/484], step: 8812, 2.960 samples/sec, batch_loss: 0.0683, batch_loss_c: 0.0615, batch_loss_s: 0.0840, time:13.5116, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:39:53 \u001b[32mINFO     \u001b[0m train.py: [18/300], [110/484], step: 8822, 2.081 samples/sec, batch_loss: 0.1074, batch_loss_c: 0.0852, batch_loss_s: 0.1591, time:19.2235, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:40:09 \u001b[32mINFO     \u001b[0m train.py: [18/300], [120/484], step: 8832, 2.469 samples/sec, batch_loss: 0.0746, batch_loss_c: 0.0597, batch_loss_s: 0.1094, time:16.2003, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:40:34 \u001b[32mINFO     \u001b[0m train.py: [18/300], [130/484], step: 8842, 1.626 samples/sec, batch_loss: 0.2891, batch_loss_c: 0.2839, batch_loss_s: 0.3013, time:24.5962, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:40:46 \u001b[32mINFO     \u001b[0m train.py: [18/300], [140/484], step: 8852, 3.327 samples/sec, batch_loss: 0.1797, batch_loss_c: 0.1854, batch_loss_s: 0.1665, time:12.0211, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:41:02 \u001b[32mINFO     \u001b[0m train.py: [18/300], [150/484], step: 8862, 2.499 samples/sec, batch_loss: 0.3614, batch_loss_c: 0.3627, batch_loss_s: 0.3583, time:16.0049, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:41:23 \u001b[32mINFO     \u001b[0m train.py: [18/300], [160/484], step: 8872, 1.892 samples/sec, batch_loss: 0.0712, batch_loss_c: 0.0540, batch_loss_s: 0.1112, time:21.1385, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:41:38 \u001b[32mINFO     \u001b[0m train.py: [18/300], [170/484], step: 8882, 2.587 samples/sec, batch_loss: 0.0782, batch_loss_c: 0.0737, batch_loss_s: 0.0887, time:15.4603, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:41:51 \u001b[32mINFO     \u001b[0m train.py: [18/300], [180/484], step: 8892, 3.099 samples/sec, batch_loss: 0.0953, batch_loss_c: 0.0789, batch_loss_s: 0.1334, time:12.9084, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:42:07 \u001b[32mINFO     \u001b[0m train.py: [18/300], [190/484], step: 8902, 2.468 samples/sec, batch_loss: 0.1806, batch_loss_c: 0.1709, batch_loss_s: 0.2032, time:16.2089, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:42:26 \u001b[32mINFO     \u001b[0m train.py: [18/300], [200/484], step: 8912, 2.167 samples/sec, batch_loss: 0.4363, batch_loss_c: 0.4212, batch_loss_s: 0.4715, time:18.4622, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:42:46 \u001b[32mINFO     \u001b[0m train.py: [18/300], [210/484], step: 8922, 1.927 samples/sec, batch_loss: 0.0855, batch_loss_c: 0.0908, batch_loss_s: 0.0732, time:20.7628, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:43:01 \u001b[32mINFO     \u001b[0m train.py: [18/300], [220/484], step: 8932, 2.694 samples/sec, batch_loss: 0.0611, batch_loss_c: 0.0540, batch_loss_s: 0.0776, time:14.8498, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:43:38 \u001b[32mINFO     \u001b[0m train.py: [18/300], [230/484], step: 8942, 1.081 samples/sec, batch_loss: 0.0622, batch_loss_c: 0.0598, batch_loss_s: 0.0678, time:37.0147, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:44:00 \u001b[32mINFO     \u001b[0m train.py: [18/300], [240/484], step: 8952, 1.832 samples/sec, batch_loss: 0.0765, batch_loss_c: 0.0666, batch_loss_s: 0.0996, time:21.8369, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:44:19 \u001b[32mINFO     \u001b[0m train.py: [18/300], [250/484], step: 8962, 2.139 samples/sec, batch_loss: 0.5709, batch_loss_c: 0.5606, batch_loss_s: 0.5950, time:18.6979, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:44:39 \u001b[32mINFO     \u001b[0m train.py: [18/300], [260/484], step: 8972, 1.954 samples/sec, batch_loss: 0.0903, batch_loss_c: 0.0832, batch_loss_s: 0.1067, time:20.4669, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:45:01 \u001b[32mINFO     \u001b[0m train.py: [18/300], [270/484], step: 8982, 1.877 samples/sec, batch_loss: 0.1238, batch_loss_c: 0.1036, batch_loss_s: 0.1710, time:21.3122, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:45:17 \u001b[32mINFO     \u001b[0m train.py: [18/300], [280/484], step: 8992, 2.493 samples/sec, batch_loss: 0.0660, batch_loss_c: 0.0597, batch_loss_s: 0.0809, time:16.0463, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:45:48 \u001b[32mINFO     \u001b[0m train.py: [18/300], [290/484], step: 9002, 1.284 samples/sec, batch_loss: 0.2851, batch_loss_c: 0.2777, batch_loss_s: 0.3023, time:31.1536, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:46:07 \u001b[32mINFO     \u001b[0m train.py: [18/300], [300/484], step: 9012, 2.077 samples/sec, batch_loss: 0.0966, batch_loss_c: 0.0909, batch_loss_s: 0.1099, time:19.2604, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:46:23 \u001b[32mINFO     \u001b[0m train.py: [18/300], [310/484], step: 9022, 2.469 samples/sec, batch_loss: 0.0487, batch_loss_c: 0.0410, batch_loss_s: 0.0668, time:16.2010, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:46:39 \u001b[32mINFO     \u001b[0m train.py: [18/300], [320/484], step: 9032, 2.624 samples/sec, batch_loss: 0.3517, batch_loss_c: 0.3371, batch_loss_s: 0.3857, time:15.2441, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:46:53 \u001b[32mINFO     \u001b[0m train.py: [18/300], [330/484], step: 9042, 2.830 samples/sec, batch_loss: 0.1316, batch_loss_c: 0.0773, batch_loss_s: 0.2585, time:14.1345, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:47:17 \u001b[32mINFO     \u001b[0m train.py: [18/300], [340/484], step: 9052, 1.675 samples/sec, batch_loss: 0.2789, batch_loss_c: 0.2755, batch_loss_s: 0.2869, time:23.8738, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:47:33 \u001b[32mINFO     \u001b[0m train.py: [18/300], [350/484], step: 9062, 2.461 samples/sec, batch_loss: 0.0601, batch_loss_c: 0.0464, batch_loss_s: 0.0919, time:16.2534, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:47:49 \u001b[32mINFO     \u001b[0m train.py: [18/300], [360/484], step: 9072, 2.488 samples/sec, batch_loss: 0.3889, batch_loss_c: 0.4070, batch_loss_s: 0.3466, time:16.0774, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:48:02 \u001b[32mINFO     \u001b[0m train.py: [18/300], [370/484], step: 9082, 2.959 samples/sec, batch_loss: 0.1330, batch_loss_c: 0.1264, batch_loss_s: 0.1484, time:13.5197, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:48:32 \u001b[32mINFO     \u001b[0m train.py: [18/300], [380/484], step: 9092, 1.343 samples/sec, batch_loss: 0.0612, batch_loss_c: 0.0552, batch_loss_s: 0.0751, time:29.7733, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:48:46 \u001b[32mINFO     \u001b[0m train.py: [18/300], [390/484], step: 9102, 2.988 samples/sec, batch_loss: 0.0457, batch_loss_c: 0.0363, batch_loss_s: 0.0678, time:13.3873, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:49:01 \u001b[32mINFO     \u001b[0m train.py: [18/300], [400/484], step: 9112, 2.650 samples/sec, batch_loss: 0.0481, batch_loss_c: 0.0405, batch_loss_s: 0.0659, time:15.0932, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:49:20 \u001b[32mINFO     \u001b[0m train.py: [18/300], [410/484], step: 9122, 2.105 samples/sec, batch_loss: 0.1488, batch_loss_c: 0.1576, batch_loss_s: 0.1285, time:19.0030, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:49:35 \u001b[32mINFO     \u001b[0m train.py: [18/300], [420/484], step: 9132, 2.571 samples/sec, batch_loss: 0.0747, batch_loss_c: 0.0720, batch_loss_s: 0.0811, time:15.5581, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:49:51 \u001b[32mINFO     \u001b[0m train.py: [18/300], [430/484], step: 9142, 2.468 samples/sec, batch_loss: 0.1058, batch_loss_c: 0.1222, batch_loss_s: 0.0675, time:16.2096, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:50:19 \u001b[32mINFO     \u001b[0m train.py: [18/300], [440/484], step: 9152, 1.448 samples/sec, batch_loss: 0.0469, batch_loss_c: 0.0413, batch_loss_s: 0.0599, time:27.6281, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:50:39 \u001b[32mINFO     \u001b[0m train.py: [18/300], [450/484], step: 9162, 1.995 samples/sec, batch_loss: 0.2985, batch_loss_c: 0.2907, batch_loss_s: 0.3167, time:20.0463, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:51:07 \u001b[32mINFO     \u001b[0m train.py: [18/300], [460/484], step: 9172, 1.448 samples/sec, batch_loss: 0.2999, batch_loss_c: 0.2931, batch_loss_s: 0.3158, time:27.6286, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:51:35 \u001b[32mINFO     \u001b[0m train.py: [18/300], [470/484], step: 9182, 1.418 samples/sec, batch_loss: 0.3532, batch_loss_c: 0.3588, batch_loss_s: 0.3401, time:28.2168, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:51:49 \u001b[32mINFO     \u001b[0m train.py: [18/300], [480/484], step: 9192, 2.929 samples/sec, batch_loss: 0.2899, batch_loss_c: 0.2848, batch_loss_s: 0.3019, time:13.6580, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:51:54 \u001b[32mINFO     \u001b[0m train.py: [18/300], train_loss: 0.1551, time: 916.7720, lr: 0.0001\u001b[0m\n",
            "2019-12-06 08:51:55 \u001b[32mINFO     \u001b[0m train.py: [19/300], [0/484], step: 9196, 32.203 samples/sec, batch_loss: 0.1284, batch_loss_c: 0.1170, batch_loss_s: 0.1550, time:1.2421, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:52:15 \u001b[32mINFO     \u001b[0m train.py: [19/300], [10/484], step: 9206, 2.022 samples/sec, batch_loss: 0.0648, batch_loss_c: 0.0575, batch_loss_s: 0.0819, time:19.7828, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:52:42 \u001b[32mINFO     \u001b[0m train.py: [19/300], [20/484], step: 9216, 1.490 samples/sec, batch_loss: 0.0475, batch_loss_c: 0.0454, batch_loss_s: 0.0524, time:26.8449, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:53:00 \u001b[32mINFO     \u001b[0m train.py: [19/300], [30/484], step: 9226, 2.217 samples/sec, batch_loss: 0.0746, batch_loss_c: 0.0695, batch_loss_s: 0.0865, time:18.0415, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:53:28 \u001b[32mINFO     \u001b[0m train.py: [19/300], [40/484], step: 9236, 1.424 samples/sec, batch_loss: 0.3197, batch_loss_c: 0.3148, batch_loss_s: 0.3311, time:28.0840, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:53:52 \u001b[32mINFO     \u001b[0m train.py: [19/300], [50/484], step: 9246, 1.672 samples/sec, batch_loss: 0.0493, batch_loss_c: 0.0434, batch_loss_s: 0.0631, time:23.9192, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:54:08 \u001b[32mINFO     \u001b[0m train.py: [19/300], [60/484], step: 9256, 2.454 samples/sec, batch_loss: 0.0874, batch_loss_c: 0.0805, batch_loss_s: 0.1036, time:16.3017, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:54:35 \u001b[32mINFO     \u001b[0m train.py: [19/300], [70/484], step: 9266, 1.506 samples/sec, batch_loss: 0.1325, batch_loss_c: 0.1283, batch_loss_s: 0.1422, time:26.5682, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:54:53 \u001b[32mINFO     \u001b[0m train.py: [19/300], [80/484], step: 9276, 2.169 samples/sec, batch_loss: 0.5044, batch_loss_c: 0.4919, batch_loss_s: 0.5336, time:18.4451, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:55:16 \u001b[32mINFO     \u001b[0m train.py: [19/300], [90/484], step: 9286, 1.737 samples/sec, batch_loss: 0.0759, batch_loss_c: 0.0739, batch_loss_s: 0.0805, time:23.0334, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:55:38 \u001b[32mINFO     \u001b[0m train.py: [19/300], [100/484], step: 9296, 1.830 samples/sec, batch_loss: 0.1053, batch_loss_c: 0.0935, batch_loss_s: 0.1327, time:21.8548, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:56:04 \u001b[32mINFO     \u001b[0m train.py: [19/300], [110/484], step: 9306, 1.536 samples/sec, batch_loss: 0.0715, batch_loss_c: 0.0636, batch_loss_s: 0.0899, time:26.0484, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:56:35 \u001b[32mINFO     \u001b[0m train.py: [19/300], [120/484], step: 9316, 1.305 samples/sec, batch_loss: 0.1418, batch_loss_c: 0.1274, batch_loss_s: 0.1754, time:30.6462, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:56:53 \u001b[32mINFO     \u001b[0m train.py: [19/300], [130/484], step: 9326, 2.208 samples/sec, batch_loss: 0.1750, batch_loss_c: 0.1760, batch_loss_s: 0.1728, time:18.1179, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:57:09 \u001b[32mINFO     \u001b[0m train.py: [19/300], [140/484], step: 9336, 2.462 samples/sec, batch_loss: 0.2825, batch_loss_c: 0.3102, batch_loss_s: 0.2179, time:16.2468, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:57:22 \u001b[32mINFO     \u001b[0m train.py: [19/300], [150/484], step: 9346, 3.143 samples/sec, batch_loss: 0.2860, batch_loss_c: 0.2813, batch_loss_s: 0.2968, time:12.7266, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:57:38 \u001b[32mINFO     \u001b[0m train.py: [19/300], [160/484], step: 9356, 2.595 samples/sec, batch_loss: 0.0922, batch_loss_c: 0.0877, batch_loss_s: 0.1027, time:15.4158, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:58:08 \u001b[32mINFO     \u001b[0m train.py: [19/300], [170/484], step: 9366, 1.320 samples/sec, batch_loss: 0.0484, batch_loss_c: 0.0351, batch_loss_s: 0.0793, time:30.3005, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:58:23 \u001b[32mINFO     \u001b[0m train.py: [19/300], [180/484], step: 9376, 2.610 samples/sec, batch_loss: 0.0886, batch_loss_c: 0.0834, batch_loss_s: 0.1005, time:15.3257, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:58:38 \u001b[32mINFO     \u001b[0m train.py: [19/300], [190/484], step: 9386, 2.753 samples/sec, batch_loss: 0.0820, batch_loss_c: 0.0721, batch_loss_s: 0.1050, time:14.5303, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:58:54 \u001b[32mINFO     \u001b[0m train.py: [19/300], [200/484], step: 9396, 2.394 samples/sec, batch_loss: 0.0685, batch_loss_c: 0.0637, batch_loss_s: 0.0795, time:16.7118, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:59:13 \u001b[32mINFO     \u001b[0m train.py: [19/300], [210/484], step: 9406, 2.212 samples/sec, batch_loss: 0.0635, batch_loss_c: 0.0597, batch_loss_s: 0.0723, time:18.0849, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:59:31 \u001b[32mINFO     \u001b[0m train.py: [19/300], [220/484], step: 9416, 2.128 samples/sec, batch_loss: 0.1139, batch_loss_c: 0.1097, batch_loss_s: 0.1239, time:18.7957, lr:0.0001\u001b[0m\n",
            "2019-12-06 08:59:46 \u001b[32mINFO     \u001b[0m train.py: [19/300], [230/484], step: 9426, 2.678 samples/sec, batch_loss: 0.0835, batch_loss_c: 0.0807, batch_loss_s: 0.0902, time:14.9355, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:00:01 \u001b[32mINFO     \u001b[0m train.py: [19/300], [240/484], step: 9436, 2.804 samples/sec, batch_loss: 0.0525, batch_loss_c: 0.0443, batch_loss_s: 0.0716, time:14.2648, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:00:15 \u001b[32mINFO     \u001b[0m train.py: [19/300], [250/484], step: 9446, 2.738 samples/sec, batch_loss: 0.0544, batch_loss_c: 0.0441, batch_loss_s: 0.0785, time:14.6099, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:00:29 \u001b[32mINFO     \u001b[0m train.py: [19/300], [260/484], step: 9456, 2.798 samples/sec, batch_loss: 0.0502, batch_loss_c: 0.0382, batch_loss_s: 0.0782, time:14.2958, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:00:49 \u001b[32mINFO     \u001b[0m train.py: [19/300], [270/484], step: 9466, 2.045 samples/sec, batch_loss: 0.1049, batch_loss_c: 0.0940, batch_loss_s: 0.1303, time:19.5638, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:01:06 \u001b[32mINFO     \u001b[0m train.py: [19/300], [280/484], step: 9476, 2.395 samples/sec, batch_loss: 0.0751, batch_loss_c: 0.0664, batch_loss_s: 0.0954, time:16.7029, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:01:21 \u001b[32mINFO     \u001b[0m train.py: [19/300], [290/484], step: 9486, 2.600 samples/sec, batch_loss: 0.1331, batch_loss_c: 0.1211, batch_loss_s: 0.1609, time:15.3868, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:01:37 \u001b[32mINFO     \u001b[0m train.py: [19/300], [300/484], step: 9496, 2.584 samples/sec, batch_loss: 0.0613, batch_loss_c: 0.0589, batch_loss_s: 0.0668, time:15.4816, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:01:55 \u001b[32mINFO     \u001b[0m train.py: [19/300], [310/484], step: 9506, 2.133 samples/sec, batch_loss: 0.1660, batch_loss_c: 0.1835, batch_loss_s: 0.1252, time:18.7559, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:02:11 \u001b[32mINFO     \u001b[0m train.py: [19/300], [320/484], step: 9516, 2.489 samples/sec, batch_loss: 0.2904, batch_loss_c: 0.2842, batch_loss_s: 0.3051, time:16.0702, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:02:31 \u001b[32mINFO     \u001b[0m train.py: [19/300], [330/484], step: 9526, 2.053 samples/sec, batch_loss: 0.1633, batch_loss_c: 0.1734, batch_loss_s: 0.1397, time:19.4809, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:02:48 \u001b[32mINFO     \u001b[0m train.py: [19/300], [340/484], step: 9536, 2.272 samples/sec, batch_loss: 0.0581, batch_loss_c: 0.0526, batch_loss_s: 0.0709, time:17.6061, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:03:12 \u001b[32mINFO     \u001b[0m train.py: [19/300], [350/484], step: 9546, 1.709 samples/sec, batch_loss: 0.1514, batch_loss_c: 0.1591, batch_loss_s: 0.1336, time:23.4081, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:03:32 \u001b[32mINFO     \u001b[0m train.py: [19/300], [360/484], step: 9556, 2.021 samples/sec, batch_loss: 0.0912, batch_loss_c: 0.0866, batch_loss_s: 0.1019, time:19.7928, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:03:45 \u001b[32mINFO     \u001b[0m train.py: [19/300], [370/484], step: 9566, 2.938 samples/sec, batch_loss: 0.0612, batch_loss_c: 0.0583, batch_loss_s: 0.0681, time:13.6155, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:04:14 \u001b[32mINFO     \u001b[0m train.py: [19/300], [380/484], step: 9576, 1.379 samples/sec, batch_loss: 0.0653, batch_loss_c: 0.0570, batch_loss_s: 0.0846, time:29.0132, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:04:39 \u001b[32mINFO     \u001b[0m train.py: [19/300], [390/484], step: 9586, 1.612 samples/sec, batch_loss: 0.4334, batch_loss_c: 0.4200, batch_loss_s: 0.4648, time:24.8136, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:05:06 \u001b[32mINFO     \u001b[0m train.py: [19/300], [400/484], step: 9596, 1.506 samples/sec, batch_loss: 0.2282, batch_loss_c: 0.1947, batch_loss_s: 0.3066, time:26.5687, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:05:35 \u001b[32mINFO     \u001b[0m train.py: [19/300], [410/484], step: 9606, 1.384 samples/sec, batch_loss: 0.0640, batch_loss_c: 0.0550, batch_loss_s: 0.0851, time:28.9025, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:05:49 \u001b[32mINFO     \u001b[0m train.py: [19/300], [420/484], step: 9616, 2.765 samples/sec, batch_loss: 0.2339, batch_loss_c: 0.2097, batch_loss_s: 0.2902, time:14.4642, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:06:02 \u001b[32mINFO     \u001b[0m train.py: [19/300], [430/484], step: 9626, 3.186 samples/sec, batch_loss: 0.2892, batch_loss_c: 0.2826, batch_loss_s: 0.3046, time:12.5549, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:06:22 \u001b[32mINFO     \u001b[0m train.py: [19/300], [440/484], step: 9636, 1.960 samples/sec, batch_loss: 0.0459, batch_loss_c: 0.0415, batch_loss_s: 0.0561, time:20.4101, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:06:40 \u001b[32mINFO     \u001b[0m train.py: [19/300], [450/484], step: 9646, 2.208 samples/sec, batch_loss: 0.2085, batch_loss_c: 0.2269, batch_loss_s: 0.1657, time:18.1141, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:07:02 \u001b[32mINFO     \u001b[0m train.py: [19/300], [460/484], step: 9656, 1.794 samples/sec, batch_loss: 0.5297, batch_loss_c: 0.5226, batch_loss_s: 0.5463, time:22.2988, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:07:18 \u001b[32mINFO     \u001b[0m train.py: [19/300], [470/484], step: 9666, 2.601 samples/sec, batch_loss: 0.0876, batch_loss_c: 0.0756, batch_loss_s: 0.1155, time:15.3799, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:07:51 \u001b[32mINFO     \u001b[0m train.py: [19/300], [480/484], step: 9676, 1.217 samples/sec, batch_loss: 0.4068, batch_loss_c: 0.4488, batch_loss_s: 0.3088, time:32.8710, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:07:56 \u001b[32mINFO     \u001b[0m train.py: [19/300], train_loss: 0.1524, time: 961.5443, lr: 0.0001\u001b[0m\n",
            "2019-12-06 09:07:58 \u001b[32mINFO     \u001b[0m train.py: [20/300], [0/484], step: 9680, 28.412 samples/sec, batch_loss: 0.0721, batch_loss_c: 0.0627, batch_loss_s: 0.0939, time:1.4079, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:08:15 \u001b[32mINFO     \u001b[0m train.py: [20/300], [10/484], step: 9690, 2.369 samples/sec, batch_loss: 0.0569, batch_loss_c: 0.0468, batch_loss_s: 0.0806, time:16.8864, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:08:36 \u001b[32mINFO     \u001b[0m train.py: [20/300], [20/484], step: 9700, 1.894 samples/sec, batch_loss: 0.3031, batch_loss_c: 0.3030, batch_loss_s: 0.3033, time:21.1243, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:09:00 \u001b[32mINFO     \u001b[0m train.py: [20/300], [30/484], step: 9710, 1.625 samples/sec, batch_loss: 0.1495, batch_loss_c: 0.1474, batch_loss_s: 0.1543, time:24.6093, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:09:19 \u001b[32mINFO     \u001b[0m train.py: [20/300], [40/484], step: 9720, 2.160 samples/sec, batch_loss: 0.0760, batch_loss_c: 0.0652, batch_loss_s: 0.1014, time:18.5175, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:09:40 \u001b[32mINFO     \u001b[0m train.py: [20/300], [50/484], step: 9730, 1.843 samples/sec, batch_loss: 0.2231, batch_loss_c: 0.1906, batch_loss_s: 0.2992, time:21.7049, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:10:08 \u001b[32mINFO     \u001b[0m train.py: [20/300], [60/484], step: 9740, 1.446 samples/sec, batch_loss: 0.1327, batch_loss_c: 0.1234, batch_loss_s: 0.1543, time:27.6604, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:10:25 \u001b[32mINFO     \u001b[0m train.py: [20/300], [70/484], step: 9750, 2.440 samples/sec, batch_loss: 0.3649, batch_loss_c: 0.3646, batch_loss_s: 0.3656, time:16.3956, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:10:51 \u001b[32mINFO     \u001b[0m train.py: [20/300], [80/484], step: 9760, 1.500 samples/sec, batch_loss: 0.0841, batch_loss_c: 0.0918, batch_loss_s: 0.0664, time:26.6714, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:11:06 \u001b[32mINFO     \u001b[0m train.py: [20/300], [90/484], step: 9770, 2.748 samples/sec, batch_loss: 0.0493, batch_loss_c: 0.0462, batch_loss_s: 0.0566, time:14.5545, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:11:19 \u001b[32mINFO     \u001b[0m train.py: [20/300], [100/484], step: 9780, 3.112 samples/sec, batch_loss: 0.0895, batch_loss_c: 0.0934, batch_loss_s: 0.0803, time:12.8518, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:11:33 \u001b[32mINFO     \u001b[0m train.py: [20/300], [110/484], step: 9790, 2.858 samples/sec, batch_loss: 0.0616, batch_loss_c: 0.0617, batch_loss_s: 0.0613, time:13.9956, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:12:02 \u001b[32mINFO     \u001b[0m train.py: [20/300], [120/484], step: 9800, 1.359 samples/sec, batch_loss: 0.3742, batch_loss_c: 0.4001, batch_loss_s: 0.3137, time:29.4321, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:12:16 \u001b[32mINFO     \u001b[0m train.py: [20/300], [130/484], step: 9810, 2.825 samples/sec, batch_loss: 0.0706, batch_loss_c: 0.0636, batch_loss_s: 0.0870, time:14.1610, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:12:42 \u001b[32mINFO     \u001b[0m train.py: [20/300], [140/484], step: 9820, 1.535 samples/sec, batch_loss: 0.3009, batch_loss_c: 0.2883, batch_loss_s: 0.3302, time:26.0639, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:13:09 \u001b[32mINFO     \u001b[0m train.py: [20/300], [150/484], step: 9830, 1.504 samples/sec, batch_loss: 0.3092, batch_loss_c: 0.3049, batch_loss_s: 0.3190, time:26.5889, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:13:26 \u001b[32mINFO     \u001b[0m train.py: [20/300], [160/484], step: 9840, 2.297 samples/sec, batch_loss: 0.0769, batch_loss_c: 0.0651, batch_loss_s: 0.1046, time:17.4142, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:13:47 \u001b[32mINFO     \u001b[0m train.py: [20/300], [170/484], step: 9850, 1.944 samples/sec, batch_loss: 0.1603, batch_loss_c: 0.1570, batch_loss_s: 0.1680, time:20.5799, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:14:08 \u001b[32mINFO     \u001b[0m train.py: [20/300], [180/484], step: 9860, 1.911 samples/sec, batch_loss: 0.3089, batch_loss_c: 0.2998, batch_loss_s: 0.3301, time:20.9273, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:14:45 \u001b[32mINFO     \u001b[0m train.py: [20/300], [190/484], step: 9870, 1.083 samples/sec, batch_loss: 0.5263, batch_loss_c: 0.5232, batch_loss_s: 0.5337, time:36.9388, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:15:00 \u001b[32mINFO     \u001b[0m train.py: [20/300], [200/484], step: 9880, 2.581 samples/sec, batch_loss: 0.0799, batch_loss_c: 0.0783, batch_loss_s: 0.0836, time:15.4966, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:15:21 \u001b[32mINFO     \u001b[0m train.py: [20/300], [210/484], step: 9890, 1.911 samples/sec, batch_loss: 0.0783, batch_loss_c: 0.0700, batch_loss_s: 0.0975, time:20.9260, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:16:00 \u001b[32mINFO     \u001b[0m train.py: [20/300], [220/484], step: 9900, 1.033 samples/sec, batch_loss: 0.0561, batch_loss_c: 0.0492, batch_loss_s: 0.0721, time:38.7398, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:16:27 \u001b[32mINFO     \u001b[0m train.py: [20/300], [230/484], step: 9910, 1.491 samples/sec, batch_loss: 0.1168, batch_loss_c: 0.0666, batch_loss_s: 0.2342, time:26.8209, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:16:41 \u001b[32mINFO     \u001b[0m train.py: [20/300], [240/484], step: 9920, 2.752 samples/sec, batch_loss: 0.1411, batch_loss_c: 0.1544, batch_loss_s: 0.1099, time:14.5367, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:16:56 \u001b[32mINFO     \u001b[0m train.py: [20/300], [250/484], step: 9930, 2.781 samples/sec, batch_loss: 0.1655, batch_loss_c: 0.1951, batch_loss_s: 0.0966, time:14.3843, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:17:27 \u001b[32mINFO     \u001b[0m train.py: [20/300], [260/484], step: 9940, 1.281 samples/sec, batch_loss: 0.1158, batch_loss_c: 0.1135, batch_loss_s: 0.1211, time:31.2148, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:17:43 \u001b[32mINFO     \u001b[0m train.py: [20/300], [270/484], step: 9950, 2.508 samples/sec, batch_loss: 0.0780, batch_loss_c: 0.0712, batch_loss_s: 0.0940, time:15.9493, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:17:56 \u001b[32mINFO     \u001b[0m train.py: [20/300], [280/484], step: 9960, 2.934 samples/sec, batch_loss: 0.0451, batch_loss_c: 0.0366, batch_loss_s: 0.0650, time:13.6351, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:18:18 \u001b[32mINFO     \u001b[0m train.py: [20/300], [290/484], step: 9970, 1.879 samples/sec, batch_loss: 0.1130, batch_loss_c: 0.1074, batch_loss_s: 0.1260, time:21.2889, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:18:35 \u001b[32mINFO     \u001b[0m train.py: [20/300], [300/484], step: 9980, 2.274 samples/sec, batch_loss: 0.0778, batch_loss_c: 0.0729, batch_loss_s: 0.0891, time:17.5893, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:18:50 \u001b[32mINFO     \u001b[0m train.py: [20/300], [310/484], step: 9990, 2.647 samples/sec, batch_loss: 0.0808, batch_loss_c: 0.0806, batch_loss_s: 0.0812, time:15.1096, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:19:09 \u001b[32mINFO     \u001b[0m train.py: [20/300], [320/484], step: 10000, 2.205 samples/sec, batch_loss: 0.3280, batch_loss_c: 0.3171, batch_loss_s: 0.3535, time:18.1376, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:19:22 \u001b[32mINFO     \u001b[0m train.py: [20/300], [330/484], step: 10010, 3.057 samples/sec, batch_loss: 0.0836, batch_loss_c: 0.0788, batch_loss_s: 0.0947, time:13.0832, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:19:47 \u001b[32mINFO     \u001b[0m train.py: [20/300], [340/484], step: 10020, 1.591 samples/sec, batch_loss: 0.2030, batch_loss_c: 0.2261, batch_loss_s: 0.1491, time:25.1375, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:20:08 \u001b[32mINFO     \u001b[0m train.py: [20/300], [350/484], step: 10030, 1.898 samples/sec, batch_loss: 0.0658, batch_loss_c: 0.0456, batch_loss_s: 0.1130, time:21.0731, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:20:25 \u001b[32mINFO     \u001b[0m train.py: [20/300], [360/484], step: 10040, 2.338 samples/sec, batch_loss: 0.0420, batch_loss_c: 0.0353, batch_loss_s: 0.0578, time:17.1059, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:20:45 \u001b[32mINFO     \u001b[0m train.py: [20/300], [370/484], step: 10050, 1.953 samples/sec, batch_loss: 0.2088, batch_loss_c: 0.2285, batch_loss_s: 0.1628, time:20.4773, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:21:14 \u001b[32mINFO     \u001b[0m train.py: [20/300], [380/484], step: 10060, 1.406 samples/sec, batch_loss: 0.1098, batch_loss_c: 0.1083, batch_loss_s: 0.1134, time:28.4482, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:21:33 \u001b[32mINFO     \u001b[0m train.py: [20/300], [390/484], step: 10070, 2.135 samples/sec, batch_loss: 0.0919, batch_loss_c: 0.0785, batch_loss_s: 0.1230, time:18.7368, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:21:49 \u001b[32mINFO     \u001b[0m train.py: [20/300], [400/484], step: 10080, 2.416 samples/sec, batch_loss: 0.3132, batch_loss_c: 0.3112, batch_loss_s: 0.3181, time:16.5594, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:22:12 \u001b[32mINFO     \u001b[0m train.py: [20/300], [410/484], step: 10090, 1.734 samples/sec, batch_loss: 0.0694, batch_loss_c: 0.0642, batch_loss_s: 0.0816, time:23.0628, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:22:39 \u001b[32mINFO     \u001b[0m train.py: [20/300], [420/484], step: 10100, 1.485 samples/sec, batch_loss: 0.2810, batch_loss_c: 0.2764, batch_loss_s: 0.2916, time:26.9441, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:23:15 \u001b[32mINFO     \u001b[0m train.py: [20/300], [430/484], step: 10110, 1.126 samples/sec, batch_loss: 0.0925, batch_loss_c: 0.0669, batch_loss_s: 0.1523, time:35.5255, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:23:31 \u001b[32mINFO     \u001b[0m train.py: [20/300], [440/484], step: 10120, 2.513 samples/sec, batch_loss: 0.0454, batch_loss_c: 0.0385, batch_loss_s: 0.0616, time:15.9183, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:23:51 \u001b[32mINFO     \u001b[0m train.py: [20/300], [450/484], step: 10130, 1.965 samples/sec, batch_loss: 0.3019, batch_loss_c: 0.2950, batch_loss_s: 0.3178, time:20.3592, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:24:05 \u001b[32mINFO     \u001b[0m train.py: [20/300], [460/484], step: 10140, 2.799 samples/sec, batch_loss: 0.0498, batch_loss_c: 0.0369, batch_loss_s: 0.0799, time:14.2916, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:24:18 \u001b[32mINFO     \u001b[0m train.py: [20/300], [470/484], step: 10150, 3.060 samples/sec, batch_loss: 0.0647, batch_loss_c: 0.0577, batch_loss_s: 0.0813, time:13.0729, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:24:56 \u001b[32mINFO     \u001b[0m train.py: [20/300], [480/484], step: 10160, 1.071 samples/sec, batch_loss: 0.1742, batch_loss_c: 0.1481, batch_loss_s: 0.2350, time:37.3408, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:25:03 \u001b[32mINFO     \u001b[0m train.py: [20/300], train_loss: 0.1646, time: 1026.3490, lr: 0.0001\u001b[0m\n",
            "2019-12-06 09:25:06 \u001b[32mINFO     \u001b[0m train.py: [21/300], [0/484], step: 10164, 13.806 samples/sec, batch_loss: 0.4522, batch_loss_c: 0.4802, batch_loss_s: 0.3869, time:2.8973, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:25:26 \u001b[32mINFO     \u001b[0m train.py: [21/300], [10/484], step: 10174, 2.016 samples/sec, batch_loss: 0.1197, batch_loss_c: 0.1262, batch_loss_s: 0.1044, time:19.8440, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:25:46 \u001b[32mINFO     \u001b[0m train.py: [21/300], [20/484], step: 10184, 1.987 samples/sec, batch_loss: 0.0791, batch_loss_c: 0.0716, batch_loss_s: 0.0966, time:20.1344, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:26:06 \u001b[32mINFO     \u001b[0m train.py: [21/300], [30/484], step: 10194, 1.954 samples/sec, batch_loss: 0.0823, batch_loss_c: 0.0749, batch_loss_s: 0.0995, time:20.4749, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:26:19 \u001b[32mINFO     \u001b[0m train.py: [21/300], [40/484], step: 10204, 3.116 samples/sec, batch_loss: 0.0764, batch_loss_c: 0.0697, batch_loss_s: 0.0919, time:12.8371, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:26:52 \u001b[32mINFO     \u001b[0m train.py: [21/300], [50/484], step: 10214, 1.214 samples/sec, batch_loss: 0.0667, batch_loss_c: 0.0690, batch_loss_s: 0.0611, time:32.9511, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:27:08 \u001b[32mINFO     \u001b[0m train.py: [21/300], [60/484], step: 10224, 2.463 samples/sec, batch_loss: 0.1462, batch_loss_c: 0.1152, batch_loss_s: 0.2186, time:16.2424, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:27:25 \u001b[32mINFO     \u001b[0m train.py: [21/300], [70/484], step: 10234, 2.410 samples/sec, batch_loss: 0.0817, batch_loss_c: 0.0783, batch_loss_s: 0.0897, time:16.6004, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:27:46 \u001b[32mINFO     \u001b[0m train.py: [21/300], [80/484], step: 10244, 1.947 samples/sec, batch_loss: 0.0633, batch_loss_c: 0.0563, batch_loss_s: 0.0797, time:20.5475, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:28:25 \u001b[32mINFO     \u001b[0m train.py: [21/300], [90/484], step: 10254, 1.012 samples/sec, batch_loss: 0.1259, batch_loss_c: 0.1292, batch_loss_s: 0.1184, time:39.5419, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:28:50 \u001b[32mINFO     \u001b[0m train.py: [21/300], [100/484], step: 10264, 1.615 samples/sec, batch_loss: 0.3281, batch_loss_c: 0.3126, batch_loss_s: 0.3642, time:24.7699, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:29:10 \u001b[32mINFO     \u001b[0m train.py: [21/300], [110/484], step: 10274, 2.004 samples/sec, batch_loss: 0.2099, batch_loss_c: 0.1884, batch_loss_s: 0.2600, time:19.9624, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:29:32 \u001b[32mINFO     \u001b[0m train.py: [21/300], [120/484], step: 10284, 1.781 samples/sec, batch_loss: 0.3312, batch_loss_c: 0.2973, batch_loss_s: 0.4104, time:22.4598, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:29:47 \u001b[32mINFO     \u001b[0m train.py: [21/300], [130/484], step: 10294, 2.658 samples/sec, batch_loss: 0.0734, batch_loss_c: 0.0571, batch_loss_s: 0.1116, time:15.0468, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:30:02 \u001b[32mINFO     \u001b[0m train.py: [21/300], [140/484], step: 10304, 2.692 samples/sec, batch_loss: 0.0792, batch_loss_c: 0.0785, batch_loss_s: 0.0809, time:14.8592, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:30:18 \u001b[32mINFO     \u001b[0m train.py: [21/300], [150/484], step: 10314, 2.512 samples/sec, batch_loss: 0.0529, batch_loss_c: 0.0491, batch_loss_s: 0.0620, time:15.9240, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:30:48 \u001b[32mINFO     \u001b[0m train.py: [21/300], [160/484], step: 10324, 1.329 samples/sec, batch_loss: 0.3397, batch_loss_c: 0.3300, batch_loss_s: 0.3624, time:30.0975, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:31:16 \u001b[32mINFO     \u001b[0m train.py: [21/300], [170/484], step: 10334, 1.427 samples/sec, batch_loss: 0.2873, batch_loss_c: 0.2843, batch_loss_s: 0.2942, time:28.0312, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:31:51 \u001b[32mINFO     \u001b[0m train.py: [21/300], [180/484], step: 10344, 1.150 samples/sec, batch_loss: 0.0878, batch_loss_c: 0.0729, batch_loss_s: 0.1226, time:34.7847, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:32:05 \u001b[32mINFO     \u001b[0m train.py: [21/300], [190/484], step: 10354, 2.928 samples/sec, batch_loss: 0.3384, batch_loss_c: 0.3133, batch_loss_s: 0.3971, time:13.6603, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:32:21 \u001b[32mINFO     \u001b[0m train.py: [21/300], [200/484], step: 10364, 2.419 samples/sec, batch_loss: 0.0896, batch_loss_c: 0.0682, batch_loss_s: 0.1395, time:16.5333, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:32:43 \u001b[32mINFO     \u001b[0m train.py: [21/300], [210/484], step: 10374, 1.820 samples/sec, batch_loss: 0.0691, batch_loss_c: 0.0652, batch_loss_s: 0.0783, time:21.9742, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:33:02 \u001b[32mINFO     \u001b[0m train.py: [21/300], [220/484], step: 10384, 2.169 samples/sec, batch_loss: 0.0712, batch_loss_c: 0.0690, batch_loss_s: 0.0763, time:18.4409, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:33:29 \u001b[32mINFO     \u001b[0m train.py: [21/300], [230/484], step: 10394, 1.457 samples/sec, batch_loss: 0.3891, batch_loss_c: 0.3565, batch_loss_s: 0.4654, time:27.4508, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:33:46 \u001b[32mINFO     \u001b[0m train.py: [21/300], [240/484], step: 10404, 2.351 samples/sec, batch_loss: 0.0516, batch_loss_c: 0.0417, batch_loss_s: 0.0747, time:17.0152, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:34:01 \u001b[32mINFO     \u001b[0m train.py: [21/300], [250/484], step: 10414, 2.754 samples/sec, batch_loss: 0.0728, batch_loss_c: 0.0652, batch_loss_s: 0.0905, time:14.5235, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:34:16 \u001b[32mINFO     \u001b[0m train.py: [21/300], [260/484], step: 10424, 2.661 samples/sec, batch_loss: 0.1518, batch_loss_c: 0.1476, batch_loss_s: 0.1615, time:15.0321, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:34:30 \u001b[32mINFO     \u001b[0m train.py: [21/300], [270/484], step: 10434, 2.884 samples/sec, batch_loss: 0.4292, batch_loss_c: 0.4506, batch_loss_s: 0.3791, time:13.8680, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:34:47 \u001b[32mINFO     \u001b[0m train.py: [21/300], [280/484], step: 10444, 2.288 samples/sec, batch_loss: 0.0773, batch_loss_c: 0.0642, batch_loss_s: 0.1080, time:17.4804, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:35:05 \u001b[32mINFO     \u001b[0m train.py: [21/300], [290/484], step: 10454, 2.196 samples/sec, batch_loss: 0.0733, batch_loss_c: 0.0679, batch_loss_s: 0.0859, time:18.2151, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:35:27 \u001b[32mINFO     \u001b[0m train.py: [21/300], [300/484], step: 10464, 1.853 samples/sec, batch_loss: 0.0537, batch_loss_c: 0.0463, batch_loss_s: 0.0710, time:21.5876, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:35:46 \u001b[32mINFO     \u001b[0m train.py: [21/300], [310/484], step: 10474, 2.123 samples/sec, batch_loss: 0.1137, batch_loss_c: 0.1049, batch_loss_s: 0.1345, time:18.8385, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:36:03 \u001b[32mINFO     \u001b[0m train.py: [21/300], [320/484], step: 10484, 2.373 samples/sec, batch_loss: 0.3029, batch_loss_c: 0.2953, batch_loss_s: 0.3207, time:16.8548, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:36:21 \u001b[32mINFO     \u001b[0m train.py: [21/300], [330/484], step: 10494, 2.234 samples/sec, batch_loss: 0.1594, batch_loss_c: 0.1807, batch_loss_s: 0.1095, time:17.9058, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:36:43 \u001b[32mINFO     \u001b[0m train.py: [21/300], [340/484], step: 10504, 1.775 samples/sec, batch_loss: 0.1021, batch_loss_c: 0.1032, batch_loss_s: 0.0996, time:22.5344, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:36:56 \u001b[32mINFO     \u001b[0m train.py: [21/300], [350/484], step: 10514, 3.028 samples/sec, batch_loss: 0.0636, batch_loss_c: 0.0510, batch_loss_s: 0.0931, time:13.2081, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:37:12 \u001b[32mINFO     \u001b[0m train.py: [21/300], [360/484], step: 10524, 2.523 samples/sec, batch_loss: 0.0651, batch_loss_c: 0.0570, batch_loss_s: 0.0840, time:15.8526, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:37:36 \u001b[32mINFO     \u001b[0m train.py: [21/300], [370/484], step: 10534, 1.703 samples/sec, batch_loss: 0.1287, batch_loss_c: 0.1419, batch_loss_s: 0.0978, time:23.4882, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:37:55 \u001b[32mINFO     \u001b[0m train.py: [21/300], [380/484], step: 10544, 2.092 samples/sec, batch_loss: 0.0632, batch_loss_c: 0.0525, batch_loss_s: 0.0882, time:19.1172, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:38:13 \u001b[32mINFO     \u001b[0m train.py: [21/300], [390/484], step: 10554, 2.250 samples/sec, batch_loss: 0.3038, batch_loss_c: 0.2990, batch_loss_s: 0.3148, time:17.7797, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:38:28 \u001b[32mINFO     \u001b[0m train.py: [21/300], [400/484], step: 10564, 2.626 samples/sec, batch_loss: 0.0786, batch_loss_c: 0.0775, batch_loss_s: 0.0814, time:15.2296, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:38:44 \u001b[32mINFO     \u001b[0m train.py: [21/300], [410/484], step: 10574, 2.425 samples/sec, batch_loss: 0.0474, batch_loss_c: 0.0462, batch_loss_s: 0.0502, time:16.4933, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:39:06 \u001b[32mINFO     \u001b[0m train.py: [21/300], [420/484], step: 10584, 1.875 samples/sec, batch_loss: 0.2408, batch_loss_c: 0.2227, batch_loss_s: 0.2831, time:21.3290, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:39:19 \u001b[32mINFO     \u001b[0m train.py: [21/300], [430/484], step: 10594, 2.902 samples/sec, batch_loss: 0.0553, batch_loss_c: 0.0506, batch_loss_s: 0.0665, time:13.7821, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:39:37 \u001b[32mINFO     \u001b[0m train.py: [21/300], [440/484], step: 10604, 2.232 samples/sec, batch_loss: 0.1753, batch_loss_c: 0.1720, batch_loss_s: 0.1831, time:17.9220, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:39:59 \u001b[32mINFO     \u001b[0m train.py: [21/300], [450/484], step: 10614, 1.849 samples/sec, batch_loss: 0.1556, batch_loss_c: 0.1661, batch_loss_s: 0.1309, time:21.6365, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:40:17 \u001b[32mINFO     \u001b[0m train.py: [21/300], [460/484], step: 10624, 2.206 samples/sec, batch_loss: 0.0373, batch_loss_c: 0.0311, batch_loss_s: 0.0519, time:18.1306, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:40:44 \u001b[32mINFO     \u001b[0m train.py: [21/300], [470/484], step: 10634, 1.475 samples/sec, batch_loss: 0.0796, batch_loss_c: 0.0689, batch_loss_s: 0.1044, time:27.1236, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:40:58 \u001b[32mINFO     \u001b[0m train.py: [21/300], [480/484], step: 10644, 2.940 samples/sec, batch_loss: 0.0813, batch_loss_c: 0.0792, batch_loss_s: 0.0862, time:13.6033, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:41:02 \u001b[32mINFO     \u001b[0m train.py: [21/300], train_loss: 0.1567, time: 958.4147, lr: 0.0001\u001b[0m\n",
            "2019-12-06 09:41:04 \u001b[32mINFO     \u001b[0m train.py: [22/300], [0/484], step: 10648, 25.119 samples/sec, batch_loss: 0.3996, batch_loss_c: 0.3895, batch_loss_s: 0.4230, time:1.5924, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:41:27 \u001b[32mINFO     \u001b[0m train.py: [22/300], [10/484], step: 10658, 1.675 samples/sec, batch_loss: 0.3382, batch_loss_c: 0.3379, batch_loss_s: 0.3390, time:23.8789, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:41:48 \u001b[32mINFO     \u001b[0m train.py: [22/300], [20/484], step: 10668, 1.970 samples/sec, batch_loss: 0.0573, batch_loss_c: 0.0554, batch_loss_s: 0.0618, time:20.3037, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:42:18 \u001b[32mINFO     \u001b[0m train.py: [22/300], [30/484], step: 10678, 1.317 samples/sec, batch_loss: 0.5978, batch_loss_c: 0.6108, batch_loss_s: 0.5673, time:30.3654, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:42:34 \u001b[32mINFO     \u001b[0m train.py: [22/300], [40/484], step: 10688, 2.582 samples/sec, batch_loss: 0.0867, batch_loss_c: 0.0714, batch_loss_s: 0.1226, time:15.4945, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:43:11 \u001b[32mINFO     \u001b[0m train.py: [22/300], [50/484], step: 10698, 1.058 samples/sec, batch_loss: 0.0814, batch_loss_c: 0.0812, batch_loss_s: 0.0818, time:37.8236, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:43:25 \u001b[32mINFO     \u001b[0m train.py: [22/300], [60/484], step: 10708, 2.923 samples/sec, batch_loss: 0.0657, batch_loss_c: 0.0522, batch_loss_s: 0.0972, time:13.6841, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:43:40 \u001b[32mINFO     \u001b[0m train.py: [22/300], [70/484], step: 10718, 2.741 samples/sec, batch_loss: 0.0557, batch_loss_c: 0.0505, batch_loss_s: 0.0677, time:14.5957, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:43:59 \u001b[32mINFO     \u001b[0m train.py: [22/300], [80/484], step: 10728, 2.085 samples/sec, batch_loss: 0.0911, batch_loss_c: 0.0754, batch_loss_s: 0.1279, time:19.1812, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:44:15 \u001b[32mINFO     \u001b[0m train.py: [22/300], [90/484], step: 10738, 2.502 samples/sec, batch_loss: 0.0925, batch_loss_c: 0.0804, batch_loss_s: 0.1208, time:15.9880, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:44:42 \u001b[32mINFO     \u001b[0m train.py: [22/300], [100/484], step: 10748, 1.460 samples/sec, batch_loss: 0.1159, batch_loss_c: 0.1046, batch_loss_s: 0.1425, time:27.3894, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:44:57 \u001b[32mINFO     \u001b[0m train.py: [22/300], [110/484], step: 10758, 2.743 samples/sec, batch_loss: 0.0601, batch_loss_c: 0.0562, batch_loss_s: 0.0691, time:14.5824, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:45:30 \u001b[32mINFO     \u001b[0m train.py: [22/300], [120/484], step: 10768, 1.191 samples/sec, batch_loss: 0.2648, batch_loss_c: 0.2550, batch_loss_s: 0.2879, time:33.5716, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:46:03 \u001b[32mINFO     \u001b[0m train.py: [22/300], [130/484], step: 10778, 1.240 samples/sec, batch_loss: 0.0870, batch_loss_c: 0.0840, batch_loss_s: 0.0938, time:32.2575, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:46:15 \u001b[32mINFO     \u001b[0m train.py: [22/300], [140/484], step: 10788, 3.219 samples/sec, batch_loss: 0.1258, batch_loss_c: 0.1187, batch_loss_s: 0.1425, time:12.4273, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:46:28 \u001b[32mINFO     \u001b[0m train.py: [22/300], [150/484], step: 10798, 3.005 samples/sec, batch_loss: 0.0872, batch_loss_c: 0.0774, batch_loss_s: 0.1099, time:13.3110, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:46:45 \u001b[32mINFO     \u001b[0m train.py: [22/300], [160/484], step: 10808, 2.407 samples/sec, batch_loss: 0.2389, batch_loss_c: 0.2334, batch_loss_s: 0.2518, time:16.6203, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:47:11 \u001b[32mINFO     \u001b[0m train.py: [22/300], [170/484], step: 10818, 1.522 samples/sec, batch_loss: 0.0471, batch_loss_c: 0.0338, batch_loss_s: 0.0782, time:26.2897, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:47:53 \u001b[32mINFO     \u001b[0m train.py: [22/300], [180/484], step: 10828, 0.969 samples/sec, batch_loss: 0.3107, batch_loss_c: 0.3112, batch_loss_s: 0.3094, time:41.2829, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:48:08 \u001b[32mINFO     \u001b[0m train.py: [22/300], [190/484], step: 10838, 2.611 samples/sec, batch_loss: 0.5320, batch_loss_c: 0.5310, batch_loss_s: 0.5343, time:15.3173, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:48:29 \u001b[32mINFO     \u001b[0m train.py: [22/300], [200/484], step: 10848, 1.863 samples/sec, batch_loss: 0.0910, batch_loss_c: 0.0741, batch_loss_s: 0.1306, time:21.4742, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:48:53 \u001b[32mINFO     \u001b[0m train.py: [22/300], [210/484], step: 10858, 1.731 samples/sec, batch_loss: 0.2449, batch_loss_c: 0.2405, batch_loss_s: 0.2553, time:23.1060, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:49:06 \u001b[32mINFO     \u001b[0m train.py: [22/300], [220/484], step: 10868, 3.062 samples/sec, batch_loss: 0.0823, batch_loss_c: 0.0737, batch_loss_s: 0.1025, time:13.0624, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:49:24 \u001b[32mINFO     \u001b[0m train.py: [22/300], [230/484], step: 10878, 2.119 samples/sec, batch_loss: 0.1541, batch_loss_c: 0.1469, batch_loss_s: 0.1710, time:18.8767, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:49:44 \u001b[32mINFO     \u001b[0m train.py: [22/300], [240/484], step: 10888, 2.077 samples/sec, batch_loss: 0.3261, batch_loss_c: 0.3271, batch_loss_s: 0.3240, time:19.2594, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:50:03 \u001b[32mINFO     \u001b[0m train.py: [22/300], [250/484], step: 10898, 2.076 samples/sec, batch_loss: 0.2329, batch_loss_c: 0.2216, batch_loss_s: 0.2594, time:19.2672, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:50:33 \u001b[32mINFO     \u001b[0m train.py: [22/300], [260/484], step: 10908, 1.344 samples/sec, batch_loss: 0.0426, batch_loss_c: 0.0357, batch_loss_s: 0.0588, time:29.7723, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:51:02 \u001b[32mINFO     \u001b[0m train.py: [22/300], [270/484], step: 10918, 1.382 samples/sec, batch_loss: 0.0646, batch_loss_c: 0.0507, batch_loss_s: 0.0970, time:28.9485, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:51:20 \u001b[32mINFO     \u001b[0m train.py: [22/300], [280/484], step: 10928, 2.140 samples/sec, batch_loss: 0.1388, batch_loss_c: 0.0760, batch_loss_s: 0.2853, time:18.6884, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:51:36 \u001b[32mINFO     \u001b[0m train.py: [22/300], [290/484], step: 10938, 2.576 samples/sec, batch_loss: 0.1704, batch_loss_c: 0.1915, batch_loss_s: 0.1212, time:15.5277, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:51:57 \u001b[32mINFO     \u001b[0m train.py: [22/300], [300/484], step: 10948, 1.931 samples/sec, batch_loss: 0.1106, batch_loss_c: 0.1274, batch_loss_s: 0.0714, time:20.7159, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:52:35 \u001b[32mINFO     \u001b[0m train.py: [22/300], [310/484], step: 10958, 1.048 samples/sec, batch_loss: 0.0483, batch_loss_c: 0.0436, batch_loss_s: 0.0593, time:38.1831, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:52:50 \u001b[32mINFO     \u001b[0m train.py: [22/300], [320/484], step: 10968, 2.706 samples/sec, batch_loss: 0.0503, batch_loss_c: 0.0462, batch_loss_s: 0.0600, time:14.7822, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:53:07 \u001b[32mINFO     \u001b[0m train.py: [22/300], [330/484], step: 10978, 2.349 samples/sec, batch_loss: 0.0596, batch_loss_c: 0.0488, batch_loss_s: 0.0849, time:17.0293, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:53:34 \u001b[32mINFO     \u001b[0m train.py: [22/300], [340/484], step: 10988, 1.487 samples/sec, batch_loss: 0.1607, batch_loss_c: 0.1931, batch_loss_s: 0.0852, time:26.8981, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:53:49 \u001b[32mINFO     \u001b[0m train.py: [22/300], [350/484], step: 10998, 2.579 samples/sec, batch_loss: 0.0404, batch_loss_c: 0.0313, batch_loss_s: 0.0616, time:15.5072, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:54:14 \u001b[32mINFO     \u001b[0m train.py: [22/300], [360/484], step: 11008, 1.610 samples/sec, batch_loss: 0.2840, batch_loss_c: 0.2750, batch_loss_s: 0.3050, time:24.8421, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:54:29 \u001b[32mINFO     \u001b[0m train.py: [22/300], [370/484], step: 11018, 2.738 samples/sec, batch_loss: 0.1555, batch_loss_c: 0.1648, batch_loss_s: 0.1338, time:14.6066, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:54:45 \u001b[32mINFO     \u001b[0m train.py: [22/300], [380/484], step: 11028, 2.473 samples/sec, batch_loss: 0.0684, batch_loss_c: 0.0609, batch_loss_s: 0.0858, time:16.1738, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:55:01 \u001b[32mINFO     \u001b[0m train.py: [22/300], [390/484], step: 11038, 2.533 samples/sec, batch_loss: 0.0782, batch_loss_c: 0.0721, batch_loss_s: 0.0926, time:15.7940, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:55:20 \u001b[32mINFO     \u001b[0m train.py: [22/300], [400/484], step: 11048, 2.022 samples/sec, batch_loss: 0.0663, batch_loss_c: 0.0605, batch_loss_s: 0.0799, time:19.7819, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:55:38 \u001b[32mINFO     \u001b[0m train.py: [22/300], [410/484], step: 11058, 2.267 samples/sec, batch_loss: 0.1895, batch_loss_c: 0.2025, batch_loss_s: 0.1590, time:17.6411, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:55:55 \u001b[32mINFO     \u001b[0m train.py: [22/300], [420/484], step: 11068, 2.411 samples/sec, batch_loss: 0.0484, batch_loss_c: 0.0426, batch_loss_s: 0.0617, time:16.5914, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:56:17 \u001b[32mINFO     \u001b[0m train.py: [22/300], [430/484], step: 11078, 1.796 samples/sec, batch_loss: 0.1796, batch_loss_c: 0.1898, batch_loss_s: 0.1557, time:22.2687, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:56:38 \u001b[32mINFO     \u001b[0m train.py: [22/300], [440/484], step: 11088, 1.855 samples/sec, batch_loss: 0.0816, batch_loss_c: 0.0861, batch_loss_s: 0.0708, time:21.5671, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:56:59 \u001b[32mINFO     \u001b[0m train.py: [22/300], [450/484], step: 11098, 1.952 samples/sec, batch_loss: 0.0562, batch_loss_c: 0.0516, batch_loss_s: 0.0671, time:20.4897, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:57:24 \u001b[32mINFO     \u001b[0m train.py: [22/300], [460/484], step: 11108, 1.615 samples/sec, batch_loss: 0.2340, batch_loss_c: 0.2508, batch_loss_s: 0.1947, time:24.7672, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:57:41 \u001b[32mINFO     \u001b[0m train.py: [22/300], [470/484], step: 11118, 2.286 samples/sec, batch_loss: 0.0447, batch_loss_c: 0.0381, batch_loss_s: 0.0601, time:17.4985, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:57:58 \u001b[32mINFO     \u001b[0m train.py: [22/300], [480/484], step: 11128, 2.380 samples/sec, batch_loss: 0.3446, batch_loss_c: 0.3400, batch_loss_s: 0.3552, time:16.8072, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:58:03 \u001b[32mINFO     \u001b[0m train.py: [22/300], train_loss: 0.1522, time: 1020.6679, lr: 0.0001\u001b[0m\n",
            "2019-12-06 09:58:04 \u001b[32mINFO     \u001b[0m train.py: [23/300], [0/484], step: 11132, 36.642 samples/sec, batch_loss: 0.0422, batch_loss_c: 0.0368, batch_loss_s: 0.0550, time:1.0916, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:58:27 \u001b[32mINFO     \u001b[0m train.py: [23/300], [10/484], step: 11142, 1.794 samples/sec, batch_loss: 0.0630, batch_loss_c: 0.0533, batch_loss_s: 0.0856, time:22.2912, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:58:46 \u001b[32mINFO     \u001b[0m train.py: [23/300], [20/484], step: 11152, 2.098 samples/sec, batch_loss: 0.1412, batch_loss_c: 0.1592, batch_loss_s: 0.0991, time:19.0678, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:59:07 \u001b[32mINFO     \u001b[0m train.py: [23/300], [30/484], step: 11162, 1.911 samples/sec, batch_loss: 0.3217, batch_loss_c: 0.3158, batch_loss_s: 0.3353, time:20.9331, lr:0.0001\u001b[0m\n",
            "2019-12-06 09:59:59 \u001b[32mINFO     \u001b[0m train.py: [23/300], [40/484], step: 11172, 0.756 samples/sec, batch_loss: 0.1529, batch_loss_c: 0.1462, batch_loss_s: 0.1686, time:52.9364, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:00:15 \u001b[32mINFO     \u001b[0m train.py: [23/300], [50/484], step: 11182, 2.610 samples/sec, batch_loss: 0.0716, batch_loss_c: 0.0627, batch_loss_s: 0.0926, time:15.3257, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:00:37 \u001b[32mINFO     \u001b[0m train.py: [23/300], [60/484], step: 11192, 1.811 samples/sec, batch_loss: 0.0535, batch_loss_c: 0.0413, batch_loss_s: 0.0822, time:22.0824, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:00:53 \u001b[32mINFO     \u001b[0m train.py: [23/300], [70/484], step: 11202, 2.420 samples/sec, batch_loss: 0.0713, batch_loss_c: 0.0628, batch_loss_s: 0.0911, time:16.5274, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:01:10 \u001b[32mINFO     \u001b[0m train.py: [23/300], [80/484], step: 11212, 2.393 samples/sec, batch_loss: 0.0666, batch_loss_c: 0.0519, batch_loss_s: 0.1010, time:16.7146, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:01:26 \u001b[32mINFO     \u001b[0m train.py: [23/300], [90/484], step: 11222, 2.516 samples/sec, batch_loss: 0.0601, batch_loss_c: 0.0539, batch_loss_s: 0.0745, time:15.9012, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:01:59 \u001b[32mINFO     \u001b[0m train.py: [23/300], [100/484], step: 11232, 1.216 samples/sec, batch_loss: 0.2904, batch_loss_c: 0.2878, batch_loss_s: 0.2965, time:32.8837, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:02:13 \u001b[32mINFO     \u001b[0m train.py: [23/300], [110/484], step: 11242, 2.768 samples/sec, batch_loss: 0.0779, batch_loss_c: 0.0795, batch_loss_s: 0.0741, time:14.4503, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:03:00 \u001b[32mINFO     \u001b[0m train.py: [23/300], [120/484], step: 11252, 0.855 samples/sec, batch_loss: 0.1617, batch_loss_c: 0.1851, batch_loss_s: 0.1070, time:46.7612, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:03:15 \u001b[32mINFO     \u001b[0m train.py: [23/300], [130/484], step: 11262, 2.774 samples/sec, batch_loss: 0.0687, batch_loss_c: 0.0655, batch_loss_s: 0.0764, time:14.4174, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:03:29 \u001b[32mINFO     \u001b[0m train.py: [23/300], [140/484], step: 11272, 2.816 samples/sec, batch_loss: 0.3058, batch_loss_c: 0.3048, batch_loss_s: 0.3081, time:14.2023, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:03:49 \u001b[32mINFO     \u001b[0m train.py: [23/300], [150/484], step: 11282, 1.929 samples/sec, batch_loss: 0.0723, batch_loss_c: 0.0650, batch_loss_s: 0.0894, time:20.7363, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:04:07 \u001b[32mINFO     \u001b[0m train.py: [23/300], [160/484], step: 11292, 2.221 samples/sec, batch_loss: 0.0874, batch_loss_c: 0.0835, batch_loss_s: 0.0966, time:18.0126, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:04:21 \u001b[32mINFO     \u001b[0m train.py: [23/300], [170/484], step: 11302, 3.052 samples/sec, batch_loss: 0.0891, batch_loss_c: 0.0799, batch_loss_s: 0.1105, time:13.1053, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:04:43 \u001b[32mINFO     \u001b[0m train.py: [23/300], [180/484], step: 11312, 1.797 samples/sec, batch_loss: 0.1256, batch_loss_c: 0.1172, batch_loss_s: 0.1452, time:22.2584, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:05:02 \u001b[32mINFO     \u001b[0m train.py: [23/300], [190/484], step: 11322, 2.140 samples/sec, batch_loss: 0.2825, batch_loss_c: 0.2746, batch_loss_s: 0.3009, time:18.6942, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:05:21 \u001b[32mINFO     \u001b[0m train.py: [23/300], [200/484], step: 11332, 2.110 samples/sec, batch_loss: 0.0726, batch_loss_c: 0.0570, batch_loss_s: 0.1088, time:18.9603, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:05:33 \u001b[32mINFO     \u001b[0m train.py: [23/300], [210/484], step: 11342, 3.173 samples/sec, batch_loss: 0.0554, batch_loss_c: 0.0466, batch_loss_s: 0.0760, time:12.6044, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:05:49 \u001b[32mINFO     \u001b[0m train.py: [23/300], [220/484], step: 11352, 2.529 samples/sec, batch_loss: 0.2482, batch_loss_c: 0.2154, batch_loss_s: 0.3247, time:15.8150, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:06:14 \u001b[32mINFO     \u001b[0m train.py: [23/300], [230/484], step: 11362, 1.608 samples/sec, batch_loss: 0.3156, batch_loss_c: 0.3014, batch_loss_s: 0.3486, time:24.8706, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:06:35 \u001b[32mINFO     \u001b[0m train.py: [23/300], [240/484], step: 11372, 1.855 samples/sec, batch_loss: 0.0827, batch_loss_c: 0.0792, batch_loss_s: 0.0911, time:21.5687, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:06:48 \u001b[32mINFO     \u001b[0m train.py: [23/300], [250/484], step: 11382, 3.080 samples/sec, batch_loss: 0.1908, batch_loss_c: 0.2024, batch_loss_s: 0.1639, time:12.9852, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:07:06 \u001b[32mINFO     \u001b[0m train.py: [23/300], [260/484], step: 11392, 2.286 samples/sec, batch_loss: 0.0972, batch_loss_c: 0.0822, batch_loss_s: 0.1322, time:17.4955, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:07:35 \u001b[32mINFO     \u001b[0m train.py: [23/300], [270/484], step: 11402, 1.379 samples/sec, batch_loss: 0.2671, batch_loss_c: 0.2584, batch_loss_s: 0.2871, time:29.0060, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:08:05 \u001b[32mINFO     \u001b[0m train.py: [23/300], [280/484], step: 11412, 1.327 samples/sec, batch_loss: 0.0663, batch_loss_c: 0.0643, batch_loss_s: 0.0710, time:30.1492, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:08:39 \u001b[32mINFO     \u001b[0m train.py: [23/300], [290/484], step: 11422, 1.184 samples/sec, batch_loss: 0.3031, batch_loss_c: 0.3048, batch_loss_s: 0.2990, time:33.7870, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:09:02 \u001b[32mINFO     \u001b[0m train.py: [23/300], [300/484], step: 11432, 1.742 samples/sec, batch_loss: 0.0618, batch_loss_c: 0.0583, batch_loss_s: 0.0700, time:22.9590, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:09:25 \u001b[32mINFO     \u001b[0m train.py: [23/300], [310/484], step: 11442, 1.736 samples/sec, batch_loss: 0.1294, batch_loss_c: 0.1163, batch_loss_s: 0.1598, time:23.0440, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:09:38 \u001b[32mINFO     \u001b[0m train.py: [23/300], [320/484], step: 11452, 3.108 samples/sec, batch_loss: 0.0701, batch_loss_c: 0.0691, batch_loss_s: 0.0722, time:12.8718, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:09:51 \u001b[32mINFO     \u001b[0m train.py: [23/300], [330/484], step: 11462, 2.922 samples/sec, batch_loss: 0.1011, batch_loss_c: 0.0810, batch_loss_s: 0.1479, time:13.6872, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:10:16 \u001b[32mINFO     \u001b[0m train.py: [23/300], [340/484], step: 11472, 1.646 samples/sec, batch_loss: 0.0512, batch_loss_c: 0.0442, batch_loss_s: 0.0678, time:24.3011, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:10:34 \u001b[32mINFO     \u001b[0m train.py: [23/300], [350/484], step: 11482, 2.139 samples/sec, batch_loss: 0.1503, batch_loss_c: 0.1513, batch_loss_s: 0.1482, time:18.7018, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:11:01 \u001b[32mINFO     \u001b[0m train.py: [23/300], [360/484], step: 11492, 1.502 samples/sec, batch_loss: 0.2769, batch_loss_c: 0.2737, batch_loss_s: 0.2842, time:26.6316, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:11:18 \u001b[32mINFO     \u001b[0m train.py: [23/300], [370/484], step: 11502, 2.391 samples/sec, batch_loss: 0.0800, batch_loss_c: 0.0827, batch_loss_s: 0.0736, time:16.7298, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:11:40 \u001b[32mINFO     \u001b[0m train.py: [23/300], [380/484], step: 11512, 1.765 samples/sec, batch_loss: 0.4479, batch_loss_c: 0.4189, batch_loss_s: 0.5158, time:22.6648, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:12:03 \u001b[32mINFO     \u001b[0m train.py: [23/300], [390/484], step: 11522, 1.738 samples/sec, batch_loss: 0.0813, batch_loss_c: 0.0802, batch_loss_s: 0.0837, time:23.0168, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:12:20 \u001b[32mINFO     \u001b[0m train.py: [23/300], [400/484], step: 11532, 2.468 samples/sec, batch_loss: 0.1244, batch_loss_c: 0.1111, batch_loss_s: 0.1554, time:16.2081, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:12:33 \u001b[32mINFO     \u001b[0m train.py: [23/300], [410/484], step: 11542, 2.952 samples/sec, batch_loss: 0.0584, batch_loss_c: 0.0543, batch_loss_s: 0.0679, time:13.5483, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:12:47 \u001b[32mINFO     \u001b[0m train.py: [23/300], [420/484], step: 11552, 2.814 samples/sec, batch_loss: 0.2712, batch_loss_c: 0.2627, batch_loss_s: 0.2910, time:14.2170, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:13:09 \u001b[32mINFO     \u001b[0m train.py: [23/300], [430/484], step: 11562, 1.866 samples/sec, batch_loss: 0.3239, batch_loss_c: 0.3126, batch_loss_s: 0.3503, time:21.4366, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:13:26 \u001b[32mINFO     \u001b[0m train.py: [23/300], [440/484], step: 11572, 2.274 samples/sec, batch_loss: 0.0503, batch_loss_c: 0.0435, batch_loss_s: 0.0663, time:17.5914, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:13:46 \u001b[32mINFO     \u001b[0m train.py: [23/300], [450/484], step: 11582, 2.045 samples/sec, batch_loss: 0.0665, batch_loss_c: 0.0564, batch_loss_s: 0.0900, time:19.5611, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:14:02 \u001b[32mINFO     \u001b[0m train.py: [23/300], [460/484], step: 11592, 2.576 samples/sec, batch_loss: 0.1019, batch_loss_c: 0.1130, batch_loss_s: 0.0759, time:15.5307, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:14:25 \u001b[32mINFO     \u001b[0m train.py: [23/300], [470/484], step: 11602, 1.714 samples/sec, batch_loss: 0.3204, batch_loss_c: 0.3115, batch_loss_s: 0.3413, time:23.3334, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:14:49 \u001b[32mINFO     \u001b[0m train.py: [23/300], [480/484], step: 11612, 1.661 samples/sec, batch_loss: 0.0756, batch_loss_c: 0.0644, batch_loss_s: 0.1017, time:24.0873, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:14:53 \u001b[32mINFO     \u001b[0m train.py: [23/300], train_loss: 0.1468, time: 1010.3562, lr: 0.0001\u001b[0m\n",
            "2019-12-06 10:14:55 \u001b[32mINFO     \u001b[0m train.py: [24/300], [0/484], step: 11616, 34.927 samples/sec, batch_loss: 0.0644, batch_loss_c: 0.0537, batch_loss_s: 0.0895, time:1.1453, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:15:10 \u001b[32mINFO     \u001b[0m train.py: [24/300], [10/484], step: 11626, 2.644 samples/sec, batch_loss: 0.5483, batch_loss_c: 0.5512, batch_loss_s: 0.5415, time:15.1258, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:15:47 \u001b[32mINFO     \u001b[0m train.py: [24/300], [20/484], step: 11636, 1.095 samples/sec, batch_loss: 0.2940, batch_loss_c: 0.2787, batch_loss_s: 0.3297, time:36.5299, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:16:19 \u001b[32mINFO     \u001b[0m train.py: [24/300], [30/484], step: 11646, 1.234 samples/sec, batch_loss: 0.2327, batch_loss_c: 0.2200, batch_loss_s: 0.2622, time:32.4210, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:16:50 \u001b[32mINFO     \u001b[0m train.py: [24/300], [40/484], step: 11656, 1.303 samples/sec, batch_loss: 0.3009, batch_loss_c: 0.2931, batch_loss_s: 0.3192, time:30.7066, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:17:04 \u001b[32mINFO     \u001b[0m train.py: [24/300], [50/484], step: 11666, 2.805 samples/sec, batch_loss: 0.1368, batch_loss_c: 0.1593, batch_loss_s: 0.0844, time:14.2593, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:17:39 \u001b[32mINFO     \u001b[0m train.py: [24/300], [60/484], step: 11676, 1.133 samples/sec, batch_loss: 0.3186, batch_loss_c: 0.3105, batch_loss_s: 0.3373, time:35.2941, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:17:53 \u001b[32mINFO     \u001b[0m train.py: [24/300], [70/484], step: 11686, 2.999 samples/sec, batch_loss: 0.2058, batch_loss_c: 0.2056, batch_loss_s: 0.2062, time:13.3389, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:18:25 \u001b[32mINFO     \u001b[0m train.py: [24/300], [80/484], step: 11696, 1.250 samples/sec, batch_loss: 0.2934, batch_loss_c: 0.2869, batch_loss_s: 0.3086, time:32.0086, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:18:41 \u001b[32mINFO     \u001b[0m train.py: [24/300], [90/484], step: 11706, 2.530 samples/sec, batch_loss: 0.1324, batch_loss_c: 0.1355, batch_loss_s: 0.1251, time:15.8127, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:18:58 \u001b[32mINFO     \u001b[0m train.py: [24/300], [100/484], step: 11716, 2.327 samples/sec, batch_loss: 0.2838, batch_loss_c: 0.2793, batch_loss_s: 0.2943, time:17.1930, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:19:18 \u001b[32mINFO     \u001b[0m train.py: [24/300], [110/484], step: 11726, 1.932 samples/sec, batch_loss: 0.0756, batch_loss_c: 0.0661, batch_loss_s: 0.0978, time:20.7038, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:19:31 \u001b[32mINFO     \u001b[0m train.py: [24/300], [120/484], step: 11736, 3.100 samples/sec, batch_loss: 0.1165, batch_loss_c: 0.1158, batch_loss_s: 0.1183, time:12.9047, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:19:46 \u001b[32mINFO     \u001b[0m train.py: [24/300], [130/484], step: 11746, 2.784 samples/sec, batch_loss: 0.0630, batch_loss_c: 0.0535, batch_loss_s: 0.0853, time:14.3677, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:20:04 \u001b[32mINFO     \u001b[0m train.py: [24/300], [140/484], step: 11756, 2.234 samples/sec, batch_loss: 0.1444, batch_loss_c: 0.1560, batch_loss_s: 0.1173, time:17.9073, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:20:25 \u001b[32mINFO     \u001b[0m train.py: [24/300], [150/484], step: 11766, 1.845 samples/sec, batch_loss: 0.0611, batch_loss_c: 0.0644, batch_loss_s: 0.0534, time:21.6816, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:20:43 \u001b[32mINFO     \u001b[0m train.py: [24/300], [160/484], step: 11776, 2.302 samples/sec, batch_loss: 0.0758, batch_loss_c: 0.0739, batch_loss_s: 0.0802, time:17.3727, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:20:57 \u001b[32mINFO     \u001b[0m train.py: [24/300], [170/484], step: 11786, 2.851 samples/sec, batch_loss: 0.0502, batch_loss_c: 0.0497, batch_loss_s: 0.0513, time:14.0296, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:21:16 \u001b[32mINFO     \u001b[0m train.py: [24/300], [180/484], step: 11796, 2.093 samples/sec, batch_loss: 0.0644, batch_loss_c: 0.0530, batch_loss_s: 0.0910, time:19.1142, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:21:30 \u001b[32mINFO     \u001b[0m train.py: [24/300], [190/484], step: 11806, 2.890 samples/sec, batch_loss: 0.0635, batch_loss_c: 0.0534, batch_loss_s: 0.0872, time:13.8403, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:21:44 \u001b[32mINFO     \u001b[0m train.py: [24/300], [200/484], step: 11816, 2.798 samples/sec, batch_loss: 0.0763, batch_loss_c: 0.0733, batch_loss_s: 0.0835, time:14.2967, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:22:16 \u001b[32mINFO     \u001b[0m train.py: [24/300], [210/484], step: 11826, 1.265 samples/sec, batch_loss: 0.0690, batch_loss_c: 0.0606, batch_loss_s: 0.0887, time:31.6286, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:22:44 \u001b[32mINFO     \u001b[0m train.py: [24/300], [220/484], step: 11836, 1.422 samples/sec, batch_loss: 0.1820, batch_loss_c: 0.1429, batch_loss_s: 0.2732, time:28.1217, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:22:59 \u001b[32mINFO     \u001b[0m train.py: [24/300], [230/484], step: 11846, 2.709 samples/sec, batch_loss: 0.1861, batch_loss_c: 0.1446, batch_loss_s: 0.2830, time:14.7653, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:23:13 \u001b[32mINFO     \u001b[0m train.py: [24/300], [240/484], step: 11856, 2.748 samples/sec, batch_loss: 0.0679, batch_loss_c: 0.0637, batch_loss_s: 0.0776, time:14.5560, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:23:27 \u001b[32mINFO     \u001b[0m train.py: [24/300], [250/484], step: 11866, 2.845 samples/sec, batch_loss: 0.0479, batch_loss_c: 0.0472, batch_loss_s: 0.0497, time:14.0610, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:23:50 \u001b[32mINFO     \u001b[0m train.py: [24/300], [260/484], step: 11876, 1.724 samples/sec, batch_loss: 0.0780, batch_loss_c: 0.0771, batch_loss_s: 0.0801, time:23.1980, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:24:11 \u001b[32mINFO     \u001b[0m train.py: [24/300], [270/484], step: 11886, 1.897 samples/sec, batch_loss: 0.0800, batch_loss_c: 0.0770, batch_loss_s: 0.0871, time:21.0903, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:24:25 \u001b[32mINFO     \u001b[0m train.py: [24/300], [280/484], step: 11896, 2.925 samples/sec, batch_loss: 0.0591, batch_loss_c: 0.0475, batch_loss_s: 0.0862, time:13.6757, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:24:42 \u001b[32mINFO     \u001b[0m train.py: [24/300], [290/484], step: 11906, 2.401 samples/sec, batch_loss: 0.4288, batch_loss_c: 0.3986, batch_loss_s: 0.4992, time:16.6598, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:25:18 \u001b[32mINFO     \u001b[0m train.py: [24/300], [300/484], step: 11916, 1.109 samples/sec, batch_loss: 0.1265, batch_loss_c: 0.1367, batch_loss_s: 0.1026, time:36.0667, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:25:35 \u001b[32mINFO     \u001b[0m train.py: [24/300], [310/484], step: 11926, 2.286 samples/sec, batch_loss: 0.3474, batch_loss_c: 0.3442, batch_loss_s: 0.3549, time:17.4952, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:25:52 \u001b[32mINFO     \u001b[0m train.py: [24/300], [320/484], step: 11936, 2.432 samples/sec, batch_loss: 0.0562, batch_loss_c: 0.0519, batch_loss_s: 0.0664, time:16.4500, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:26:09 \u001b[32mINFO     \u001b[0m train.py: [24/300], [330/484], step: 11946, 2.334 samples/sec, batch_loss: 0.1701, batch_loss_c: 0.1654, batch_loss_s: 0.1810, time:17.1345, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:26:23 \u001b[32mINFO     \u001b[0m train.py: [24/300], [340/484], step: 11956, 2.850 samples/sec, batch_loss: 0.0752, batch_loss_c: 0.0609, batch_loss_s: 0.1086, time:14.0374, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:26:39 \u001b[32mINFO     \u001b[0m train.py: [24/300], [350/484], step: 11966, 2.534 samples/sec, batch_loss: 0.0386, batch_loss_c: 0.0302, batch_loss_s: 0.0583, time:15.7824, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:26:56 \u001b[32mINFO     \u001b[0m train.py: [24/300], [360/484], step: 11976, 2.318 samples/sec, batch_loss: 0.0833, batch_loss_c: 0.0736, batch_loss_s: 0.1059, time:17.2577, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:27:14 \u001b[32mINFO     \u001b[0m train.py: [24/300], [370/484], step: 11986, 2.260 samples/sec, batch_loss: 0.1124, batch_loss_c: 0.1031, batch_loss_s: 0.1342, time:17.7015, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:27:28 \u001b[32mINFO     \u001b[0m train.py: [24/300], [380/484], step: 11996, 2.751 samples/sec, batch_loss: 0.3111, batch_loss_c: 0.3107, batch_loss_s: 0.3121, time:14.5391, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:27:48 \u001b[32mINFO     \u001b[0m train.py: [24/300], [390/484], step: 12006, 2.021 samples/sec, batch_loss: 0.0639, batch_loss_c: 0.0549, batch_loss_s: 0.0848, time:19.7875, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:28:06 \u001b[32mINFO     \u001b[0m train.py: [24/300], [400/484], step: 12016, 2.220 samples/sec, batch_loss: 0.1705, batch_loss_c: 0.1928, batch_loss_s: 0.1184, time:18.0180, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:28:29 \u001b[32mINFO     \u001b[0m train.py: [24/300], [410/484], step: 12026, 1.770 samples/sec, batch_loss: 0.1592, batch_loss_c: 0.1503, batch_loss_s: 0.1799, time:22.6026, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:28:44 \u001b[32mINFO     \u001b[0m train.py: [24/300], [420/484], step: 12036, 2.532 samples/sec, batch_loss: 0.2856, batch_loss_c: 0.2798, batch_loss_s: 0.2991, time:15.7960, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:28:59 \u001b[32mINFO     \u001b[0m train.py: [24/300], [430/484], step: 12046, 2.687 samples/sec, batch_loss: 0.0970, batch_loss_c: 0.0972, batch_loss_s: 0.0963, time:14.8870, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:29:25 \u001b[32mINFO     \u001b[0m train.py: [24/300], [440/484], step: 12056, 1.553 samples/sec, batch_loss: 0.0705, batch_loss_c: 0.0580, batch_loss_s: 0.0996, time:25.7516, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:29:42 \u001b[32mINFO     \u001b[0m train.py: [24/300], [450/484], step: 12066, 2.359 samples/sec, batch_loss: 0.1487, batch_loss_c: 0.1459, batch_loss_s: 0.1553, time:16.9549, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:30:09 \u001b[32mINFO     \u001b[0m train.py: [24/300], [460/484], step: 12076, 1.505 samples/sec, batch_loss: 0.0702, batch_loss_c: 0.0695, batch_loss_s: 0.0718, time:26.5765, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:30:41 \u001b[32mINFO     \u001b[0m train.py: [24/300], [470/484], step: 12086, 1.230 samples/sec, batch_loss: 0.0879, batch_loss_c: 0.0950, batch_loss_s: 0.0715, time:32.5176, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:31:01 \u001b[32mINFO     \u001b[0m train.py: [24/300], [480/484], step: 12096, 2.067 samples/sec, batch_loss: 0.0506, batch_loss_c: 0.0433, batch_loss_s: 0.0675, time:19.3551, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:31:17 \u001b[32mINFO     \u001b[0m train.py: [24/300], train_loss: 0.1493, time: 982.5948, lr: 0.0001\u001b[0m\n",
            "2019-12-06 10:31:18 \u001b[32mINFO     \u001b[0m train.py: [25/300], [0/484], step: 12100, 39.742 samples/sec, batch_loss: 0.0700, batch_loss_c: 0.0687, batch_loss_s: 0.0728, time:1.0065, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:31:41 \u001b[32mINFO     \u001b[0m train.py: [25/300], [10/484], step: 12110, 1.779 samples/sec, batch_loss: 0.0660, batch_loss_c: 0.0550, batch_loss_s: 0.0919, time:22.4804, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:31:58 \u001b[32mINFO     \u001b[0m train.py: [25/300], [20/484], step: 12120, 2.286 samples/sec, batch_loss: 0.2388, batch_loss_c: 0.2619, batch_loss_s: 0.1850, time:17.4974, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:32:13 \u001b[32mINFO     \u001b[0m train.py: [25/300], [30/484], step: 12130, 2.748 samples/sec, batch_loss: 0.1685, batch_loss_c: 0.1604, batch_loss_s: 0.1872, time:14.5555, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:32:25 \u001b[32mINFO     \u001b[0m train.py: [25/300], [40/484], step: 12140, 3.107 samples/sec, batch_loss: 0.3274, batch_loss_c: 0.3215, batch_loss_s: 0.3412, time:12.8760, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:32:49 \u001b[32mINFO     \u001b[0m train.py: [25/300], [50/484], step: 12150, 1.669 samples/sec, batch_loss: 0.0980, batch_loss_c: 0.0891, batch_loss_s: 0.1187, time:23.9724, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:33:05 \u001b[32mINFO     \u001b[0m train.py: [25/300], [60/484], step: 12160, 2.538 samples/sec, batch_loss: 0.0831, batch_loss_c: 0.0810, batch_loss_s: 0.0879, time:15.7600, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:33:23 \u001b[32mINFO     \u001b[0m train.py: [25/300], [70/484], step: 12170, 2.299 samples/sec, batch_loss: 0.1669, batch_loss_c: 0.1765, batch_loss_s: 0.1443, time:17.3980, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:33:39 \u001b[32mINFO     \u001b[0m train.py: [25/300], [80/484], step: 12180, 2.375 samples/sec, batch_loss: 0.2732, batch_loss_c: 0.2683, batch_loss_s: 0.2847, time:16.8408, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:33:55 \u001b[32mINFO     \u001b[0m train.py: [25/300], [90/484], step: 12190, 2.518 samples/sec, batch_loss: 0.0525, batch_loss_c: 0.0471, batch_loss_s: 0.0649, time:15.8850, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:34:21 \u001b[32mINFO     \u001b[0m train.py: [25/300], [100/484], step: 12200, 1.561 samples/sec, batch_loss: 0.1423, batch_loss_c: 0.1413, batch_loss_s: 0.1447, time:25.6254, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:34:54 \u001b[32mINFO     \u001b[0m train.py: [25/300], [110/484], step: 12210, 1.227 samples/sec, batch_loss: 0.3006, batch_loss_c: 0.2899, batch_loss_s: 0.3256, time:32.6078, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:35:26 \u001b[32mINFO     \u001b[0m train.py: [25/300], [120/484], step: 12220, 1.230 samples/sec, batch_loss: 0.0656, batch_loss_c: 0.0592, batch_loss_s: 0.0806, time:32.5202, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:35:44 \u001b[32mINFO     \u001b[0m train.py: [25/300], [130/484], step: 12230, 2.254 samples/sec, batch_loss: 0.0667, batch_loss_c: 0.0620, batch_loss_s: 0.0777, time:17.7436, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:36:16 \u001b[32mINFO     \u001b[0m train.py: [25/300], [140/484], step: 12240, 1.257 samples/sec, batch_loss: 0.1813, batch_loss_c: 0.1717, batch_loss_s: 0.2039, time:31.8245, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:36:43 \u001b[32mINFO     \u001b[0m train.py: [25/300], [150/484], step: 12250, 1.485 samples/sec, batch_loss: 0.3631, batch_loss_c: 0.3590, batch_loss_s: 0.3727, time:26.9348, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:37:00 \u001b[32mINFO     \u001b[0m train.py: [25/300], [160/484], step: 12260, 2.301 samples/sec, batch_loss: 0.1022, batch_loss_c: 0.0999, batch_loss_s: 0.1074, time:17.3832, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:37:32 \u001b[32mINFO     \u001b[0m train.py: [25/300], [170/484], step: 12270, 1.231 samples/sec, batch_loss: 0.2621, batch_loss_c: 0.2224, batch_loss_s: 0.3547, time:32.4922, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:37:50 \u001b[32mINFO     \u001b[0m train.py: [25/300], [180/484], step: 12280, 2.314 samples/sec, batch_loss: 0.5784, batch_loss_c: 0.5646, batch_loss_s: 0.6106, time:17.2839, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:38:18 \u001b[32mINFO     \u001b[0m train.py: [25/300], [190/484], step: 12290, 1.424 samples/sec, batch_loss: 0.0534, batch_loss_c: 0.0463, batch_loss_s: 0.0702, time:28.0843, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:38:32 \u001b[32mINFO     \u001b[0m train.py: [25/300], [200/484], step: 12300, 2.882 samples/sec, batch_loss: 0.3190, batch_loss_c: 0.3220, batch_loss_s: 0.3118, time:13.8804, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:38:51 \u001b[32mINFO     \u001b[0m train.py: [25/300], [210/484], step: 12310, 2.031 samples/sec, batch_loss: 0.0585, batch_loss_c: 0.0521, batch_loss_s: 0.0736, time:19.6923, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:39:12 \u001b[32mINFO     \u001b[0m train.py: [25/300], [220/484], step: 12320, 1.953 samples/sec, batch_loss: 0.2234, batch_loss_c: 0.1925, batch_loss_s: 0.2957, time:20.4763, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:39:29 \u001b[32mINFO     \u001b[0m train.py: [25/300], [230/484], step: 12330, 2.317 samples/sec, batch_loss: 0.3086, batch_loss_c: 0.3012, batch_loss_s: 0.3259, time:17.2669, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:39:48 \u001b[32mINFO     \u001b[0m train.py: [25/300], [240/484], step: 12340, 2.092 samples/sec, batch_loss: 0.1030, batch_loss_c: 0.1004, batch_loss_s: 0.1091, time:19.1198, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:40:03 \u001b[32mINFO     \u001b[0m train.py: [25/300], [250/484], step: 12350, 2.656 samples/sec, batch_loss: 0.0499, batch_loss_c: 0.0389, batch_loss_s: 0.0757, time:15.0587, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:40:21 \u001b[32mINFO     \u001b[0m train.py: [25/300], [260/484], step: 12360, 2.238 samples/sec, batch_loss: 0.0558, batch_loss_c: 0.0492, batch_loss_s: 0.0710, time:17.8714, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:40:38 \u001b[32mINFO     \u001b[0m train.py: [25/300], [270/484], step: 12370, 2.380 samples/sec, batch_loss: 0.0897, batch_loss_c: 0.0833, batch_loss_s: 0.1047, time:16.8068, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:40:52 \u001b[32mINFO     \u001b[0m train.py: [25/300], [280/484], step: 12380, 2.777 samples/sec, batch_loss: 0.0611, batch_loss_c: 0.0512, batch_loss_s: 0.0841, time:14.4053, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:41:08 \u001b[32mINFO     \u001b[0m train.py: [25/300], [290/484], step: 12390, 2.588 samples/sec, batch_loss: 0.1217, batch_loss_c: 0.1146, batch_loss_s: 0.1382, time:15.4570, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:41:28 \u001b[32mINFO     \u001b[0m train.py: [25/300], [300/484], step: 12400, 1.957 samples/sec, batch_loss: 0.0953, batch_loss_c: 0.0781, batch_loss_s: 0.1355, time:20.4437, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:41:44 \u001b[32mINFO     \u001b[0m train.py: [25/300], [310/484], step: 12410, 2.638 samples/sec, batch_loss: 0.1361, batch_loss_c: 0.1463, batch_loss_s: 0.1123, time:15.1613, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:42:06 \u001b[32mINFO     \u001b[0m train.py: [25/300], [320/484], step: 12420, 1.787 samples/sec, batch_loss: 0.4026, batch_loss_c: 0.4326, batch_loss_s: 0.3326, time:22.3788, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:42:28 \u001b[32mINFO     \u001b[0m train.py: [25/300], [330/484], step: 12430, 1.782 samples/sec, batch_loss: 0.1306, batch_loss_c: 0.1376, batch_loss_s: 0.1142, time:22.4457, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:42:47 \u001b[32mINFO     \u001b[0m train.py: [25/300], [340/484], step: 12440, 2.096 samples/sec, batch_loss: 0.1217, batch_loss_c: 0.1136, batch_loss_s: 0.1405, time:19.0838, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:43:04 \u001b[32mINFO     \u001b[0m train.py: [25/300], [350/484], step: 12450, 2.487 samples/sec, batch_loss: 0.0630, batch_loss_c: 0.0543, batch_loss_s: 0.0832, time:16.0859, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:43:22 \u001b[32mINFO     \u001b[0m train.py: [25/300], [360/484], step: 12460, 2.204 samples/sec, batch_loss: 0.2188, batch_loss_c: 0.1988, batch_loss_s: 0.2653, time:18.1505, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:43:49 \u001b[32mINFO     \u001b[0m train.py: [25/300], [370/484], step: 12470, 1.477 samples/sec, batch_loss: 0.0733, batch_loss_c: 0.0693, batch_loss_s: 0.0826, time:27.0735, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:44:07 \u001b[32mINFO     \u001b[0m train.py: [25/300], [380/484], step: 12480, 2.162 samples/sec, batch_loss: 0.7165, batch_loss_c: 0.7105, batch_loss_s: 0.7305, time:18.5036, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:44:23 \u001b[32mINFO     \u001b[0m train.py: [25/300], [390/484], step: 12490, 2.550 samples/sec, batch_loss: 0.1244, batch_loss_c: 0.1362, batch_loss_s: 0.0966, time:15.6844, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:44:39 \u001b[32mINFO     \u001b[0m train.py: [25/300], [400/484], step: 12500, 2.550 samples/sec, batch_loss: 0.2984, batch_loss_c: 0.2914, batch_loss_s: 0.3147, time:15.6887, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:44:53 \u001b[32mINFO     \u001b[0m train.py: [25/300], [410/484], step: 12510, 2.692 samples/sec, batch_loss: 0.2162, batch_loss_c: 0.2089, batch_loss_s: 0.2332, time:14.8578, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:45:12 \u001b[32mINFO     \u001b[0m train.py: [25/300], [420/484], step: 12520, 2.158 samples/sec, batch_loss: 0.0792, batch_loss_c: 0.0815, batch_loss_s: 0.0737, time:18.5384, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:45:38 \u001b[32mINFO     \u001b[0m train.py: [25/300], [430/484], step: 12530, 1.512 samples/sec, batch_loss: 0.1921, batch_loss_c: 0.1844, batch_loss_s: 0.2101, time:26.4584, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:45:53 \u001b[32mINFO     \u001b[0m train.py: [25/300], [440/484], step: 12540, 2.687 samples/sec, batch_loss: 0.0829, batch_loss_c: 0.0742, batch_loss_s: 0.1030, time:14.8845, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:46:08 \u001b[32mINFO     \u001b[0m train.py: [25/300], [450/484], step: 12550, 2.650 samples/sec, batch_loss: 0.0458, batch_loss_c: 0.0397, batch_loss_s: 0.0601, time:15.0959, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:46:35 \u001b[32mINFO     \u001b[0m train.py: [25/300], [460/484], step: 12560, 1.492 samples/sec, batch_loss: 0.3181, batch_loss_c: 0.3158, batch_loss_s: 0.3236, time:26.8054, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:47:01 \u001b[32mINFO     \u001b[0m train.py: [25/300], [470/484], step: 12570, 1.555 samples/sec, batch_loss: 0.0872, batch_loss_c: 0.0881, batch_loss_s: 0.0852, time:25.7201, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:47:17 \u001b[32mINFO     \u001b[0m train.py: [25/300], [480/484], step: 12580, 2.511 samples/sec, batch_loss: 0.0737, batch_loss_c: 0.0609, batch_loss_s: 0.1033, time:15.9305, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:47:21 \u001b[32mINFO     \u001b[0m train.py: [25/300], train_loss: 0.1469, time: 964.1163, lr: 0.0001\u001b[0m\n",
            "2019-12-06 10:47:23 \u001b[32mINFO     \u001b[0m train.py: [26/300], [0/484], step: 12584, 38.632 samples/sec, batch_loss: 0.0794, batch_loss_c: 0.0633, batch_loss_s: 0.1170, time:1.0354, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:47:37 \u001b[32mINFO     \u001b[0m train.py: [26/300], [10/484], step: 12594, 2.786 samples/sec, batch_loss: 0.0746, batch_loss_c: 0.0644, batch_loss_s: 0.0984, time:14.3594, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:47:58 \u001b[32mINFO     \u001b[0m train.py: [26/300], [20/484], step: 12604, 1.946 samples/sec, batch_loss: 0.2909, batch_loss_c: 0.2774, batch_loss_s: 0.3224, time:20.5516, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:48:17 \u001b[32mINFO     \u001b[0m train.py: [26/300], [30/484], step: 12614, 2.117 samples/sec, batch_loss: 0.3082, batch_loss_c: 0.3055, batch_loss_s: 0.3146, time:18.8974, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:48:32 \u001b[32mINFO     \u001b[0m train.py: [26/300], [40/484], step: 12624, 2.642 samples/sec, batch_loss: 0.0654, batch_loss_c: 0.0603, batch_loss_s: 0.0773, time:15.1374, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:48:56 \u001b[32mINFO     \u001b[0m train.py: [26/300], [50/484], step: 12634, 1.614 samples/sec, batch_loss: 0.0965, batch_loss_c: 0.0935, batch_loss_s: 0.1035, time:24.7871, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:49:15 \u001b[32mINFO     \u001b[0m train.py: [26/300], [60/484], step: 12644, 2.105 samples/sec, batch_loss: 0.1336, batch_loss_c: 0.1045, batch_loss_s: 0.2013, time:19.0064, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:49:38 \u001b[32mINFO     \u001b[0m train.py: [26/300], [70/484], step: 12654, 1.775 samples/sec, batch_loss: 0.0596, batch_loss_c: 0.0498, batch_loss_s: 0.0826, time:22.5293, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:49:57 \u001b[32mINFO     \u001b[0m train.py: [26/300], [80/484], step: 12664, 2.060 samples/sec, batch_loss: 0.2942, batch_loss_c: 0.2867, batch_loss_s: 0.3118, time:19.4131, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:50:20 \u001b[32mINFO     \u001b[0m train.py: [26/300], [90/484], step: 12674, 1.743 samples/sec, batch_loss: 0.3390, batch_loss_c: 0.3298, batch_loss_s: 0.3606, time:22.9433, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:50:53 \u001b[32mINFO     \u001b[0m train.py: [26/300], [100/484], step: 12684, 1.218 samples/sec, batch_loss: 0.0692, batch_loss_c: 0.0655, batch_loss_s: 0.0780, time:32.8397, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:51:06 \u001b[32mINFO     \u001b[0m train.py: [26/300], [110/484], step: 12694, 3.068 samples/sec, batch_loss: 0.1683, batch_loss_c: 0.1359, batch_loss_s: 0.2438, time:13.0386, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:51:24 \u001b[32mINFO     \u001b[0m train.py: [26/300], [120/484], step: 12704, 2.224 samples/sec, batch_loss: 0.1374, batch_loss_c: 0.1425, batch_loss_s: 0.1254, time:17.9843, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:51:39 \u001b[32mINFO     \u001b[0m train.py: [26/300], [130/484], step: 12714, 2.614 samples/sec, batch_loss: 0.1797, batch_loss_c: 0.1636, batch_loss_s: 0.2174, time:15.3018, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:51:55 \u001b[32mINFO     \u001b[0m train.py: [26/300], [140/484], step: 12724, 2.540 samples/sec, batch_loss: 0.3095, batch_loss_c: 0.3040, batch_loss_s: 0.3225, time:15.7475, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:52:13 \u001b[32mINFO     \u001b[0m train.py: [26/300], [150/484], step: 12734, 2.277 samples/sec, batch_loss: 0.0665, batch_loss_c: 0.0588, batch_loss_s: 0.0846, time:17.5638, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:52:41 \u001b[32mINFO     \u001b[0m train.py: [26/300], [160/484], step: 12744, 1.408 samples/sec, batch_loss: 0.0572, batch_loss_c: 0.0536, batch_loss_s: 0.0659, time:28.4028, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:53:09 \u001b[32mINFO     \u001b[0m train.py: [26/300], [170/484], step: 12754, 1.454 samples/sec, batch_loss: 0.0449, batch_loss_c: 0.0410, batch_loss_s: 0.0541, time:27.5014, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:53:23 \u001b[32mINFO     \u001b[0m train.py: [26/300], [180/484], step: 12764, 2.836 samples/sec, batch_loss: 0.2434, batch_loss_c: 0.1999, batch_loss_s: 0.3449, time:14.1062, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:53:37 \u001b[32mINFO     \u001b[0m train.py: [26/300], [190/484], step: 12774, 2.877 samples/sec, batch_loss: 0.0699, batch_loss_c: 0.0630, batch_loss_s: 0.0859, time:13.9053, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:53:51 \u001b[32mINFO     \u001b[0m train.py: [26/300], [200/484], step: 12784, 2.890 samples/sec, batch_loss: 0.1704, batch_loss_c: 0.1699, batch_loss_s: 0.1715, time:13.8385, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:54:07 \u001b[32mINFO     \u001b[0m train.py: [26/300], [210/484], step: 12794, 2.381 samples/sec, batch_loss: 0.0860, batch_loss_c: 0.0825, batch_loss_s: 0.0943, time:16.7973, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:54:37 \u001b[32mINFO     \u001b[0m train.py: [26/300], [220/484], step: 12804, 1.368 samples/sec, batch_loss: 0.2112, batch_loss_c: 0.2315, batch_loss_s: 0.1638, time:29.2415, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:55:05 \u001b[32mINFO     \u001b[0m train.py: [26/300], [230/484], step: 12814, 1.413 samples/sec, batch_loss: 0.0789, batch_loss_c: 0.0736, batch_loss_s: 0.0911, time:28.3103, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:55:20 \u001b[32mINFO     \u001b[0m train.py: [26/300], [240/484], step: 12824, 2.734 samples/sec, batch_loss: 0.0759, batch_loss_c: 0.0589, batch_loss_s: 0.1155, time:14.6322, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:55:48 \u001b[32mINFO     \u001b[0m train.py: [26/300], [250/484], step: 12834, 1.389 samples/sec, batch_loss: 0.1015, batch_loss_c: 0.0995, batch_loss_s: 0.1060, time:28.7942, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:56:14 \u001b[32mINFO     \u001b[0m train.py: [26/300], [260/484], step: 12844, 1.580 samples/sec, batch_loss: 0.0916, batch_loss_c: 0.0995, batch_loss_s: 0.0731, time:25.3088, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:56:29 \u001b[32mINFO     \u001b[0m train.py: [26/300], [270/484], step: 12854, 2.618 samples/sec, batch_loss: 0.1850, batch_loss_c: 0.1706, batch_loss_s: 0.2186, time:15.2760, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:56:46 \u001b[32mINFO     \u001b[0m train.py: [26/300], [280/484], step: 12864, 2.291 samples/sec, batch_loss: 0.0765, batch_loss_c: 0.0698, batch_loss_s: 0.0921, time:17.4587, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:57:08 \u001b[32mINFO     \u001b[0m train.py: [26/300], [290/484], step: 12874, 1.892 samples/sec, batch_loss: 0.3046, batch_loss_c: 0.2943, batch_loss_s: 0.3286, time:21.1455, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:57:28 \u001b[32mINFO     \u001b[0m train.py: [26/300], [300/484], step: 12884, 2.002 samples/sec, batch_loss: 0.0725, batch_loss_c: 0.0632, batch_loss_s: 0.0943, time:19.9811, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:57:57 \u001b[32mINFO     \u001b[0m train.py: [26/300], [310/484], step: 12894, 1.355 samples/sec, batch_loss: 0.1986, batch_loss_c: 0.1818, batch_loss_s: 0.2377, time:29.5224, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:58:13 \u001b[32mINFO     \u001b[0m train.py: [26/300], [320/484], step: 12904, 2.522 samples/sec, batch_loss: 0.1506, batch_loss_c: 0.1594, batch_loss_s: 0.1300, time:15.8613, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:58:29 \u001b[32mINFO     \u001b[0m train.py: [26/300], [330/484], step: 12914, 2.468 samples/sec, batch_loss: 0.0624, batch_loss_c: 0.0507, batch_loss_s: 0.0896, time:16.2085, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:58:45 \u001b[32mINFO     \u001b[0m train.py: [26/300], [340/484], step: 12924, 2.587 samples/sec, batch_loss: 0.3203, batch_loss_c: 0.3158, batch_loss_s: 0.3310, time:15.4615, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:59:11 \u001b[32mINFO     \u001b[0m train.py: [26/300], [350/484], step: 12934, 1.489 samples/sec, batch_loss: 0.0611, batch_loss_c: 0.0507, batch_loss_s: 0.0852, time:26.8714, lr:0.0001\u001b[0m\n",
            "2019-12-06 10:59:30 \u001b[32mINFO     \u001b[0m train.py: [26/300], [360/484], step: 12944, 2.134 samples/sec, batch_loss: 0.1231, batch_loss_c: 0.1224, batch_loss_s: 0.1247, time:18.7462, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:00:00 \u001b[32mINFO     \u001b[0m train.py: [26/300], [370/484], step: 12954, 1.320 samples/sec, batch_loss: 0.0842, batch_loss_c: 0.0911, batch_loss_s: 0.0679, time:30.2975, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:00:20 \u001b[32mINFO     \u001b[0m train.py: [26/300], [380/484], step: 12964, 2.016 samples/sec, batch_loss: 0.2965, batch_loss_c: 0.2901, batch_loss_s: 0.3117, time:19.8442, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:00:37 \u001b[32mINFO     \u001b[0m train.py: [26/300], [390/484], step: 12974, 2.425 samples/sec, batch_loss: 0.0807, batch_loss_c: 0.0761, batch_loss_s: 0.0914, time:16.4916, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:01:06 \u001b[32mINFO     \u001b[0m train.py: [26/300], [400/484], step: 12984, 1.370 samples/sec, batch_loss: 0.0639, batch_loss_c: 0.0599, batch_loss_s: 0.0733, time:29.1900, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:01:25 \u001b[32mINFO     \u001b[0m train.py: [26/300], [410/484], step: 12994, 2.089 samples/sec, batch_loss: 0.2907, batch_loss_c: 0.2889, batch_loss_s: 0.2948, time:19.1484, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:01:41 \u001b[32mINFO     \u001b[0m train.py: [26/300], [420/484], step: 13004, 2.558 samples/sec, batch_loss: 0.0580, batch_loss_c: 0.0447, batch_loss_s: 0.0891, time:15.6342, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:02:00 \u001b[32mINFO     \u001b[0m train.py: [26/300], [430/484], step: 13014, 2.072 samples/sec, batch_loss: 0.5330, batch_loss_c: 0.5262, batch_loss_s: 0.5488, time:19.3006, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:02:30 \u001b[32mINFO     \u001b[0m train.py: [26/300], [440/484], step: 13024, 1.325 samples/sec, batch_loss: 0.0484, batch_loss_c: 0.0411, batch_loss_s: 0.0656, time:30.1988, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:02:46 \u001b[32mINFO     \u001b[0m train.py: [26/300], [450/484], step: 13034, 2.540 samples/sec, batch_loss: 0.1346, batch_loss_c: 0.1488, batch_loss_s: 0.1014, time:15.7478, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:03:14 \u001b[32mINFO     \u001b[0m train.py: [26/300], [460/484], step: 13044, 1.418 samples/sec, batch_loss: 0.0686, batch_loss_c: 0.0493, batch_loss_s: 0.1136, time:28.2011, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:03:32 \u001b[32mINFO     \u001b[0m train.py: [26/300], [470/484], step: 13054, 2.279 samples/sec, batch_loss: 0.0961, batch_loss_c: 0.0893, batch_loss_s: 0.1121, time:17.5516, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:03:47 \u001b[32mINFO     \u001b[0m train.py: [26/300], [480/484], step: 13064, 2.552 samples/sec, batch_loss: 0.2828, batch_loss_c: 0.2773, batch_loss_s: 0.2957, time:15.6741, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:03:52 \u001b[32mINFO     \u001b[0m train.py: [26/300], train_loss: 0.1528, time: 989.9092, lr: 0.0001\u001b[0m\n",
            "2019-12-06 11:03:53 \u001b[32mINFO     \u001b[0m train.py: [27/300], [0/484], step: 13068, 28.806 samples/sec, batch_loss: 0.2728, batch_loss_c: 0.2377, batch_loss_s: 0.3548, time:1.3886, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:04:13 \u001b[32mINFO     \u001b[0m train.py: [27/300], [10/484], step: 13078, 2.057 samples/sec, batch_loss: 0.0712, batch_loss_c: 0.0720, batch_loss_s: 0.0693, time:19.4449, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:04:36 \u001b[32mINFO     \u001b[0m train.py: [27/300], [20/484], step: 13088, 1.725 samples/sec, batch_loss: 0.0415, batch_loss_c: 0.0384, batch_loss_s: 0.0486, time:23.1863, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:04:52 \u001b[32mINFO     \u001b[0m train.py: [27/300], [30/484], step: 13098, 2.488 samples/sec, batch_loss: 0.1571, batch_loss_c: 0.1524, batch_loss_s: 0.1680, time:16.0760, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:05:08 \u001b[32mINFO     \u001b[0m train.py: [27/300], [40/484], step: 13108, 2.517 samples/sec, batch_loss: 0.1192, batch_loss_c: 0.1189, batch_loss_s: 0.1199, time:15.8940, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:05:23 \u001b[32mINFO     \u001b[0m train.py: [27/300], [50/484], step: 13118, 2.741 samples/sec, batch_loss: 0.2981, batch_loss_c: 0.2935, batch_loss_s: 0.3088, time:14.5911, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:05:42 \u001b[32mINFO     \u001b[0m train.py: [27/300], [60/484], step: 13128, 2.039 samples/sec, batch_loss: 0.1326, batch_loss_c: 0.1292, batch_loss_s: 0.1404, time:19.6174, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:06:08 \u001b[32mINFO     \u001b[0m train.py: [27/300], [70/484], step: 13138, 1.543 samples/sec, batch_loss: 0.0830, batch_loss_c: 0.0799, batch_loss_s: 0.0900, time:25.9288, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:06:51 \u001b[32mINFO     \u001b[0m train.py: [27/300], [80/484], step: 13148, 0.945 samples/sec, batch_loss: 0.0893, batch_loss_c: 0.0750, batch_loss_s: 0.1227, time:42.3346, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:07:05 \u001b[32mINFO     \u001b[0m train.py: [27/300], [90/484], step: 13158, 2.700 samples/sec, batch_loss: 0.0717, batch_loss_c: 0.0694, batch_loss_s: 0.0772, time:14.8149, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:07:28 \u001b[32mINFO     \u001b[0m train.py: [27/300], [100/484], step: 13168, 1.744 samples/sec, batch_loss: 0.2938, batch_loss_c: 0.2858, batch_loss_s: 0.3123, time:22.9344, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:07:41 \u001b[32mINFO     \u001b[0m train.py: [27/300], [110/484], step: 13178, 3.130 samples/sec, batch_loss: 0.0676, batch_loss_c: 0.0697, batch_loss_s: 0.0626, time:12.7813, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:07:58 \u001b[32mINFO     \u001b[0m train.py: [27/300], [120/484], step: 13188, 2.424 samples/sec, batch_loss: 0.0767, batch_loss_c: 0.0742, batch_loss_s: 0.0826, time:16.5006, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:08:14 \u001b[32mINFO     \u001b[0m train.py: [27/300], [130/484], step: 13198, 2.423 samples/sec, batch_loss: 0.1677, batch_loss_c: 0.1721, batch_loss_s: 0.1573, time:16.5058, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:08:29 \u001b[32mINFO     \u001b[0m train.py: [27/300], [140/484], step: 13208, 2.653 samples/sec, batch_loss: 0.0576, batch_loss_c: 0.0528, batch_loss_s: 0.0687, time:15.0762, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:08:51 \u001b[32mINFO     \u001b[0m train.py: [27/300], [150/484], step: 13218, 1.844 samples/sec, batch_loss: 0.3212, batch_loss_c: 0.3048, batch_loss_s: 0.3595, time:21.6862, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:09:18 \u001b[32mINFO     \u001b[0m train.py: [27/300], [160/484], step: 13228, 1.463 samples/sec, batch_loss: 0.5225, batch_loss_c: 0.5210, batch_loss_s: 0.5260, time:27.3473, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:09:36 \u001b[32mINFO     \u001b[0m train.py: [27/300], [170/484], step: 13238, 2.229 samples/sec, batch_loss: 0.0564, batch_loss_c: 0.0518, batch_loss_s: 0.0670, time:17.9432, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:09:56 \u001b[32mINFO     \u001b[0m train.py: [27/300], [180/484], step: 13248, 2.063 samples/sec, batch_loss: 0.0995, batch_loss_c: 0.0943, batch_loss_s: 0.1116, time:19.3888, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:10:26 \u001b[32mINFO     \u001b[0m train.py: [27/300], [190/484], step: 13258, 1.314 samples/sec, batch_loss: 0.3016, batch_loss_c: 0.2944, batch_loss_s: 0.3184, time:30.4306, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:10:41 \u001b[32mINFO     \u001b[0m train.py: [27/300], [200/484], step: 13268, 2.707 samples/sec, batch_loss: 0.0852, batch_loss_c: 0.0764, batch_loss_s: 0.1059, time:14.7752, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:10:55 \u001b[32mINFO     \u001b[0m train.py: [27/300], [210/484], step: 13278, 2.870 samples/sec, batch_loss: 0.3109, batch_loss_c: 0.3355, batch_loss_s: 0.2536, time:13.9390, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:11:07 \u001b[32mINFO     \u001b[0m train.py: [27/300], [220/484], step: 13288, 3.251 samples/sec, batch_loss: 0.0646, batch_loss_c: 0.0575, batch_loss_s: 0.0813, time:12.3040, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:11:30 \u001b[32mINFO     \u001b[0m train.py: [27/300], [230/484], step: 13298, 1.754 samples/sec, batch_loss: 0.5264, batch_loss_c: 0.5203, batch_loss_s: 0.5408, time:22.8075, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:11:49 \u001b[32mINFO     \u001b[0m train.py: [27/300], [240/484], step: 13308, 2.054 samples/sec, batch_loss: 0.0697, batch_loss_c: 0.0565, batch_loss_s: 0.1005, time:19.4713, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:12:01 \u001b[32mINFO     \u001b[0m train.py: [27/300], [250/484], step: 13318, 3.317 samples/sec, batch_loss: 0.1122, batch_loss_c: 0.1018, batch_loss_s: 0.1366, time:12.0609, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:12:15 \u001b[32mINFO     \u001b[0m train.py: [27/300], [260/484], step: 13328, 3.025 samples/sec, batch_loss: 0.1872, batch_loss_c: 0.1678, batch_loss_s: 0.2327, time:13.2224, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:12:39 \u001b[32mINFO     \u001b[0m train.py: [27/300], [270/484], step: 13338, 1.666 samples/sec, batch_loss: 0.0689, batch_loss_c: 0.0629, batch_loss_s: 0.0828, time:24.0050, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:13:09 \u001b[32mINFO     \u001b[0m train.py: [27/300], [280/484], step: 13348, 1.321 samples/sec, batch_loss: 0.0918, batch_loss_c: 0.0764, batch_loss_s: 0.1279, time:30.2779, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:13:23 \u001b[32mINFO     \u001b[0m train.py: [27/300], [290/484], step: 13358, 2.901 samples/sec, batch_loss: 0.0779, batch_loss_c: 0.0679, batch_loss_s: 0.1015, time:13.7889, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:13:45 \u001b[32mINFO     \u001b[0m train.py: [27/300], [300/484], step: 13368, 1.758 samples/sec, batch_loss: 0.0740, batch_loss_c: 0.0627, batch_loss_s: 0.1002, time:22.7531, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:14:00 \u001b[32mINFO     \u001b[0m train.py: [27/300], [310/484], step: 13378, 2.795 samples/sec, batch_loss: 0.1036, batch_loss_c: 0.1018, batch_loss_s: 0.1079, time:14.3132, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:14:16 \u001b[32mINFO     \u001b[0m train.py: [27/300], [320/484], step: 13388, 2.508 samples/sec, batch_loss: 0.0549, batch_loss_c: 0.0436, batch_loss_s: 0.0812, time:15.9512, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:14:34 \u001b[32mINFO     \u001b[0m train.py: [27/300], [330/484], step: 13398, 2.177 samples/sec, batch_loss: 0.0636, batch_loss_c: 0.0545, batch_loss_s: 0.0846, time:18.3753, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:14:58 \u001b[32mINFO     \u001b[0m train.py: [27/300], [340/484], step: 13408, 1.691 samples/sec, batch_loss: 0.0765, batch_loss_c: 0.0680, batch_loss_s: 0.0962, time:23.6606, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:15:19 \u001b[32mINFO     \u001b[0m train.py: [27/300], [350/484], step: 13418, 1.916 samples/sec, batch_loss: 0.3227, batch_loss_c: 0.2998, batch_loss_s: 0.3764, time:20.8745, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:15:39 \u001b[32mINFO     \u001b[0m train.py: [27/300], [360/484], step: 13428, 1.966 samples/sec, batch_loss: 0.2933, batch_loss_c: 0.2896, batch_loss_s: 0.3018, time:20.3437, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:15:52 \u001b[32mINFO     \u001b[0m train.py: [27/300], [370/484], step: 13438, 2.997 samples/sec, batch_loss: 0.0591, batch_loss_c: 0.0510, batch_loss_s: 0.0781, time:13.3487, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:16:11 \u001b[32mINFO     \u001b[0m train.py: [27/300], [380/484], step: 13448, 2.155 samples/sec, batch_loss: 0.0484, batch_loss_c: 0.0384, batch_loss_s: 0.0716, time:18.5615, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:16:27 \u001b[32mINFO     \u001b[0m train.py: [27/300], [390/484], step: 13458, 2.479 samples/sec, batch_loss: 0.2158, batch_loss_c: 0.1971, batch_loss_s: 0.2596, time:16.1374, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:16:51 \u001b[32mINFO     \u001b[0m train.py: [27/300], [400/484], step: 13468, 1.634 samples/sec, batch_loss: 0.1287, batch_loss_c: 0.1210, batch_loss_s: 0.1467, time:24.4800, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:17:11 \u001b[32mINFO     \u001b[0m train.py: [27/300], [410/484], step: 13478, 2.080 samples/sec, batch_loss: 0.0675, batch_loss_c: 0.0569, batch_loss_s: 0.0924, time:19.2307, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:17:31 \u001b[32mINFO     \u001b[0m train.py: [27/300], [420/484], step: 13488, 2.005 samples/sec, batch_loss: 0.3005, batch_loss_c: 0.2977, batch_loss_s: 0.3071, time:19.9457, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:17:46 \u001b[32mINFO     \u001b[0m train.py: [27/300], [430/484], step: 13498, 2.650 samples/sec, batch_loss: 0.3065, batch_loss_c: 0.3017, batch_loss_s: 0.3178, time:15.0964, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:18:04 \u001b[32mINFO     \u001b[0m train.py: [27/300], [440/484], step: 13508, 2.219 samples/sec, batch_loss: 0.0637, batch_loss_c: 0.0587, batch_loss_s: 0.0753, time:18.0260, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:18:23 \u001b[32mINFO     \u001b[0m train.py: [27/300], [450/484], step: 13518, 2.025 samples/sec, batch_loss: 0.1018, batch_loss_c: 0.1063, batch_loss_s: 0.0913, time:19.7509, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:18:38 \u001b[32mINFO     \u001b[0m train.py: [27/300], [460/484], step: 13528, 2.827 samples/sec, batch_loss: 0.0430, batch_loss_c: 0.0392, batch_loss_s: 0.0518, time:14.1502, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:18:56 \u001b[32mINFO     \u001b[0m train.py: [27/300], [470/484], step: 13538, 2.219 samples/sec, batch_loss: 0.0892, batch_loss_c: 0.0854, batch_loss_s: 0.0982, time:18.0246, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:19:19 \u001b[32mINFO     \u001b[0m train.py: [27/300], [480/484], step: 13548, 1.749 samples/sec, batch_loss: 0.0468, batch_loss_c: 0.0412, batch_loss_s: 0.0599, time:22.8752, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:19:23 \u001b[32mINFO     \u001b[0m train.py: [27/300], train_loss: 0.1506, time: 930.6817, lr: 0.0001\u001b[0m\n",
            "2019-12-06 11:19:26 \u001b[32mINFO     \u001b[0m train.py: [28/300], [0/484], step: 13552, 15.118 samples/sec, batch_loss: 0.2036, batch_loss_c: 0.2133, batch_loss_s: 0.1810, time:2.6458, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:19:48 \u001b[32mINFO     \u001b[0m train.py: [28/300], [10/484], step: 13562, 1.841 samples/sec, batch_loss: 0.1189, batch_loss_c: 0.1208, batch_loss_s: 0.1143, time:21.7327, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:20:04 \u001b[32mINFO     \u001b[0m train.py: [28/300], [20/484], step: 13572, 2.395 samples/sec, batch_loss: 0.0489, batch_loss_c: 0.0398, batch_loss_s: 0.0701, time:16.7018, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:20:26 \u001b[32mINFO     \u001b[0m train.py: [28/300], [30/484], step: 13582, 1.824 samples/sec, batch_loss: 0.3615, batch_loss_c: 0.3573, batch_loss_s: 0.3711, time:21.9307, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:20:48 \u001b[32mINFO     \u001b[0m train.py: [28/300], [40/484], step: 13592, 1.867 samples/sec, batch_loss: 0.3075, batch_loss_c: 0.3043, batch_loss_s: 0.3147, time:21.4234, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:21:10 \u001b[32mINFO     \u001b[0m train.py: [28/300], [50/484], step: 13602, 1.769 samples/sec, batch_loss: 0.0569, batch_loss_c: 0.0375, batch_loss_s: 0.1021, time:22.6144, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:21:32 \u001b[32mINFO     \u001b[0m train.py: [28/300], [60/484], step: 13612, 1.849 samples/sec, batch_loss: 0.2361, batch_loss_c: 0.2228, batch_loss_s: 0.2670, time:21.6364, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:21:54 \u001b[32mINFO     \u001b[0m train.py: [28/300], [70/484], step: 13622, 1.803 samples/sec, batch_loss: 0.0960, batch_loss_c: 0.0864, batch_loss_s: 0.1184, time:22.1825, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:22:29 \u001b[32mINFO     \u001b[0m train.py: [28/300], [80/484], step: 13632, 1.131 samples/sec, batch_loss: 0.1771, batch_loss_c: 0.1342, batch_loss_s: 0.2770, time:35.3517, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:22:53 \u001b[32mINFO     \u001b[0m train.py: [28/300], [90/484], step: 13642, 1.702 samples/sec, batch_loss: 0.2946, batch_loss_c: 0.2879, batch_loss_s: 0.3102, time:23.5080, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:23:10 \u001b[32mINFO     \u001b[0m train.py: [28/300], [100/484], step: 13652, 2.338 samples/sec, batch_loss: 0.2882, batch_loss_c: 0.2830, batch_loss_s: 0.3003, time:17.1091, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:23:26 \u001b[32mINFO     \u001b[0m train.py: [28/300], [110/484], step: 13662, 2.495 samples/sec, batch_loss: 0.1019, batch_loss_c: 0.0932, batch_loss_s: 0.1223, time:16.0352, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:23:39 \u001b[32mINFO     \u001b[0m train.py: [28/300], [120/484], step: 13672, 3.189 samples/sec, batch_loss: 0.1611, batch_loss_c: 0.1792, batch_loss_s: 0.1188, time:12.5422, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:23:58 \u001b[32mINFO     \u001b[0m train.py: [28/300], [130/484], step: 13682, 2.110 samples/sec, batch_loss: 0.1420, batch_loss_c: 0.1507, batch_loss_s: 0.1219, time:18.9618, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:24:19 \u001b[32mINFO     \u001b[0m train.py: [28/300], [140/484], step: 13692, 1.867 samples/sec, batch_loss: 0.0447, batch_loss_c: 0.0386, batch_loss_s: 0.0591, time:21.4222, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:24:33 \u001b[32mINFO     \u001b[0m train.py: [28/300], [150/484], step: 13702, 2.940 samples/sec, batch_loss: 0.0954, batch_loss_c: 0.0882, batch_loss_s: 0.1121, time:13.6062, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:24:48 \u001b[32mINFO     \u001b[0m train.py: [28/300], [160/484], step: 13712, 2.681 samples/sec, batch_loss: 0.0586, batch_loss_c: 0.0504, batch_loss_s: 0.0779, time:14.9220, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:25:01 \u001b[32mINFO     \u001b[0m train.py: [28/300], [170/484], step: 13722, 3.051 samples/sec, batch_loss: 0.0649, batch_loss_c: 0.0445, batch_loss_s: 0.1124, time:13.1115, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:25:24 \u001b[32mINFO     \u001b[0m train.py: [28/300], [180/484], step: 13732, 1.739 samples/sec, batch_loss: 0.0430, batch_loss_c: 0.0386, batch_loss_s: 0.0532, time:23.0066, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:25:38 \u001b[32mINFO     \u001b[0m train.py: [28/300], [190/484], step: 13742, 2.729 samples/sec, batch_loss: 0.0580, batch_loss_c: 0.0458, batch_loss_s: 0.0864, time:14.6561, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:25:53 \u001b[32mINFO     \u001b[0m train.py: [28/300], [200/484], step: 13752, 2.805 samples/sec, batch_loss: 0.0456, batch_loss_c: 0.0364, batch_loss_s: 0.0672, time:14.2589, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:26:19 \u001b[32mINFO     \u001b[0m train.py: [28/300], [210/484], step: 13762, 1.528 samples/sec, batch_loss: 0.2518, batch_loss_c: 0.2923, batch_loss_s: 0.1572, time:26.1743, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:26:31 \u001b[32mINFO     \u001b[0m train.py: [28/300], [220/484], step: 13772, 3.191 samples/sec, batch_loss: 0.0464, batch_loss_c: 0.0383, batch_loss_s: 0.0654, time:12.5351, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:26:52 \u001b[32mINFO     \u001b[0m train.py: [28/300], [230/484], step: 13782, 1.941 samples/sec, batch_loss: 0.1338, batch_loss_c: 0.1623, batch_loss_s: 0.0673, time:20.6046, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:27:08 \u001b[32mINFO     \u001b[0m train.py: [28/300], [240/484], step: 13792, 2.533 samples/sec, batch_loss: 0.1679, batch_loss_c: 0.1632, batch_loss_s: 0.1788, time:15.7899, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:27:24 \u001b[32mINFO     \u001b[0m train.py: [28/300], [250/484], step: 13802, 2.385 samples/sec, batch_loss: 0.2950, batch_loss_c: 0.2874, batch_loss_s: 0.3128, time:16.7690, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:27:48 \u001b[32mINFO     \u001b[0m train.py: [28/300], [260/484], step: 13812, 1.690 samples/sec, batch_loss: 0.3016, batch_loss_c: 0.2941, batch_loss_s: 0.3191, time:23.6697, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:28:07 \u001b[32mINFO     \u001b[0m train.py: [28/300], [270/484], step: 13822, 2.170 samples/sec, batch_loss: 0.0415, batch_loss_c: 0.0370, batch_loss_s: 0.0519, time:18.4350, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:28:22 \u001b[32mINFO     \u001b[0m train.py: [28/300], [280/484], step: 13832, 2.588 samples/sec, batch_loss: 0.0721, batch_loss_c: 0.0631, batch_loss_s: 0.0932, time:15.4550, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:28:39 \u001b[32mINFO     \u001b[0m train.py: [28/300], [290/484], step: 13842, 2.315 samples/sec, batch_loss: 0.1736, batch_loss_c: 0.1642, batch_loss_s: 0.1956, time:17.2751, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:28:53 \u001b[32mINFO     \u001b[0m train.py: [28/300], [300/484], step: 13852, 2.814 samples/sec, batch_loss: 0.1325, batch_loss_c: 0.1210, batch_loss_s: 0.1591, time:14.2124, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:29:14 \u001b[32mINFO     \u001b[0m train.py: [28/300], [310/484], step: 13862, 1.934 samples/sec, batch_loss: 0.0840, batch_loss_c: 0.0745, batch_loss_s: 0.1062, time:20.6778, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:29:40 \u001b[32mINFO     \u001b[0m train.py: [28/300], [320/484], step: 13872, 1.519 samples/sec, batch_loss: 0.1674, batch_loss_c: 0.1652, batch_loss_s: 0.1728, time:26.3264, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:30:17 \u001b[32mINFO     \u001b[0m train.py: [28/300], [330/484], step: 13882, 1.092 samples/sec, batch_loss: 0.3925, batch_loss_c: 0.3818, batch_loss_s: 0.4174, time:36.6197, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:30:45 \u001b[32mINFO     \u001b[0m train.py: [28/300], [340/484], step: 13892, 1.455 samples/sec, batch_loss: 0.2686, batch_loss_c: 0.2588, batch_loss_s: 0.2913, time:27.4919, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:31:05 \u001b[32mINFO     \u001b[0m train.py: [28/300], [350/484], step: 13902, 1.923 samples/sec, batch_loss: 0.2074, batch_loss_c: 0.2066, batch_loss_s: 0.2092, time:20.8049, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:31:25 \u001b[32mINFO     \u001b[0m train.py: [28/300], [360/484], step: 13912, 2.071 samples/sec, batch_loss: 0.1003, batch_loss_c: 0.0933, batch_loss_s: 0.1168, time:19.3187, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:31:49 \u001b[32mINFO     \u001b[0m train.py: [28/300], [370/484], step: 13922, 1.646 samples/sec, batch_loss: 0.1999, batch_loss_c: 0.1931, batch_loss_s: 0.2160, time:24.3010, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:32:04 \u001b[32mINFO     \u001b[0m train.py: [28/300], [380/484], step: 13932, 2.644 samples/sec, batch_loss: 0.0532, batch_loss_c: 0.0462, batch_loss_s: 0.0695, time:15.1296, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:32:24 \u001b[32mINFO     \u001b[0m train.py: [28/300], [390/484], step: 13942, 1.976 samples/sec, batch_loss: 0.3017, batch_loss_c: 0.2961, batch_loss_s: 0.3145, time:20.2475, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:32:41 \u001b[32mINFO     \u001b[0m train.py: [28/300], [400/484], step: 13952, 2.445 samples/sec, batch_loss: 0.4580, batch_loss_c: 0.4301, batch_loss_s: 0.5231, time:16.3620, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:33:05 \u001b[32mINFO     \u001b[0m train.py: [28/300], [410/484], step: 13962, 1.680 samples/sec, batch_loss: 0.1026, batch_loss_c: 0.0999, batch_loss_s: 0.1088, time:23.8050, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:33:18 \u001b[32mINFO     \u001b[0m train.py: [28/300], [420/484], step: 13972, 3.001 samples/sec, batch_loss: 0.0676, batch_loss_c: 0.0576, batch_loss_s: 0.0909, time:13.3295, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:33:31 \u001b[32mINFO     \u001b[0m train.py: [28/300], [430/484], step: 13982, 2.947 samples/sec, batch_loss: 0.0883, batch_loss_c: 0.0888, batch_loss_s: 0.0869, time:13.5723, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:33:49 \u001b[32mINFO     \u001b[0m train.py: [28/300], [440/484], step: 13992, 2.310 samples/sec, batch_loss: 0.0610, batch_loss_c: 0.0506, batch_loss_s: 0.0851, time:17.3194, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:34:15 \u001b[32mINFO     \u001b[0m train.py: [28/300], [450/484], step: 14002, 1.539 samples/sec, batch_loss: 0.0633, batch_loss_c: 0.0579, batch_loss_s: 0.0759, time:25.9853, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:34:29 \u001b[32mINFO     \u001b[0m train.py: [28/300], [460/484], step: 14012, 2.787 samples/sec, batch_loss: 0.0862, batch_loss_c: 0.0822, batch_loss_s: 0.0954, time:14.3547, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:34:45 \u001b[32mINFO     \u001b[0m train.py: [28/300], [470/484], step: 14022, 2.566 samples/sec, batch_loss: 0.1076, batch_loss_c: 0.1113, batch_loss_s: 0.0990, time:15.5890, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:35:19 \u001b[32mINFO     \u001b[0m train.py: [28/300], [480/484], step: 14032, 1.168 samples/sec, batch_loss: 0.0570, batch_loss_c: 0.0548, batch_loss_s: 0.0620, time:34.2469, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:35:23 \u001b[32mINFO     \u001b[0m train.py: [28/300], train_loss: 0.1521, time: 959.4176, lr: 0.0001\u001b[0m\n",
            "2019-12-06 11:35:24 \u001b[32mINFO     \u001b[0m train.py: [29/300], [0/484], step: 14036, 27.526 samples/sec, batch_loss: 0.1561, batch_loss_c: 0.0960, batch_loss_s: 0.2962, time:1.4532, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:35:37 \u001b[32mINFO     \u001b[0m train.py: [29/300], [10/484], step: 14046, 3.074 samples/sec, batch_loss: 0.1130, batch_loss_c: 0.1063, batch_loss_s: 0.1287, time:13.0118, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:35:51 \u001b[32mINFO     \u001b[0m train.py: [29/300], [20/484], step: 14056, 2.926 samples/sec, batch_loss: 0.1039, batch_loss_c: 0.1015, batch_loss_s: 0.1095, time:13.6703, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:36:15 \u001b[32mINFO     \u001b[0m train.py: [29/300], [30/484], step: 14066, 1.677 samples/sec, batch_loss: 0.1313, batch_loss_c: 0.1228, batch_loss_s: 0.1512, time:23.8564, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:36:31 \u001b[32mINFO     \u001b[0m train.py: [29/300], [40/484], step: 14076, 2.533 samples/sec, batch_loss: 0.0667, batch_loss_c: 0.0542, batch_loss_s: 0.0960, time:15.7895, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:36:47 \u001b[32mINFO     \u001b[0m train.py: [29/300], [50/484], step: 14086, 2.519 samples/sec, batch_loss: 0.0573, batch_loss_c: 0.0516, batch_loss_s: 0.0706, time:15.8768, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:37:22 \u001b[32mINFO     \u001b[0m train.py: [29/300], [60/484], step: 14096, 1.133 samples/sec, batch_loss: 0.3988, batch_loss_c: 0.3751, batch_loss_s: 0.4542, time:35.3019, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:37:46 \u001b[32mINFO     \u001b[0m train.py: [29/300], [70/484], step: 14106, 1.654 samples/sec, batch_loss: 0.0974, batch_loss_c: 0.0673, batch_loss_s: 0.1676, time:24.1902, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:38:04 \u001b[32mINFO     \u001b[0m train.py: [29/300], [80/484], step: 14116, 2.207 samples/sec, batch_loss: 0.1000, batch_loss_c: 0.0776, batch_loss_s: 0.1524, time:18.1272, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:38:19 \u001b[32mINFO     \u001b[0m train.py: [29/300], [90/484], step: 14126, 2.769 samples/sec, batch_loss: 0.0818, batch_loss_c: 0.0723, batch_loss_s: 0.1039, time:14.4460, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:38:41 \u001b[32mINFO     \u001b[0m train.py: [29/300], [100/484], step: 14136, 1.806 samples/sec, batch_loss: 0.0839, batch_loss_c: 0.0830, batch_loss_s: 0.0860, time:22.1435, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:39:04 \u001b[32mINFO     \u001b[0m train.py: [29/300], [110/484], step: 14146, 1.729 samples/sec, batch_loss: 0.0475, batch_loss_c: 0.0402, batch_loss_s: 0.0644, time:23.1310, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:39:18 \u001b[32mINFO     \u001b[0m train.py: [29/300], [120/484], step: 14156, 2.844 samples/sec, batch_loss: 0.0514, batch_loss_c: 0.0463, batch_loss_s: 0.0635, time:14.0665, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:40:06 \u001b[32mINFO     \u001b[0m train.py: [29/300], [130/484], step: 14166, 0.841 samples/sec, batch_loss: 0.0947, batch_loss_c: 0.0972, batch_loss_s: 0.0888, time:47.5635, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:40:25 \u001b[32mINFO     \u001b[0m train.py: [29/300], [140/484], step: 14176, 2.113 samples/sec, batch_loss: 0.1159, batch_loss_c: 0.0967, batch_loss_s: 0.1606, time:18.9348, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:40:49 \u001b[32mINFO     \u001b[0m train.py: [29/300], [150/484], step: 14186, 1.655 samples/sec, batch_loss: 0.2933, batch_loss_c: 0.2865, batch_loss_s: 0.3091, time:24.1689, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:41:07 \u001b[32mINFO     \u001b[0m train.py: [29/300], [160/484], step: 14196, 2.189 samples/sec, batch_loss: 0.0969, batch_loss_c: 0.0912, batch_loss_s: 0.1104, time:18.2741, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:41:20 \u001b[32mINFO     \u001b[0m train.py: [29/300], [170/484], step: 14206, 2.978 samples/sec, batch_loss: 0.0870, batch_loss_c: 0.0732, batch_loss_s: 0.1194, time:13.4313, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:41:33 \u001b[32mINFO     \u001b[0m train.py: [29/300], [180/484], step: 14216, 3.100 samples/sec, batch_loss: 0.2925, batch_loss_c: 0.2846, batch_loss_s: 0.3109, time:12.9051, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:41:56 \u001b[32mINFO     \u001b[0m train.py: [29/300], [190/484], step: 14226, 1.731 samples/sec, batch_loss: 0.0557, batch_loss_c: 0.0482, batch_loss_s: 0.0731, time:23.1098, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:42:09 \u001b[32mINFO     \u001b[0m train.py: [29/300], [200/484], step: 14236, 3.215 samples/sec, batch_loss: 0.0688, batch_loss_c: 0.0609, batch_loss_s: 0.0872, time:12.4425, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:42:36 \u001b[32mINFO     \u001b[0m train.py: [29/300], [210/484], step: 14246, 1.459 samples/sec, batch_loss: 0.0591, batch_loss_c: 0.0524, batch_loss_s: 0.0748, time:27.4086, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:42:59 \u001b[32mINFO     \u001b[0m train.py: [29/300], [220/484], step: 14256, 1.749 samples/sec, batch_loss: 0.0671, batch_loss_c: 0.0490, batch_loss_s: 0.1091, time:22.8658, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:43:14 \u001b[32mINFO     \u001b[0m train.py: [29/300], [230/484], step: 14266, 2.715 samples/sec, batch_loss: 0.0594, batch_loss_c: 0.0450, batch_loss_s: 0.0929, time:14.7310, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:43:42 \u001b[32mINFO     \u001b[0m train.py: [29/300], [240/484], step: 14276, 1.451 samples/sec, batch_loss: 0.0435, batch_loss_c: 0.0361, batch_loss_s: 0.0607, time:27.5750, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:44:08 \u001b[32mINFO     \u001b[0m train.py: [29/300], [250/484], step: 14286, 1.484 samples/sec, batch_loss: 0.0447, batch_loss_c: 0.0383, batch_loss_s: 0.0597, time:26.9585, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:44:44 \u001b[32mINFO     \u001b[0m train.py: [29/300], [260/484], step: 14296, 1.131 samples/sec, batch_loss: 0.3136, batch_loss_c: 0.3065, batch_loss_s: 0.3301, time:35.3564, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:44:58 \u001b[32mINFO     \u001b[0m train.py: [29/300], [270/484], step: 14306, 2.808 samples/sec, batch_loss: 0.3010, batch_loss_c: 0.2930, batch_loss_s: 0.3195, time:14.2435, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:45:19 \u001b[32mINFO     \u001b[0m train.py: [29/300], [280/484], step: 14316, 1.907 samples/sec, batch_loss: 0.0838, batch_loss_c: 0.0767, batch_loss_s: 0.1003, time:20.9722, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:45:37 \u001b[32mINFO     \u001b[0m train.py: [29/300], [290/484], step: 14326, 2.196 samples/sec, batch_loss: 0.1242, batch_loss_c: 0.1259, batch_loss_s: 0.1201, time:18.2133, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:45:53 \u001b[32mINFO     \u001b[0m train.py: [29/300], [300/484], step: 14336, 2.589 samples/sec, batch_loss: 0.1640, batch_loss_c: 0.1727, batch_loss_s: 0.1435, time:15.4502, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:46:07 \u001b[32mINFO     \u001b[0m train.py: [29/300], [310/484], step: 14346, 2.759 samples/sec, batch_loss: 0.0566, batch_loss_c: 0.0538, batch_loss_s: 0.0630, time:14.5006, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:46:31 \u001b[32mINFO     \u001b[0m train.py: [29/300], [320/484], step: 14356, 1.696 samples/sec, batch_loss: 0.1991, batch_loss_c: 0.1811, batch_loss_s: 0.2410, time:23.5802, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:46:51 \u001b[32mINFO     \u001b[0m train.py: [29/300], [330/484], step: 14366, 2.030 samples/sec, batch_loss: 0.0640, batch_loss_c: 0.0529, batch_loss_s: 0.0900, time:19.7018, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:47:04 \u001b[32mINFO     \u001b[0m train.py: [29/300], [340/484], step: 14376, 2.884 samples/sec, batch_loss: 0.1388, batch_loss_c: 0.1072, batch_loss_s: 0.2124, time:13.8709, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:47:51 \u001b[32mINFO     \u001b[0m train.py: [29/300], [350/484], step: 14386, 0.861 samples/sec, batch_loss: 0.0965, batch_loss_c: 0.0899, batch_loss_s: 0.1117, time:46.4697, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:48:11 \u001b[32mINFO     \u001b[0m train.py: [29/300], [360/484], step: 14396, 1.967 samples/sec, batch_loss: 0.2183, batch_loss_c: 0.2032, batch_loss_s: 0.2536, time:20.3377, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:48:34 \u001b[32mINFO     \u001b[0m train.py: [29/300], [370/484], step: 14406, 1.744 samples/sec, batch_loss: 0.0902, batch_loss_c: 0.0839, batch_loss_s: 0.1049, time:22.9391, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:49:01 \u001b[32mINFO     \u001b[0m train.py: [29/300], [380/484], step: 14416, 1.484 samples/sec, batch_loss: 0.0484, batch_loss_c: 0.0441, batch_loss_s: 0.0584, time:26.9557, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:49:19 \u001b[32mINFO     \u001b[0m train.py: [29/300], [390/484], step: 14426, 2.264 samples/sec, batch_loss: 0.0598, batch_loss_c: 0.0538, batch_loss_s: 0.0737, time:17.6713, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:49:32 \u001b[32mINFO     \u001b[0m train.py: [29/300], [400/484], step: 14436, 2.960 samples/sec, batch_loss: 0.0869, batch_loss_c: 0.0789, batch_loss_s: 0.1056, time:13.5131, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:49:49 \u001b[32mINFO     \u001b[0m train.py: [29/300], [410/484], step: 14446, 2.407 samples/sec, batch_loss: 0.0586, batch_loss_c: 0.0560, batch_loss_s: 0.0645, time:16.6166, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:50:15 \u001b[32mINFO     \u001b[0m train.py: [29/300], [420/484], step: 14456, 1.539 samples/sec, batch_loss: 0.0662, batch_loss_c: 0.0611, batch_loss_s: 0.0782, time:25.9985, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:50:41 \u001b[32mINFO     \u001b[0m train.py: [29/300], [430/484], step: 14466, 1.504 samples/sec, batch_loss: 0.0899, batch_loss_c: 0.0826, batch_loss_s: 0.1070, time:26.6026, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:51:00 \u001b[32mINFO     \u001b[0m train.py: [29/300], [440/484], step: 14476, 2.212 samples/sec, batch_loss: 0.0919, batch_loss_c: 0.0968, batch_loss_s: 0.0804, time:18.0860, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:51:14 \u001b[32mINFO     \u001b[0m train.py: [29/300], [450/484], step: 14486, 2.682 samples/sec, batch_loss: 0.0531, batch_loss_c: 0.0440, batch_loss_s: 0.0743, time:14.9170, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:51:48 \u001b[32mINFO     \u001b[0m train.py: [29/300], [460/484], step: 14496, 1.180 samples/sec, batch_loss: 0.0558, batch_loss_c: 0.0488, batch_loss_s: 0.0720, time:33.9092, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:52:17 \u001b[32mINFO     \u001b[0m train.py: [29/300], [470/484], step: 14506, 1.391 samples/sec, batch_loss: 0.1756, batch_loss_c: 0.1728, batch_loss_s: 0.1821, time:28.7612, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:52:40 \u001b[32mINFO     \u001b[0m train.py: [29/300], [480/484], step: 14516, 1.784 samples/sec, batch_loss: 0.0435, batch_loss_c: 0.0380, batch_loss_s: 0.0563, time:22.4229, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:52:49 \u001b[32mINFO     \u001b[0m train.py: [29/300], train_loss: 0.1502, time: 1045.8513, lr: 0.0001\u001b[0m\n",
            "2019-12-06 11:52:51 \u001b[32mINFO     \u001b[0m train.py: [30/300], [0/484], step: 14520, 27.874 samples/sec, batch_loss: 0.1206, batch_loss_c: 0.1107, batch_loss_s: 0.1438, time:1.4350, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:53:06 \u001b[32mINFO     \u001b[0m train.py: [30/300], [10/484], step: 14530, 2.578 samples/sec, batch_loss: 0.0511, batch_loss_c: 0.0436, batch_loss_s: 0.0687, time:15.5168, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:53:26 \u001b[32mINFO     \u001b[0m train.py: [30/300], [20/484], step: 14540, 1.985 samples/sec, batch_loss: 0.3308, batch_loss_c: 0.3239, batch_loss_s: 0.3469, time:20.1545, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:53:42 \u001b[32mINFO     \u001b[0m train.py: [30/300], [30/484], step: 14550, 2.619 samples/sec, batch_loss: 0.0955, batch_loss_c: 0.0828, batch_loss_s: 0.1250, time:15.2746, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:53:58 \u001b[32mINFO     \u001b[0m train.py: [30/300], [40/484], step: 14560, 2.439 samples/sec, batch_loss: 0.3000, batch_loss_c: 0.2929, batch_loss_s: 0.3165, time:16.4003, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:54:15 \u001b[32mINFO     \u001b[0m train.py: [30/300], [50/484], step: 14570, 2.325 samples/sec, batch_loss: 0.0505, batch_loss_c: 0.0469, batch_loss_s: 0.0590, time:17.2061, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:54:29 \u001b[32mINFO     \u001b[0m train.py: [30/300], [60/484], step: 14580, 3.021 samples/sec, batch_loss: 0.0963, batch_loss_c: 0.0829, batch_loss_s: 0.1274, time:13.2395, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:54:45 \u001b[32mINFO     \u001b[0m train.py: [30/300], [70/484], step: 14590, 2.465 samples/sec, batch_loss: 0.1028, batch_loss_c: 0.0852, batch_loss_s: 0.1439, time:16.2271, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:55:00 \u001b[32mINFO     \u001b[0m train.py: [30/300], [80/484], step: 14600, 2.706 samples/sec, batch_loss: 0.1286, batch_loss_c: 0.1224, batch_loss_s: 0.1431, time:14.7806, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:55:20 \u001b[32mINFO     \u001b[0m train.py: [30/300], [90/484], step: 14610, 1.954 samples/sec, batch_loss: 0.0717, batch_loss_c: 0.0578, batch_loss_s: 0.1042, time:20.4739, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:55:40 \u001b[32mINFO     \u001b[0m train.py: [30/300], [100/484], step: 14620, 1.989 samples/sec, batch_loss: 0.0949, batch_loss_c: 0.0833, batch_loss_s: 0.1219, time:20.1125, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:56:09 \u001b[32mINFO     \u001b[0m train.py: [30/300], [110/484], step: 14630, 1.397 samples/sec, batch_loss: 0.0789, batch_loss_c: 0.0746, batch_loss_s: 0.0890, time:28.6339, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:56:28 \u001b[32mINFO     \u001b[0m train.py: [30/300], [120/484], step: 14640, 2.061 samples/sec, batch_loss: 0.0469, batch_loss_c: 0.0412, batch_loss_s: 0.0604, time:19.4120, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:56:43 \u001b[32mINFO     \u001b[0m train.py: [30/300], [130/484], step: 14650, 2.760 samples/sec, batch_loss: 0.0907, batch_loss_c: 0.0904, batch_loss_s: 0.0915, time:14.4911, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:57:13 \u001b[32mINFO     \u001b[0m train.py: [30/300], [140/484], step: 14660, 1.333 samples/sec, batch_loss: 0.0609, batch_loss_c: 0.0537, batch_loss_s: 0.0776, time:30.0072, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:57:40 \u001b[32mINFO     \u001b[0m train.py: [30/300], [150/484], step: 14670, 1.477 samples/sec, batch_loss: 0.2492, batch_loss_c: 0.2218, batch_loss_s: 0.3133, time:27.0847, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:57:58 \u001b[32mINFO     \u001b[0m train.py: [30/300], [160/484], step: 14680, 2.246 samples/sec, batch_loss: 0.0561, batch_loss_c: 0.0494, batch_loss_s: 0.0717, time:17.8117, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:58:16 \u001b[32mINFO     \u001b[0m train.py: [30/300], [170/484], step: 14690, 2.199 samples/sec, batch_loss: 0.0666, batch_loss_c: 0.0645, batch_loss_s: 0.0716, time:18.1904, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:58:32 \u001b[32mINFO     \u001b[0m train.py: [30/300], [180/484], step: 14700, 2.496 samples/sec, batch_loss: 0.3301, batch_loss_c: 0.3396, batch_loss_s: 0.3081, time:16.0264, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:58:46 \u001b[32mINFO     \u001b[0m train.py: [30/300], [190/484], step: 14710, 2.874 samples/sec, batch_loss: 0.0541, batch_loss_c: 0.0505, batch_loss_s: 0.0622, time:13.9190, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:59:19 \u001b[32mINFO     \u001b[0m train.py: [30/300], [200/484], step: 14720, 1.221 samples/sec, batch_loss: 0.0876, batch_loss_c: 0.0831, batch_loss_s: 0.0980, time:32.7499, lr:0.0001\u001b[0m\n",
            "2019-12-06 11:59:50 \u001b[32mINFO     \u001b[0m train.py: [30/300], [210/484], step: 14730, 1.274 samples/sec, batch_loss: 0.0799, batch_loss_c: 0.0751, batch_loss_s: 0.0910, time:31.3952, lr:0.0001\u001b[0m\n",
            "2019-12-06 12:00:02 \u001b[32mINFO     \u001b[0m train.py: [30/300], [220/484], step: 14740, 3.248 samples/sec, batch_loss: 0.0833, batch_loss_c: 0.0725, batch_loss_s: 0.1083, time:12.3137, lr:0.0001\u001b[0m\n",
            "2019-12-06 12:00:18 \u001b[32mINFO     \u001b[0m train.py: [30/300], [230/484], step: 14750, 2.569 samples/sec, batch_loss: 0.0533, batch_loss_c: 0.0514, batch_loss_s: 0.0576, time:15.5674, lr:0.0001\u001b[0m\n",
            "2019-12-06 12:00:31 \u001b[32mINFO     \u001b[0m train.py: [30/300], [240/484], step: 14760, 3.049 samples/sec, batch_loss: 0.0585, batch_loss_c: 0.0505, batch_loss_s: 0.0774, time:13.1201, lr:0.0001\u001b[0m\n",
            "2019-12-06 12:00:58 \u001b[32mINFO     \u001b[0m train.py: [30/300], [250/484], step: 14770, 1.460 samples/sec, batch_loss: 0.0901, batch_loss_c: 0.0949, batch_loss_s: 0.0787, time:27.3978, lr:0.0001\u001b[0m\n",
            "2019-12-06 12:01:29 \u001b[32mINFO     \u001b[0m train.py: [30/300], [260/484], step: 14780, 1.288 samples/sec, batch_loss: 0.1126, batch_loss_c: 0.1205, batch_loss_s: 0.0943, time:31.0575, lr:0.0001\u001b[0m\n",
            "2019-12-06 12:01:46 \u001b[32mINFO     \u001b[0m train.py: [30/300], [270/484], step: 14790, 2.364 samples/sec, batch_loss: 0.0814, batch_loss_c: 0.0718, batch_loss_s: 0.1041, time:16.9180, lr:0.0001\u001b[0m\n",
            "2019-12-06 12:02:06 \u001b[32mINFO     \u001b[0m train.py: [30/300], [280/484], step: 14800, 2.002 samples/sec, batch_loss: 0.0534, batch_loss_c: 0.0471, batch_loss_s: 0.0681, time:19.9795, lr:0.0001\u001b[0m\n",
            "2019-12-06 12:02:27 \u001b[32mINFO     \u001b[0m train.py: [30/300], [290/484], step: 14810, 1.924 samples/sec, batch_loss: 0.0600, batch_loss_c: 0.0570, batch_loss_s: 0.0669, time:20.7868, lr:0.0001\u001b[0m\n",
            "2019-12-06 12:02:50 \u001b[32mINFO     \u001b[0m train.py: [30/300], [300/484], step: 14820, 1.775 samples/sec, batch_loss: 0.0892, batch_loss_c: 0.0723, batch_loss_s: 0.1288, time:22.5372, lr:0.0001\u001b[0m\n",
            "2019-12-06 12:03:07 \u001b[32mINFO     \u001b[0m train.py: [30/300], [310/484], step: 14830, 2.367 samples/sec, batch_loss: 0.5222, batch_loss_c: 0.5184, batch_loss_s: 0.5313, time:16.9003, lr:0.0001\u001b[0m\n",
            "2019-12-06 12:03:22 \u001b[32mINFO     \u001b[0m train.py: [30/300], [320/484], step: 14840, 2.606 samples/sec, batch_loss: 0.0685, batch_loss_c: 0.0605, batch_loss_s: 0.0872, time:15.3478, lr:0.0001\u001b[0m\n",
            "2019-12-06 12:03:53 \u001b[32mINFO     \u001b[0m train.py: [30/300], [330/484], step: 14850, 1.291 samples/sec, batch_loss: 0.1433, batch_loss_c: 0.1442, batch_loss_s: 0.1410, time:30.9897, lr:0.0001\u001b[0m\n",
            "2019-12-06 12:04:16 \u001b[32mINFO     \u001b[0m train.py: [30/300], [340/484], step: 14860, 1.693 samples/sec, batch_loss: 0.5308, batch_loss_c: 0.5276, batch_loss_s: 0.5383, time:23.6310, lr:0.0001\u001b[0m\n",
            "2019-12-06 12:04:41 \u001b[32mINFO     \u001b[0m train.py: [30/300], [350/484], step: 14870, 1.657 samples/sec, batch_loss: 0.1410, batch_loss_c: 0.1069, batch_loss_s: 0.2204, time:24.1393, lr:0.0001\u001b[0m\n",
            "2019-12-06 12:05:00 \u001b[32mINFO     \u001b[0m train.py: [30/300], [360/484], step: 14880, 2.092 samples/sec, batch_loss: 0.1212, batch_loss_c: 0.1287, batch_loss_s: 0.1037, time:19.1212, lr:0.0001\u001b[0m\n",
            "tcmalloc: large alloc 6326804480 bytes == 0x7fe8e0fba000 @  0x7feeb13d01e7 0x7feea59b4f71 0x7feea5a1855d 0x7feea5a1be28 0x7feea5a1c3e5 0x7feea5ab2fc2 0x50ac25 0x50d390 0x508245 0x50a080 0x50aa7d 0x50d390 0x509455 0x595311 0x5a067e 0x557d28 0x50c88b 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x509455 0x595311 0x5a522c 0x557f7e 0x4b5eff 0x50c810\n",
            "2019-12-06 12:05:55 \u001b[32mINFO     \u001b[0m train.py: [30/300], [370/484], step: 14890, 0.719 samples/sec, batch_loss: 0.3991, batch_loss_c: 0.4077, batch_loss_s: 0.3792, time:55.6598, lr:0.0001\u001b[0m\n",
            "2019-12-06 12:06:20 \u001b[32mINFO     \u001b[0m train.py: [30/300], [380/484], step: 14900, 1.625 samples/sec, batch_loss: 0.0610, batch_loss_c: 0.0507, batch_loss_s: 0.0850, time:24.6148, lr:0.0001\u001b[0m\n",
            "2019-12-06 12:06:36 \u001b[32mINFO     \u001b[0m train.py: [30/300], [390/484], step: 14910, 2.545 samples/sec, batch_loss: 0.5502, batch_loss_c: 0.5416, batch_loss_s: 0.5703, time:15.7175, lr:0.0001\u001b[0m\n",
            "2019-12-06 12:06:59 \u001b[32mINFO     \u001b[0m train.py: [30/300], [400/484], step: 14920, 1.752 samples/sec, batch_loss: 0.0667, batch_loss_c: 0.0542, batch_loss_s: 0.0960, time:22.8278, lr:0.0001\u001b[0m\n",
            "2019-12-06 12:07:19 \u001b[32mINFO     \u001b[0m train.py: [30/300], [410/484], step: 14930, 1.952 samples/sec, batch_loss: 0.0845, batch_loss_c: 0.0770, batch_loss_s: 0.1021, time:20.4879, lr:0.0001\u001b[0m\n",
            "2019-12-06 12:07:36 \u001b[32mINFO     \u001b[0m train.py: [30/300], [420/484], step: 14940, 2.413 samples/sec, batch_loss: 0.0590, batch_loss_c: 0.0527, batch_loss_s: 0.0738, time:16.5735, lr:0.0001\u001b[0m\n",
            "2019-12-06 12:07:48 \u001b[32mINFO     \u001b[0m train.py: [30/300], [430/484], step: 14950, 3.277 samples/sec, batch_loss: 0.0796, batch_loss_c: 0.0762, batch_loss_s: 0.0876, time:12.2070, lr:0.0001\u001b[0m\n",
            "2019-12-06 12:08:16 \u001b[32mINFO     \u001b[0m train.py: [30/300], [440/484], step: 14960, 1.435 samples/sec, batch_loss: 0.1646, batch_loss_c: 0.1494, batch_loss_s: 0.2000, time:27.8825, lr:0.0001\u001b[0m\n",
            "2019-12-06 12:08:32 \u001b[32mINFO     \u001b[0m train.py: [30/300], [450/484], step: 14970, 2.408 samples/sec, batch_loss: 0.0723, batch_loss_c: 0.0674, batch_loss_s: 0.0836, time:16.6105, lr:0.0001\u001b[0m\n",
            "2019-12-06 12:08:47 \u001b[32mINFO     \u001b[0m train.py: [30/300], [460/484], step: 14980, 2.722 samples/sec, batch_loss: 0.1798, batch_loss_c: 0.2167, batch_loss_s: 0.0936, time:14.6932, lr:0.0001\u001b[0m\n",
            "2019-12-06 12:09:04 \u001b[32mINFO     \u001b[0m train.py: [30/300], [470/484], step: 14990, 2.306 samples/sec, batch_loss: 0.0601, batch_loss_c: 0.0548, batch_loss_s: 0.0724, time:17.3473, lr:0.0001\u001b[0m\n",
            "2019-12-06 12:09:35 \u001b[32mINFO     \u001b[0m train.py: [30/300], [480/484], step: 15000, 1.328 samples/sec, batch_loss: 0.1075, batch_loss_c: 0.0625, batch_loss_s: 0.2123, time:30.1299, lr:0.0001\u001b[0m\n",
            "2019-12-06 12:09:38 \u001b[32mINFO     \u001b[0m train.py: [30/300], train_loss: 0.1450, time: 1008.6246, lr: 0.0001\u001b[0m\n",
            "2019-12-06 12:09:40 \u001b[32mINFO     \u001b[0m train.py: [31/300], [0/484], step: 15004, 33.180 samples/sec, batch_loss: 0.0988, batch_loss_c: 0.0957, batch_loss_s: 0.1061, time:1.2056, lr:0.0001\u001b[0m\n",
            "2019-12-06 12:09:53 \u001b[32mINFO     \u001b[0m train.py: [31/300], [10/484], step: 15014, 3.101 samples/sec, batch_loss: 0.1033, batch_loss_c: 0.0952, batch_loss_s: 0.1221, time:12.8975, lr:0.0001\u001b[0m\n",
            "2019-12-06 12:10:18 \u001b[32mINFO     \u001b[0m train.py: [31/300], [20/484], step: 15024, 1.608 samples/sec, batch_loss: 0.0712, batch_loss_c: 0.0759, batch_loss_s: 0.0602, time:24.8761, lr:0.0001\u001b[0m\n",
            "2019-12-06 12:10:36 \u001b[32mINFO     \u001b[0m train.py: [31/300], [30/484], step: 15034, 2.117 samples/sec, batch_loss: 0.0625, batch_loss_c: 0.0543, batch_loss_s: 0.0818, time:18.8931, lr:0.0001\u001b[0m\n",
            "2019-12-06 12:11:18 \u001b[32mINFO     \u001b[0m train.py: [31/300], [40/484], step: 15044, 0.951 samples/sec, batch_loss: 0.0846, batch_loss_c: 0.0773, batch_loss_s: 0.1016, time:42.0708, lr:0.0001\u001b[0m\n",
            "2019-12-06 12:11:35 \u001b[32mINFO     \u001b[0m train.py: [31/300], [50/484], step: 15054, 2.364 samples/sec, batch_loss: 0.0501, batch_loss_c: 0.0412, batch_loss_s: 0.0710, time:16.9194, lr:0.0001\u001b[0m\n",
            "2019-12-06 12:11:55 \u001b[32mINFO     \u001b[0m train.py: [31/300], [60/484], step: 15064, 2.013 samples/sec, batch_loss: 0.0330, batch_loss_c: 0.0258, batch_loss_s: 0.0498, time:19.8673, lr:0.0001\u001b[0m\n",
            "2019-12-06 12:12:31 \u001b[32mINFO     \u001b[0m train.py: [31/300], [70/484], step: 15074, 1.106 samples/sec, batch_loss: 0.5356, batch_loss_c: 0.5347, batch_loss_s: 0.5376, time:36.1794, lr:0.0001\u001b[0m\n",
            "2019-12-06 12:12:58 \u001b[32mINFO     \u001b[0m train.py: [31/300], [80/484], step: 15084, 1.513 samples/sec, batch_loss: 0.0503, batch_loss_c: 0.0449, batch_loss_s: 0.0628, time:26.4404, lr:0.0001\u001b[0m\n",
            "2019-12-06 12:13:12 \u001b[32mINFO     \u001b[0m train.py: [31/300], [90/484], step: 15094, 2.881 samples/sec, batch_loss: 0.0683, batch_loss_c: 0.0606, batch_loss_s: 0.0863, time:13.8837, lr:0.0001\u001b[0m\n",
            "2019-12-06 12:13:37 \u001b[32mINFO     \u001b[0m train.py: [31/300], [100/484], step: 15104, 1.604 samples/sec, batch_loss: 0.1119, batch_loss_c: 0.1171, batch_loss_s: 0.0998, time:24.9304, lr:0.0001\u001b[0m\n",
            "2019-12-06 12:13:56 \u001b[32mINFO     \u001b[0m train.py: [31/300], [110/484], step: 15114, 2.038 samples/sec, batch_loss: 0.0779, batch_loss_c: 0.0420, batch_loss_s: 0.1617, time:19.6304, lr:0.0001\u001b[0m\n",
            "2019-12-06 12:14:14 \u001b[32mINFO     \u001b[0m train.py: [31/300], [120/484], step: 15124, 2.286 samples/sec, batch_loss: 0.3307, batch_loss_c: 0.2702, batch_loss_s: 0.4720, time:17.4975, lr:0.0001\u001b[0m\n",
            "2019-12-06 12:14:29 \u001b[32mINFO     \u001b[0m train.py: [31/300], [130/484], step: 15134, 2.709 samples/sec, batch_loss: 0.0840, batch_loss_c: 0.0764, batch_loss_s: 0.1020, time:14.7649, lr:0.0001\u001b[0m\n",
            "2019-12-06 12:14:45 \u001b[32mINFO     \u001b[0m train.py: [31/300], [140/484], step: 15144, 2.416 samples/sec, batch_loss: 0.0682, batch_loss_c: 0.0634, batch_loss_s: 0.0794, time:16.5596, lr:0.0001\u001b[0m\n",
            "2019-12-06 12:15:15 \u001b[32mINFO     \u001b[0m train.py: [31/300], [150/484], step: 15154, 1.328 samples/sec, batch_loss: 0.1307, batch_loss_c: 0.1356, batch_loss_s: 0.1191, time:30.1218, lr:0.0001\u001b[0m\n",
            "2019-12-06 12:15:43 \u001b[32mINFO     \u001b[0m train.py: [31/300], [160/484], step: 15164, 1.461 samples/sec, batch_loss: 0.0595, batch_loss_c: 0.0530, batch_loss_s: 0.0748, time:27.3733, lr:0.0001\u001b[0m\n",
            "2019-12-06 12:16:02 \u001b[32mINFO     \u001b[0m train.py: [31/300], [170/484], step: 15174, 2.104 samples/sec, batch_loss: 0.2814, batch_loss_c: 0.2767, batch_loss_s: 0.2923, time:19.0148, lr:0.0001\u001b[0m\n",
            "2019-12-06 12:16:22 \u001b[32mINFO     \u001b[0m train.py: [31/300], [180/484], step: 15184, 1.982 samples/sec, batch_loss: 0.4022, batch_loss_c: 0.3977, batch_loss_s: 0.4128, time:20.1794, lr:0.0001\u001b[0m\n",
            "2019-12-06 12:16:36 \u001b[32mINFO     \u001b[0m train.py: [31/300], [190/484], step: 15194, 2.876 samples/sec, batch_loss: 0.3060, batch_loss_c: 0.2984, batch_loss_s: 0.3238, time:13.9093, lr:0.0001\u001b[0m\n",
            "2019-12-06 12:17:08 \u001b[32mINFO     \u001b[0m train.py: [31/300], [200/484], step: 15204, 1.232 samples/sec, batch_loss: 0.5243, batch_loss_c: 0.5143, batch_loss_s: 0.5478, time:32.4800, lr:0.0001\u001b[0m\n",
            "2019-12-06 12:17:25 \u001b[32mINFO     \u001b[0m train.py: [31/300], [210/484], step: 15214, 2.382 samples/sec, batch_loss: 0.2940, batch_loss_c: 0.2803, batch_loss_s: 0.3259, time:16.7921, lr:0.0001\u001b[0m\n",
            "2019-12-06 12:17:45 \u001b[32mINFO     \u001b[0m train.py: [31/300], [220/484], step: 15224, 2.006 samples/sec, batch_loss: 0.0464, batch_loss_c: 0.0401, batch_loss_s: 0.0611, time:19.9399, lr:0.0001\u001b[0m\n",
            "2019-12-06 12:18:03 \u001b[32mINFO     \u001b[0m train.py: [31/300], [230/484], step: 15234, 2.242 samples/sec, batch_loss: 0.0819, batch_loss_c: 0.0886, batch_loss_s: 0.0665, time:17.8445, lr:0.0001\u001b[0m\n",
            "2019-12-06 12:18:20 \u001b[32mINFO     \u001b[0m train.py: [31/300], [240/484], step: 15244, 2.323 samples/sec, batch_loss: 0.1322, batch_loss_c: 0.1358, batch_loss_s: 0.1236, time:17.2164, lr:0.0001\u001b[0m\n",
            "2019-12-06 12:18:36 \u001b[32mINFO     \u001b[0m train.py: [31/300], [250/484], step: 15254, 2.462 samples/sec, batch_loss: 0.0729, batch_loss_c: 0.0596, batch_loss_s: 0.1040, time:16.2497, lr:0.0001\u001b[0m\n",
            "2019-12-06 12:18:50 \u001b[32mINFO     \u001b[0m train.py: [31/300], [260/484], step: 15264, 2.845 samples/sec, batch_loss: 0.1037, batch_loss_c: 0.1085, batch_loss_s: 0.0927, time:14.0573, lr:0.0001\u001b[0m\n",
            "2019-12-06 12:19:06 \u001b[32mINFO     \u001b[0m train.py: [31/300], [270/484], step: 15274, 2.513 samples/sec, batch_loss: 0.3922, batch_loss_c: 0.3710, batch_loss_s: 0.4417, time:15.9196, lr:0.0001\u001b[0m\n",
            "2019-12-06 12:19:25 \u001b[32mINFO     \u001b[0m train.py: [31/300], [280/484], step: 15284, 2.152 samples/sec, batch_loss: 0.0622, batch_loss_c: 0.0483, batch_loss_s: 0.0947, time:18.5876, lr:0.0001\u001b[0m\n",
            "2019-12-06 12:19:41 \u001b[32mINFO     \u001b[0m train.py: [31/300], [290/484], step: 15294, 2.451 samples/sec, batch_loss: 0.2887, batch_loss_c: 0.2841, batch_loss_s: 0.2995, time:16.3216, lr:0.0001\u001b[0m\n",
            "2019-12-06 12:20:07 \u001b[32mINFO     \u001b[0m train.py: [31/300], [300/484], step: 15304, 1.521 samples/sec, batch_loss: 0.0638, batch_loss_c: 0.0570, batch_loss_s: 0.0797, time:26.2982, lr:0.0001\u001b[0m\n",
            "2019-12-06 12:20:27 \u001b[32mINFO     \u001b[0m train.py: [31/300], [310/484], step: 15314, 2.058 samples/sec, batch_loss: 0.5241, batch_loss_c: 0.5203, batch_loss_s: 0.5330, time:19.4331, lr:0.0001\u001b[0m\n",
            "2019-12-06 12:20:46 \u001b[32mINFO     \u001b[0m train.py: [31/300], [320/484], step: 15324, 2.077 samples/sec, batch_loss: 0.0803, batch_loss_c: 0.0745, batch_loss_s: 0.0938, time:19.2548, lr:0.0001\u001b[0m\n",
            "2019-12-06 12:21:11 \u001b[32mINFO     \u001b[0m train.py: [31/300], [330/484], step: 15334, 1.614 samples/sec, batch_loss: 0.1037, batch_loss_c: 0.1078, batch_loss_s: 0.0941, time:24.7892, lr:0.0001\u001b[0m\n",
            "2019-12-06 12:21:25 \u001b[32mINFO     \u001b[0m train.py: [31/300], [340/484], step: 15344, 2.847 samples/sec, batch_loss: 0.2719, batch_loss_c: 0.2124, batch_loss_s: 0.4108, time:14.0487, lr:0.0001\u001b[0m\n",
            "2019-12-06 12:21:50 \u001b[32mINFO     \u001b[0m train.py: [31/300], [350/484], step: 15354, 1.584 samples/sec, batch_loss: 0.2754, batch_loss_c: 0.2672, batch_loss_s: 0.2946, time:25.2551, lr:0.0001\u001b[0m\n",
            "2019-12-06 12:22:13 \u001b[32mINFO     \u001b[0m train.py: [31/300], [360/484], step: 15364, 1.777 samples/sec, batch_loss: 0.0852, batch_loss_c: 0.0880, batch_loss_s: 0.0787, time:22.5053, lr:0.0001\u001b[0m\n",
            "2019-12-06 12:22:47 \u001b[32mINFO     \u001b[0m train.py: [31/300], [370/484], step: 15374, 1.178 samples/sec, batch_loss: 0.2857, batch_loss_c: 0.2833, batch_loss_s: 0.2912, time:33.9675, lr:0.0001\u001b[0m\n",
            "2019-12-06 12:22:59 \u001b[32mINFO     \u001b[0m train.py: [31/300], [380/484], step: 15384, 3.224 samples/sec, batch_loss: 0.0796, batch_loss_c: 0.0668, batch_loss_s: 0.1097, time:12.4051, lr:0.0001\u001b[0m\n",
            "2019-12-06 12:23:26 \u001b[32mINFO     \u001b[0m train.py: [31/300], [390/484], step: 15394, 1.464 samples/sec, batch_loss: 0.0817, batch_loss_c: 0.0702, batch_loss_s: 0.1086, time:27.3194, lr:0.0001\u001b[0m\n",
            "2019-12-06 12:23:40 \u001b[32mINFO     \u001b[0m train.py: [31/300], [400/484], step: 15404, 2.960 samples/sec, batch_loss: 0.2079, batch_loss_c: 0.1976, batch_loss_s: 0.2320, time:13.5152, lr:0.0001\u001b[0m\n",
            "2019-12-06 12:24:16 \u001b[32mINFO     \u001b[0m train.py: [31/300], [410/484], step: 15414, 1.098 samples/sec, batch_loss: 0.0425, batch_loss_c: 0.0315, batch_loss_s: 0.0681, time:36.4199, lr:0.0001\u001b[0m\n",
            "2019-12-06 12:24:33 \u001b[32mINFO     \u001b[0m train.py: [31/300], [420/484], step: 15424, 2.471 samples/sec, batch_loss: 0.0677, batch_loss_c: 0.0624, batch_loss_s: 0.0801, time:16.1878, lr:0.0001\u001b[0m\n",
            "2019-12-06 12:24:47 \u001b[32mINFO     \u001b[0m train.py: [31/300], [430/484], step: 15434, 2.877 samples/sec, batch_loss: 0.0663, batch_loss_c: 0.0529, batch_loss_s: 0.0977, time:13.9030, lr:0.0001\u001b[0m\n",
            "2019-12-06 12:25:03 \u001b[32mINFO     \u001b[0m train.py: [31/300], [440/484], step: 15444, 2.494 samples/sec, batch_loss: 0.0617, batch_loss_c: 0.0572, batch_loss_s: 0.0722, time:16.0400, lr:0.0001\u001b[0m\n",
            "2019-12-06 12:25:31 \u001b[32mINFO     \u001b[0m train.py: [31/300], [450/484], step: 15454, 1.411 samples/sec, batch_loss: 0.2857, batch_loss_c: 0.2794, batch_loss_s: 0.3002, time:28.3535, lr:0.0001\u001b[0m\n",
            "2019-12-06 12:25:51 \u001b[32mINFO     \u001b[0m train.py: [31/300], [460/484], step: 15464, 2.018 samples/sec, batch_loss: 0.0598, batch_loss_c: 0.0539, batch_loss_s: 0.0737, time:19.8214, lr:0.0001\u001b[0m\n",
            "2019-12-06 12:26:05 \u001b[32mINFO     \u001b[0m train.py: [31/300], [470/484], step: 15474, 2.784 samples/sec, batch_loss: 0.2891, batch_loss_c: 0.2798, batch_loss_s: 0.3106, time:14.3671, lr:0.0001\u001b[0m\n",
            "2019-12-06 12:26:37 \u001b[32mINFO     \u001b[0m train.py: [31/300], [480/484], step: 15484, 1.252 samples/sec, batch_loss: 0.0919, batch_loss_c: 0.0848, batch_loss_s: 0.1085, time:31.9408, lr:0.0001\u001b[0m\n",
            "2019-12-06 12:26:41 \u001b[32mINFO     \u001b[0m train.py: [31/300], train_loss: 0.1478, time: 1022.0383, lr: 0.0001\u001b[0m\n",
            "2019-12-06 12:26:43 \u001b[32mINFO     \u001b[0m train.py: [32/300], [0/484], step: 15488, 18.232 samples/sec, batch_loss: 0.0499, batch_loss_c: 0.0439, batch_loss_s: 0.0638, time:2.1939, lr:0.0001\u001b[0m\n",
            "2019-12-06 12:26:59 \u001b[32mINFO     \u001b[0m train.py: [32/300], [10/484], step: 15498, 2.470 samples/sec, batch_loss: 0.3310, batch_loss_c: 0.3220, batch_loss_s: 0.3520, time:16.1934, lr:0.0001\u001b[0m\n",
            "2019-12-06 12:27:15 \u001b[32mINFO     \u001b[0m train.py: [32/300], [20/484], step: 15508, 2.600 samples/sec, batch_loss: 0.0639, batch_loss_c: 0.0516, batch_loss_s: 0.0925, time:15.3832, lr:0.0001\u001b[0m\n",
            "2019-12-06 12:27:33 \u001b[32mINFO     \u001b[0m train.py: [32/300], [30/484], step: 15518, 2.189 samples/sec, batch_loss: 0.2894, batch_loss_c: 0.2900, batch_loss_s: 0.2878, time:18.2703, lr:0.0001\u001b[0m\n",
            "2019-12-06 12:27:58 \u001b[32mINFO     \u001b[0m train.py: [32/300], [40/484], step: 15528, 1.577 samples/sec, batch_loss: 0.0956, batch_loss_c: 0.0859, batch_loss_s: 0.1183, time:25.3703, lr:0.0001\u001b[0m\n",
            "2019-12-06 12:28:15 \u001b[32mINFO     \u001b[0m train.py: [32/300], [50/484], step: 15538, 2.416 samples/sec, batch_loss: 0.0638, batch_loss_c: 0.0574, batch_loss_s: 0.0788, time:16.5549, lr:0.0001\u001b[0m\n",
            "2019-12-06 12:28:49 \u001b[32mINFO     \u001b[0m train.py: [32/300], [60/484], step: 15548, 1.181 samples/sec, batch_loss: 0.3051, batch_loss_c: 0.2955, batch_loss_s: 0.3274, time:33.8782, lr:0.0001\u001b[0m\n",
            "2019-12-06 12:29:04 \u001b[32mINFO     \u001b[0m train.py: [32/300], [70/484], step: 15558, 2.602 samples/sec, batch_loss: 0.0834, batch_loss_c: 0.0903, batch_loss_s: 0.0675, time:15.3708, lr:0.0001\u001b[0m\n",
            "2019-12-06 12:29:29 \u001b[32mINFO     \u001b[0m train.py: [32/300], [80/484], step: 15568, 1.622 samples/sec, batch_loss: 0.0410, batch_loss_c: 0.0360, batch_loss_s: 0.0528, time:24.6599, lr:0.0001\u001b[0m\n",
            "2019-12-06 12:29:48 \u001b[32mINFO     \u001b[0m train.py: [32/300], [90/484], step: 15578, 2.146 samples/sec, batch_loss: 0.0539, batch_loss_c: 0.0440, batch_loss_s: 0.0770, time:18.6430, lr:0.0001\u001b[0m\n",
            "2019-12-06 12:30:02 \u001b[32mINFO     \u001b[0m train.py: [32/300], [100/484], step: 15588, 2.771 samples/sec, batch_loss: 0.0450, batch_loss_c: 0.0378, batch_loss_s: 0.0618, time:14.4351, lr:0.0001\u001b[0m\n",
            "2019-12-06 12:30:31 \u001b[32mINFO     \u001b[0m train.py: [32/300], [110/484], step: 15598, 1.366 samples/sec, batch_loss: 0.4259, batch_loss_c: 0.4614, batch_loss_s: 0.3433, time:29.2750, lr:0.0001\u001b[0m\n",
            "2019-12-06 12:30:51 \u001b[32mINFO     \u001b[0m train.py: [32/300], [120/484], step: 15608, 2.022 samples/sec, batch_loss: 0.2987, batch_loss_c: 0.2934, batch_loss_s: 0.3111, time:19.7830, lr:0.0001\u001b[0m\n",
            "2019-12-06 12:31:12 \u001b[32mINFO     \u001b[0m train.py: [32/300], [130/484], step: 15618, 1.938 samples/sec, batch_loss: 0.1388, batch_loss_c: 0.1074, batch_loss_s: 0.2123, time:20.6374, lr:0.0001\u001b[0m\n",
            "2019-12-06 12:31:43 \u001b[32mINFO     \u001b[0m train.py: [32/300], [140/484], step: 15628, 1.287 samples/sec, batch_loss: 0.2217, batch_loss_c: 0.1970, batch_loss_s: 0.2792, time:31.0797, lr:0.0001\u001b[0m\n",
            "2019-12-06 12:31:56 \u001b[32mINFO     \u001b[0m train.py: [32/300], [150/484], step: 15638, 2.998 samples/sec, batch_loss: 0.0509, batch_loss_c: 0.0413, batch_loss_s: 0.0735, time:13.3401, lr:0.0001\u001b[0m\n",
            "2019-12-06 12:32:19 \u001b[32mINFO     \u001b[0m train.py: [32/300], [160/484], step: 15648, 1.784 samples/sec, batch_loss: 0.0821, batch_loss_c: 0.0784, batch_loss_s: 0.0908, time:22.4272, lr:0.0001\u001b[0m\n",
            "2019-12-06 12:32:41 \u001b[32mINFO     \u001b[0m train.py: [32/300], [170/484], step: 15658, 1.819 samples/sec, batch_loss: 0.3556, batch_loss_c: 0.3343, batch_loss_s: 0.4051, time:21.9860, lr:0.0001\u001b[0m\n",
            "2019-12-06 12:33:04 \u001b[32mINFO     \u001b[0m train.py: [32/300], [180/484], step: 15668, 1.738 samples/sec, batch_loss: 0.0798, batch_loss_c: 0.0704, batch_loss_s: 0.1018, time:23.0179, lr:0.0001\u001b[0m\n",
            "2019-12-06 12:33:16 \u001b[32mINFO     \u001b[0m train.py: [32/300], [190/484], step: 15678, 3.236 samples/sec, batch_loss: 0.3449, batch_loss_c: 0.3400, batch_loss_s: 0.3562, time:12.3601, lr:0.0001\u001b[0m\n",
            "2019-12-06 12:33:53 \u001b[32mINFO     \u001b[0m train.py: [32/300], [200/484], step: 15688, 1.080 samples/sec, batch_loss: 0.0528, batch_loss_c: 0.0450, batch_loss_s: 0.0709, time:37.0431, lr:0.0001\u001b[0m\n",
            "2019-12-06 12:34:08 \u001b[32mINFO     \u001b[0m train.py: [32/300], [210/484], step: 15698, 2.660 samples/sec, batch_loss: 0.0454, batch_loss_c: 0.0390, batch_loss_s: 0.0601, time:15.0376, lr:0.0001\u001b[0m\n",
            "2019-12-06 12:34:29 \u001b[32mINFO     \u001b[0m train.py: [32/300], [220/484], step: 15708, 1.936 samples/sec, batch_loss: 0.1317, batch_loss_c: 0.0985, batch_loss_s: 0.2093, time:20.6662, lr:0.0001\u001b[0m\n",
            "2019-12-06 12:34:53 \u001b[32mINFO     \u001b[0m train.py: [32/300], [230/484], step: 15718, 1.677 samples/sec, batch_loss: 0.1093, batch_loss_c: 0.0917, batch_loss_s: 0.1503, time:23.8463, lr:0.0001\u001b[0m\n",
            "2019-12-06 12:35:11 \u001b[32mINFO     \u001b[0m train.py: [32/300], [240/484], step: 15728, 2.146 samples/sec, batch_loss: 0.2898, batch_loss_c: 0.2819, batch_loss_s: 0.3081, time:18.6354, lr:0.0001\u001b[0m\n",
            "2019-12-06 12:35:26 \u001b[32mINFO     \u001b[0m train.py: [32/300], [250/484], step: 15738, 2.701 samples/sec, batch_loss: 0.0507, batch_loss_c: 0.0432, batch_loss_s: 0.0683, time:14.8083, lr:0.0001\u001b[0m\n",
            "2019-12-06 12:35:57 \u001b[32mINFO     \u001b[0m train.py: [32/300], [260/484], step: 15748, 1.305 samples/sec, batch_loss: 0.0737, batch_loss_c: 0.0703, batch_loss_s: 0.0817, time:30.6513, lr:0.0001\u001b[0m\n",
            "2019-12-06 12:36:09 \u001b[32mINFO     \u001b[0m train.py: [32/300], [270/484], step: 15758, 3.154 samples/sec, batch_loss: 0.0737, batch_loss_c: 0.0552, batch_loss_s: 0.1169, time:12.6824, lr:0.0001\u001b[0m\n",
            "2019-12-06 12:36:28 \u001b[32mINFO     \u001b[0m train.py: [32/300], [280/484], step: 15768, 2.186 samples/sec, batch_loss: 0.0738, batch_loss_c: 0.0674, batch_loss_s: 0.0886, time:18.2951, lr:0.0001\u001b[0m\n",
            "2019-12-06 12:36:48 \u001b[32mINFO     \u001b[0m train.py: [32/300], [290/484], step: 15778, 1.965 samples/sec, batch_loss: 0.0828, batch_loss_c: 0.0784, batch_loss_s: 0.0929, time:20.3522, lr:0.0001\u001b[0m\n",
            "2019-12-06 12:37:01 \u001b[32mINFO     \u001b[0m train.py: [32/300], [300/484], step: 15788, 2.955 samples/sec, batch_loss: 0.3014, batch_loss_c: 0.2872, batch_loss_s: 0.3344, time:13.5359, lr:0.0001\u001b[0m\n",
            "2019-12-06 12:37:26 \u001b[32mINFO     \u001b[0m train.py: [32/300], [310/484], step: 15798, 1.625 samples/sec, batch_loss: 0.0871, batch_loss_c: 0.0761, batch_loss_s: 0.1126, time:24.6165, lr:0.0001\u001b[0m\n",
            "2019-12-06 12:37:52 \u001b[32mINFO     \u001b[0m train.py: [32/300], [320/484], step: 15808, 1.552 samples/sec, batch_loss: 0.2941, batch_loss_c: 0.2877, batch_loss_s: 0.3090, time:25.7697, lr:0.0001\u001b[0m\n",
            "2019-12-06 12:38:05 \u001b[32mINFO     \u001b[0m train.py: [32/300], [330/484], step: 15818, 3.032 samples/sec, batch_loss: 0.2767, batch_loss_c: 0.2579, batch_loss_s: 0.3208, time:13.1915, lr:0.0001\u001b[0m\n",
            "2019-12-06 12:38:37 \u001b[32mINFO     \u001b[0m train.py: [32/300], [340/484], step: 15828, 1.242 samples/sec, batch_loss: 0.2132, batch_loss_c: 0.2459, batch_loss_s: 0.1369, time:32.2011, lr:0.0001\u001b[0m\n",
            "2019-12-06 12:38:55 \u001b[32mINFO     \u001b[0m train.py: [32/300], [350/484], step: 15838, 2.257 samples/sec, batch_loss: 0.2038, batch_loss_c: 0.2038, batch_loss_s: 0.2037, time:17.7257, lr:0.0001\u001b[0m\n",
            "2019-12-06 12:39:11 \u001b[32mINFO     \u001b[0m train.py: [32/300], [360/484], step: 15848, 2.526 samples/sec, batch_loss: 0.1390, batch_loss_c: 0.1177, batch_loss_s: 0.1887, time:15.8326, lr:0.0001\u001b[0m\n",
            "2019-12-06 12:39:41 \u001b[32mINFO     \u001b[0m train.py: [32/300], [370/484], step: 15858, 1.343 samples/sec, batch_loss: 0.0632, batch_loss_c: 0.0520, batch_loss_s: 0.0891, time:29.7791, lr:0.0001\u001b[0m\n",
            "2019-12-06 12:39:57 \u001b[32mINFO     \u001b[0m train.py: [32/300], [380/484], step: 15868, 2.378 samples/sec, batch_loss: 0.2944, batch_loss_c: 0.2875, batch_loss_s: 0.3107, time:16.8227, lr:0.0001\u001b[0m\n",
            "2019-12-06 12:40:10 \u001b[32mINFO     \u001b[0m train.py: [32/300], [390/484], step: 15878, 3.153 samples/sec, batch_loss: 0.2812, batch_loss_c: 0.2782, batch_loss_s: 0.2881, time:12.6862, lr:0.0001\u001b[0m\n",
            "2019-12-06 12:40:24 \u001b[32mINFO     \u001b[0m train.py: [32/300], [400/484], step: 15888, 2.868 samples/sec, batch_loss: 0.1062, batch_loss_c: 0.1001, batch_loss_s: 0.1204, time:13.9455, lr:0.0001\u001b[0m\n",
            "2019-12-06 12:40:40 \u001b[32mINFO     \u001b[0m train.py: [32/300], [410/484], step: 15898, 2.520 samples/sec, batch_loss: 0.0880, batch_loss_c: 0.0849, batch_loss_s: 0.0952, time:15.8756, lr:0.0001\u001b[0m\n",
            "2019-12-06 12:40:52 \u001b[32mINFO     \u001b[0m train.py: [32/300], [420/484], step: 15908, 3.195 samples/sec, batch_loss: 0.1265, batch_loss_c: 0.1260, batch_loss_s: 0.1275, time:12.5188, lr:0.0001\u001b[0m\n",
            "2019-12-06 12:41:31 \u001b[32mINFO     \u001b[0m train.py: [32/300], [430/484], step: 15918, 1.045 samples/sec, batch_loss: 0.1859, batch_loss_c: 0.1822, batch_loss_s: 0.1945, time:38.2872, lr:0.0001\u001b[0m\n",
            "2019-12-06 12:41:46 \u001b[32mINFO     \u001b[0m train.py: [32/300], [440/484], step: 15928, 2.551 samples/sec, batch_loss: 0.3227, batch_loss_c: 0.3165, batch_loss_s: 0.3373, time:15.6824, lr:0.0001\u001b[0m\n",
            "2019-12-06 12:41:59 \u001b[32mINFO     \u001b[0m train.py: [32/300], [450/484], step: 15938, 3.119 samples/sec, batch_loss: 0.1102, batch_loss_c: 0.1152, batch_loss_s: 0.0985, time:12.8235, lr:0.0001\u001b[0m\n",
            "2019-12-06 12:42:20 \u001b[32mINFO     \u001b[0m train.py: [32/300], [460/484], step: 15948, 1.892 samples/sec, batch_loss: 0.0690, batch_loss_c: 0.0670, batch_loss_s: 0.0738, time:21.1447, lr:0.0001\u001b[0m\n",
            "2019-12-06 12:42:35 \u001b[32mINFO     \u001b[0m train.py: [32/300], [470/484], step: 15958, 2.686 samples/sec, batch_loss: 0.0623, batch_loss_c: 0.0456, batch_loss_s: 0.1013, time:14.8903, lr:0.0001\u001b[0m\n",
            "2019-12-06 12:43:07 \u001b[32mINFO     \u001b[0m train.py: [32/300], [480/484], step: 15968, 1.262 samples/sec, batch_loss: 0.2966, batch_loss_c: 0.2894, batch_loss_s: 0.3134, time:31.6897, lr:0.0001\u001b[0m\n",
            "2019-12-06 12:43:12 \u001b[32mINFO     \u001b[0m train.py: [32/300], train_loss: 0.1474, time: 990.7393, lr: 0.0001\u001b[0m\n",
            "2019-12-06 12:43:14 \u001b[32mINFO     \u001b[0m train.py: [33/300], [0/484], step: 15972, 25.504 samples/sec, batch_loss: 0.1470, batch_loss_c: 0.1389, batch_loss_s: 0.1657, time:1.5684, lr:0.0001\u001b[0m\n",
            "2019-12-06 12:43:30 \u001b[32mINFO     \u001b[0m train.py: [33/300], [10/484], step: 15982, 2.404 samples/sec, batch_loss: 0.3252, batch_loss_c: 0.3299, batch_loss_s: 0.3143, time:16.6356, lr:0.0001\u001b[0m\n",
            "2019-12-06 12:43:45 \u001b[32mINFO     \u001b[0m train.py: [33/300], [20/484], step: 15992, 2.749 samples/sec, batch_loss: 0.1012, batch_loss_c: 0.0951, batch_loss_s: 0.1154, time:14.5524, lr:0.0001\u001b[0m\n",
            "2019-12-06 12:44:02 \u001b[32mINFO     \u001b[0m train.py: [33/300], [30/484], step: 16002, 2.381 samples/sec, batch_loss: 0.4007, batch_loss_c: 0.3502, batch_loss_s: 0.5185, time:16.7979, lr:0.0001\u001b[0m\n",
            "2019-12-06 12:44:21 \u001b[32mINFO     \u001b[0m train.py: [33/300], [40/484], step: 16012, 2.077 samples/sec, batch_loss: 0.0534, batch_loss_c: 0.0471, batch_loss_s: 0.0680, time:19.2586, lr:0.0001\u001b[0m\n",
            "2019-12-06 12:44:48 \u001b[32mINFO     \u001b[0m train.py: [33/300], [50/484], step: 16022, 1.469 samples/sec, batch_loss: 0.0973, batch_loss_c: 0.0931, batch_loss_s: 0.1070, time:27.2220, lr:0.0001\u001b[0m\n",
            "2019-12-06 12:45:11 \u001b[32mINFO     \u001b[0m train.py: [33/300], [60/484], step: 16032, 1.785 samples/sec, batch_loss: 0.0577, batch_loss_c: 0.0509, batch_loss_s: 0.0735, time:22.4029, lr:0.0001\u001b[0m\n",
            "2019-12-06 12:45:32 \u001b[32mINFO     \u001b[0m train.py: [33/300], [70/484], step: 16042, 1.918 samples/sec, batch_loss: 0.0437, batch_loss_c: 0.0365, batch_loss_s: 0.0603, time:20.8552, lr:0.0001\u001b[0m\n",
            "2019-12-06 12:45:45 \u001b[32mINFO     \u001b[0m train.py: [33/300], [80/484], step: 16052, 2.972 samples/sec, batch_loss: 0.0663, batch_loss_c: 0.0609, batch_loss_s: 0.0789, time:13.4580, lr:0.0001\u001b[0m\n",
            "2019-12-06 12:46:02 \u001b[32mINFO     \u001b[0m train.py: [33/300], [90/484], step: 16062, 2.393 samples/sec, batch_loss: 0.0705, batch_loss_c: 0.0629, batch_loss_s: 0.0884, time:16.7147, lr:0.0001\u001b[0m\n",
            "2019-12-06 12:46:21 \u001b[32mINFO     \u001b[0m train.py: [33/300], [100/484], step: 16072, 2.039 samples/sec, batch_loss: 0.0564, batch_loss_c: 0.0363, batch_loss_s: 0.1033, time:19.6209, lr:0.0001\u001b[0m\n",
            "2019-12-06 12:46:37 \u001b[32mINFO     \u001b[0m train.py: [33/300], [110/484], step: 16082, 2.588 samples/sec, batch_loss: 0.0820, batch_loss_c: 0.0739, batch_loss_s: 0.1009, time:15.4571, lr:0.0001\u001b[0m\n",
            "2019-12-06 12:46:56 \u001b[32mINFO     \u001b[0m train.py: [33/300], [120/484], step: 16092, 2.068 samples/sec, batch_loss: 0.0949, batch_loss_c: 0.0960, batch_loss_s: 0.0921, time:19.3397, lr:0.0001\u001b[0m\n",
            "2019-12-06 12:47:14 \u001b[32mINFO     \u001b[0m train.py: [33/300], [130/484], step: 16102, 2.242 samples/sec, batch_loss: 0.0510, batch_loss_c: 0.0379, batch_loss_s: 0.0815, time:17.8438, lr:0.0001\u001b[0m\n",
            "2019-12-06 12:47:38 \u001b[32mINFO     \u001b[0m utils.py: models saved to /content/drive/My Drive/PSENet_2/final.pth\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}