{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PSENet_trial_run.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kavyajeetbora/PSENet.pytorch/blob/master/PSENet_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UByxwp0F3QUp",
        "colab_type": "text"
      },
      "source": [
        "## Installing softwares and other dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GSEHnmCxK_o9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, Dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y99kqKlHoYkz",
        "colab_type": "code",
        "outputId": "e61561a6-e18d-4259-ba94-1ce55e414b19",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "!pip install pyclipper"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyclipper\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/67/2691f7cbb28fb9dbf423f2302fe489f9cee34d9a50a743c95032a24ac597/pyclipper-1.1.0.post1-cp36-cp36m-manylinux1_x86_64.whl (129kB)\n",
            "\r\u001b[K     |██▌                             | 10kB 9.8MB/s eta 0:00:01\r\u001b[K     |█████                           | 20kB 4.5MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 30kB 6.4MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 40kB 8.1MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 51kB 5.0MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 61kB 5.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 71kB 6.6MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 81kB 7.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 92kB 8.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 102kB 6.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 112kB 6.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 122kB 6.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 133kB 6.5MB/s \n",
            "\u001b[?25hInstalling collected packages: pyclipper\n",
            "Successfully installed pyclipper-1.1.0.post1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ncoynOlfnIW1",
        "colab_type": "code",
        "outputId": "508bd8af-65c3-4a05-95c6-de49d64dd559",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hhJ-6yyXcnRx",
        "colab_type": "code",
        "outputId": "2894386b-9d6a-4bb1-d8ed-4414c84e9d08",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        }
      },
      "source": [
        "import shutil, os\n",
        "os.chdir('/content')\n",
        "directory = '/content/cloned-repo'\n",
        "if os.path.exists(directory):\n",
        "  shutil.rmtree(directory)\n",
        "\n",
        "!git clone https://github.com/kavyajeetbora/PSENet.pytorch.git /content/cloned-repo\n",
        "print(\"Cloned the repository\")\n",
        "os.chdir('/content/cloned-repo')\n",
        "!ls"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into '/content/cloned-repo'...\n",
            "remote: Enumerating objects: 110, done.\u001b[K\n",
            "remote: Counting objects:   0% (1/110)\u001b[K\rremote: Counting objects:   1% (2/110)\u001b[K\rremote: Counting objects:   2% (3/110)\u001b[K\rremote: Counting objects:   3% (4/110)\u001b[K\rremote: Counting objects:   4% (5/110)\u001b[K\rremote: Counting objects:   5% (6/110)\u001b[K\rremote: Counting objects:   6% (7/110)\u001b[K\rremote: Counting objects:   7% (8/110)\u001b[K\rremote: Counting objects:   8% (9/110)\u001b[K\rremote: Counting objects:   9% (10/110)\u001b[K\rremote: Counting objects:  10% (11/110)\u001b[K\rremote: Counting objects:  11% (13/110)\u001b[K\rremote: Counting objects:  12% (14/110)\u001b[K\rremote: Counting objects:  13% (15/110)\u001b[K\rremote: Counting objects:  14% (16/110)\u001b[K\rremote: Counting objects:  15% (17/110)\u001b[K\rremote: Counting objects:  16% (18/110)\u001b[K\rremote: Counting objects:  17% (19/110)\u001b[K\rremote: Counting objects:  18% (20/110)\u001b[K\rremote: Counting objects:  19% (21/110)\u001b[K\rremote: Counting objects:  20% (22/110)\u001b[K\rremote: Counting objects:  21% (24/110)\u001b[K\rremote: Counting objects:  22% (25/110)\u001b[K\rremote: Counting objects:  23% (26/110)\u001b[K\rremote: Counting objects:  24% (27/110)\u001b[K\rremote: Counting objects:  25% (28/110)\u001b[K\rremote: Counting objects:  26% (29/110)\u001b[K\rremote: Counting objects:  27% (30/110)\u001b[K\rremote: Counting objects:  28% (31/110)\u001b[K\rremote: Counting objects:  29% (32/110)\u001b[K\rremote: Counting objects:  30% (33/110)\u001b[K\rremote: Counting objects:  31% (35/110)\u001b[K\rremote: Counting objects:  32% (36/110)\u001b[K\rremote: Counting objects:  33% (37/110)\u001b[K\rremote: Counting objects:  34% (38/110)\u001b[K\rremote: Counting objects:  35% (39/110)\u001b[K\rremote: Counting objects:  36% (40/110)\u001b[K\rremote: Counting objects:  37% (41/110)\u001b[K\rremote: Counting objects:  38% (42/110)\u001b[K\rremote: Counting objects:  39% (43/110)\u001b[K\rremote: Counting objects:  40% (44/110)\u001b[K\rremote: Counting objects:  41% (46/110)\u001b[K\rremote: Counting objects:  42% (47/110)\u001b[K\rremote: Counting objects:  43% (48/110)\u001b[K\rremote: Counting objects:  44% (49/110)\u001b[K\rremote: Counting objects:  45% (50/110)\u001b[K\rremote: Counting objects:  46% (51/110)\u001b[K\rremote: Counting objects:  47% (52/110)\u001b[K\rremote: Counting objects:  48% (53/110)\u001b[K\rremote: Counting objects:  49% (54/110)\u001b[K\rremote: Counting objects:  50% (55/110)\u001b[K\rremote: Counting objects:  51% (57/110)\u001b[K\rremote: Counting objects:  52% (58/110)\u001b[K\rremote: Counting objects:  53% (59/110)\u001b[K\rremote: Counting objects:  54% (60/110)\u001b[K\rremote: Counting objects:  55% (61/110)\u001b[K\rremote: Counting objects:  56% (62/110)\u001b[K\rremote: Counting objects:  57% (63/110)\u001b[K\rremote: Counting objects:  58% (64/110)\u001b[K\rremote: Counting objects:  59% (65/110)\u001b[K\rremote: Counting objects:  60% (66/110)\u001b[K\rremote: Counting objects:  61% (68/110)\u001b[K\rremote: Counting objects:  62% (69/110)\u001b[K\rremote: Counting objects:  63% (70/110)\u001b[K\rremote: Counting objects:  64% (71/110)\u001b[K\rremote: Counting objects:  65% (72/110)\u001b[K\rremote: Counting objects:  66% (73/110)\u001b[K\rremote: Counting objects:  67% (74/110)\u001b[K\rremote: Counting objects:  68% (75/110)\u001b[K\rremote: Counting objects:  69% (76/110)\u001b[K\rremote: Counting objects:  70% (77/110)\u001b[K\rremote: Counting objects:  71% (79/110)\u001b[K\rremote: Counting objects:  72% (80/110)\u001b[K\rremote: Counting objects:  73% (81/110)\u001b[K\rremote: Counting objects:  74% (82/110)\u001b[K\rremote: Counting objects:  75% (83/110)\u001b[K\rremote: Counting objects:  76% (84/110)\u001b[K\rremote: Counting objects:  77% (85/110)\u001b[K\rremote: Counting objects:  78% (86/110)\u001b[K\rremote: Counting objects:  79% (87/110)\u001b[K\rremote: Counting objects:  80% (88/110)\u001b[K\rremote: Counting objects:  81% (90/110)\u001b[K\rremote: Counting objects:  82% (91/110)\u001b[K\rremote: Counting objects:  83% (92/110)\u001b[K\rremote: Counting objects:  84% (93/110)\u001b[K\rremote: Counting objects:  85% (94/110)\u001b[K\rremote: Counting objects:  86% (95/110)\u001b[K\rremote: Counting objects:  87% (96/110)\u001b[K\rremote: Counting objects:  88% (97/110)\u001b[K\rremote: Counting objects:  89% (98/110)\u001b[K\rremote: Counting objects:  90% (99/110)\u001b[K\rremote: Counting objects:  91% (101/110)\u001b[K\rremote: Counting objects:  92% (102/110)\u001b[K\rremote: Counting objects:  93% (103/110)\u001b[K\rremote: Counting objects:  94% (104/110)\u001b[K\rremote: Counting objects:  95% (105/110)\u001b[K\rremote: Counting objects:  96% (106/110)\u001b[K\rremote: Counting objects:  97% (107/110)\u001b[K\rremote: Counting objects:  98% (108/110)\u001b[K\rremote: Counting objects:  99% (109/110)\u001b[K\rremote: Counting objects: 100% (110/110)\u001b[K\rremote: Counting objects: 100% (110/110), done.\u001b[K\n",
            "remote: Compressing objects: 100% (110/110), done.\u001b[K\n",
            "remote: Total 510 (delta 65), reused 0 (delta 0), pack-reused 400\u001b[K\n",
            "Receiving objects: 100% (510/510), 8.73 MiB | 9.54 MiB/s, done.\n",
            "Resolving deltas: 100% (259/259), done.\n",
            "Cloned the repository\n",
            "cal_recall  install_dependencies.sh  PSENet.ipynb\t     train.py\n",
            "config.py   LICENSE\t\t     PSENet_predict.ipynb    utils\n",
            "dataset     models\t\t     PSENet_training.ipynb\n",
            "eval.py     predict.py\t\t     PSENet_trial_run.ipynb\n",
            "imgs\t    pse\t\t\t     README.md\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PrOmfo6_3X2I",
        "colab_type": "text"
      },
      "source": [
        "## Extracting the data and setting up the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dGLMxTujlm0w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## unzipping the files\n",
        "from zipfile import ZipFile\n",
        "\n",
        "def unzip_files(file,output_dir):\n",
        "  with ZipFile(file, 'r') as zipObj:\n",
        "    # Extract all the contents of zip file in current directory\n",
        "    zipObj.extractall(output_dir)\n",
        "  print('Extracted to',output_dir)\n",
        "\n",
        "def make_directory(directory):\n",
        "  if os.path.isdir(directory):\n",
        "    shutil.rmtree(directory)\n",
        "  \n",
        "  os.mkdir(directory)\n",
        "  print('Created a new directory')\n",
        "\n",
        "training_data_zip = '/content/drive/My Drive/Colab Notebooks/padh.ai.notebooks/15. Object Detection/AI4Bharat Dataset/Training Set/Random 5000.zip'\n",
        "test_data_zip = '/content/drive/My Drive/Colab Notebooks/padh.ai.notebooks/15. Object Detection/AI4Bharat Dataset/Test Set/real_Image_dataset_Detection.zip'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FqJ6xb5u2dgg",
        "colab_type": "code",
        "outputId": "30d8527c-2cc6-41a7-e2d2-d23b3cc4b347",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# make directories\n",
        "make_directory('Training Set')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Created a new directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FFSaSXr-m-K4",
        "colab_type": "code",
        "outputId": "1fa2e2f2-7958-454e-b2bc-306f1f235456",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "%%time\n",
        "unzip_files(test_data_zip,'Training Set')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracted to Training Set\n",
            "CPU times: user 550 ms, sys: 140 ms, total: 691 ms\n",
            "Wall time: 6.14 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YRSeyluknVH4",
        "colab_type": "code",
        "outputId": "8ce00ff4-c218-4393-ddc3-229e75d2f354",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(len(os.listdir('Training Set/Images')))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "428\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MOp4JsZ6nXvn",
        "colab_type": "code",
        "outputId": "050b48a8-1e90-43d7-cc0d-5ed4dbc225c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(len(os.listdir('Training Set/Annotations')))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "428\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mhvg89mXn8zG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from dataset.data_utils import *\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader, Dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4JeHTz3G6Lyj",
        "colab_type": "code",
        "outputId": "8a9cb817-4cef-4bdf-f967-40f8e4900eea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_data = MyDataset('Training Set',transform=transforms.ToTensor())\n",
        "len(train_data)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "428"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "odLKMn-5SBAz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device(\"cuda:0\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9a9PsyF3dqo",
        "colab_type": "text"
      },
      "source": [
        "## Training "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9x221Ya0RBGO",
        "outputId": "c23acc0e-c736-43c0-fd2f-2aaa195b5eda",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        }
      },
      "source": [
        "!chmod +x install_dependencies.sh # make shell script executable\n",
        "!./install_dependencies.sh # run the shell script"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyclipper in /usr/local/lib/python3.6/dist-packages (1.1.0.post1)\n",
            "Collecting Polygon3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/73/a0/d07a4f3e80ed7020a33f3111db217f54ac44a485ff45da3c21ce49f65041/Polygon3-3.0.8.tar.gz (71kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 4.4MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: Polygon3\n",
            "  Building wheel for Polygon3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for Polygon3: filename=Polygon3-3.0.8-cp36-cp36m-linux_x86_64.whl size=101488 sha256=9e922a3cdbe3a735d235f95523cc338874fcd54d3e3225162deeec12ee45acdb\n",
            "  Stored in directory: /root/.cache/pip/wheels/95/32/f1/5525b233996d9d99cbce2f0a8da60d137ddddc555d3e8b0e2a\n",
            "Successfully built Polygon3\n",
            "Installing collected packages: Polygon3\n",
            "Successfully installed Polygon3-3.0.8\n",
            "Collecting colorlog\n",
            "  Downloading https://files.pythonhosted.org/packages/68/4d/892728b0c14547224f0ac40884e722a3d00cb54e7a146aea0b3186806c9e/colorlog-4.0.2-py2.py3-none-any.whl\n",
            "Installing collected packages: colorlog\n",
            "Successfully installed colorlog-4.0.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xD4GuTKC3wXY",
        "colab_type": "code",
        "outputId": "6e1a6561-4b3e-4409-ea46-cfae709fbea1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!python3 train.py"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "make: Entering directory '/content/cloned-repo/pse'\n",
            "make: 'pse.so' is up to date.\n",
            "make: Leaving directory '/content/cloned-repo/pse'\n",
            "2019-11-26 15:51:02 \u001b[32mINFO     \u001b[0m utils.py: logger init finished\u001b[0m\n",
            "2019-11-26 15:51:02 \u001b[32mINFO     \u001b[0m train.py: {'Lambda': 0.7,\n",
            " 'OHEM_ratio': 3,\n",
            " 'backbone': 'resnet50',\n",
            " 'checkpoint': '',\n",
            " 'data_shape': 640,\n",
            " 'display_input_images': False,\n",
            " 'display_interval': 10,\n",
            " 'display_output_images': False,\n",
            " 'end_lr': 1e-07,\n",
            " 'epochs': 50,\n",
            " 'gpu_id': '0',\n",
            " 'lr': 0.001,\n",
            " 'lr_decay_step': [200, 400],\n",
            " 'lr_gamma': 0.1,\n",
            " 'm': 0.5,\n",
            " 'n': 6,\n",
            " 'output_dir': '/content/drive/My Drive/PSENet_2',\n",
            " 'pretrained': False,\n",
            " 'pretrained_path': '/content/drive/My Drive/PSENet_2/PSENet_resnet50.pth',\n",
            " 'restart_training': False,\n",
            " 'scale': 1,\n",
            " 'seed': 2,\n",
            " 'show_images_interval': 50,\n",
            " 'start_epoch': 0,\n",
            " 'testroot': 'Test Set',\n",
            " 'train_batch_size': 4,\n",
            " 'trainroot': 'Training Set',\n",
            " 'warm_up_epoch': 6,\n",
            " 'warm_up_lr': 0.0001,\n",
            " 'weight_decay': 0.0005,\n",
            " 'workers': 12}\u001b[0m\n",
            "2019-11-26 15:51:02 \u001b[32mINFO     \u001b[0m train.py: train with gpu 0 and pytorch 1.3.1\u001b[0m\n",
            "2019-11-26 15:51:13 \u001b[32mINFO     \u001b[0m train.py: train dataset has 428 samples,107 in dataloader\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:100: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule.See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "2019-11-26 15:51:36 \u001b[32mINFO     \u001b[0m train.py: [0/50], [0/107], step: 0, 1.702 samples/sec, batch_loss: 0.3087, batch_loss_c: 0.3027, batch_loss_s: 0.3228, time:23.4951, lr:0.001\u001b[0m\n",
            "2019-11-26 15:51:43 \u001b[32mINFO     \u001b[0m train.py: [0/50], [10/107], step: 10, 5.935 samples/sec, batch_loss: 0.1624, batch_loss_c: 0.1806, batch_loss_s: 0.1198, time:6.7391, lr:0.001\u001b[0m\n",
            "2019-11-26 15:51:50 \u001b[32mINFO     \u001b[0m train.py: [0/50], [20/107], step: 20, 5.775 samples/sec, batch_loss: 0.1639, batch_loss_c: 0.1687, batch_loss_s: 0.1528, time:6.9259, lr:0.001\u001b[0m\n",
            "2019-11-26 15:51:57 \u001b[32mINFO     \u001b[0m train.py: [0/50], [30/107], step: 30, 5.645 samples/sec, batch_loss: 0.0780, batch_loss_c: 0.0686, batch_loss_s: 0.0998, time:7.0862, lr:0.001\u001b[0m\n",
            "2019-11-26 15:52:06 \u001b[32mINFO     \u001b[0m train.py: [0/50], [40/107], step: 40, 4.467 samples/sec, batch_loss: 0.3172, batch_loss_c: 0.3094, batch_loss_s: 0.3354, time:8.9543, lr:0.001\u001b[0m\n",
            "2019-11-26 15:52:15 \u001b[32mINFO     \u001b[0m train.py: [0/50], [50/107], step: 50, 4.641 samples/sec, batch_loss: 0.4201, batch_loss_c: 0.4011, batch_loss_s: 0.4645, time:8.6191, lr:0.001\u001b[0m\n",
            "2019-11-26 15:52:22 \u001b[32mINFO     \u001b[0m train.py: [0/50], [60/107], step: 60, 5.786 samples/sec, batch_loss: 0.3003, batch_loss_c: 0.3260, batch_loss_s: 0.2403, time:6.9129, lr:0.001\u001b[0m\n",
            "2019-11-26 15:52:28 \u001b[32mINFO     \u001b[0m train.py: [0/50], [70/107], step: 70, 6.218 samples/sec, batch_loss: 0.2998, batch_loss_c: 0.2441, batch_loss_s: 0.4297, time:6.4334, lr:0.001\u001b[0m\n",
            "2019-11-26 15:52:35 \u001b[32mINFO     \u001b[0m train.py: [0/50], [80/107], step: 80, 5.715 samples/sec, batch_loss: 0.0721, batch_loss_c: 0.0630, batch_loss_s: 0.0934, time:6.9991, lr:0.001\u001b[0m\n",
            "2019-11-26 15:52:40 \u001b[32mINFO     \u001b[0m train.py: [0/50], [90/107], step: 90, 7.619 samples/sec, batch_loss: 0.0798, batch_loss_c: 0.0588, batch_loss_s: 0.1286, time:5.2499, lr:0.001\u001b[0m\n",
            "2019-11-26 15:52:45 \u001b[32mINFO     \u001b[0m train.py: [0/50], [100/107], step: 100, 7.900 samples/sec, batch_loss: 0.0694, batch_loss_c: 0.0524, batch_loss_s: 0.1091, time:5.0634, lr:0.001\u001b[0m\n",
            "2019-11-26 15:52:49 \u001b[32mINFO     \u001b[0m train.py: [0/50], train_loss: 0.1491, time: 95.8734, lr: 0.001\u001b[0m\n",
            "2019-11-26 15:52:55 \u001b[32mINFO     \u001b[0m train.py: [1/50], [0/107], step: 107, 6.941 samples/sec, batch_loss: 0.1447, batch_loss_c: 0.1549, batch_loss_s: 0.1211, time:5.7629, lr:0.001\u001b[0m\n",
            "2019-11-26 15:53:08 \u001b[32mINFO     \u001b[0m train.py: [1/50], [10/107], step: 117, 3.079 samples/sec, batch_loss: 0.0907, batch_loss_c: 0.0812, batch_loss_s: 0.1128, time:12.9893, lr:0.001\u001b[0m\n",
            "2019-11-26 15:53:16 \u001b[32mINFO     \u001b[0m train.py: [1/50], [20/107], step: 127, 4.898 samples/sec, batch_loss: 0.0817, batch_loss_c: 0.0694, batch_loss_s: 0.1105, time:8.1665, lr:0.001\u001b[0m\n",
            "2019-11-26 15:53:24 \u001b[32mINFO     \u001b[0m train.py: [1/50], [30/107], step: 137, 5.078 samples/sec, batch_loss: 0.0914, batch_loss_c: 0.0851, batch_loss_s: 0.1061, time:7.8774, lr:0.001\u001b[0m\n",
            "2019-11-26 15:53:32 \u001b[32mINFO     \u001b[0m train.py: [1/50], [40/107], step: 147, 5.000 samples/sec, batch_loss: 0.1201, batch_loss_c: 0.1104, batch_loss_s: 0.1426, time:7.9997, lr:0.001\u001b[0m\n",
            "2019-11-26 15:53:41 \u001b[32mINFO     \u001b[0m train.py: [1/50], [50/107], step: 157, 4.574 samples/sec, batch_loss: 0.3204, batch_loss_c: 0.3108, batch_loss_s: 0.3429, time:8.7456, lr:0.001\u001b[0m\n",
            "2019-11-26 15:53:48 \u001b[32mINFO     \u001b[0m train.py: [1/50], [60/107], step: 167, 5.403 samples/sec, batch_loss: 0.1491, batch_loss_c: 0.1372, batch_loss_s: 0.1768, time:7.4036, lr:0.001\u001b[0m\n",
            "2019-11-26 15:53:55 \u001b[32mINFO     \u001b[0m train.py: [1/50], [70/107], step: 177, 5.694 samples/sec, batch_loss: 0.1325, batch_loss_c: 0.1069, batch_loss_s: 0.1922, time:7.0244, lr:0.001\u001b[0m\n",
            "2019-11-26 15:54:03 \u001b[32mINFO     \u001b[0m train.py: [1/50], [80/107], step: 187, 5.446 samples/sec, batch_loss: 0.1058, batch_loss_c: 0.1086, batch_loss_s: 0.0993, time:7.3451, lr:0.001\u001b[0m\n",
            "2019-11-26 15:54:09 \u001b[32mINFO     \u001b[0m train.py: [1/50], [90/107], step: 197, 6.481 samples/sec, batch_loss: 0.1407, batch_loss_c: 0.1199, batch_loss_s: 0.1891, time:6.1719, lr:0.001\u001b[0m\n",
            "2019-11-26 15:54:14 \u001b[32mINFO     \u001b[0m train.py: [1/50], [100/107], step: 207, 8.054 samples/sec, batch_loss: 0.1036, batch_loss_c: 0.0867, batch_loss_s: 0.1432, time:4.9663, lr:0.001\u001b[0m\n",
            "2019-11-26 15:54:17 \u001b[32mINFO     \u001b[0m train.py: [1/50], train_loss: 0.1499, time: 87.9128, lr: 0.001\u001b[0m\n",
            "2019-11-26 15:54:25 \u001b[32mINFO     \u001b[0m train.py: [2/50], [0/107], step: 214, 5.520 samples/sec, batch_loss: 0.1382, batch_loss_c: 0.1296, batch_loss_s: 0.1583, time:7.2460, lr:0.001\u001b[0m\n",
            "2019-11-26 15:54:37 \u001b[32mINFO     \u001b[0m train.py: [2/50], [10/107], step: 224, 3.287 samples/sec, batch_loss: 0.1019, batch_loss_c: 0.0937, batch_loss_s: 0.1209, time:12.1705, lr:0.001\u001b[0m\n",
            "2019-11-26 15:54:44 \u001b[32mINFO     \u001b[0m train.py: [2/50], [20/107], step: 234, 5.781 samples/sec, batch_loss: 0.0966, batch_loss_c: 0.0987, batch_loss_s: 0.0917, time:6.9190, lr:0.001\u001b[0m\n",
            "2019-11-26 15:54:53 \u001b[32mINFO     \u001b[0m train.py: [2/50], [30/107], step: 244, 4.197 samples/sec, batch_loss: 0.1576, batch_loss_c: 0.1768, batch_loss_s: 0.1126, time:9.5316, lr:0.001\u001b[0m\n",
            "2019-11-26 15:55:05 \u001b[32mINFO     \u001b[0m train.py: [2/50], [40/107], step: 254, 3.412 samples/sec, batch_loss: 0.4885, batch_loss_c: 0.4708, batch_loss_s: 0.5298, time:11.7241, lr:0.001\u001b[0m\n",
            "2019-11-26 15:55:19 \u001b[32mINFO     \u001b[0m train.py: [2/50], [50/107], step: 264, 2.958 samples/sec, batch_loss: 0.1360, batch_loss_c: 0.1156, batch_loss_s: 0.1836, time:13.5224, lr:0.001\u001b[0m\n",
            "tcmalloc: large alloc 1638408192 bytes == 0xae184000 @  0x7fb8703161e7 0x7fb8659e9f71 0x7fb865a4d55d 0x7fb865a50e28 0x7fb865a513e5 0x7fb865ae7fc2 0x50abc5 0x50d320 0x5081d5 0x50a020 0x50aa1d 0x50d320 0x5093e5 0x5951c1 0x5a04ce 0x557878 0x50c81e 0x5081d5 0x50a020 0x50aa1d 0x50c549 0x5081d5 0x50a020 0x50aa1d 0x50c549 0x5081d5 0x5895e1 0x5a04ce 0x50d8f5 0x509ce8 0x50aa1d\n",
            "2019-11-26 15:55:27 \u001b[32mINFO     \u001b[0m train.py: [2/50], [60/107], step: 274, 4.790 samples/sec, batch_loss: 0.3983, batch_loss_c: 0.4143, batch_loss_s: 0.3611, time:8.3505, lr:0.001\u001b[0m\n",
            "2019-11-26 15:55:35 \u001b[32mINFO     \u001b[0m train.py: [2/50], [70/107], step: 284, 4.878 samples/sec, batch_loss: 0.2170, batch_loss_c: 0.2228, batch_loss_s: 0.2035, time:8.1999, lr:0.001\u001b[0m\n",
            "2019-11-26 15:55:43 \u001b[32mINFO     \u001b[0m train.py: [2/50], [80/107], step: 294, 4.942 samples/sec, batch_loss: 0.0789, batch_loss_c: 0.0741, batch_loss_s: 0.0902, time:8.0932, lr:0.001\u001b[0m\n",
            "2019-11-26 15:55:49 \u001b[32mINFO     \u001b[0m train.py: [2/50], [90/107], step: 304, 6.660 samples/sec, batch_loss: 0.0602, batch_loss_c: 0.0542, batch_loss_s: 0.0742, time:6.0064, lr:0.001\u001b[0m\n",
            "2019-11-26 15:55:54 \u001b[32mINFO     \u001b[0m train.py: [2/50], [100/107], step: 314, 7.846 samples/sec, batch_loss: 0.3290, batch_loss_c: 0.3287, batch_loss_s: 0.3297, time:5.0983, lr:0.001\u001b[0m\n",
            "2019-11-26 15:55:58 \u001b[32mINFO     \u001b[0m train.py: [2/50], train_loss: 0.1606, time: 100.2935, lr: 0.001\u001b[0m\n",
            "2019-11-26 15:56:07 \u001b[32mINFO     \u001b[0m train.py: [3/50], [0/107], step: 321, 4.389 samples/sec, batch_loss: 0.2190, batch_loss_c: 0.1779, batch_loss_s: 0.3149, time:9.1130, lr:0.001\u001b[0m\n",
            "2019-11-26 15:56:15 \u001b[32mINFO     \u001b[0m train.py: [3/50], [10/107], step: 331, 5.272 samples/sec, batch_loss: 0.0617, batch_loss_c: 0.0525, batch_loss_s: 0.0833, time:7.5877, lr:0.001\u001b[0m\n",
            "2019-11-26 15:56:22 \u001b[32mINFO     \u001b[0m train.py: [3/50], [20/107], step: 341, 5.272 samples/sec, batch_loss: 0.0935, batch_loss_c: 0.0960, batch_loss_s: 0.0876, time:7.5871, lr:0.001\u001b[0m\n",
            "2019-11-26 15:56:30 \u001b[32mINFO     \u001b[0m train.py: [3/50], [30/107], step: 351, 5.472 samples/sec, batch_loss: 0.1457, batch_loss_c: 0.1336, batch_loss_s: 0.1739, time:7.3098, lr:0.001\u001b[0m\n",
            "2019-11-26 15:56:43 \u001b[32mINFO     \u001b[0m train.py: [3/50], [40/107], step: 361, 3.028 samples/sec, batch_loss: 0.1436, batch_loss_c: 0.1351, batch_loss_s: 0.1634, time:13.2117, lr:0.001\u001b[0m\n",
            "2019-11-26 15:56:53 \u001b[32mINFO     \u001b[0m train.py: [3/50], [50/107], step: 371, 4.050 samples/sec, batch_loss: 0.0783, batch_loss_c: 0.0633, batch_loss_s: 0.1131, time:9.8757, lr:0.001\u001b[0m\n",
            "2019-11-26 15:57:00 \u001b[32mINFO     \u001b[0m train.py: [3/50], [60/107], step: 381, 5.670 samples/sec, batch_loss: 0.0883, batch_loss_c: 0.0653, batch_loss_s: 0.1418, time:7.0550, lr:0.001\u001b[0m\n",
            "2019-11-26 15:57:07 \u001b[32mINFO     \u001b[0m train.py: [3/50], [70/107], step: 391, 5.366 samples/sec, batch_loss: 0.2336, batch_loss_c: 0.2422, batch_loss_s: 0.2134, time:7.4549, lr:0.001\u001b[0m\n",
            "2019-11-26 15:57:15 \u001b[32mINFO     \u001b[0m train.py: [3/50], [80/107], step: 401, 5.179 samples/sec, batch_loss: 0.2496, batch_loss_c: 0.2218, batch_loss_s: 0.3145, time:7.7238, lr:0.001\u001b[0m\n",
            "2019-11-26 15:57:22 \u001b[32mINFO     \u001b[0m train.py: [3/50], [90/107], step: 411, 5.988 samples/sec, batch_loss: 0.2077, batch_loss_c: 0.2285, batch_loss_s: 0.1592, time:6.6795, lr:0.001\u001b[0m\n",
            "2019-11-26 15:57:27 \u001b[32mINFO     \u001b[0m train.py: [3/50], [100/107], step: 421, 7.849 samples/sec, batch_loss: 0.1280, batch_loss_c: 0.1200, batch_loss_s: 0.1467, time:5.0961, lr:0.001\u001b[0m\n",
            "2019-11-26 15:57:30 \u001b[32mINFO     \u001b[0m train.py: [3/50], train_loss: 0.1642, time: 92.0922, lr: 0.001\u001b[0m\n",
            "2019-11-26 15:57:44 \u001b[32mINFO     \u001b[0m train.py: [4/50], [0/107], step: 428, 2.898 samples/sec, batch_loss: 0.0673, batch_loss_c: 0.0570, batch_loss_s: 0.0913, time:13.8034, lr:0.001\u001b[0m\n",
            "2019-11-26 15:57:53 \u001b[32mINFO     \u001b[0m train.py: [4/50], [10/107], step: 438, 4.459 samples/sec, batch_loss: 0.1893, batch_loss_c: 0.2057, batch_loss_s: 0.1508, time:8.9697, lr:0.001\u001b[0m\n",
            "2019-11-26 15:58:00 \u001b[32mINFO     \u001b[0m train.py: [4/50], [20/107], step: 448, 6.013 samples/sec, batch_loss: 0.0941, batch_loss_c: 0.0918, batch_loss_s: 0.0993, time:6.6527, lr:0.001\u001b[0m\n",
            "2019-11-26 15:58:10 \u001b[32mINFO     \u001b[0m train.py: [4/50], [30/107], step: 458, 3.998 samples/sec, batch_loss: 0.2113, batch_loss_c: 0.1852, batch_loss_s: 0.2722, time:10.0060, lr:0.001\u001b[0m\n",
            "tcmalloc: large alloc 1376698368 bytes == 0xa113c000 @  0x7fb8703161e7 0x7fb8659e9f71 0x7fb865a4d55d 0x7fb865a50e28 0x7fb865a513e5 0x7fb865ae7fc2 0x50abc5 0x50d320 0x5081d5 0x50a020 0x50aa1d 0x50d320 0x5093e5 0x5951c1 0x5a04ce 0x557878 0x50c81e 0x5081d5 0x50a020 0x50aa1d 0x50c549 0x5081d5 0x50a020 0x50aa1d 0x50c549 0x5081d5 0x5895e1 0x5a04ce 0x50d8f5 0x509ce8 0x50aa1d\n",
            "2019-11-26 15:58:19 \u001b[32mINFO     \u001b[0m train.py: [4/50], [40/107], step: 468, 4.543 samples/sec, batch_loss: 0.3238, batch_loss_c: 0.3171, batch_loss_s: 0.3395, time:8.8055, lr:0.001\u001b[0m\n",
            "2019-11-26 15:58:27 \u001b[32mINFO     \u001b[0m train.py: [4/50], [50/107], step: 478, 5.274 samples/sec, batch_loss: 0.1235, batch_loss_c: 0.1156, batch_loss_s: 0.1418, time:7.5848, lr:0.001\u001b[0m\n",
            "2019-11-26 15:58:33 \u001b[32mINFO     \u001b[0m train.py: [4/50], [60/107], step: 488, 6.440 samples/sec, batch_loss: 0.0723, batch_loss_c: 0.0638, batch_loss_s: 0.0919, time:6.2112, lr:0.001\u001b[0m\n",
            "2019-11-26 15:58:40 \u001b[32mINFO     \u001b[0m train.py: [4/50], [70/107], step: 498, 5.513 samples/sec, batch_loss: 0.0541, batch_loss_c: 0.0474, batch_loss_s: 0.0698, time:7.2551, lr:0.001\u001b[0m\n",
            "2019-11-26 15:58:49 \u001b[32mINFO     \u001b[0m train.py: [4/50], [80/107], step: 508, 4.264 samples/sec, batch_loss: 0.0871, batch_loss_c: 0.0827, batch_loss_s: 0.0975, time:9.3811, lr:0.001\u001b[0m\n",
            "2019-11-26 15:58:55 \u001b[32mINFO     \u001b[0m train.py: [4/50], [90/107], step: 518, 7.708 samples/sec, batch_loss: 0.0732, batch_loss_c: 0.0582, batch_loss_s: 0.1080, time:5.1894, lr:0.001\u001b[0m\n",
            "2019-11-26 15:59:00 \u001b[32mINFO     \u001b[0m train.py: [4/50], [100/107], step: 528, 7.867 samples/sec, batch_loss: 0.1473, batch_loss_c: 0.1497, batch_loss_s: 0.1416, time:5.0845, lr:0.001\u001b[0m\n",
            "2019-11-26 15:59:03 \u001b[32mINFO     \u001b[0m train.py: [4/50], train_loss: 0.1525, time: 92.3797, lr: 0.001\u001b[0m\n",
            "2019-11-26 15:59:13 \u001b[32mINFO     \u001b[0m train.py: [5/50], [0/107], step: 535, 4.106 samples/sec, batch_loss: 0.0826, batch_loss_c: 0.0778, batch_loss_s: 0.0937, time:9.7427, lr:0.001\u001b[0m\n",
            "2019-11-26 15:59:23 \u001b[32mINFO     \u001b[0m train.py: [5/50], [10/107], step: 545, 4.165 samples/sec, batch_loss: 0.1570, batch_loss_c: 0.1332, batch_loss_s: 0.2124, time:9.6037, lr:0.001\u001b[0m\n",
            "2019-11-26 15:59:30 \u001b[32mINFO     \u001b[0m train.py: [5/50], [20/107], step: 555, 5.375 samples/sec, batch_loss: 0.1170, batch_loss_c: 0.0926, batch_loss_s: 0.1739, time:7.4415, lr:0.001\u001b[0m\n",
            "2019-11-26 15:59:38 \u001b[32mINFO     \u001b[0m train.py: [5/50], [30/107], step: 565, 5.495 samples/sec, batch_loss: 0.2045, batch_loss_c: 0.1810, batch_loss_s: 0.2593, time:7.2798, lr:0.001\u001b[0m\n",
            "2019-11-26 15:59:45 \u001b[32mINFO     \u001b[0m train.py: [5/50], [40/107], step: 575, 5.111 samples/sec, batch_loss: 0.0848, batch_loss_c: 0.0670, batch_loss_s: 0.1265, time:7.8260, lr:0.001\u001b[0m\n",
            "2019-11-26 15:59:52 \u001b[32mINFO     \u001b[0m train.py: [5/50], [50/107], step: 585, 5.952 samples/sec, batch_loss: 0.0869, batch_loss_c: 0.0738, batch_loss_s: 0.1174, time:6.7205, lr:0.001\u001b[0m\n",
            "2019-11-26 15:59:58 \u001b[32mINFO     \u001b[0m train.py: [5/50], [60/107], step: 595, 7.113 samples/sec, batch_loss: 0.0695, batch_loss_c: 0.0681, batch_loss_s: 0.0728, time:5.6232, lr:0.001\u001b[0m\n",
            "2019-11-26 16:00:04 \u001b[32mINFO     \u001b[0m train.py: [5/50], [70/107], step: 605, 6.347 samples/sec, batch_loss: 0.1240, batch_loss_c: 0.1101, batch_loss_s: 0.1564, time:6.3018, lr:0.001\u001b[0m\n",
            "2019-11-26 16:00:12 \u001b[32mINFO     \u001b[0m train.py: [5/50], [80/107], step: 615, 5.397 samples/sec, batch_loss: 0.2938, batch_loss_c: 0.3434, batch_loss_s: 0.1783, time:7.4116, lr:0.001\u001b[0m\n",
            "2019-11-26 16:00:17 \u001b[32mINFO     \u001b[0m train.py: [5/50], [90/107], step: 625, 7.257 samples/sec, batch_loss: 0.0860, batch_loss_c: 0.0802, batch_loss_s: 0.0994, time:5.5121, lr:0.001\u001b[0m\n",
            "2019-11-26 16:00:22 \u001b[32mINFO     \u001b[0m train.py: [5/50], [100/107], step: 635, 7.950 samples/sec, batch_loss: 0.3012, batch_loss_c: 0.2893, batch_loss_s: 0.3290, time:5.0315, lr:0.001\u001b[0m\n",
            "2019-11-26 16:00:25 \u001b[32mINFO     \u001b[0m train.py: [5/50], train_loss: 0.1537, time: 81.8357, lr: 0.001\u001b[0m\n",
            "2019-11-26 16:00:34 \u001b[32mINFO     \u001b[0m train.py: [6/50], [0/107], step: 642, 4.928 samples/sec, batch_loss: 0.0757, batch_loss_c: 0.0582, batch_loss_s: 0.1166, time:8.1167, lr:0.001\u001b[0m\n",
            "tcmalloc: large alloc 1083916288 bytes == 0xa7bda000 @  0x7fb8703161e7 0x7fb8659e9f71 0x7fb865a4d55d 0x7fb865a50e28 0x7fb865a513e5 0x7fb865ae7fc2 0x50abc5 0x50d320 0x5081d5 0x50a020 0x50aa1d 0x50d320 0x5093e5 0x5951c1 0x5a04ce 0x557878 0x50c81e 0x5081d5 0x50a020 0x50aa1d 0x50c549 0x5081d5 0x50a020 0x50aa1d 0x50c549 0x5081d5 0x5895e1 0x5a04ce 0x50d8f5 0x509ce8 0x50aa1d\n",
            "2019-11-26 16:00:48 \u001b[32mINFO     \u001b[0m train.py: [6/50], [10/107], step: 652, 2.906 samples/sec, batch_loss: 0.1219, batch_loss_c: 0.1019, batch_loss_s: 0.1686, time:13.7628, lr:0.001\u001b[0m\n",
            "2019-11-26 16:00:55 \u001b[32mINFO     \u001b[0m train.py: [6/50], [20/107], step: 662, 5.943 samples/sec, batch_loss: 0.0844, batch_loss_c: 0.0702, batch_loss_s: 0.1176, time:6.7309, lr:0.001\u001b[0m\n",
            "2019-11-26 16:01:03 \u001b[32mINFO     \u001b[0m train.py: [6/50], [30/107], step: 672, 4.863 samples/sec, batch_loss: 0.0765, batch_loss_c: 0.0650, batch_loss_s: 0.1034, time:8.2248, lr:0.001\u001b[0m\n",
            "2019-11-26 16:01:10 \u001b[32mINFO     \u001b[0m train.py: [6/50], [40/107], step: 682, 5.368 samples/sec, batch_loss: 0.1095, batch_loss_c: 0.1024, batch_loss_s: 0.1261, time:7.4519, lr:0.001\u001b[0m\n",
            "2019-11-26 16:01:17 \u001b[32mINFO     \u001b[0m train.py: [6/50], [50/107], step: 692, 6.070 samples/sec, batch_loss: 0.1453, batch_loss_c: 0.1315, batch_loss_s: 0.1776, time:6.5902, lr:0.001\u001b[0m\n",
            "2019-11-26 16:01:23 \u001b[32mINFO     \u001b[0m train.py: [6/50], [60/107], step: 702, 6.956 samples/sec, batch_loss: 0.2956, batch_loss_c: 0.2820, batch_loss_s: 0.3273, time:5.7507, lr:0.001\u001b[0m\n",
            "2019-11-26 16:01:29 \u001b[32mINFO     \u001b[0m train.py: [6/50], [70/107], step: 712, 6.594 samples/sec, batch_loss: 0.0988, batch_loss_c: 0.0886, batch_loss_s: 0.1227, time:6.0661, lr:0.001\u001b[0m\n",
            "2019-11-26 16:01:36 \u001b[32mINFO     \u001b[0m train.py: [6/50], [80/107], step: 722, 5.598 samples/sec, batch_loss: 0.0601, batch_loss_c: 0.0506, batch_loss_s: 0.0823, time:7.1449, lr:0.001\u001b[0m\n",
            "2019-11-26 16:01:41 \u001b[32mINFO     \u001b[0m train.py: [6/50], [90/107], step: 732, 7.170 samples/sec, batch_loss: 0.0879, batch_loss_c: 0.0785, batch_loss_s: 0.1097, time:5.5791, lr:0.001\u001b[0m\n",
            "2019-11-26 16:01:46 \u001b[32mINFO     \u001b[0m train.py: [6/50], [100/107], step: 742, 7.927 samples/sec, batch_loss: 0.2423, batch_loss_c: 0.2176, batch_loss_s: 0.2998, time:5.0462, lr:0.001\u001b[0m\n",
            "2019-11-26 16:01:50 \u001b[32mINFO     \u001b[0m train.py: [6/50], train_loss: 0.1518, time: 83.8716, lr: 0.001\u001b[0m\n",
            "2019-11-26 16:02:02 \u001b[32mINFO     \u001b[0m train.py: [7/50], [0/107], step: 749, 3.398 samples/sec, batch_loss: 0.2203, batch_loss_c: 0.2458, batch_loss_s: 0.1608, time:11.7731, lr:0.001\u001b[0m\n",
            "2019-11-26 16:02:13 \u001b[32mINFO     \u001b[0m train.py: [7/50], [10/107], step: 759, 3.582 samples/sec, batch_loss: 0.1640, batch_loss_c: 0.1302, batch_loss_s: 0.2429, time:11.1675, lr:0.001\u001b[0m\n",
            "2019-11-26 16:02:22 \u001b[32mINFO     \u001b[0m train.py: [7/50], [20/107], step: 769, 4.762 samples/sec, batch_loss: 0.3288, batch_loss_c: 0.3154, batch_loss_s: 0.3599, time:8.4006, lr:0.001\u001b[0m\n",
            "2019-11-26 16:02:30 \u001b[32mINFO     \u001b[0m train.py: [7/50], [30/107], step: 779, 4.553 samples/sec, batch_loss: 0.2856, batch_loss_c: 0.2736, batch_loss_s: 0.3138, time:8.7860, lr:0.001\u001b[0m\n",
            "2019-11-26 16:02:38 \u001b[32mINFO     \u001b[0m train.py: [7/50], [40/107], step: 789, 5.125 samples/sec, batch_loss: 0.0634, batch_loss_c: 0.0487, batch_loss_s: 0.0979, time:7.8050, lr:0.001\u001b[0m\n",
            "2019-11-26 16:02:48 \u001b[32mINFO     \u001b[0m train.py: [7/50], [50/107], step: 799, 4.166 samples/sec, batch_loss: 0.4348, batch_loss_c: 0.4644, batch_loss_s: 0.3660, time:9.6023, lr:0.001\u001b[0m\n",
            "2019-11-26 16:02:55 \u001b[32mINFO     \u001b[0m train.py: [7/50], [60/107], step: 809, 5.228 samples/sec, batch_loss: 0.0676, batch_loss_c: 0.0427, batch_loss_s: 0.1257, time:7.6514, lr:0.001\u001b[0m\n",
            "2019-11-26 16:03:03 \u001b[32mINFO     \u001b[0m train.py: [7/50], [70/107], step: 819, 5.138 samples/sec, batch_loss: 0.1501, batch_loss_c: 0.1494, batch_loss_s: 0.1518, time:7.7851, lr:0.001\u001b[0m\n",
            "2019-11-26 16:03:10 \u001b[32mINFO     \u001b[0m train.py: [7/50], [80/107], step: 829, 5.631 samples/sec, batch_loss: 0.2136, batch_loss_c: 0.2147, batch_loss_s: 0.2111, time:7.1040, lr:0.001\u001b[0m\n",
            "2019-11-26 16:03:16 \u001b[32mINFO     \u001b[0m train.py: [7/50], [90/107], step: 839, 7.048 samples/sec, batch_loss: 0.3033, batch_loss_c: 0.2987, batch_loss_s: 0.3141, time:5.6753, lr:0.001\u001b[0m\n",
            "2019-11-26 16:03:21 \u001b[32mINFO     \u001b[0m train.py: [7/50], [100/107], step: 849, 7.918 samples/sec, batch_loss: 0.1796, batch_loss_c: 0.1965, batch_loss_s: 0.1403, time:5.0516, lr:0.001\u001b[0m\n",
            "2019-11-26 16:03:24 \u001b[32mINFO     \u001b[0m train.py: [7/50], train_loss: 0.1718, time: 94.1548, lr: 0.001\u001b[0m\n",
            "2019-11-26 16:03:31 \u001b[32mINFO     \u001b[0m train.py: [8/50], [0/107], step: 856, 6.964 samples/sec, batch_loss: 0.1058, batch_loss_c: 0.0960, batch_loss_s: 0.1286, time:5.7437, lr:0.001\u001b[0m\n",
            "2019-11-26 16:03:44 \u001b[32mINFO     \u001b[0m train.py: [8/50], [10/107], step: 866, 3.094 samples/sec, batch_loss: 0.0870, batch_loss_c: 0.0740, batch_loss_s: 0.1176, time:12.9283, lr:0.001\u001b[0m\n",
            "2019-11-26 16:03:51 \u001b[32mINFO     \u001b[0m train.py: [8/50], [20/107], step: 876, 5.508 samples/sec, batch_loss: 0.1063, batch_loss_c: 0.0994, batch_loss_s: 0.1224, time:7.2621, lr:0.001\u001b[0m\n",
            "2019-11-26 16:04:01 \u001b[32mINFO     \u001b[0m train.py: [8/50], [30/107], step: 886, 3.844 samples/sec, batch_loss: 0.0851, batch_loss_c: 0.0593, batch_loss_s: 0.1453, time:10.4070, lr:0.001\u001b[0m\n",
            "2019-11-26 16:04:14 \u001b[32mINFO     \u001b[0m train.py: [8/50], [40/107], step: 896, 3.140 samples/sec, batch_loss: 0.0566, batch_loss_c: 0.0498, batch_loss_s: 0.0727, time:12.7385, lr:0.001\u001b[0m\n",
            "tcmalloc: large alloc 1346297856 bytes == 0xab7aa000 @  0x7fb8703161e7 0x7fb8659e9f71 0x7fb865a4d55d 0x7fb865a50e28 0x7fb865a513e5 0x7fb865ae7fc2 0x50abc5 0x50d320 0x5081d5 0x50a020 0x50aa1d 0x50d320 0x5093e5 0x5951c1 0x5a04ce 0x557878 0x50c81e 0x5081d5 0x50a020 0x50aa1d 0x50c549 0x5081d5 0x50a020 0x50aa1d 0x50c549 0x5081d5 0x5895e1 0x5a04ce 0x50d8f5 0x509ce8 0x50aa1d\n",
            "2019-11-26 16:04:25 \u001b[32mINFO     \u001b[0m train.py: [8/50], [50/107], step: 906, 3.756 samples/sec, batch_loss: 0.2299, batch_loss_c: 0.2645, batch_loss_s: 0.1493, time:10.6485, lr:0.001\u001b[0m\n",
            "2019-11-26 16:04:31 \u001b[32mINFO     \u001b[0m train.py: [8/50], [60/107], step: 916, 5.930 samples/sec, batch_loss: 0.1033, batch_loss_c: 0.0955, batch_loss_s: 0.1215, time:6.7451, lr:0.001\u001b[0m\n",
            "2019-11-26 16:04:38 \u001b[32mINFO     \u001b[0m train.py: [8/50], [70/107], step: 926, 6.016 samples/sec, batch_loss: 0.0791, batch_loss_c: 0.0738, batch_loss_s: 0.0913, time:6.6484, lr:0.001\u001b[0m\n",
            "2019-11-26 16:04:45 \u001b[32mINFO     \u001b[0m train.py: [8/50], [80/107], step: 936, 5.487 samples/sec, batch_loss: 0.1115, batch_loss_c: 0.1056, batch_loss_s: 0.1252, time:7.2902, lr:0.001\u001b[0m\n",
            "2019-11-26 16:04:51 \u001b[32mINFO     \u001b[0m train.py: [8/50], [90/107], step: 946, 7.431 samples/sec, batch_loss: 0.2208, batch_loss_c: 0.2063, batch_loss_s: 0.2547, time:5.3826, lr:0.001\u001b[0m\n",
            "2019-11-26 16:04:56 \u001b[32mINFO     \u001b[0m train.py: [8/50], [100/107], step: 956, 7.898 samples/sec, batch_loss: 0.1445, batch_loss_c: 0.1576, batch_loss_s: 0.1139, time:5.0648, lr:0.001\u001b[0m\n",
            "2019-11-26 16:04:59 \u001b[32mINFO     \u001b[0m train.py: [8/50], train_loss: 0.1423, time: 94.2973, lr: 0.001\u001b[0m\n",
            "2019-11-26 16:05:06 \u001b[32mINFO     \u001b[0m train.py: [9/50], [0/107], step: 963, 6.264 samples/sec, batch_loss: 0.0993, batch_loss_c: 0.0933, batch_loss_s: 0.1132, time:6.3854, lr:0.001\u001b[0m\n",
            "2019-11-26 16:05:19 \u001b[32mINFO     \u001b[0m train.py: [9/50], [10/107], step: 973, 3.149 samples/sec, batch_loss: 0.3388, batch_loss_c: 0.3048, batch_loss_s: 0.4179, time:12.7029, lr:0.001\u001b[0m\n",
            "2019-11-26 16:05:27 \u001b[32mINFO     \u001b[0m train.py: [9/50], [20/107], step: 983, 4.679 samples/sec, batch_loss: 0.1228, batch_loss_c: 0.0983, batch_loss_s: 0.1800, time:8.5479, lr:0.001\u001b[0m\n",
            "2019-11-26 16:05:35 \u001b[32mINFO     \u001b[0m train.py: [9/50], [30/107], step: 993, 5.403 samples/sec, batch_loss: 0.1862, batch_loss_c: 0.2147, batch_loss_s: 0.1198, time:7.4029, lr:0.001\u001b[0m\n",
            "2019-11-26 16:05:43 \u001b[32mINFO     \u001b[0m train.py: [9/50], [40/107], step: 1003, 4.775 samples/sec, batch_loss: 0.3023, batch_loss_c: 0.2965, batch_loss_s: 0.3159, time:8.3766, lr:0.001\u001b[0m\n",
            "2019-11-26 16:05:53 \u001b[32mINFO     \u001b[0m train.py: [9/50], [50/107], step: 1013, 4.108 samples/sec, batch_loss: 0.3118, batch_loss_c: 0.3084, batch_loss_s: 0.3197, time:9.7379, lr:0.001\u001b[0m\n",
            "2019-11-26 16:05:59 \u001b[32mINFO     \u001b[0m train.py: [9/50], [60/107], step: 1023, 6.241 samples/sec, batch_loss: 0.1283, batch_loss_c: 0.1219, batch_loss_s: 0.1430, time:6.4093, lr:0.001\u001b[0m\n",
            "2019-11-26 16:06:07 \u001b[32mINFO     \u001b[0m train.py: [9/50], [70/107], step: 1033, 5.245 samples/sec, batch_loss: 0.2068, batch_loss_c: 0.1992, batch_loss_s: 0.2246, time:7.6265, lr:0.001\u001b[0m\n",
            "2019-11-26 16:06:15 \u001b[32mINFO     \u001b[0m train.py: [9/50], [80/107], step: 1043, 4.828 samples/sec, batch_loss: 0.1023, batch_loss_c: 0.0935, batch_loss_s: 0.1227, time:8.2856, lr:0.001\u001b[0m\n",
            "2019-11-26 16:06:22 \u001b[32mINFO     \u001b[0m train.py: [9/50], [90/107], step: 1053, 5.767 samples/sec, batch_loss: 0.0993, batch_loss_c: 0.0771, batch_loss_s: 0.1511, time:6.9361, lr:0.001\u001b[0m\n",
            "2019-11-26 16:06:27 \u001b[32mINFO     \u001b[0m train.py: [9/50], [100/107], step: 1063, 7.656 samples/sec, batch_loss: 0.1118, batch_loss_c: 0.0978, batch_loss_s: 0.1443, time:5.2247, lr:0.001\u001b[0m\n",
            "2019-11-26 16:06:31 \u001b[32mINFO     \u001b[0m train.py: [9/50], train_loss: 0.1609, time: 91.0391, lr: 0.001\u001b[0m\n",
            "2019-11-26 16:06:37 \u001b[32mINFO     \u001b[0m train.py: [10/50], [0/107], step: 1070, 6.182 samples/sec, batch_loss: 0.0778, batch_loss_c: 0.0590, batch_loss_s: 0.1217, time:6.4703, lr:0.001\u001b[0m\n",
            "2019-11-26 16:06:55 \u001b[32mINFO     \u001b[0m train.py: [10/50], [10/107], step: 1080, 2.290 samples/sec, batch_loss: 0.3150, batch_loss_c: 0.3765, batch_loss_s: 0.1714, time:17.4684, lr:0.001\u001b[0m\n",
            "2019-11-26 16:07:03 \u001b[32mINFO     \u001b[0m train.py: [10/50], [20/107], step: 1090, 4.766 samples/sec, batch_loss: 0.0812, batch_loss_c: 0.0723, batch_loss_s: 0.1021, time:8.3931, lr:0.001\u001b[0m\n",
            "2019-11-26 16:07:10 \u001b[32mINFO     \u001b[0m train.py: [10/50], [30/107], step: 1100, 5.733 samples/sec, batch_loss: 0.0852, batch_loss_c: 0.0732, batch_loss_s: 0.1134, time:6.9777, lr:0.001\u001b[0m\n",
            "2019-11-26 16:07:19 \u001b[32mINFO     \u001b[0m train.py: [10/50], [40/107], step: 1110, 4.598 samples/sec, batch_loss: 0.2501, batch_loss_c: 0.2255, batch_loss_s: 0.3076, time:8.7000, lr:0.001\u001b[0m\n",
            "2019-11-26 16:07:26 \u001b[32mINFO     \u001b[0m train.py: [10/50], [50/107], step: 1120, 5.613 samples/sec, batch_loss: 0.1173, batch_loss_c: 0.1080, batch_loss_s: 0.1389, time:7.1261, lr:0.001\u001b[0m\n",
            "2019-11-26 16:07:32 \u001b[32mINFO     \u001b[0m train.py: [10/50], [60/107], step: 1130, 6.376 samples/sec, batch_loss: 0.2396, batch_loss_c: 0.2541, batch_loss_s: 0.2056, time:6.2737, lr:0.001\u001b[0m\n",
            "2019-11-26 16:07:39 \u001b[32mINFO     \u001b[0m train.py: [10/50], [70/107], step: 1140, 5.757 samples/sec, batch_loss: 0.2651, batch_loss_c: 0.2411, batch_loss_s: 0.3211, time:6.9480, lr:0.001\u001b[0m\n",
            "2019-11-26 16:07:46 \u001b[32mINFO     \u001b[0m train.py: [10/50], [80/107], step: 1150, 6.193 samples/sec, batch_loss: 0.1289, batch_loss_c: 0.1380, batch_loss_s: 0.1077, time:6.4589, lr:0.001\u001b[0m\n",
            "2019-11-26 16:07:52 \u001b[32mINFO     \u001b[0m train.py: [10/50], [90/107], step: 1160, 6.790 samples/sec, batch_loss: 0.0930, batch_loss_c: 0.0881, batch_loss_s: 0.1044, time:5.8912, lr:0.001\u001b[0m\n",
            "2019-11-26 16:07:57 \u001b[32mINFO     \u001b[0m train.py: [10/50], [100/107], step: 1170, 7.846 samples/sec, batch_loss: 0.0667, batch_loss_c: 0.0590, batch_loss_s: 0.0847, time:5.0980, lr:0.001\u001b[0m\n",
            "2019-11-26 16:08:00 \u001b[32mINFO     \u001b[0m train.py: [10/50], train_loss: 0.1533, time: 89.1430, lr: 0.001\u001b[0m\n",
            "2019-11-26 16:08:08 \u001b[32mINFO     \u001b[0m train.py: [11/50], [0/107], step: 1177, 5.080 samples/sec, batch_loss: 0.0568, batch_loss_c: 0.0482, batch_loss_s: 0.0767, time:7.8734, lr:0.001\u001b[0m\n",
            "2019-11-26 16:08:20 \u001b[32mINFO     \u001b[0m train.py: [11/50], [10/107], step: 1187, 3.358 samples/sec, batch_loss: 0.0686, batch_loss_c: 0.0543, batch_loss_s: 0.1021, time:11.9136, lr:0.001\u001b[0m\n",
            "tcmalloc: large alloc 1083916288 bytes == 0x9bb66000 @  0x7fb8703161e7 0x7fb8659e9f71 0x7fb865a4d55d 0x7fb865a50e28 0x7fb865a513e5 0x7fb865ae7fc2 0x50abc5 0x50d320 0x5081d5 0x50a020 0x50aa1d 0x50d320 0x5093e5 0x5951c1 0x5a04ce 0x557878 0x50c81e 0x5081d5 0x50a020 0x50aa1d 0x50c549 0x5081d5 0x50a020 0x50aa1d 0x50c549 0x5081d5 0x5895e1 0x5a04ce 0x50d8f5 0x509ce8 0x50aa1d\n",
            "2019-11-26 16:08:28 \u001b[32mINFO     \u001b[0m train.py: [11/50], [20/107], step: 1197, 5.563 samples/sec, batch_loss: 0.3132, batch_loss_c: 0.3085, batch_loss_s: 0.3242, time:7.1899, lr:0.001\u001b[0m\n",
            "2019-11-26 16:08:39 \u001b[32mINFO     \u001b[0m train.py: [11/50], [30/107], step: 1207, 3.548 samples/sec, batch_loss: 0.1352, batch_loss_c: 0.1290, batch_loss_s: 0.1496, time:11.2741, lr:0.001\u001b[0m\n",
            "2019-11-26 16:08:49 \u001b[32mINFO     \u001b[0m train.py: [11/50], [40/107], step: 1217, 4.144 samples/sec, batch_loss: 0.1699, batch_loss_c: 0.1719, batch_loss_s: 0.1652, time:9.6526, lr:0.001\u001b[0m\n",
            "2019-11-26 16:08:56 \u001b[32mINFO     \u001b[0m train.py: [11/50], [50/107], step: 1227, 5.747 samples/sec, batch_loss: 0.1529, batch_loss_c: 0.1401, batch_loss_s: 0.1829, time:6.9603, lr:0.001\u001b[0m\n",
            "2019-11-26 16:09:04 \u001b[32mINFO     \u001b[0m train.py: [11/50], [60/107], step: 1237, 4.691 samples/sec, batch_loss: 0.1087, batch_loss_c: 0.1029, batch_loss_s: 0.1221, time:8.5274, lr:0.001\u001b[0m\n",
            "2019-11-26 16:09:11 \u001b[32mINFO     \u001b[0m train.py: [11/50], [70/107], step: 1247, 5.782 samples/sec, batch_loss: 0.4747, batch_loss_c: 0.5077, batch_loss_s: 0.3977, time:6.9180, lr:0.001\u001b[0m\n",
            "2019-11-26 16:09:18 \u001b[32mINFO     \u001b[0m train.py: [11/50], [80/107], step: 1257, 5.621 samples/sec, batch_loss: 0.1110, batch_loss_c: 0.1141, batch_loss_s: 0.1036, time:7.1168, lr:0.001\u001b[0m\n",
            "2019-11-26 16:09:24 \u001b[32mINFO     \u001b[0m train.py: [11/50], [90/107], step: 1267, 7.302 samples/sec, batch_loss: 0.2925, batch_loss_c: 0.2745, batch_loss_s: 0.3344, time:5.4782, lr:0.001\u001b[0m\n",
            "2019-11-26 16:09:29 \u001b[32mINFO     \u001b[0m train.py: [11/50], [100/107], step: 1277, 7.916 samples/sec, batch_loss: 0.1051, batch_loss_c: 0.1042, batch_loss_s: 0.1073, time:5.0528, lr:0.001\u001b[0m\n",
            "2019-11-26 16:09:32 \u001b[32mINFO     \u001b[0m train.py: [11/50], train_loss: 0.1374, time: 91.3319, lr: 0.001\u001b[0m\n",
            "2019-11-26 16:09:41 \u001b[32mINFO     \u001b[0m train.py: [12/50], [0/107], step: 1284, 4.955 samples/sec, batch_loss: 0.0817, batch_loss_c: 0.0766, batch_loss_s: 0.0937, time:8.0724, lr:0.001\u001b[0m\n",
            "2019-11-26 16:09:52 \u001b[32mINFO     \u001b[0m train.py: [12/50], [10/107], step: 1294, 3.542 samples/sec, batch_loss: 0.0720, batch_loss_c: 0.0613, batch_loss_s: 0.0970, time:11.2920, lr:0.001\u001b[0m\n",
            "2019-11-26 16:09:59 \u001b[32mINFO     \u001b[0m train.py: [12/50], [20/107], step: 1304, 5.406 samples/sec, batch_loss: 0.0896, batch_loss_c: 0.0784, batch_loss_s: 0.1157, time:7.3992, lr:0.001\u001b[0m\n",
            "2019-11-26 16:10:07 \u001b[32mINFO     \u001b[0m train.py: [12/50], [30/107], step: 1314, 5.222 samples/sec, batch_loss: 0.0730, batch_loss_c: 0.0616, batch_loss_s: 0.0996, time:7.6602, lr:0.001\u001b[0m\n",
            "2019-11-26 16:10:14 \u001b[32mINFO     \u001b[0m train.py: [12/50], [40/107], step: 1324, 5.407 samples/sec, batch_loss: 0.1476, batch_loss_c: 0.1445, batch_loss_s: 0.1548, time:7.3984, lr:0.001\u001b[0m\n",
            "2019-11-26 16:10:22 \u001b[32mINFO     \u001b[0m train.py: [12/50], [50/107], step: 1334, 5.390 samples/sec, batch_loss: 0.0806, batch_loss_c: 0.0738, batch_loss_s: 0.0962, time:7.4214, lr:0.001\u001b[0m\n",
            "2019-11-26 16:10:28 \u001b[32mINFO     \u001b[0m train.py: [12/50], [60/107], step: 1344, 6.560 samples/sec, batch_loss: 0.0762, batch_loss_c: 0.0733, batch_loss_s: 0.0829, time:6.0980, lr:0.001\u001b[0m\n",
            "2019-11-26 16:10:34 \u001b[32mINFO     \u001b[0m train.py: [12/50], [70/107], step: 1354, 6.735 samples/sec, batch_loss: 0.0817, batch_loss_c: 0.0712, batch_loss_s: 0.1063, time:5.9391, lr:0.001\u001b[0m\n",
            "2019-11-26 16:10:41 \u001b[32mINFO     \u001b[0m train.py: [12/50], [80/107], step: 1364, 5.274 samples/sec, batch_loss: 0.0830, batch_loss_c: 0.0672, batch_loss_s: 0.1199, time:7.5840, lr:0.001\u001b[0m\n",
            "2019-11-26 16:10:50 \u001b[32mINFO     \u001b[0m train.py: [12/50], [90/107], step: 1374, 4.895 samples/sec, batch_loss: 0.0693, batch_loss_c: 0.0557, batch_loss_s: 0.1010, time:8.1717, lr:0.001\u001b[0m\n",
            "2019-11-26 16:10:55 \u001b[32mINFO     \u001b[0m train.py: [12/50], [100/107], step: 1384, 7.839 samples/sec, batch_loss: 0.3909, batch_loss_c: 0.3797, batch_loss_s: 0.4171, time:5.1028, lr:0.001\u001b[0m\n",
            "2019-11-26 16:10:58 \u001b[32mINFO     \u001b[0m train.py: [12/50], train_loss: 0.1328, time: 85.5811, lr: 0.001\u001b[0m\n",
            "2019-11-26 16:11:10 \u001b[32mINFO     \u001b[0m train.py: [13/50], [0/107], step: 1391, 3.573 samples/sec, batch_loss: 0.1551, batch_loss_c: 0.1861, batch_loss_s: 0.0829, time:11.1935, lr:0.001\u001b[0m\n",
            "2019-11-26 16:11:18 \u001b[32mINFO     \u001b[0m train.py: [13/50], [10/107], step: 1401, 5.163 samples/sec, batch_loss: 0.1281, batch_loss_c: 0.1201, batch_loss_s: 0.1468, time:7.7469, lr:0.001\u001b[0m\n",
            "2019-11-26 16:11:25 \u001b[32mINFO     \u001b[0m train.py: [13/50], [20/107], step: 1411, 5.552 samples/sec, batch_loss: 0.3814, batch_loss_c: 0.4035, batch_loss_s: 0.3298, time:7.2051, lr:0.001\u001b[0m\n",
            "2019-11-26 16:11:34 \u001b[32mINFO     \u001b[0m train.py: [13/50], [30/107], step: 1421, 4.606 samples/sec, batch_loss: 0.0977, batch_loss_c: 0.0854, batch_loss_s: 0.1264, time:8.6844, lr:0.001\u001b[0m\n",
            "2019-11-26 16:11:42 \u001b[32mINFO     \u001b[0m train.py: [13/50], [40/107], step: 1431, 4.964 samples/sec, batch_loss: 0.0848, batch_loss_c: 0.0702, batch_loss_s: 0.1187, time:8.0575, lr:0.001\u001b[0m\n",
            "2019-11-26 16:11:49 \u001b[32mINFO     \u001b[0m train.py: [13/50], [50/107], step: 1441, 5.271 samples/sec, batch_loss: 0.0896, batch_loss_c: 0.0762, batch_loss_s: 0.1210, time:7.5889, lr:0.001\u001b[0m\n",
            "2019-11-26 16:11:56 \u001b[32mINFO     \u001b[0m train.py: [13/50], [60/107], step: 1451, 6.207 samples/sec, batch_loss: 0.0934, batch_loss_c: 0.0911, batch_loss_s: 0.0988, time:6.4440, lr:0.001\u001b[0m\n",
            "2019-11-26 16:12:02 \u001b[32mINFO     \u001b[0m train.py: [13/50], [70/107], step: 1461, 6.179 samples/sec, batch_loss: 0.1457, batch_loss_c: 0.1366, batch_loss_s: 0.1669, time:6.4735, lr:0.001\u001b[0m\n",
            "2019-11-26 16:12:10 \u001b[32mINFO     \u001b[0m train.py: [13/50], [80/107], step: 1471, 5.217 samples/sec, batch_loss: 0.0607, batch_loss_c: 0.0526, batch_loss_s: 0.0797, time:7.6679, lr:0.001\u001b[0m\n",
            "2019-11-26 16:12:16 \u001b[32mINFO     \u001b[0m train.py: [13/50], [90/107], step: 1481, 7.188 samples/sec, batch_loss: 0.0644, batch_loss_c: 0.0570, batch_loss_s: 0.0814, time:5.5645, lr:0.001\u001b[0m\n",
            "2019-11-26 16:12:21 \u001b[32mINFO     \u001b[0m train.py: [13/50], [100/107], step: 1491, 7.861 samples/sec, batch_loss: 0.0698, batch_loss_c: 0.0591, batch_loss_s: 0.0948, time:5.0883, lr:0.001\u001b[0m\n",
            "2019-11-26 16:12:24 \u001b[32mINFO     \u001b[0m train.py: [13/50], train_loss: 0.1399, time: 85.0015, lr: 0.001\u001b[0m\n",
            "2019-11-26 16:12:31 \u001b[32mINFO     \u001b[0m train.py: [14/50], [0/107], step: 1498, 5.720 samples/sec, batch_loss: 0.2249, batch_loss_c: 0.2101, batch_loss_s: 0.2594, time:6.9933, lr:0.001\u001b[0m\n",
            "2019-11-26 16:12:45 \u001b[32mINFO     \u001b[0m train.py: [14/50], [10/107], step: 1508, 3.022 samples/sec, batch_loss: 0.0881, batch_loss_c: 0.0827, batch_loss_s: 0.1006, time:13.2373, lr:0.001\u001b[0m\n",
            "2019-11-26 16:12:51 \u001b[32mINFO     \u001b[0m train.py: [14/50], [20/107], step: 1518, 6.658 samples/sec, batch_loss: 0.1428, batch_loss_c: 0.1508, batch_loss_s: 0.1243, time:6.0075, lr:0.001\u001b[0m\n",
            "2019-11-26 16:12:58 \u001b[32mINFO     \u001b[0m train.py: [14/50], [30/107], step: 1528, 5.554 samples/sec, batch_loss: 0.0784, batch_loss_c: 0.0683, batch_loss_s: 0.1020, time:7.2015, lr:0.001\u001b[0m\n",
            "2019-11-26 16:13:06 \u001b[32mINFO     \u001b[0m train.py: [14/50], [40/107], step: 1538, 5.154 samples/sec, batch_loss: 0.1309, batch_loss_c: 0.1197, batch_loss_s: 0.1573, time:7.7612, lr:0.001\u001b[0m\n",
            "2019-11-26 16:13:12 \u001b[32mINFO     \u001b[0m train.py: [14/50], [50/107], step: 1548, 6.108 samples/sec, batch_loss: 0.1953, batch_loss_c: 0.1906, batch_loss_s: 0.2062, time:6.5489, lr:0.001\u001b[0m\n",
            "2019-11-26 16:13:20 \u001b[32mINFO     \u001b[0m train.py: [14/50], [60/107], step: 1558, 5.288 samples/sec, batch_loss: 0.3167, batch_loss_c: 0.3171, batch_loss_s: 0.3160, time:7.5649, lr:0.001\u001b[0m\n",
            "2019-11-26 16:13:27 \u001b[32mINFO     \u001b[0m train.py: [14/50], [70/107], step: 1568, 5.858 samples/sec, batch_loss: 0.0658, batch_loss_c: 0.0573, batch_loss_s: 0.0855, time:6.8286, lr:0.001\u001b[0m\n",
            "2019-11-26 16:13:35 \u001b[32mINFO     \u001b[0m train.py: [14/50], [80/107], step: 1578, 4.537 samples/sec, batch_loss: 0.1314, batch_loss_c: 0.1520, batch_loss_s: 0.0835, time:8.8157, lr:0.001\u001b[0m\n",
            "2019-11-26 16:13:42 \u001b[32mINFO     \u001b[0m train.py: [14/50], [90/107], step: 1588, 6.490 samples/sec, batch_loss: 0.0694, batch_loss_c: 0.0603, batch_loss_s: 0.0906, time:6.1633, lr:0.001\u001b[0m\n",
            "2019-11-26 16:13:47 \u001b[32mINFO     \u001b[0m train.py: [14/50], [100/107], step: 1598, 7.759 samples/sec, batch_loss: 0.0730, batch_loss_c: 0.0646, batch_loss_s: 0.0928, time:5.1550, lr:0.001\u001b[0m\n",
            "2019-11-26 16:13:50 \u001b[32mINFO     \u001b[0m train.py: [14/50], train_loss: 0.1397, time: 85.6483, lr: 0.001\u001b[0m\n",
            "2019-11-26 16:14:00 \u001b[32mINFO     \u001b[0m train.py: [15/50], [0/107], step: 1605, 4.212 samples/sec, batch_loss: 0.3422, batch_loss_c: 0.3285, batch_loss_s: 0.3742, time:9.4970, lr:0.001\u001b[0m\n",
            "2019-11-26 16:14:09 \u001b[32mINFO     \u001b[0m train.py: [15/50], [10/107], step: 1615, 4.693 samples/sec, batch_loss: 0.0967, batch_loss_c: 0.0809, batch_loss_s: 0.1337, time:8.5229, lr:0.001\u001b[0m\n",
            "2019-11-26 16:14:19 \u001b[32mINFO     \u001b[0m train.py: [15/50], [20/107], step: 1625, 4.000 samples/sec, batch_loss: 0.0722, batch_loss_c: 0.0645, batch_loss_s: 0.0904, time:9.9992, lr:0.001\u001b[0m\n",
            "2019-11-26 16:14:29 \u001b[32mINFO     \u001b[0m train.py: [15/50], [30/107], step: 1635, 3.969 samples/sec, batch_loss: 0.1349, batch_loss_c: 0.1282, batch_loss_s: 0.1507, time:10.0793, lr:0.001\u001b[0m\n",
            "tcmalloc: large alloc 1431633920 bytes == 0xb8442000 @  0x7fb8703161e7 0x7fb8659e9f71 0x7fb865a4d55d 0x7fb865a50e28 0x7fb865a513e5 0x7fb865ae7fc2 0x50abc5 0x50d320 0x5081d5 0x50a020 0x50aa1d 0x50d320 0x5093e5 0x5951c1 0x5a04ce 0x557878 0x50c81e 0x5081d5 0x50a020 0x50aa1d 0x50c549 0x5081d5 0x50a020 0x50aa1d 0x50c549 0x5081d5 0x5895e1 0x5a04ce 0x50d8f5 0x509ce8 0x50aa1d\n",
            "2019-11-26 16:14:39 \u001b[32mINFO     \u001b[0m train.py: [15/50], [40/107], step: 1645, 4.037 samples/sec, batch_loss: 0.1174, batch_loss_c: 0.1098, batch_loss_s: 0.1350, time:9.9074, lr:0.001\u001b[0m\n",
            "2019-11-26 16:14:48 \u001b[32mINFO     \u001b[0m train.py: [15/50], [50/107], step: 1655, 4.253 samples/sec, batch_loss: 0.3464, batch_loss_c: 0.3320, batch_loss_s: 0.3799, time:9.4053, lr:0.001\u001b[0m\n",
            "2019-11-26 16:14:55 \u001b[32mINFO     \u001b[0m train.py: [15/50], [60/107], step: 1665, 5.454 samples/sec, batch_loss: 0.0740, batch_loss_c: 0.0620, batch_loss_s: 0.1019, time:7.3344, lr:0.001\u001b[0m\n",
            "2019-11-26 16:15:02 \u001b[32mINFO     \u001b[0m train.py: [15/50], [70/107], step: 1675, 6.271 samples/sec, batch_loss: 0.1096, batch_loss_c: 0.0978, batch_loss_s: 0.1371, time:6.3781, lr:0.001\u001b[0m\n",
            "2019-11-26 16:15:09 \u001b[32mINFO     \u001b[0m train.py: [15/50], [80/107], step: 1685, 5.734 samples/sec, batch_loss: 0.0752, batch_loss_c: 0.0690, batch_loss_s: 0.0896, time:6.9754, lr:0.001\u001b[0m\n",
            "2019-11-26 16:15:14 \u001b[32mINFO     \u001b[0m train.py: [15/50], [90/107], step: 1695, 7.182 samples/sec, batch_loss: 0.0599, batch_loss_c: 0.0530, batch_loss_s: 0.0759, time:5.5698, lr:0.001\u001b[0m\n",
            "2019-11-26 16:15:19 \u001b[32mINFO     \u001b[0m train.py: [15/50], [100/107], step: 1705, 7.922 samples/sec, batch_loss: 0.1025, batch_loss_c: 0.0990, batch_loss_s: 0.1107, time:5.0492, lr:0.001\u001b[0m\n",
            "2019-11-26 16:15:23 \u001b[32mINFO     \u001b[0m train.py: [15/50], train_loss: 0.1486, time: 92.1450, lr: 0.001\u001b[0m\n",
            "2019-11-26 16:15:38 \u001b[32mINFO     \u001b[0m train.py: [16/50], [0/107], step: 1712, 2.757 samples/sec, batch_loss: 0.1397, batch_loss_c: 0.1550, batch_loss_s: 0.1040, time:14.5067, lr:0.001\u001b[0m\n",
            "2019-11-26 16:15:44 \u001b[32mINFO     \u001b[0m train.py: [16/50], [10/107], step: 1722, 5.921 samples/sec, batch_loss: 0.0897, batch_loss_c: 0.0802, batch_loss_s: 0.1120, time:6.7557, lr:0.001\u001b[0m\n",
            "2019-11-26 16:15:51 \u001b[32mINFO     \u001b[0m train.py: [16/50], [20/107], step: 1732, 5.785 samples/sec, batch_loss: 0.0808, batch_loss_c: 0.0791, batch_loss_s: 0.0848, time:6.9149, lr:0.001\u001b[0m\n",
            "2019-11-26 16:15:59 \u001b[32mINFO     \u001b[0m train.py: [16/50], [30/107], step: 1742, 5.176 samples/sec, batch_loss: 0.1139, batch_loss_c: 0.0958, batch_loss_s: 0.1562, time:7.7277, lr:0.001\u001b[0m\n",
            "2019-11-26 16:16:07 \u001b[32mINFO     \u001b[0m train.py: [16/50], [40/107], step: 1752, 5.368 samples/sec, batch_loss: 0.1023, batch_loss_c: 0.0989, batch_loss_s: 0.1104, time:7.4516, lr:0.001\u001b[0m\n",
            "2019-11-26 16:16:14 \u001b[32mINFO     \u001b[0m train.py: [16/50], [50/107], step: 1762, 5.285 samples/sec, batch_loss: 0.2315, batch_loss_c: 0.2402, batch_loss_s: 0.2113, time:7.5685, lr:0.001\u001b[0m\n",
            "2019-11-26 16:16:21 \u001b[32mINFO     \u001b[0m train.py: [16/50], [60/107], step: 1772, 6.113 samples/sec, batch_loss: 0.0696, batch_loss_c: 0.0664, batch_loss_s: 0.0771, time:6.5433, lr:0.001\u001b[0m\n",
            "2019-11-26 16:16:28 \u001b[32mINFO     \u001b[0m train.py: [16/50], [70/107], step: 1782, 5.768 samples/sec, batch_loss: 0.0862, batch_loss_c: 0.0741, batch_loss_s: 0.1144, time:6.9343, lr:0.001\u001b[0m\n",
            "2019-11-26 16:16:35 \u001b[32mINFO     \u001b[0m train.py: [16/50], [80/107], step: 1792, 5.354 samples/sec, batch_loss: 0.1481, batch_loss_c: 0.1518, batch_loss_s: 0.1395, time:7.4716, lr:0.001\u001b[0m\n",
            "2019-11-26 16:16:41 \u001b[32mINFO     \u001b[0m train.py: [16/50], [90/107], step: 1802, 6.569 samples/sec, batch_loss: 0.1876, batch_loss_c: 0.1792, batch_loss_s: 0.2071, time:6.0889, lr:0.001\u001b[0m\n",
            "2019-11-26 16:16:46 \u001b[32mINFO     \u001b[0m train.py: [16/50], [100/107], step: 1812, 7.930 samples/sec, batch_loss: 0.1051, batch_loss_c: 0.1052, batch_loss_s: 0.1050, time:5.0439, lr:0.001\u001b[0m\n",
            "2019-11-26 16:16:50 \u001b[32mINFO     \u001b[0m train.py: [16/50], train_loss: 0.1376, time: 86.3988, lr: 0.001\u001b[0m\n",
            "2019-11-26 16:16:57 \u001b[32mINFO     \u001b[0m train.py: [17/50], [0/107], step: 1819, 5.598 samples/sec, batch_loss: 0.0761, batch_loss_c: 0.0667, batch_loss_s: 0.0983, time:7.1451, lr:0.001\u001b[0m\n",
            "2019-11-26 16:17:08 \u001b[32mINFO     \u001b[0m train.py: [17/50], [10/107], step: 1829, 3.541 samples/sec, batch_loss: 0.0830, batch_loss_c: 0.0647, batch_loss_s: 0.1256, time:11.2951, lr:0.001\u001b[0m\n",
            "2019-11-26 16:17:16 \u001b[32mINFO     \u001b[0m train.py: [17/50], [20/107], step: 1839, 5.138 samples/sec, batch_loss: 0.1298, batch_loss_c: 0.1319, batch_loss_s: 0.1249, time:7.7851, lr:0.001\u001b[0m\n",
            "2019-11-26 16:17:23 \u001b[32mINFO     \u001b[0m train.py: [17/50], [30/107], step: 1849, 6.120 samples/sec, batch_loss: 0.0813, batch_loss_c: 0.0659, batch_loss_s: 0.1173, time:6.5359, lr:0.001\u001b[0m\n",
            "2019-11-26 16:17:30 \u001b[32mINFO     \u001b[0m train.py: [17/50], [40/107], step: 1859, 5.603 samples/sec, batch_loss: 0.1317, batch_loss_c: 0.1290, batch_loss_s: 0.1379, time:7.1387, lr:0.001\u001b[0m\n",
            "2019-11-26 16:17:37 \u001b[32mINFO     \u001b[0m train.py: [17/50], [50/107], step: 1869, 5.557 samples/sec, batch_loss: 0.2684, batch_loss_c: 0.2850, batch_loss_s: 0.2299, time:7.1975, lr:0.001\u001b[0m\n",
            "2019-11-26 16:17:43 \u001b[32mINFO     \u001b[0m train.py: [17/50], [60/107], step: 1879, 6.553 samples/sec, batch_loss: 0.3128, batch_loss_c: 0.3111, batch_loss_s: 0.3168, time:6.1039, lr:0.001\u001b[0m\n",
            "2019-11-26 16:17:50 \u001b[32mINFO     \u001b[0m train.py: [17/50], [70/107], step: 1889, 5.997 samples/sec, batch_loss: 0.3029, batch_loss_c: 0.2925, batch_loss_s: 0.3271, time:6.6697, lr:0.001\u001b[0m\n",
            "2019-11-26 16:17:58 \u001b[32mINFO     \u001b[0m train.py: [17/50], [80/107], step: 1899, 4.870 samples/sec, batch_loss: 0.0727, batch_loss_c: 0.0605, batch_loss_s: 0.1010, time:8.2128, lr:0.001\u001b[0m\n",
            "2019-11-26 16:18:06 \u001b[32mINFO     \u001b[0m train.py: [17/50], [90/107], step: 1909, 5.367 samples/sec, batch_loss: 0.0839, batch_loss_c: 0.0746, batch_loss_s: 0.1056, time:7.4524, lr:0.001\u001b[0m\n",
            "2019-11-26 16:18:11 \u001b[32mINFO     \u001b[0m train.py: [17/50], [100/107], step: 1919, 7.827 samples/sec, batch_loss: 0.0919, batch_loss_c: 0.0908, batch_loss_s: 0.0945, time:5.1103, lr:0.001\u001b[0m\n",
            "2019-11-26 16:18:14 \u001b[32mINFO     \u001b[0m train.py: [17/50], train_loss: 0.1475, time: 83.9872, lr: 0.001\u001b[0m\n",
            "2019-11-26 16:18:20 \u001b[32mINFO     \u001b[0m train.py: [18/50], [0/107], step: 1926, 7.041 samples/sec, batch_loss: 0.1152, batch_loss_c: 0.1338, batch_loss_s: 0.0718, time:5.6808, lr:0.001\u001b[0m\n",
            "2019-11-26 16:18:31 \u001b[32mINFO     \u001b[0m train.py: [18/50], [10/107], step: 1936, 3.717 samples/sec, batch_loss: 0.1156, batch_loss_c: 0.1152, batch_loss_s: 0.1165, time:10.7626, lr:0.001\u001b[0m\n",
            "2019-11-26 16:18:39 \u001b[32mINFO     \u001b[0m train.py: [18/50], [20/107], step: 1946, 4.950 samples/sec, batch_loss: 0.0881, batch_loss_c: 0.0890, batch_loss_s: 0.0859, time:8.0811, lr:0.001\u001b[0m\n",
            "2019-11-26 16:18:46 \u001b[32mINFO     \u001b[0m train.py: [18/50], [30/107], step: 1956, 5.667 samples/sec, batch_loss: 0.1874, batch_loss_c: 0.1680, batch_loss_s: 0.2327, time:7.0582, lr:0.001\u001b[0m\n",
            "2019-11-26 16:18:53 \u001b[32mINFO     \u001b[0m train.py: [18/50], [40/107], step: 1966, 5.561 samples/sec, batch_loss: 0.1236, batch_loss_c: 0.1196, batch_loss_s: 0.1330, time:7.1926, lr:0.001\u001b[0m\n",
            "2019-11-26 16:19:00 \u001b[32mINFO     \u001b[0m train.py: [18/50], [50/107], step: 1976, 6.097 samples/sec, batch_loss: 0.1130, batch_loss_c: 0.1038, batch_loss_s: 0.1345, time:6.5605, lr:0.001\u001b[0m\n",
            "2019-11-26 16:19:06 \u001b[32mINFO     \u001b[0m train.py: [18/50], [60/107], step: 1986, 6.748 samples/sec, batch_loss: 0.3272, batch_loss_c: 0.3200, batch_loss_s: 0.3442, time:5.9280, lr:0.001\u001b[0m\n",
            "2019-11-26 16:19:14 \u001b[32mINFO     \u001b[0m train.py: [18/50], [70/107], step: 1996, 4.832 samples/sec, batch_loss: 0.0701, batch_loss_c: 0.0566, batch_loss_s: 0.1018, time:8.2787, lr:0.001\u001b[0m\n",
            "2019-11-26 16:19:21 \u001b[32mINFO     \u001b[0m train.py: [18/50], [80/107], step: 2006, 6.127 samples/sec, batch_loss: 0.3803, batch_loss_c: 0.3845, batch_loss_s: 0.3706, time:6.5287, lr:0.001\u001b[0m\n",
            "2019-11-26 16:19:27 \u001b[32mINFO     \u001b[0m train.py: [18/50], [90/107], step: 2016, 6.781 samples/sec, batch_loss: 0.1341, batch_loss_c: 0.1307, batch_loss_s: 0.1420, time:5.8985, lr:0.001\u001b[0m\n",
            "2019-11-26 16:19:32 \u001b[32mINFO     \u001b[0m train.py: [18/50], [100/107], step: 2026, 8.027 samples/sec, batch_loss: 0.0833, batch_loss_c: 0.0811, batch_loss_s: 0.0884, time:4.9834, lr:0.001\u001b[0m\n",
            "2019-11-26 16:19:35 \u001b[32mINFO     \u001b[0m train.py: [18/50], train_loss: 0.1186, time: 80.3097, lr: 0.001\u001b[0m\n",
            "2019-11-26 16:19:45 \u001b[32mINFO     \u001b[0m train.py: [19/50], [0/107], step: 2033, 3.900 samples/sec, batch_loss: 0.0656, batch_loss_c: 0.0588, batch_loss_s: 0.0816, time:10.2558, lr:0.001\u001b[0m\n",
            "2019-11-26 16:19:57 \u001b[32mINFO     \u001b[0m train.py: [19/50], [10/107], step: 2043, 3.596 samples/sec, batch_loss: 0.0599, batch_loss_c: 0.0489, batch_loss_s: 0.0855, time:11.1236, lr:0.001\u001b[0m\n",
            "2019-11-26 16:20:03 \u001b[32mINFO     \u001b[0m train.py: [19/50], [20/107], step: 2053, 6.393 samples/sec, batch_loss: 0.0548, batch_loss_c: 0.0412, batch_loss_s: 0.0866, time:6.2570, lr:0.001\u001b[0m\n",
            "2019-11-26 16:20:11 \u001b[32mINFO     \u001b[0m train.py: [19/50], [30/107], step: 2063, 4.712 samples/sec, batch_loss: 0.0700, batch_loss_c: 0.0616, batch_loss_s: 0.0898, time:8.4881, lr:0.001\u001b[0m\n",
            "2019-11-26 16:20:23 \u001b[32mINFO     \u001b[0m train.py: [19/50], [40/107], step: 2073, 3.368 samples/sec, batch_loss: 0.1875, batch_loss_c: 0.1759, batch_loss_s: 0.2146, time:11.8757, lr:0.001\u001b[0m\n",
            "2019-11-26 16:20:31 \u001b[32mINFO     \u001b[0m train.py: [19/50], [50/107], step: 2083, 5.149 samples/sec, batch_loss: 0.1101, batch_loss_c: 0.1047, batch_loss_s: 0.1227, time:7.7682, lr:0.001\u001b[0m\n",
            "2019-11-26 16:20:37 \u001b[32mINFO     \u001b[0m train.py: [19/50], [60/107], step: 2093, 6.552 samples/sec, batch_loss: 0.3298, batch_loss_c: 0.3237, batch_loss_s: 0.3440, time:6.1049, lr:0.001\u001b[0m\n",
            "2019-11-26 16:20:43 \u001b[32mINFO     \u001b[0m train.py: [19/50], [70/107], step: 2103, 6.820 samples/sec, batch_loss: 0.0968, batch_loss_c: 0.0919, batch_loss_s: 0.1084, time:5.8650, lr:0.001\u001b[0m\n",
            "2019-11-26 16:20:50 \u001b[32mINFO     \u001b[0m train.py: [19/50], [80/107], step: 2113, 5.343 samples/sec, batch_loss: 0.0730, batch_loss_c: 0.0667, batch_loss_s: 0.0877, time:7.4865, lr:0.001\u001b[0m\n",
            "2019-11-26 16:20:56 \u001b[32mINFO     \u001b[0m train.py: [19/50], [90/107], step: 2123, 6.829 samples/sec, batch_loss: 0.0861, batch_loss_c: 0.0716, batch_loss_s: 0.1200, time:5.8575, lr:0.001\u001b[0m\n",
            "2019-11-26 16:21:01 \u001b[32mINFO     \u001b[0m train.py: [19/50], [100/107], step: 2133, 7.887 samples/sec, batch_loss: 0.0672, batch_loss_c: 0.0522, batch_loss_s: 0.1021, time:5.0714, lr:0.001\u001b[0m\n",
            "2019-11-26 16:21:05 \u001b[32mINFO     \u001b[0m train.py: [19/50], train_loss: 0.1368, time: 89.5476, lr: 0.001\u001b[0m\n",
            "2019-11-26 16:21:11 \u001b[32mINFO     \u001b[0m train.py: [20/50], [0/107], step: 2140, 7.093 samples/sec, batch_loss: 0.0691, batch_loss_c: 0.0643, batch_loss_s: 0.0804, time:5.6391, lr:0.001\u001b[0m\n",
            "2019-11-26 16:21:27 \u001b[32mINFO     \u001b[0m train.py: [20/50], [10/107], step: 2150, 2.528 samples/sec, batch_loss: 0.0867, batch_loss_c: 0.0798, batch_loss_s: 0.1030, time:15.8219, lr:0.001\u001b[0m\n",
            "2019-11-26 16:21:34 \u001b[32mINFO     \u001b[0m train.py: [20/50], [20/107], step: 2160, 5.250 samples/sec, batch_loss: 0.0760, batch_loss_c: 0.0578, batch_loss_s: 0.1184, time:7.6189, lr:0.001\u001b[0m\n",
            "2019-11-26 16:21:41 \u001b[32mINFO     \u001b[0m train.py: [20/50], [30/107], step: 2170, 6.406 samples/sec, batch_loss: 0.0654, batch_loss_c: 0.0501, batch_loss_s: 0.1010, time:6.2437, lr:0.001\u001b[0m\n",
            "2019-11-26 16:21:50 \u001b[32mINFO     \u001b[0m train.py: [20/50], [40/107], step: 2180, 4.357 samples/sec, batch_loss: 0.0857, batch_loss_c: 0.0780, batch_loss_s: 0.1039, time:9.1804, lr:0.001\u001b[0m\n",
            "2019-11-26 16:21:57 \u001b[32mINFO     \u001b[0m train.py: [20/50], [50/107], step: 2190, 5.397 samples/sec, batch_loss: 0.0967, batch_loss_c: 0.0734, batch_loss_s: 0.1509, time:7.4111, lr:0.001\u001b[0m\n",
            "2019-11-26 16:22:03 \u001b[32mINFO     \u001b[0m train.py: [20/50], [60/107], step: 2200, 6.797 samples/sec, batch_loss: 0.1138, batch_loss_c: 0.1058, batch_loss_s: 0.1324, time:5.8847, lr:0.001\u001b[0m\n",
            "2019-11-26 16:22:10 \u001b[32mINFO     \u001b[0m train.py: [20/50], [70/107], step: 2210, 5.547 samples/sec, batch_loss: 0.2370, batch_loss_c: 0.2259, batch_loss_s: 0.2629, time:7.2105, lr:0.001\u001b[0m\n",
            "2019-11-26 16:22:18 \u001b[32mINFO     \u001b[0m train.py: [20/50], [80/107], step: 2220, 5.485 samples/sec, batch_loss: 0.1056, batch_loss_c: 0.0950, batch_loss_s: 0.1302, time:7.2929, lr:0.001\u001b[0m\n",
            "2019-11-26 16:22:24 \u001b[32mINFO     \u001b[0m train.py: [20/50], [90/107], step: 2230, 6.673 samples/sec, batch_loss: 0.0714, batch_loss_c: 0.0674, batch_loss_s: 0.0809, time:5.9944, lr:0.001\u001b[0m\n",
            "2019-11-26 16:22:29 \u001b[32mINFO     \u001b[0m train.py: [20/50], [100/107], step: 2240, 7.876 samples/sec, batch_loss: 0.3132, batch_loss_c: 0.3094, batch_loss_s: 0.3220, time:5.0785, lr:0.001\u001b[0m\n",
            "2019-11-26 16:22:32 \u001b[32mINFO     \u001b[0m train.py: [20/50], train_loss: 0.1543, time: 86.7481, lr: 0.001\u001b[0m\n",
            "2019-11-26 16:22:40 \u001b[32mINFO     \u001b[0m train.py: [21/50], [0/107], step: 2247, 5.238 samples/sec, batch_loss: 0.0861, batch_loss_c: 0.0757, batch_loss_s: 0.1103, time:7.6370, lr:0.001\u001b[0m\n",
            "tcmalloc: large alloc 1083916288 bytes == 0x97550000 @  0x7fb8703161e7 0x7fb8659e9f71 0x7fb865a4d55d 0x7fb865a50e28 0x7fb865a513e5 0x7fb865ae7fc2 0x50abc5 0x50d320 0x5081d5 0x50a020 0x50aa1d 0x50d320 0x5093e5 0x5951c1 0x5a04ce 0x557878 0x50c81e 0x5081d5 0x50a020 0x50aa1d 0x50c549 0x5081d5 0x50a020 0x50aa1d 0x50c549 0x5081d5 0x5895e1 0x5a04ce 0x50d8f5 0x509ce8 0x50aa1d\n",
            "2019-11-26 16:22:54 \u001b[32mINFO     \u001b[0m train.py: [21/50], [10/107], step: 2257, 2.885 samples/sec, batch_loss: 0.0784, batch_loss_c: 0.0631, batch_loss_s: 0.1141, time:13.8643, lr:0.001\u001b[0m\n",
            "2019-11-26 16:23:01 \u001b[32mINFO     \u001b[0m train.py: [21/50], [20/107], step: 2267, 5.623 samples/sec, batch_loss: 0.0711, batch_loss_c: 0.0625, batch_loss_s: 0.0912, time:7.1138, lr:0.001\u001b[0m\n",
            "2019-11-26 16:23:08 \u001b[32mINFO     \u001b[0m train.py: [21/50], [30/107], step: 2277, 6.147 samples/sec, batch_loss: 0.2765, batch_loss_c: 0.2616, batch_loss_s: 0.3112, time:6.5069, lr:0.001\u001b[0m\n",
            "2019-11-26 16:23:16 \u001b[32mINFO     \u001b[0m train.py: [21/50], [40/107], step: 2287, 4.773 samples/sec, batch_loss: 0.0944, batch_loss_c: 0.0840, batch_loss_s: 0.1188, time:8.3800, lr:0.001\u001b[0m\n",
            "2019-11-26 16:23:25 \u001b[32mINFO     \u001b[0m train.py: [21/50], [50/107], step: 2297, 4.409 samples/sec, batch_loss: 0.3372, batch_loss_c: 0.3287, batch_loss_s: 0.3569, time:9.0714, lr:0.001\u001b[0m\n",
            "2019-11-26 16:23:31 \u001b[32mINFO     \u001b[0m train.py: [21/50], [60/107], step: 2307, 7.085 samples/sec, batch_loss: 0.2951, batch_loss_c: 0.2822, batch_loss_s: 0.3253, time:5.6455, lr:0.001\u001b[0m\n",
            "2019-11-26 16:23:37 \u001b[32mINFO     \u001b[0m train.py: [21/50], [70/107], step: 2317, 6.571 samples/sec, batch_loss: 0.2934, batch_loss_c: 0.2884, batch_loss_s: 0.3050, time:6.0876, lr:0.001\u001b[0m\n",
            "2019-11-26 16:23:45 \u001b[32mINFO     \u001b[0m train.py: [21/50], [80/107], step: 2327, 5.034 samples/sec, batch_loss: 0.0832, batch_loss_c: 0.0815, batch_loss_s: 0.0873, time:7.9456, lr:0.001\u001b[0m\n",
            "2019-11-26 16:23:51 \u001b[32mINFO     \u001b[0m train.py: [21/50], [90/107], step: 2337, 6.846 samples/sec, batch_loss: 0.0776, batch_loss_c: 0.0675, batch_loss_s: 0.1013, time:5.8426, lr:0.001\u001b[0m\n",
            "2019-11-26 16:23:56 \u001b[32mINFO     \u001b[0m train.py: [21/50], [100/107], step: 2347, 7.830 samples/sec, batch_loss: 0.0969, batch_loss_c: 0.0785, batch_loss_s: 0.1396, time:5.1089, lr:0.001\u001b[0m\n",
            "2019-11-26 16:23:59 \u001b[32mINFO     \u001b[0m train.py: [21/50], train_loss: 0.1415, time: 86.5924, lr: 0.001\u001b[0m\n",
            "2019-11-26 16:24:10 \u001b[32mINFO     \u001b[0m train.py: [22/50], [0/107], step: 2354, 3.932 samples/sec, batch_loss: 0.3450, batch_loss_c: 0.3359, batch_loss_s: 0.3664, time:10.1734, lr:0.001\u001b[0m\n",
            "2019-11-26 16:24:17 \u001b[32mINFO     \u001b[0m train.py: [22/50], [10/107], step: 2364, 5.250 samples/sec, batch_loss: 0.0886, batch_loss_c: 0.0819, batch_loss_s: 0.1041, time:7.6192, lr:0.001\u001b[0m\n",
            "2019-11-26 16:24:27 \u001b[32mINFO     \u001b[0m train.py: [22/50], [20/107], step: 2374, 4.295 samples/sec, batch_loss: 0.0999, batch_loss_c: 0.0977, batch_loss_s: 0.1050, time:9.3140, lr:0.001\u001b[0m\n",
            "2019-11-26 16:24:38 \u001b[32mINFO     \u001b[0m train.py: [22/50], [30/107], step: 2384, 3.621 samples/sec, batch_loss: 0.2500, batch_loss_c: 0.2209, batch_loss_s: 0.3178, time:11.0478, lr:0.001\u001b[0m\n",
            "2019-11-26 16:24:45 \u001b[32mINFO     \u001b[0m train.py: [22/50], [40/107], step: 2394, 5.367 samples/sec, batch_loss: 0.1071, batch_loss_c: 0.1054, batch_loss_s: 0.1112, time:7.4534, lr:0.001\u001b[0m\n",
            "2019-11-26 16:24:53 \u001b[32mINFO     \u001b[0m train.py: [22/50], [50/107], step: 2404, 5.189 samples/sec, batch_loss: 0.1670, batch_loss_c: 0.1694, batch_loss_s: 0.1615, time:7.7082, lr:0.001\u001b[0m\n",
            "2019-11-26 16:24:59 \u001b[32mINFO     \u001b[0m train.py: [22/50], [60/107], step: 2414, 6.125 samples/sec, batch_loss: 0.0657, batch_loss_c: 0.0529, batch_loss_s: 0.0957, time:6.5310, lr:0.001\u001b[0m\n",
            "2019-11-26 16:25:06 \u001b[32mINFO     \u001b[0m train.py: [22/50], [70/107], step: 2424, 5.700 samples/sec, batch_loss: 0.0660, batch_loss_c: 0.0524, batch_loss_s: 0.0979, time:7.0180, lr:0.001\u001b[0m\n",
            "2019-11-26 16:25:14 \u001b[32mINFO     \u001b[0m train.py: [22/50], [80/107], step: 2434, 5.116 samples/sec, batch_loss: 0.3054, batch_loss_c: 0.2913, batch_loss_s: 0.3386, time:7.8182, lr:0.001\u001b[0m\n",
            "2019-11-26 16:25:20 \u001b[32mINFO     \u001b[0m train.py: [22/50], [90/107], step: 2444, 6.444 samples/sec, batch_loss: 0.1057, batch_loss_c: 0.0832, batch_loss_s: 0.1582, time:6.2074, lr:0.001\u001b[0m\n",
            "2019-11-26 16:25:26 \u001b[32mINFO     \u001b[0m train.py: [22/50], [100/107], step: 2454, 7.945 samples/sec, batch_loss: 0.2942, batch_loss_c: 0.2844, batch_loss_s: 0.3171, time:5.0347, lr:0.001\u001b[0m\n",
            "2019-11-26 16:25:29 \u001b[32mINFO     \u001b[0m train.py: [22/50], train_loss: 0.1503, time: 89.3393, lr: 0.001\u001b[0m\n",
            "2019-11-26 16:25:35 \u001b[32mINFO     \u001b[0m train.py: [23/50], [0/107], step: 2461, 6.531 samples/sec, batch_loss: 0.0460, batch_loss_c: 0.0354, batch_loss_s: 0.0708, time:6.1246, lr:0.001\u001b[0m\n",
            "2019-11-26 16:25:54 \u001b[32mINFO     \u001b[0m train.py: [23/50], [10/107], step: 2471, 2.216 samples/sec, batch_loss: 0.1000, batch_loss_c: 0.0876, batch_loss_s: 0.1290, time:18.0484, lr:0.001\u001b[0m\n",
            "2019-11-26 16:26:03 \u001b[32mINFO     \u001b[0m train.py: [23/50], [20/107], step: 2481, 4.309 samples/sec, batch_loss: 0.0912, batch_loss_c: 0.0840, batch_loss_s: 0.1081, time:9.2821, lr:0.001\u001b[0m\n",
            "2019-11-26 16:26:11 \u001b[32mINFO     \u001b[0m train.py: [23/50], [30/107], step: 2491, 4.640 samples/sec, batch_loss: 0.1314, batch_loss_c: 0.1112, batch_loss_s: 0.1787, time:8.6211, lr:0.001\u001b[0m\n",
            "2019-11-26 16:26:19 \u001b[32mINFO     \u001b[0m train.py: [23/50], [40/107], step: 2501, 5.437 samples/sec, batch_loss: 0.3156, batch_loss_c: 0.2979, batch_loss_s: 0.3572, time:7.3568, lr:0.001\u001b[0m\n",
            "2019-11-26 16:26:25 \u001b[32mINFO     \u001b[0m train.py: [23/50], [50/107], step: 2511, 6.321 samples/sec, batch_loss: 0.1320, batch_loss_c: 0.1330, batch_loss_s: 0.1294, time:6.3280, lr:0.001\u001b[0m\n",
            "2019-11-26 16:26:31 \u001b[32mINFO     \u001b[0m train.py: [23/50], [60/107], step: 2521, 6.759 samples/sec, batch_loss: 0.0977, batch_loss_c: 0.0948, batch_loss_s: 0.1047, time:5.9178, lr:0.001\u001b[0m\n",
            "2019-11-26 16:26:37 \u001b[32mINFO     \u001b[0m train.py: [23/50], [70/107], step: 2531, 6.786 samples/sec, batch_loss: 0.1485, batch_loss_c: 0.1324, batch_loss_s: 0.1862, time:5.8947, lr:0.001\u001b[0m\n",
            "2019-11-26 16:26:45 \u001b[32mINFO     \u001b[0m train.py: [23/50], [80/107], step: 2541, 4.977 samples/sec, batch_loss: 0.2403, batch_loss_c: 0.2945, batch_loss_s: 0.1136, time:8.0369, lr:0.001\u001b[0m\n",
            "2019-11-26 16:26:52 \u001b[32mINFO     \u001b[0m train.py: [23/50], [90/107], step: 2551, 5.381 samples/sec, batch_loss: 0.0641, batch_loss_c: 0.0594, batch_loss_s: 0.0753, time:7.4330, lr:0.001\u001b[0m\n",
            "tcmalloc: large alloc 1255759872 bytes == 0xbbdee000 @  0x7fb8703161e7 0x7fb8659e9f71 0x7fb865a4d55d 0x7fb865a50e28 0x7fb865a513e5 0x7fb865ae7fc2 0x50abc5 0x50d320 0x5081d5 0x50a020 0x50aa1d 0x50d320 0x5093e5 0x5951c1 0x5a04ce 0x557878 0x50c81e 0x5081d5 0x50a020 0x50aa1d 0x50c549 0x5081d5 0x50a020 0x50aa1d 0x50c549 0x5081d5 0x5895e1 0x5a04ce 0x50d8f5 0x509ce8 0x50aa1d\n",
            "2019-11-26 16:26:58 \u001b[32mINFO     \u001b[0m train.py: [23/50], [100/107], step: 2561, 6.881 samples/sec, batch_loss: 0.1735, batch_loss_c: 0.1450, batch_loss_s: 0.2400, time:5.8132, lr:0.001\u001b[0m\n",
            "2019-11-26 16:27:02 \u001b[32mINFO     \u001b[0m train.py: [23/50], train_loss: 0.1473, time: 92.1704, lr: 0.001\u001b[0m\n",
            "2019-11-26 16:27:14 \u001b[32mINFO     \u001b[0m train.py: [24/50], [0/107], step: 2568, 3.316 samples/sec, batch_loss: 0.1298, batch_loss_c: 0.1327, batch_loss_s: 0.1229, time:12.0626, lr:0.001\u001b[0m\n",
            "2019-11-26 16:27:22 \u001b[32mINFO     \u001b[0m train.py: [24/50], [10/107], step: 2578, 5.083 samples/sec, batch_loss: 0.1766, batch_loss_c: 0.1741, batch_loss_s: 0.1824, time:7.8694, lr:0.001\u001b[0m\n",
            "2019-11-26 16:27:29 \u001b[32mINFO     \u001b[0m train.py: [24/50], [20/107], step: 2588, 5.905 samples/sec, batch_loss: 0.0729, batch_loss_c: 0.0668, batch_loss_s: 0.0873, time:6.7739, lr:0.001\u001b[0m\n",
            "2019-11-26 16:27:36 \u001b[32mINFO     \u001b[0m train.py: [24/50], [30/107], step: 2598, 5.641 samples/sec, batch_loss: 0.0988, batch_loss_c: 0.0790, batch_loss_s: 0.1450, time:7.0912, lr:0.001\u001b[0m\n",
            "2019-11-26 16:27:45 \u001b[32mINFO     \u001b[0m train.py: [24/50], [40/107], step: 2608, 4.374 samples/sec, batch_loss: 0.0697, batch_loss_c: 0.0571, batch_loss_s: 0.0992, time:9.1449, lr:0.001\u001b[0m\n",
            "2019-11-26 16:27:53 \u001b[32mINFO     \u001b[0m train.py: [24/50], [50/107], step: 2618, 4.933 samples/sec, batch_loss: 0.0751, batch_loss_c: 0.0679, batch_loss_s: 0.0918, time:8.1095, lr:0.001\u001b[0m\n",
            "2019-11-26 16:28:00 \u001b[32mINFO     \u001b[0m train.py: [24/50], [60/107], step: 2628, 5.964 samples/sec, batch_loss: 0.3656, batch_loss_c: 0.3565, batch_loss_s: 0.3867, time:6.7064, lr:0.001\u001b[0m\n",
            "2019-11-26 16:28:08 \u001b[32mINFO     \u001b[0m train.py: [24/50], [70/107], step: 2638, 5.051 samples/sec, batch_loss: 0.1470, batch_loss_c: 0.1600, batch_loss_s: 0.1167, time:7.9199, lr:0.001\u001b[0m\n",
            "2019-11-26 16:28:16 \u001b[32mINFO     \u001b[0m train.py: [24/50], [80/107], step: 2648, 4.575 samples/sec, batch_loss: 0.1209, batch_loss_c: 0.1081, batch_loss_s: 0.1510, time:8.7435, lr:0.001\u001b[0m\n",
            "2019-11-26 16:28:22 \u001b[32mINFO     \u001b[0m train.py: [24/50], [90/107], step: 2658, 7.497 samples/sec, batch_loss: 0.1643, batch_loss_c: 0.1884, batch_loss_s: 0.1079, time:5.3357, lr:0.001\u001b[0m\n",
            "2019-11-26 16:28:27 \u001b[32mINFO     \u001b[0m train.py: [24/50], [100/107], step: 2668, 7.935 samples/sec, batch_loss: 0.4411, batch_loss_c: 0.4322, batch_loss_s: 0.4617, time:5.0410, lr:0.001\u001b[0m\n",
            "2019-11-26 16:28:30 \u001b[32mINFO     \u001b[0m train.py: [24/50], train_loss: 0.1351, time: 88.1580, lr: 0.001\u001b[0m\n",
            "2019-11-26 16:28:41 \u001b[32mINFO     \u001b[0m train.py: [25/50], [0/107], step: 2675, 3.863 samples/sec, batch_loss: 0.3024, batch_loss_c: 0.2963, batch_loss_s: 0.3166, time:10.3553, lr:0.001\u001b[0m\n",
            "2019-11-26 16:28:49 \u001b[32mINFO     \u001b[0m train.py: [25/50], [10/107], step: 2685, 4.740 samples/sec, batch_loss: 0.1015, batch_loss_c: 0.0932, batch_loss_s: 0.1211, time:8.4392, lr:0.001\u001b[0m\n",
            "2019-11-26 16:28:55 \u001b[32mINFO     \u001b[0m train.py: [25/50], [20/107], step: 2695, 6.694 samples/sec, batch_loss: 0.1333, batch_loss_c: 0.1390, batch_loss_s: 0.1199, time:5.9753, lr:0.001\u001b[0m\n",
            "2019-11-26 16:29:04 \u001b[32mINFO     \u001b[0m train.py: [25/50], [30/107], step: 2705, 4.605 samples/sec, batch_loss: 0.1334, batch_loss_c: 0.1331, batch_loss_s: 0.1342, time:8.6861, lr:0.001\u001b[0m\n",
            "2019-11-26 16:29:13 \u001b[32mINFO     \u001b[0m train.py: [25/50], [40/107], step: 2715, 4.588 samples/sec, batch_loss: 0.0531, batch_loss_c: 0.0440, batch_loss_s: 0.0743, time:8.7183, lr:0.001\u001b[0m\n",
            "2019-11-26 16:29:22 \u001b[32mINFO     \u001b[0m train.py: [25/50], [50/107], step: 2725, 4.331 samples/sec, batch_loss: 0.3105, batch_loss_c: 0.3088, batch_loss_s: 0.3144, time:9.2349, lr:0.001\u001b[0m\n",
            "2019-11-26 16:29:28 \u001b[32mINFO     \u001b[0m train.py: [25/50], [60/107], step: 2735, 6.725 samples/sec, batch_loss: 0.0974, batch_loss_c: 0.0892, batch_loss_s: 0.1164, time:5.9482, lr:0.001\u001b[0m\n",
            "2019-11-26 16:29:34 \u001b[32mINFO     \u001b[0m train.py: [25/50], [70/107], step: 2745, 6.288 samples/sec, batch_loss: 0.1663, batch_loss_c: 0.1522, batch_loss_s: 0.1991, time:6.3618, lr:0.001\u001b[0m\n",
            "2019-11-26 16:29:41 \u001b[32mINFO     \u001b[0m train.py: [25/50], [80/107], step: 2755, 5.919 samples/sec, batch_loss: 0.1158, batch_loss_c: 0.1136, batch_loss_s: 0.1211, time:6.7582, lr:0.001\u001b[0m\n",
            "2019-11-26 16:29:47 \u001b[32mINFO     \u001b[0m train.py: [25/50], [90/107], step: 2765, 7.020 samples/sec, batch_loss: 0.1095, batch_loss_c: 0.0959, batch_loss_s: 0.1412, time:5.6982, lr:0.001\u001b[0m\n",
            "2019-11-26 16:29:52 \u001b[32mINFO     \u001b[0m train.py: [25/50], [100/107], step: 2775, 7.927 samples/sec, batch_loss: 0.0555, batch_loss_c: 0.0495, batch_loss_s: 0.0694, time:5.0458, lr:0.001\u001b[0m\n",
            "2019-11-26 16:29:55 \u001b[32mINFO     \u001b[0m train.py: [25/50], train_loss: 0.1434, time: 84.5105, lr: 0.001\u001b[0m\n",
            "2019-11-26 16:30:07 \u001b[32mINFO     \u001b[0m train.py: [26/50], [0/107], step: 2782, 3.397 samples/sec, batch_loss: 0.0988, batch_loss_c: 0.0890, batch_loss_s: 0.1218, time:11.7752, lr:0.001\u001b[0m\n",
            "2019-11-26 16:30:16 \u001b[32mINFO     \u001b[0m train.py: [26/50], [10/107], step: 2792, 4.541 samples/sec, batch_loss: 0.3351, batch_loss_c: 0.3333, batch_loss_s: 0.3392, time:8.8090, lr:0.001\u001b[0m\n",
            "2019-11-26 16:30:23 \u001b[32mINFO     \u001b[0m train.py: [26/50], [20/107], step: 2802, 5.672 samples/sec, batch_loss: 0.0603, batch_loss_c: 0.0547, batch_loss_s: 0.0735, time:7.0527, lr:0.001\u001b[0m\n",
            "2019-11-26 16:30:30 \u001b[32mINFO     \u001b[0m train.py: [26/50], [30/107], step: 2812, 5.995 samples/sec, batch_loss: 0.0584, batch_loss_c: 0.0530, batch_loss_s: 0.0710, time:6.6721, lr:0.001\u001b[0m\n",
            "2019-11-26 16:30:39 \u001b[32mINFO     \u001b[0m train.py: [26/50], [40/107], step: 2822, 4.394 samples/sec, batch_loss: 0.0945, batch_loss_c: 0.0840, batch_loss_s: 0.1191, time:9.1033, lr:0.001\u001b[0m\n",
            "2019-11-26 16:30:45 \u001b[32mINFO     \u001b[0m train.py: [26/50], [50/107], step: 2832, 6.833 samples/sec, batch_loss: 0.0644, batch_loss_c: 0.0591, batch_loss_s: 0.0766, time:5.8537, lr:0.001\u001b[0m\n",
            "2019-11-26 16:30:51 \u001b[32mINFO     \u001b[0m train.py: [26/50], [60/107], step: 2842, 6.859 samples/sec, batch_loss: 0.1057, batch_loss_c: 0.1046, batch_loss_s: 0.1084, time:5.8318, lr:0.001\u001b[0m\n",
            "2019-11-26 16:30:57 \u001b[32mINFO     \u001b[0m train.py: [26/50], [70/107], step: 2852, 6.587 samples/sec, batch_loss: 0.1041, batch_loss_c: 0.0934, batch_loss_s: 0.1290, time:6.0722, lr:0.001\u001b[0m\n",
            "2019-11-26 16:31:04 \u001b[32mINFO     \u001b[0m train.py: [26/50], [80/107], step: 2862, 5.240 samples/sec, batch_loss: 0.0570, batch_loss_c: 0.0423, batch_loss_s: 0.0914, time:7.6336, lr:0.001\u001b[0m\n",
            "2019-11-26 16:31:10 \u001b[32mINFO     \u001b[0m train.py: [26/50], [90/107], step: 2872, 6.713 samples/sec, batch_loss: 0.1166, batch_loss_c: 0.1155, batch_loss_s: 0.1191, time:5.9589, lr:0.001\u001b[0m\n",
            "2019-11-26 16:31:15 \u001b[32mINFO     \u001b[0m train.py: [26/50], [100/107], step: 2882, 8.009 samples/sec, batch_loss: 0.1093, batch_loss_c: 0.1041, batch_loss_s: 0.1213, time:4.9942, lr:0.001\u001b[0m\n",
            "2019-11-26 16:31:19 \u001b[32mINFO     \u001b[0m train.py: [26/50], train_loss: 0.1304, time: 83.1951, lr: 0.001\u001b[0m\n",
            "2019-11-26 16:31:26 \u001b[32mINFO     \u001b[0m train.py: [27/50], [0/107], step: 2889, 5.805 samples/sec, batch_loss: 0.0973, batch_loss_c: 0.0873, batch_loss_s: 0.1208, time:6.8911, lr:0.001\u001b[0m\n",
            "2019-11-26 16:31:40 \u001b[32mINFO     \u001b[0m train.py: [27/50], [10/107], step: 2899, 2.911 samples/sec, batch_loss: 0.3207, batch_loss_c: 0.3121, batch_loss_s: 0.3409, time:13.7409, lr:0.001\u001b[0m\n",
            "2019-11-26 16:31:50 \u001b[32mINFO     \u001b[0m train.py: [27/50], [20/107], step: 2909, 4.059 samples/sec, batch_loss: 0.1466, batch_loss_c: 0.1203, batch_loss_s: 0.2081, time:9.8556, lr:0.001\u001b[0m\n",
            "tcmalloc: large alloc 1342701568 bytes == 0xa347a000 @  0x7fb8703161e7 0x7fb8659e9f71 0x7fb865a4d55d 0x7fb865a50e28 0x7fb865a513e5 0x7fb865ae7fc2 0x50abc5 0x50d320 0x5081d5 0x50a020 0x50aa1d 0x50d320 0x5093e5 0x5951c1 0x5a04ce 0x557878 0x50c81e 0x5081d5 0x50a020 0x50aa1d 0x50c549 0x5081d5 0x50a020 0x50aa1d 0x50c549 0x5081d5 0x5895e1 0x5a04ce 0x50d8f5 0x509ce8 0x50aa1d\n",
            "2019-11-26 16:31:58 \u001b[32mINFO     \u001b[0m train.py: [27/50], [30/107], step: 2919, 4.670 samples/sec, batch_loss: 0.0742, batch_loss_c: 0.0637, batch_loss_s: 0.0987, time:8.5661, lr:0.001\u001b[0m\n",
            "2019-11-26 16:32:06 \u001b[32mINFO     \u001b[0m train.py: [27/50], [40/107], step: 2929, 5.182 samples/sec, batch_loss: 0.4739, batch_loss_c: 0.4472, batch_loss_s: 0.5362, time:7.7194, lr:0.001\u001b[0m\n",
            "2019-11-26 16:32:14 \u001b[32mINFO     \u001b[0m train.py: [27/50], [50/107], step: 2939, 5.285 samples/sec, batch_loss: 0.0729, batch_loss_c: 0.0568, batch_loss_s: 0.1104, time:7.5682, lr:0.001\u001b[0m\n",
            "2019-11-26 16:32:20 \u001b[32mINFO     \u001b[0m train.py: [27/50], [60/107], step: 2949, 6.632 samples/sec, batch_loss: 0.0966, batch_loss_c: 0.0929, batch_loss_s: 0.1051, time:6.0315, lr:0.001\u001b[0m\n",
            "2019-11-26 16:32:27 \u001b[32mINFO     \u001b[0m train.py: [27/50], [70/107], step: 2959, 5.805 samples/sec, batch_loss: 0.1385, batch_loss_c: 0.1373, batch_loss_s: 0.1412, time:6.8912, lr:0.001\u001b[0m\n",
            "2019-11-26 16:32:34 \u001b[32mINFO     \u001b[0m train.py: [27/50], [80/107], step: 2969, 5.271 samples/sec, batch_loss: 0.0863, batch_loss_c: 0.0851, batch_loss_s: 0.0890, time:7.5891, lr:0.001\u001b[0m\n",
            "2019-11-26 16:32:41 \u001b[32mINFO     \u001b[0m train.py: [27/50], [90/107], step: 2979, 6.195 samples/sec, batch_loss: 0.0650, batch_loss_c: 0.0563, batch_loss_s: 0.0854, time:6.4564, lr:0.001\u001b[0m\n",
            "2019-11-26 16:32:46 \u001b[32mINFO     \u001b[0m train.py: [27/50], [100/107], step: 2989, 7.899 samples/sec, batch_loss: 0.2629, batch_loss_c: 0.2612, batch_loss_s: 0.2667, time:5.0639, lr:0.001\u001b[0m\n",
            "2019-11-26 16:32:49 \u001b[32mINFO     \u001b[0m train.py: [27/50], train_loss: 0.1400, time: 89.8238, lr: 0.001\u001b[0m\n",
            "2019-11-26 16:33:00 \u001b[32mINFO     \u001b[0m train.py: [28/50], [0/107], step: 2996, 3.652 samples/sec, batch_loss: 0.1225, batch_loss_c: 0.1115, batch_loss_s: 0.1483, time:10.9530, lr:0.001\u001b[0m\n",
            "2019-11-26 16:33:09 \u001b[32mINFO     \u001b[0m train.py: [28/50], [10/107], step: 3006, 4.718 samples/sec, batch_loss: 0.0931, batch_loss_c: 0.0831, batch_loss_s: 0.1164, time:8.4776, lr:0.001\u001b[0m\n",
            "2019-11-26 16:33:15 \u001b[32mINFO     \u001b[0m train.py: [28/50], [20/107], step: 3016, 6.258 samples/sec, batch_loss: 0.0902, batch_loss_c: 0.0680, batch_loss_s: 0.1420, time:6.3918, lr:0.001\u001b[0m\n",
            "tcmalloc: large alloc 1342701568 bytes == 0xa1178000 @  0x7fb8703161e7 0x7fb8659e9f71 0x7fb865a4d55d 0x7fb865a50e28 0x7fb865a513e5 0x7fb865ae7fc2 0x50abc5 0x50d320 0x5081d5 0x50a020 0x50aa1d 0x50d320 0x5093e5 0x5951c1 0x5a04ce 0x557878 0x50c81e 0x5081d5 0x50a020 0x50aa1d 0x50c549 0x5081d5 0x50a020 0x50aa1d 0x50c549 0x5081d5 0x5895e1 0x5a04ce 0x50d8f5 0x509ce8 0x50aa1d\n",
            "2019-11-26 16:33:23 \u001b[32mINFO     \u001b[0m train.py: [28/50], [30/107], step: 3026, 5.234 samples/sec, batch_loss: 0.1072, batch_loss_c: 0.1144, batch_loss_s: 0.0905, time:7.6424, lr:0.001\u001b[0m\n",
            "2019-11-26 16:33:34 \u001b[32mINFO     \u001b[0m train.py: [28/50], [40/107], step: 3036, 3.750 samples/sec, batch_loss: 0.0555, batch_loss_c: 0.0452, batch_loss_s: 0.0793, time:10.6675, lr:0.001\u001b[0m\n",
            "2019-11-26 16:33:41 \u001b[32mINFO     \u001b[0m train.py: [28/50], [50/107], step: 3046, 5.415 samples/sec, batch_loss: 0.0791, batch_loss_c: 0.0760, batch_loss_s: 0.0864, time:7.3862, lr:0.001\u001b[0m\n",
            "2019-11-26 16:33:47 \u001b[32mINFO     \u001b[0m train.py: [28/50], [60/107], step: 3056, 6.699 samples/sec, batch_loss: 0.1759, batch_loss_c: 0.1711, batch_loss_s: 0.1871, time:5.9708, lr:0.001\u001b[0m\n",
            "2019-11-26 16:33:53 \u001b[32mINFO     \u001b[0m train.py: [28/50], [70/107], step: 3066, 6.372 samples/sec, batch_loss: 0.1051, batch_loss_c: 0.1022, batch_loss_s: 0.1117, time:6.2772, lr:0.001\u001b[0m\n",
            "2019-11-26 16:34:04 \u001b[32mINFO     \u001b[0m train.py: [28/50], [80/107], step: 3076, 3.785 samples/sec, batch_loss: 0.0657, batch_loss_c: 0.0554, batch_loss_s: 0.0896, time:10.5679, lr:0.001\u001b[0m\n",
            "2019-11-26 16:34:11 \u001b[32mINFO     \u001b[0m train.py: [28/50], [90/107], step: 3086, 5.351 samples/sec, batch_loss: 0.1061, batch_loss_c: 0.1090, batch_loss_s: 0.0993, time:7.4755, lr:0.001\u001b[0m\n",
            "2019-11-26 16:34:16 \u001b[32mINFO     \u001b[0m train.py: [28/50], [100/107], step: 3096, 7.946 samples/sec, batch_loss: 0.1255, batch_loss_c: 0.1152, batch_loss_s: 0.1497, time:5.0341, lr:0.001\u001b[0m\n",
            "2019-11-26 16:34:20 \u001b[32mINFO     \u001b[0m train.py: [28/50], train_loss: 0.1435, time: 90.2304, lr: 0.001\u001b[0m\n",
            "2019-11-26 16:34:26 \u001b[32mINFO     \u001b[0m train.py: [29/50], [0/107], step: 3103, 6.404 samples/sec, batch_loss: 0.0565, batch_loss_c: 0.0494, batch_loss_s: 0.0731, time:6.2462, lr:0.001\u001b[0m\n",
            "2019-11-26 16:34:39 \u001b[32mINFO     \u001b[0m train.py: [29/50], [10/107], step: 3113, 3.215 samples/sec, batch_loss: 0.0690, batch_loss_c: 0.0646, batch_loss_s: 0.0794, time:12.4418, lr:0.001\u001b[0m\n",
            "2019-11-26 16:34:46 \u001b[32mINFO     \u001b[0m train.py: [29/50], [20/107], step: 3123, 5.797 samples/sec, batch_loss: 0.0703, batch_loss_c: 0.0688, batch_loss_s: 0.0738, time:6.8998, lr:0.001\u001b[0m\n",
            "2019-11-26 16:34:55 \u001b[32mINFO     \u001b[0m train.py: [29/50], [30/107], step: 3133, 4.525 samples/sec, batch_loss: 0.0570, batch_loss_c: 0.0463, batch_loss_s: 0.0820, time:8.8391, lr:0.001\u001b[0m\n",
            "2019-11-26 16:35:02 \u001b[32mINFO     \u001b[0m train.py: [29/50], [40/107], step: 3143, 5.639 samples/sec, batch_loss: 0.2307, batch_loss_c: 0.2689, batch_loss_s: 0.1416, time:7.0930, lr:0.001\u001b[0m\n",
            "2019-11-26 16:35:11 \u001b[32mINFO     \u001b[0m train.py: [29/50], [50/107], step: 3153, 4.424 samples/sec, batch_loss: 0.1325, batch_loss_c: 0.1097, batch_loss_s: 0.1858, time:9.0413, lr:0.001\u001b[0m\n",
            "2019-11-26 16:35:17 \u001b[32mINFO     \u001b[0m train.py: [29/50], [60/107], step: 3163, 6.620 samples/sec, batch_loss: 0.0633, batch_loss_c: 0.0535, batch_loss_s: 0.0863, time:6.0426, lr:0.001\u001b[0m\n",
            "2019-11-26 16:35:23 \u001b[32mINFO     \u001b[0m train.py: [29/50], [70/107], step: 3173, 6.152 samples/sec, batch_loss: 0.1040, batch_loss_c: 0.1058, batch_loss_s: 0.0998, time:6.5014, lr:0.001\u001b[0m\n",
            "2019-11-26 16:35:32 \u001b[32mINFO     \u001b[0m train.py: [29/50], [80/107], step: 3183, 4.430 samples/sec, batch_loss: 0.1007, batch_loss_c: 0.0925, batch_loss_s: 0.1198, time:9.0287, lr:0.001\u001b[0m\n",
            "2019-11-26 16:35:39 \u001b[32mINFO     \u001b[0m train.py: [29/50], [90/107], step: 3193, 6.356 samples/sec, batch_loss: 0.3391, batch_loss_c: 0.3372, batch_loss_s: 0.3436, time:6.2934, lr:0.001\u001b[0m\n",
            "2019-11-26 16:35:44 \u001b[32mINFO     \u001b[0m train.py: [29/50], [100/107], step: 3203, 7.863 samples/sec, batch_loss: 0.0539, batch_loss_c: 0.0483, batch_loss_s: 0.0670, time:5.0871, lr:0.001\u001b[0m\n",
            "2019-11-26 16:35:47 \u001b[32mINFO     \u001b[0m train.py: [29/50], train_loss: 0.1232, time: 86.9558, lr: 0.001\u001b[0m\n",
            "2019-11-26 16:35:56 \u001b[32mINFO     \u001b[0m train.py: [30/50], [0/107], step: 3210, 4.795 samples/sec, batch_loss: 0.0733, batch_loss_c: 0.0705, batch_loss_s: 0.0799, time:8.3415, lr:0.001\u001b[0m\n",
            "2019-11-26 16:36:09 \u001b[32mINFO     \u001b[0m train.py: [30/50], [10/107], step: 3220, 3.115 samples/sec, batch_loss: 0.0665, batch_loss_c: 0.0567, batch_loss_s: 0.0896, time:12.8412, lr:0.001\u001b[0m\n",
            "2019-11-26 16:36:15 \u001b[32mINFO     \u001b[0m train.py: [30/50], [20/107], step: 3230, 6.096 samples/sec, batch_loss: 0.0611, batch_loss_c: 0.0420, batch_loss_s: 0.1058, time:6.5622, lr:0.001\u001b[0m\n",
            "2019-11-26 16:36:23 \u001b[32mINFO     \u001b[0m train.py: [30/50], [30/107], step: 3240, 4.939 samples/sec, batch_loss: 0.1154, batch_loss_c: 0.1131, batch_loss_s: 0.1207, time:8.0995, lr:0.001\u001b[0m\n",
            "tcmalloc: large alloc 1083916288 bytes == 0xa0672000 @  0x7fb8703161e7 0x7fb8659e9f71 0x7fb865a4d55d 0x7fb865a50e28 0x7fb865a513e5 0x7fb865ae7fc2 0x50abc5 0x50d320 0x5081d5 0x50a020 0x50aa1d 0x50d320 0x5093e5 0x5951c1 0x5a04ce 0x557878 0x50c81e 0x5081d5 0x50a020 0x50aa1d 0x50c549 0x5081d5 0x50a020 0x50aa1d 0x50c549 0x5081d5 0x5895e1 0x5a04ce 0x50d8f5 0x509ce8 0x50aa1d\n",
            "2019-11-26 16:36:32 \u001b[32mINFO     \u001b[0m train.py: [30/50], [40/107], step: 3250, 4.593 samples/sec, batch_loss: 0.1151, batch_loss_c: 0.1127, batch_loss_s: 0.1208, time:8.7093, lr:0.001\u001b[0m\n",
            "2019-11-26 16:36:38 \u001b[32mINFO     \u001b[0m train.py: [30/50], [50/107], step: 3260, 6.409 samples/sec, batch_loss: 0.1424, batch_loss_c: 0.1366, batch_loss_s: 0.1559, time:6.2415, lr:0.001\u001b[0m\n",
            "2019-11-26 16:36:45 \u001b[32mINFO     \u001b[0m train.py: [30/50], [60/107], step: 3270, 6.527 samples/sec, batch_loss: 0.0876, batch_loss_c: 0.0910, batch_loss_s: 0.0796, time:6.1285, lr:0.001\u001b[0m\n",
            "2019-11-26 16:36:52 \u001b[32mINFO     \u001b[0m train.py: [30/50], [70/107], step: 3280, 5.766 samples/sec, batch_loss: 0.1644, batch_loss_c: 0.1781, batch_loss_s: 0.1323, time:6.9377, lr:0.001\u001b[0m\n",
            "2019-11-26 16:36:59 \u001b[32mINFO     \u001b[0m train.py: [30/50], [80/107], step: 3290, 5.600 samples/sec, batch_loss: 0.1578, batch_loss_c: 0.1537, batch_loss_s: 0.1672, time:7.1424, lr:0.001\u001b[0m\n",
            "2019-11-26 16:37:04 \u001b[32mINFO     \u001b[0m train.py: [30/50], [90/107], step: 3300, 7.428 samples/sec, batch_loss: 0.0755, batch_loss_c: 0.0687, batch_loss_s: 0.0913, time:5.3853, lr:0.001\u001b[0m\n",
            "2019-11-26 16:37:09 \u001b[32mINFO     \u001b[0m train.py: [30/50], [100/107], step: 3310, 7.822 samples/sec, batch_loss: 0.0918, batch_loss_c: 0.0945, batch_loss_s: 0.0857, time:5.1137, lr:0.001\u001b[0m\n",
            "2019-11-26 16:37:12 \u001b[32mINFO     \u001b[0m train.py: [30/50], train_loss: 0.1385, time: 84.8849, lr: 0.001\u001b[0m\n",
            "2019-11-26 16:37:21 \u001b[32mINFO     \u001b[0m train.py: [31/50], [0/107], step: 3317, 4.741 samples/sec, batch_loss: 0.2747, batch_loss_c: 0.2915, batch_loss_s: 0.2356, time:8.4366, lr:0.001\u001b[0m\n",
            "2019-11-26 16:37:33 \u001b[32mINFO     \u001b[0m train.py: [31/50], [10/107], step: 3327, 3.459 samples/sec, batch_loss: 0.0620, batch_loss_c: 0.0534, batch_loss_s: 0.0820, time:11.5645, lr:0.001\u001b[0m\n",
            "2019-11-26 16:37:40 \u001b[32mINFO     \u001b[0m train.py: [31/50], [20/107], step: 3337, 6.128 samples/sec, batch_loss: 0.0642, batch_loss_c: 0.0521, batch_loss_s: 0.0926, time:6.5274, lr:0.001\u001b[0m\n",
            "2019-11-26 16:37:47 \u001b[32mINFO     \u001b[0m train.py: [31/50], [30/107], step: 3347, 5.178 samples/sec, batch_loss: 0.0667, batch_loss_c: 0.0594, batch_loss_s: 0.0837, time:7.7254, lr:0.001\u001b[0m\n",
            "2019-11-26 16:37:55 \u001b[32mINFO     \u001b[0m train.py: [31/50], [40/107], step: 3357, 4.916 samples/sec, batch_loss: 0.0836, batch_loss_c: 0.0681, batch_loss_s: 0.1198, time:8.1373, lr:0.001\u001b[0m\n",
            "2019-11-26 16:38:03 \u001b[32mINFO     \u001b[0m train.py: [31/50], [50/107], step: 3367, 5.510 samples/sec, batch_loss: 0.0776, batch_loss_c: 0.0736, batch_loss_s: 0.0870, time:7.2590, lr:0.001\u001b[0m\n",
            "2019-11-26 16:38:08 \u001b[32mINFO     \u001b[0m train.py: [31/50], [60/107], step: 3377, 6.819 samples/sec, batch_loss: 0.1382, batch_loss_c: 0.1400, batch_loss_s: 0.1340, time:5.8656, lr:0.001\u001b[0m\n",
            "2019-11-26 16:38:15 \u001b[32mINFO     \u001b[0m train.py: [31/50], [70/107], step: 3387, 6.597 samples/sec, batch_loss: 0.0905, batch_loss_c: 0.0774, batch_loss_s: 0.1212, time:6.0630, lr:0.001\u001b[0m\n",
            "2019-11-26 16:38:22 \u001b[32mINFO     \u001b[0m train.py: [31/50], [80/107], step: 3397, 5.128 samples/sec, batch_loss: 0.0774, batch_loss_c: 0.0672, batch_loss_s: 0.1013, time:7.7998, lr:0.001\u001b[0m\n",
            "2019-11-26 16:38:28 \u001b[32mINFO     \u001b[0m train.py: [31/50], [90/107], step: 3407, 7.556 samples/sec, batch_loss: 0.0810, batch_loss_c: 0.0706, batch_loss_s: 0.1055, time:5.2936, lr:0.001\u001b[0m\n",
            "2019-11-26 16:38:33 \u001b[32mINFO     \u001b[0m train.py: [31/50], [100/107], step: 3417, 7.930 samples/sec, batch_loss: 0.0842, batch_loss_c: 0.0754, batch_loss_s: 0.1048, time:5.0440, lr:0.001\u001b[0m\n",
            "2019-11-26 16:38:36 \u001b[32mINFO     \u001b[0m train.py: [31/50], train_loss: 0.1357, time: 83.0412, lr: 0.001\u001b[0m\n",
            "2019-11-26 16:38:45 \u001b[32mINFO     \u001b[0m train.py: [32/50], [0/107], step: 3424, 4.519 samples/sec, batch_loss: 0.1077, batch_loss_c: 0.1014, batch_loss_s: 0.1223, time:8.8513, lr:0.001\u001b[0m\n",
            "2019-11-26 16:38:56 \u001b[32mINFO     \u001b[0m train.py: [32/50], [10/107], step: 3434, 3.930 samples/sec, batch_loss: 0.1184, batch_loss_c: 0.1237, batch_loss_s: 0.1062, time:10.1788, lr:0.001\u001b[0m\n",
            "2019-11-26 16:39:03 \u001b[32mINFO     \u001b[0m train.py: [32/50], [20/107], step: 3444, 5.677 samples/sec, batch_loss: 0.3748, batch_loss_c: 0.3744, batch_loss_s: 0.3756, time:7.0465, lr:0.001\u001b[0m\n",
            "2019-11-26 16:39:10 \u001b[32mINFO     \u001b[0m train.py: [32/50], [30/107], step: 3454, 5.584 samples/sec, batch_loss: 0.0691, batch_loss_c: 0.0613, batch_loss_s: 0.0872, time:7.1633, lr:0.001\u001b[0m\n",
            "2019-11-26 16:39:17 \u001b[32mINFO     \u001b[0m train.py: [32/50], [40/107], step: 3464, 5.232 samples/sec, batch_loss: 0.0749, batch_loss_c: 0.0615, batch_loss_s: 0.1063, time:7.6455, lr:0.001\u001b[0m\n",
            "2019-11-26 16:39:24 \u001b[32mINFO     \u001b[0m train.py: [32/50], [50/107], step: 3474, 6.335 samples/sec, batch_loss: 0.0837, batch_loss_c: 0.0777, batch_loss_s: 0.0979, time:6.3137, lr:0.001\u001b[0m\n",
            "2019-11-26 16:39:30 \u001b[32mINFO     \u001b[0m train.py: [32/50], [60/107], step: 3484, 6.807 samples/sec, batch_loss: 0.2124, batch_loss_c: 0.2221, batch_loss_s: 0.1897, time:5.8765, lr:0.001\u001b[0m\n",
            "2019-11-26 16:39:37 \u001b[32mINFO     \u001b[0m train.py: [32/50], [70/107], step: 3494, 5.688 samples/sec, batch_loss: 0.0906, batch_loss_c: 0.0813, batch_loss_s: 0.1123, time:7.0329, lr:0.001\u001b[0m\n",
            "2019-11-26 16:39:44 \u001b[32mINFO     \u001b[0m train.py: [32/50], [80/107], step: 3504, 5.394 samples/sec, batch_loss: 0.0978, batch_loss_c: 0.0873, batch_loss_s: 0.1224, time:7.4154, lr:0.001\u001b[0m\n",
            "2019-11-26 16:39:50 \u001b[32mINFO     \u001b[0m train.py: [32/50], [90/107], step: 3514, 7.138 samples/sec, batch_loss: 0.3006, batch_loss_c: 0.2883, batch_loss_s: 0.3292, time:5.6040, lr:0.001\u001b[0m\n",
            "2019-11-26 16:39:55 \u001b[32mINFO     \u001b[0m train.py: [32/50], [100/107], step: 3524, 7.887 samples/sec, batch_loss: 0.0814, batch_loss_c: 0.0820, batch_loss_s: 0.0800, time:5.0717, lr:0.001\u001b[0m\n",
            "2019-11-26 16:39:58 \u001b[32mINFO     \u001b[0m train.py: [32/50], train_loss: 0.1354, time: 81.5643, lr: 0.001\u001b[0m\n",
            "2019-11-26 16:40:10 \u001b[32mINFO     \u001b[0m train.py: [33/50], [0/107], step: 3531, 3.603 samples/sec, batch_loss: 0.1404, batch_loss_c: 0.1486, batch_loss_s: 0.1213, time:11.1027, lr:0.001\u001b[0m\n",
            "2019-11-26 16:40:21 \u001b[32mINFO     \u001b[0m train.py: [33/50], [10/107], step: 3541, 3.443 samples/sec, batch_loss: 0.0750, batch_loss_c: 0.0556, batch_loss_s: 0.1200, time:11.6178, lr:0.001\u001b[0m\n",
            "2019-11-26 16:40:27 \u001b[32mINFO     \u001b[0m train.py: [33/50], [20/107], step: 3551, 6.709 samples/sec, batch_loss: 0.1319, batch_loss_c: 0.1513, batch_loss_s: 0.0867, time:5.9618, lr:0.001\u001b[0m\n",
            "2019-11-26 16:40:36 \u001b[32mINFO     \u001b[0m train.py: [33/50], [30/107], step: 3561, 4.388 samples/sec, batch_loss: 0.1829, batch_loss_c: 0.1417, batch_loss_s: 0.2789, time:9.1154, lr:0.001\u001b[0m\n",
            "2019-11-26 16:40:43 \u001b[32mINFO     \u001b[0m train.py: [33/50], [40/107], step: 3571, 5.808 samples/sec, batch_loss: 0.1280, batch_loss_c: 0.1125, batch_loss_s: 0.1642, time:6.8867, lr:0.001\u001b[0m\n",
            "2019-11-26 16:40:50 \u001b[32mINFO     \u001b[0m train.py: [33/50], [50/107], step: 3581, 5.653 samples/sec, batch_loss: 0.1199, batch_loss_c: 0.1135, batch_loss_s: 0.1349, time:7.0760, lr:0.001\u001b[0m\n",
            "2019-11-26 16:40:58 \u001b[32mINFO     \u001b[0m train.py: [33/50], [60/107], step: 3591, 5.457 samples/sec, batch_loss: 0.0649, batch_loss_c: 0.0525, batch_loss_s: 0.0939, time:7.3301, lr:0.001\u001b[0m\n",
            "2019-11-26 16:41:04 \u001b[32mINFO     \u001b[0m train.py: [33/50], [70/107], step: 3601, 6.739 samples/sec, batch_loss: 0.2872, batch_loss_c: 0.2588, batch_loss_s: 0.3534, time:5.9356, lr:0.001\u001b[0m\n",
            "2019-11-26 16:41:11 \u001b[32mINFO     \u001b[0m train.py: [33/50], [80/107], step: 3611, 5.317 samples/sec, batch_loss: 0.1055, batch_loss_c: 0.0895, batch_loss_s: 0.1426, time:7.5236, lr:0.001\u001b[0m\n",
            "2019-11-26 16:41:16 \u001b[32mINFO     \u001b[0m train.py: [33/50], [90/107], step: 3621, 7.437 samples/sec, batch_loss: 0.0928, batch_loss_c: 0.0920, batch_loss_s: 0.0946, time:5.3788, lr:0.001\u001b[0m\n",
            "2019-11-26 16:41:21 \u001b[32mINFO     \u001b[0m train.py: [33/50], [100/107], step: 3631, 8.031 samples/sec, batch_loss: 0.0521, batch_loss_c: 0.0447, batch_loss_s: 0.0691, time:4.9804, lr:0.001\u001b[0m\n",
            "2019-11-26 16:41:25 \u001b[32mINFO     \u001b[0m train.py: [33/50], train_loss: 0.1304, time: 86.3369, lr: 0.001\u001b[0m\n",
            "2019-11-26 16:41:36 \u001b[32mINFO     \u001b[0m train.py: [34/50], [0/107], step: 3638, 3.598 samples/sec, batch_loss: 0.1021, batch_loss_c: 0.0837, batch_loss_s: 0.1452, time:11.1178, lr:0.001\u001b[0m\n",
            "2019-11-26 16:41:45 \u001b[32mINFO     \u001b[0m train.py: [34/50], [10/107], step: 3648, 4.616 samples/sec, batch_loss: 0.0746, batch_loss_c: 0.0700, batch_loss_s: 0.0853, time:8.6650, lr:0.001\u001b[0m\n",
            "2019-11-26 16:41:53 \u001b[32mINFO     \u001b[0m train.py: [34/50], [20/107], step: 3658, 5.087 samples/sec, batch_loss: 0.0901, batch_loss_c: 0.0771, batch_loss_s: 0.1204, time:7.8636, lr:0.001\u001b[0m\n",
            "2019-11-26 16:42:01 \u001b[32mINFO     \u001b[0m train.py: [34/50], [30/107], step: 3668, 4.732 samples/sec, batch_loss: 0.2105, batch_loss_c: 0.1995, batch_loss_s: 0.2362, time:8.4530, lr:0.001\u001b[0m\n",
            "2019-11-26 16:42:10 \u001b[32mINFO     \u001b[0m train.py: [34/50], [40/107], step: 3678, 4.861 samples/sec, batch_loss: 0.0731, batch_loss_c: 0.0636, batch_loss_s: 0.0951, time:8.2294, lr:0.001\u001b[0m\n",
            "2019-11-26 16:42:16 \u001b[32mINFO     \u001b[0m train.py: [34/50], [50/107], step: 3688, 6.062 samples/sec, batch_loss: 0.0958, batch_loss_c: 0.0779, batch_loss_s: 0.1376, time:6.5989, lr:0.001\u001b[0m\n",
            "2019-11-26 16:42:23 \u001b[32mINFO     \u001b[0m train.py: [34/50], [60/107], step: 3698, 6.190 samples/sec, batch_loss: 0.0818, batch_loss_c: 0.0706, batch_loss_s: 0.1079, time:6.4622, lr:0.001\u001b[0m\n",
            "2019-11-26 16:42:29 \u001b[32mINFO     \u001b[0m train.py: [34/50], [70/107], step: 3708, 6.393 samples/sec, batch_loss: 0.5054, batch_loss_c: 0.4736, batch_loss_s: 0.5797, time:6.2572, lr:0.001\u001b[0m\n",
            "2019-11-26 16:42:36 \u001b[32mINFO     \u001b[0m train.py: [34/50], [80/107], step: 3718, 5.684 samples/sec, batch_loss: 0.0765, batch_loss_c: 0.0679, batch_loss_s: 0.0967, time:7.0372, lr:0.001\u001b[0m\n",
            "2019-11-26 16:42:41 \u001b[32mINFO     \u001b[0m train.py: [34/50], [90/107], step: 3728, 7.359 samples/sec, batch_loss: 0.0695, batch_loss_c: 0.0579, batch_loss_s: 0.0965, time:5.4358, lr:0.001\u001b[0m\n",
            "2019-11-26 16:42:46 \u001b[32mINFO     \u001b[0m train.py: [34/50], [100/107], step: 3738, 7.926 samples/sec, batch_loss: 0.0727, batch_loss_c: 0.0571, batch_loss_s: 0.1091, time:5.0468, lr:0.001\u001b[0m\n",
            "2019-11-26 16:42:50 \u001b[32mINFO     \u001b[0m train.py: [34/50], train_loss: 0.1325, time: 84.5094, lr: 0.001\u001b[0m\n",
            "2019-11-26 16:42:58 \u001b[32mINFO     \u001b[0m train.py: [35/50], [0/107], step: 3745, 5.154 samples/sec, batch_loss: 0.1027, batch_loss_c: 0.0973, batch_loss_s: 0.1151, time:7.7604, lr:0.001\u001b[0m\n",
            "2019-11-26 16:43:12 \u001b[32mINFO     \u001b[0m train.py: [35/50], [10/107], step: 3755, 2.760 samples/sec, batch_loss: 0.1114, batch_loss_c: 0.0969, batch_loss_s: 0.1451, time:14.4942, lr:0.001\u001b[0m\n",
            "2019-11-26 16:43:19 \u001b[32mINFO     \u001b[0m train.py: [35/50], [20/107], step: 3765, 6.034 samples/sec, batch_loss: 0.1041, batch_loss_c: 0.1020, batch_loss_s: 0.1090, time:6.6287, lr:0.001\u001b[0m\n",
            "2019-11-26 16:43:25 \u001b[32mINFO     \u001b[0m train.py: [35/50], [30/107], step: 3775, 6.530 samples/sec, batch_loss: 0.0948, batch_loss_c: 0.0911, batch_loss_s: 0.1035, time:6.1255, lr:0.001\u001b[0m\n",
            "2019-11-26 16:43:35 \u001b[32mINFO     \u001b[0m train.py: [35/50], [40/107], step: 3785, 4.307 samples/sec, batch_loss: 0.0750, batch_loss_c: 0.0713, batch_loss_s: 0.0836, time:9.2864, lr:0.001\u001b[0m\n",
            "2019-11-26 16:43:41 \u001b[32mINFO     \u001b[0m train.py: [35/50], [50/107], step: 3795, 6.256 samples/sec, batch_loss: 0.0928, batch_loss_c: 0.0879, batch_loss_s: 0.1045, time:6.3935, lr:0.001\u001b[0m\n",
            "2019-11-26 16:43:47 \u001b[32mINFO     \u001b[0m train.py: [35/50], [60/107], step: 3805, 7.151 samples/sec, batch_loss: 0.0802, batch_loss_c: 0.0743, batch_loss_s: 0.0938, time:5.5937, lr:0.001\u001b[0m\n",
            "2019-11-26 16:43:52 \u001b[32mINFO     \u001b[0m train.py: [35/50], [70/107], step: 3815, 6.723 samples/sec, batch_loss: 0.1621, batch_loss_c: 0.1408, batch_loss_s: 0.2118, time:5.9499, lr:0.001\u001b[0m\n",
            "2019-11-26 16:43:59 \u001b[32mINFO     \u001b[0m train.py: [35/50], [80/107], step: 3825, 6.104 samples/sec, batch_loss: 0.0595, batch_loss_c: 0.0478, batch_loss_s: 0.0866, time:6.5530, lr:0.001\u001b[0m\n",
            "2019-11-26 16:44:06 \u001b[32mINFO     \u001b[0m train.py: [35/50], [90/107], step: 3835, 5.861 samples/sec, batch_loss: 0.3083, batch_loss_c: 0.2981, batch_loss_s: 0.3322, time:6.8254, lr:0.001\u001b[0m\n",
            "2019-11-26 16:44:11 \u001b[32mINFO     \u001b[0m train.py: [35/50], [100/107], step: 3845, 7.861 samples/sec, batch_loss: 0.0700, batch_loss_c: 0.0665, batch_loss_s: 0.0782, time:5.0883, lr:0.001\u001b[0m\n",
            "2019-11-26 16:44:14 \u001b[32mINFO     \u001b[0m train.py: [35/50], train_loss: 0.1450, time: 84.0544, lr: 0.001\u001b[0m\n",
            "2019-11-26 16:44:23 \u001b[32mINFO     \u001b[0m train.py: [36/50], [0/107], step: 3852, 4.571 samples/sec, batch_loss: 0.1847, batch_loss_c: 0.1828, batch_loss_s: 0.1892, time:8.7518, lr:0.001\u001b[0m\n",
            "2019-11-26 16:44:34 \u001b[32mINFO     \u001b[0m train.py: [36/50], [10/107], step: 3862, 3.834 samples/sec, batch_loss: 0.0705, batch_loss_c: 0.0566, batch_loss_s: 0.1029, time:10.4343, lr:0.001\u001b[0m\n",
            "2019-11-26 16:44:41 \u001b[32mINFO     \u001b[0m train.py: [36/50], [20/107], step: 3872, 5.534 samples/sec, batch_loss: 0.1021, batch_loss_c: 0.0890, batch_loss_s: 0.1327, time:7.2274, lr:0.001\u001b[0m\n",
            "2019-11-26 16:44:48 \u001b[32mINFO     \u001b[0m train.py: [36/50], [30/107], step: 3882, 5.651 samples/sec, batch_loss: 0.0632, batch_loss_c: 0.0530, batch_loss_s: 0.0871, time:7.0785, lr:0.001\u001b[0m\n",
            "2019-11-26 16:45:00 \u001b[32mINFO     \u001b[0m train.py: [36/50], [40/107], step: 3892, 3.494 samples/sec, batch_loss: 0.2924, batch_loss_c: 0.2809, batch_loss_s: 0.3191, time:11.4491, lr:0.001\u001b[0m\n",
            "2019-11-26 16:45:07 \u001b[32mINFO     \u001b[0m train.py: [36/50], [50/107], step: 3902, 5.284 samples/sec, batch_loss: 0.0664, batch_loss_c: 0.0681, batch_loss_s: 0.0624, time:7.5707, lr:0.001\u001b[0m\n",
            "2019-11-26 16:45:16 \u001b[32mINFO     \u001b[0m train.py: [36/50], [60/107], step: 3912, 4.420 samples/sec, batch_loss: 0.1166, batch_loss_c: 0.1042, batch_loss_s: 0.1453, time:9.0501, lr:0.001\u001b[0m\n",
            "tcmalloc: large alloc 1086898176 bytes == 0x9ea98000 @  0x7fb8703161e7 0x7fb8659e9f71 0x7fb865a4d55d 0x7fb865a50e28 0x7fb865a513e5 0x7fb865ae7fc2 0x50abc5 0x50d320 0x5081d5 0x50a020 0x50aa1d 0x50d320 0x5093e5 0x5951c1 0x5a04ce 0x557878 0x50c81e 0x5081d5 0x50a020 0x50aa1d 0x50c549 0x5081d5 0x50a020 0x50aa1d 0x50c549 0x5081d5 0x5895e1 0x5a04ce 0x50d8f5 0x509ce8 0x50aa1d\n",
            "2019-11-26 16:45:24 \u001b[32mINFO     \u001b[0m train.py: [36/50], [70/107], step: 3922, 5.149 samples/sec, batch_loss: 0.2951, batch_loss_c: 0.2898, batch_loss_s: 0.3075, time:7.7678, lr:0.001\u001b[0m\n",
            "2019-11-26 16:45:34 \u001b[32mINFO     \u001b[0m train.py: [36/50], [80/107], step: 3932, 4.221 samples/sec, batch_loss: 0.0710, batch_loss_c: 0.0629, batch_loss_s: 0.0900, time:9.4756, lr:0.001\u001b[0m\n",
            "2019-11-26 16:45:40 \u001b[32mINFO     \u001b[0m train.py: [36/50], [90/107], step: 3942, 6.562 samples/sec, batch_loss: 0.0515, batch_loss_c: 0.0427, batch_loss_s: 0.0720, time:6.0956, lr:0.001\u001b[0m\n",
            "2019-11-26 16:45:45 \u001b[32mINFO     \u001b[0m train.py: [36/50], [100/107], step: 3952, 7.873 samples/sec, batch_loss: 0.0616, batch_loss_c: 0.0572, batch_loss_s: 0.0720, time:5.0806, lr:0.001\u001b[0m\n",
            "2019-11-26 16:45:48 \u001b[32mINFO     \u001b[0m train.py: [36/50], train_loss: 0.1216, time: 93.3955, lr: 0.001\u001b[0m\n",
            "2019-11-26 16:45:54 \u001b[32mINFO     \u001b[0m train.py: [37/50], [0/107], step: 3959, 6.902 samples/sec, batch_loss: 0.0590, batch_loss_c: 0.0498, batch_loss_s: 0.0805, time:5.7958, lr:0.001\u001b[0m\n",
            "2019-11-26 16:46:07 \u001b[32mINFO     \u001b[0m train.py: [37/50], [10/107], step: 3969, 3.098 samples/sec, batch_loss: 0.0822, batch_loss_c: 0.0771, batch_loss_s: 0.0941, time:12.9131, lr:0.001\u001b[0m\n",
            "2019-11-26 16:46:15 \u001b[32mINFO     \u001b[0m train.py: [37/50], [20/107], step: 3979, 4.932 samples/sec, batch_loss: 0.2893, batch_loss_c: 0.2774, batch_loss_s: 0.3170, time:8.1103, lr:0.001\u001b[0m\n",
            "2019-11-26 16:46:23 \u001b[32mINFO     \u001b[0m train.py: [37/50], [30/107], step: 3989, 5.414 samples/sec, batch_loss: 0.1719, batch_loss_c: 0.1676, batch_loss_s: 0.1818, time:7.3882, lr:0.001\u001b[0m\n",
            "2019-11-26 16:46:32 \u001b[32mINFO     \u001b[0m train.py: [37/50], [40/107], step: 3999, 4.253 samples/sec, batch_loss: 0.0897, batch_loss_c: 0.0833, batch_loss_s: 0.1045, time:9.4057, lr:0.001\u001b[0m\n",
            "2019-11-26 16:46:39 \u001b[32mINFO     \u001b[0m train.py: [37/50], [50/107], step: 4009, 6.239 samples/sec, batch_loss: 0.3474, batch_loss_c: 0.3473, batch_loss_s: 0.3478, time:6.4113, lr:0.001\u001b[0m\n",
            "2019-11-26 16:46:45 \u001b[32mINFO     \u001b[0m train.py: [37/50], [60/107], step: 4019, 6.434 samples/sec, batch_loss: 0.1131, batch_loss_c: 0.1155, batch_loss_s: 0.1075, time:6.2168, lr:0.001\u001b[0m\n",
            "2019-11-26 16:46:53 \u001b[32mINFO     \u001b[0m train.py: [37/50], [70/107], step: 4029, 5.023 samples/sec, batch_loss: 0.0583, batch_loss_c: 0.0501, batch_loss_s: 0.0772, time:7.9626, lr:0.001\u001b[0m\n",
            "2019-11-26 16:46:59 \u001b[32mINFO     \u001b[0m train.py: [37/50], [80/107], step: 4039, 6.301 samples/sec, batch_loss: 0.0791, batch_loss_c: 0.0671, batch_loss_s: 0.1069, time:6.3482, lr:0.001\u001b[0m\n",
            "2019-11-26 16:47:06 \u001b[32mINFO     \u001b[0m train.py: [37/50], [90/107], step: 4049, 6.060 samples/sec, batch_loss: 0.1729, batch_loss_c: 0.1749, batch_loss_s: 0.1682, time:6.6007, lr:0.001\u001b[0m\n",
            "2019-11-26 16:47:11 \u001b[32mINFO     \u001b[0m train.py: [37/50], [100/107], step: 4059, 7.898 samples/sec, batch_loss: 0.1523, batch_loss_c: 0.1142, batch_loss_s: 0.2412, time:5.0643, lr:0.001\u001b[0m\n",
            "2019-11-26 16:47:14 \u001b[32mINFO     \u001b[0m train.py: [37/50], train_loss: 0.1229, time: 85.6409, lr: 0.001\u001b[0m\n",
            "2019-11-26 16:47:21 \u001b[32mINFO     \u001b[0m train.py: [38/50], [0/107], step: 4066, 6.699 samples/sec, batch_loss: 0.0982, batch_loss_c: 0.0938, batch_loss_s: 0.1083, time:5.9707, lr:0.001\u001b[0m\n",
            "2019-11-26 16:47:36 \u001b[32mINFO     \u001b[0m train.py: [38/50], [10/107], step: 4076, 2.670 samples/sec, batch_loss: 0.0669, batch_loss_c: 0.0573, batch_loss_s: 0.0894, time:14.9817, lr:0.001\u001b[0m\n",
            "2019-11-26 16:47:43 \u001b[32mINFO     \u001b[0m train.py: [38/50], [20/107], step: 4086, 5.513 samples/sec, batch_loss: 0.0971, batch_loss_c: 0.0937, batch_loss_s: 0.1050, time:7.2553, lr:0.001\u001b[0m\n",
            "2019-11-26 16:47:50 \u001b[32mINFO     \u001b[0m train.py: [38/50], [30/107], step: 4096, 5.324 samples/sec, batch_loss: 0.0564, batch_loss_c: 0.0450, batch_loss_s: 0.0830, time:7.5137, lr:0.001\u001b[0m\n",
            "2019-11-26 16:47:57 \u001b[32mINFO     \u001b[0m train.py: [38/50], [40/107], step: 4106, 5.839 samples/sec, batch_loss: 0.0631, batch_loss_c: 0.0590, batch_loss_s: 0.0727, time:6.8510, lr:0.001\u001b[0m\n",
            "2019-11-26 16:48:05 \u001b[32mINFO     \u001b[0m train.py: [38/50], [50/107], step: 4116, 4.905 samples/sec, batch_loss: 0.0565, batch_loss_c: 0.0540, batch_loss_s: 0.0624, time:8.1552, lr:0.001\u001b[0m\n",
            "2019-11-26 16:48:12 \u001b[32mINFO     \u001b[0m train.py: [38/50], [60/107], step: 4126, 6.536 samples/sec, batch_loss: 0.0654, batch_loss_c: 0.0493, batch_loss_s: 0.1030, time:6.1200, lr:0.001\u001b[0m\n",
            "2019-11-26 16:48:18 \u001b[32mINFO     \u001b[0m train.py: [38/50], [70/107], step: 4136, 6.549 samples/sec, batch_loss: 0.2211, batch_loss_c: 0.2030, batch_loss_s: 0.2632, time:6.1079, lr:0.001\u001b[0m\n",
            "2019-11-26 16:48:25 \u001b[32mINFO     \u001b[0m train.py: [38/50], [80/107], step: 4146, 5.808 samples/sec, batch_loss: 0.0632, batch_loss_c: 0.0569, batch_loss_s: 0.0779, time:6.8875, lr:0.001\u001b[0m\n",
            "2019-11-26 16:48:30 \u001b[32mINFO     \u001b[0m train.py: [38/50], [90/107], step: 4156, 6.708 samples/sec, batch_loss: 0.3460, batch_loss_c: 0.3480, batch_loss_s: 0.3415, time:5.9635, lr:0.001\u001b[0m\n",
            "2019-11-26 16:48:36 \u001b[32mINFO     \u001b[0m train.py: [38/50], [100/107], step: 4166, 7.956 samples/sec, batch_loss: 0.3029, batch_loss_c: 0.2966, batch_loss_s: 0.3175, time:5.0279, lr:0.001\u001b[0m\n",
            "2019-11-26 16:48:39 \u001b[32mINFO     \u001b[0m train.py: [38/50], train_loss: 0.1398, time: 84.2249, lr: 0.001\u001b[0m\n",
            "2019-11-26 16:48:49 \u001b[32mINFO     \u001b[0m train.py: [39/50], [0/107], step: 4173, 4.077 samples/sec, batch_loss: 0.0862, batch_loss_c: 0.0752, batch_loss_s: 0.1117, time:9.8105, lr:0.001\u001b[0m\n",
            "2019-11-26 16:49:01 \u001b[32mINFO     \u001b[0m train.py: [39/50], [10/107], step: 4183, 3.431 samples/sec, batch_loss: 0.0805, batch_loss_c: 0.0734, batch_loss_s: 0.0970, time:11.6590, lr:0.001\u001b[0m\n",
            "2019-11-26 16:49:09 \u001b[32mINFO     \u001b[0m train.py: [39/50], [20/107], step: 4193, 4.884 samples/sec, batch_loss: 0.0797, batch_loss_c: 0.0785, batch_loss_s: 0.0826, time:8.1898, lr:0.001\u001b[0m\n",
            "2019-11-26 16:49:19 \u001b[32mINFO     \u001b[0m train.py: [39/50], [30/107], step: 4203, 4.152 samples/sec, batch_loss: 0.1233, batch_loss_c: 0.1133, batch_loss_s: 0.1466, time:9.6340, lr:0.001\u001b[0m\n",
            "2019-11-26 16:49:31 \u001b[32mINFO     \u001b[0m train.py: [39/50], [40/107], step: 4213, 3.170 samples/sec, batch_loss: 0.2943, batch_loss_c: 0.2835, batch_loss_s: 0.3195, time:12.6191, lr:0.001\u001b[0m\n",
            "tcmalloc: large alloc 1617436672 bytes == 0xa8850000 @  0x7fb8703161e7 0x7fb8659e9f71 0x7fb865a4d55d 0x7fb865a50e28 0x7fb865a513e5 0x7fb865ae7fc2 0x50abc5 0x50d320 0x5081d5 0x50a020 0x50aa1d 0x50d320 0x5093e5 0x5951c1 0x5a04ce 0x557878 0x50c81e 0x5081d5 0x50a020 0x50aa1d 0x50c549 0x5081d5 0x50a020 0x50aa1d 0x50c549 0x5081d5 0x5895e1 0x5a04ce 0x50d8f5 0x509ce8 0x50aa1d\n",
            "2019-11-26 16:49:44 \u001b[32mINFO     \u001b[0m train.py: [39/50], [50/107], step: 4223, 3.084 samples/sec, batch_loss: 0.0699, batch_loss_c: 0.0591, batch_loss_s: 0.0949, time:12.9715, lr:0.001\u001b[0m\n",
            "2019-11-26 16:49:51 \u001b[32mINFO     \u001b[0m train.py: [39/50], [60/107], step: 4233, 6.120 samples/sec, batch_loss: 0.3256, batch_loss_c: 0.3279, batch_loss_s: 0.3202, time:6.5362, lr:0.001\u001b[0m\n",
            "2019-11-26 16:49:58 \u001b[32mINFO     \u001b[0m train.py: [39/50], [70/107], step: 4243, 5.574 samples/sec, batch_loss: 0.0825, batch_loss_c: 0.0741, batch_loss_s: 0.1018, time:7.1765, lr:0.001\u001b[0m\n",
            "2019-11-26 16:50:05 \u001b[32mINFO     \u001b[0m train.py: [39/50], [80/107], step: 4253, 5.956 samples/sec, batch_loss: 0.0643, batch_loss_c: 0.0556, batch_loss_s: 0.0845, time:6.7156, lr:0.001\u001b[0m\n",
            "2019-11-26 16:50:11 \u001b[32mINFO     \u001b[0m train.py: [39/50], [90/107], step: 4263, 6.121 samples/sec, batch_loss: 0.0917, batch_loss_c: 0.0881, batch_loss_s: 0.1002, time:6.5349, lr:0.001\u001b[0m\n",
            "2019-11-26 16:50:16 \u001b[32mINFO     \u001b[0m train.py: [39/50], [100/107], step: 4273, 7.895 samples/sec, batch_loss: 0.1695, batch_loss_c: 0.1613, batch_loss_s: 0.1886, time:5.0666, lr:0.001\u001b[0m\n",
            "2019-11-26 16:50:20 \u001b[32mINFO     \u001b[0m train.py: [39/50], train_loss: 0.1377, time: 100.3648, lr: 0.001\u001b[0m\n",
            "2019-11-26 16:50:26 \u001b[32mINFO     \u001b[0m train.py: [40/50], [0/107], step: 4280, 6.508 samples/sec, batch_loss: 0.0762, batch_loss_c: 0.0636, batch_loss_s: 0.1055, time:6.1463, lr:0.001\u001b[0m\n",
            "2019-11-26 16:50:38 \u001b[32mINFO     \u001b[0m train.py: [40/50], [10/107], step: 4290, 3.304 samples/sec, batch_loss: 0.0671, batch_loss_c: 0.0579, batch_loss_s: 0.0884, time:12.1054, lr:0.001\u001b[0m\n",
            "2019-11-26 16:50:45 \u001b[32mINFO     \u001b[0m train.py: [40/50], [20/107], step: 4300, 6.101 samples/sec, batch_loss: 0.0803, batch_loss_c: 0.0744, batch_loss_s: 0.0939, time:6.5559, lr:0.001\u001b[0m\n",
            "2019-11-26 16:50:52 \u001b[32mINFO     \u001b[0m train.py: [40/50], [30/107], step: 4310, 5.396 samples/sec, batch_loss: 0.0822, batch_loss_c: 0.0754, batch_loss_s: 0.0979, time:7.4127, lr:0.001\u001b[0m\n",
            "2019-11-26 16:51:04 \u001b[32mINFO     \u001b[0m train.py: [40/50], [40/107], step: 4320, 3.484 samples/sec, batch_loss: 0.0674, batch_loss_c: 0.0523, batch_loss_s: 0.1026, time:11.4798, lr:0.001\u001b[0m\n",
            "2019-11-26 16:51:11 \u001b[32mINFO     \u001b[0m train.py: [40/50], [50/107], step: 4330, 5.821 samples/sec, batch_loss: 0.0633, batch_loss_c: 0.0527, batch_loss_s: 0.0880, time:6.8721, lr:0.001\u001b[0m\n",
            "2019-11-26 16:51:17 \u001b[32mINFO     \u001b[0m train.py: [40/50], [60/107], step: 4340, 6.580 samples/sec, batch_loss: 0.1440, batch_loss_c: 0.1423, batch_loss_s: 0.1481, time:6.0787, lr:0.001\u001b[0m\n",
            "2019-11-26 16:51:23 \u001b[32mINFO     \u001b[0m train.py: [40/50], [70/107], step: 4350, 6.931 samples/sec, batch_loss: 0.2264, batch_loss_c: 0.1935, batch_loss_s: 0.3032, time:5.7711, lr:0.001\u001b[0m\n",
            "2019-11-26 16:51:31 \u001b[32mINFO     \u001b[0m train.py: [40/50], [80/107], step: 4360, 4.683 samples/sec, batch_loss: 0.1046, batch_loss_c: 0.0840, batch_loss_s: 0.1527, time:8.5409, lr:0.001\u001b[0m\n",
            "2019-11-26 16:51:37 \u001b[32mINFO     \u001b[0m train.py: [40/50], [90/107], step: 4370, 6.558 samples/sec, batch_loss: 0.0793, batch_loss_c: 0.0714, batch_loss_s: 0.0977, time:6.0992, lr:0.001\u001b[0m\n",
            "2019-11-26 16:51:42 \u001b[32mINFO     \u001b[0m train.py: [40/50], [100/107], step: 4380, 7.691 samples/sec, batch_loss: 0.0947, batch_loss_c: 0.0907, batch_loss_s: 0.1039, time:5.2010, lr:0.001\u001b[0m\n",
            "2019-11-26 16:51:46 \u001b[32mINFO     \u001b[0m train.py: [40/50], train_loss: 0.1356, time: 85.7136, lr: 0.001\u001b[0m\n",
            "2019-11-26 16:51:54 \u001b[32mINFO     \u001b[0m train.py: [41/50], [0/107], step: 4387, 5.172 samples/sec, batch_loss: 0.0606, batch_loss_c: 0.0508, batch_loss_s: 0.0834, time:7.7338, lr:0.001\u001b[0m\n",
            "2019-11-26 16:52:04 \u001b[32mINFO     \u001b[0m train.py: [41/50], [10/107], step: 4397, 4.020 samples/sec, batch_loss: 0.0663, batch_loss_c: 0.0492, batch_loss_s: 0.1062, time:9.9503, lr:0.001\u001b[0m\n",
            "2019-11-26 16:52:10 \u001b[32mINFO     \u001b[0m train.py: [41/50], [20/107], step: 4407, 6.462 samples/sec, batch_loss: 0.3154, batch_loss_c: 0.3149, batch_loss_s: 0.3167, time:6.1902, lr:0.001\u001b[0m\n",
            "2019-11-26 16:52:17 \u001b[32mINFO     \u001b[0m train.py: [41/50], [30/107], step: 4417, 6.113 samples/sec, batch_loss: 0.0683, batch_loss_c: 0.0584, batch_loss_s: 0.0913, time:6.5435, lr:0.001\u001b[0m\n",
            "2019-11-26 16:52:25 \u001b[32mINFO     \u001b[0m train.py: [41/50], [40/107], step: 4427, 4.696 samples/sec, batch_loss: 0.1020, batch_loss_c: 0.0961, batch_loss_s: 0.1159, time:8.5181, lr:0.001\u001b[0m\n",
            "2019-11-26 16:52:32 \u001b[32mINFO     \u001b[0m train.py: [41/50], [50/107], step: 4437, 5.538 samples/sec, batch_loss: 0.1733, batch_loss_c: 0.1927, batch_loss_s: 0.1281, time:7.2232, lr:0.001\u001b[0m\n",
            "2019-11-26 16:52:39 \u001b[32mINFO     \u001b[0m train.py: [41/50], [60/107], step: 4447, 5.765 samples/sec, batch_loss: 0.0750, batch_loss_c: 0.0590, batch_loss_s: 0.1124, time:6.9389, lr:0.001\u001b[0m\n",
            "2019-11-26 16:52:46 \u001b[32mINFO     \u001b[0m train.py: [41/50], [70/107], step: 4457, 6.456 samples/sec, batch_loss: 0.3279, batch_loss_c: 0.3238, batch_loss_s: 0.3374, time:6.1959, lr:0.001\u001b[0m\n",
            "2019-11-26 16:52:52 \u001b[32mINFO     \u001b[0m train.py: [41/50], [80/107], step: 4467, 5.868 samples/sec, batch_loss: 0.0652, batch_loss_c: 0.0554, batch_loss_s: 0.0879, time:6.8170, lr:0.001\u001b[0m\n",
            "2019-11-26 16:52:58 \u001b[32mINFO     \u001b[0m train.py: [41/50], [90/107], step: 4477, 6.549 samples/sec, batch_loss: 0.0563, batch_loss_c: 0.0460, batch_loss_s: 0.0803, time:6.1081, lr:0.001\u001b[0m\n",
            "2019-11-26 16:53:04 \u001b[32mINFO     \u001b[0m train.py: [41/50], [100/107], step: 4487, 7.803 samples/sec, batch_loss: 0.0607, batch_loss_c: 0.0504, batch_loss_s: 0.0848, time:5.1260, lr:0.001\u001b[0m\n",
            "2019-11-26 16:53:07 \u001b[32mINFO     \u001b[0m train.py: [41/50], train_loss: 0.1440, time: 80.6544, lr: 0.001\u001b[0m\n",
            "2019-11-26 16:53:15 \u001b[32mINFO     \u001b[0m train.py: [42/50], [0/107], step: 4494, 5.261 samples/sec, batch_loss: 0.1470, batch_loss_c: 0.1665, batch_loss_s: 0.1013, time:7.6028, lr:0.001\u001b[0m\n",
            "2019-11-26 16:53:27 \u001b[32mINFO     \u001b[0m train.py: [42/50], [10/107], step: 4504, 3.461 samples/sec, batch_loss: 0.0608, batch_loss_c: 0.0485, batch_loss_s: 0.0897, time:11.5564, lr:0.001\u001b[0m\n",
            "2019-11-26 16:53:34 \u001b[32mINFO     \u001b[0m train.py: [42/50], [20/107], step: 4514, 5.574 samples/sec, batch_loss: 0.0819, batch_loss_c: 0.0634, batch_loss_s: 0.1252, time:7.1759, lr:0.001\u001b[0m\n",
            "2019-11-26 16:53:41 \u001b[32mINFO     \u001b[0m train.py: [42/50], [30/107], step: 4524, 5.532 samples/sec, batch_loss: 0.0567, batch_loss_c: 0.0504, batch_loss_s: 0.0714, time:7.2301, lr:0.001\u001b[0m\n",
            "2019-11-26 16:53:48 \u001b[32mINFO     \u001b[0m train.py: [42/50], [40/107], step: 4534, 5.482 samples/sec, batch_loss: 0.0800, batch_loss_c: 0.0734, batch_loss_s: 0.0954, time:7.2969, lr:0.001\u001b[0m\n",
            "2019-11-26 16:53:55 \u001b[32mINFO     \u001b[0m train.py: [42/50], [50/107], step: 4544, 5.781 samples/sec, batch_loss: 0.3048, batch_loss_c: 0.3025, batch_loss_s: 0.3100, time:6.9190, lr:0.001\u001b[0m\n",
            "2019-11-26 16:54:01 \u001b[32mINFO     \u001b[0m train.py: [42/50], [60/107], step: 4554, 6.842 samples/sec, batch_loss: 0.1743, batch_loss_c: 0.1672, batch_loss_s: 0.1910, time:5.8459, lr:0.001\u001b[0m\n",
            "2019-11-26 16:54:07 \u001b[32mINFO     \u001b[0m train.py: [42/50], [70/107], step: 4564, 6.451 samples/sec, batch_loss: 0.0583, batch_loss_c: 0.0526, batch_loss_s: 0.0718, time:6.2003, lr:0.001\u001b[0m\n",
            "2019-11-26 16:54:13 \u001b[32mINFO     \u001b[0m train.py: [42/50], [80/107], step: 4574, 6.609 samples/sec, batch_loss: 0.2142, batch_loss_c: 0.1808, batch_loss_s: 0.2923, time:6.0526, lr:0.001\u001b[0m\n",
            "2019-11-26 16:54:19 \u001b[32mINFO     \u001b[0m train.py: [42/50], [90/107], step: 4584, 7.308 samples/sec, batch_loss: 0.0805, batch_loss_c: 0.0811, batch_loss_s: 0.0790, time:5.4734, lr:0.001\u001b[0m\n",
            "2019-11-26 16:54:24 \u001b[32mINFO     \u001b[0m train.py: [42/50], [100/107], step: 4594, 8.054 samples/sec, batch_loss: 0.0588, batch_loss_c: 0.0477, batch_loss_s: 0.0845, time:4.9664, lr:0.001\u001b[0m\n",
            "2019-11-26 16:54:27 \u001b[32mINFO     \u001b[0m train.py: [42/50], train_loss: 0.1367, time: 79.6997, lr: 0.001\u001b[0m\n",
            "2019-11-26 16:54:36 \u001b[32mINFO     \u001b[0m train.py: [43/50], [0/107], step: 4601, 4.510 samples/sec, batch_loss: 0.1056, batch_loss_c: 0.1025, batch_loss_s: 0.1129, time:8.8701, lr:0.001\u001b[0m\n",
            "2019-11-26 16:54:47 \u001b[32mINFO     \u001b[0m train.py: [43/50], [10/107], step: 4611, 3.907 samples/sec, batch_loss: 0.0831, batch_loss_c: 0.0829, batch_loss_s: 0.0838, time:10.2390, lr:0.001\u001b[0m\n",
            "2019-11-26 16:54:55 \u001b[32mINFO     \u001b[0m train.py: [43/50], [20/107], step: 4621, 4.982 samples/sec, batch_loss: 0.0765, batch_loss_c: 0.0715, batch_loss_s: 0.0882, time:8.0281, lr:0.001\u001b[0m\n",
            "2019-11-26 16:55:03 \u001b[32mINFO     \u001b[0m train.py: [43/50], [30/107], step: 4631, 4.603 samples/sec, batch_loss: 0.0634, batch_loss_c: 0.0553, batch_loss_s: 0.0821, time:8.6905, lr:0.001\u001b[0m\n",
            "2019-11-26 16:55:11 \u001b[32mINFO     \u001b[0m train.py: [43/50], [40/107], step: 4641, 5.064 samples/sec, batch_loss: 0.0756, batch_loss_c: 0.0726, batch_loss_s: 0.0824, time:7.8985, lr:0.001\u001b[0m\n",
            "2019-11-26 16:55:20 \u001b[32mINFO     \u001b[0m train.py: [43/50], [50/107], step: 4651, 4.616 samples/sec, batch_loss: 0.2688, batch_loss_c: 0.3015, batch_loss_s: 0.1924, time:8.6656, lr:0.001\u001b[0m\n",
            "tcmalloc: large alloc 1332600832 bytes == 0xa7d62000 @  0x7fb8703161e7 0x7fb8659e9f71 0x7fb865a4d55d 0x7fb865a50e28 0x7fb865a513e5 0x7fb865ae7fc2 0x50abc5 0x50d320 0x5081d5 0x50a020 0x50aa1d 0x50d320 0x5093e5 0x5951c1 0x5a04ce 0x557878 0x50c81e 0x5081d5 0x50a020 0x50aa1d 0x50c549 0x5081d5 0x50a020 0x50aa1d 0x50c549 0x5081d5 0x5895e1 0x5a04ce 0x50d8f5 0x509ce8 0x50aa1d\n",
            "2019-11-26 16:55:27 \u001b[32mINFO     \u001b[0m train.py: [43/50], [60/107], step: 4661, 5.539 samples/sec, batch_loss: 0.2815, batch_loss_c: 0.2549, batch_loss_s: 0.3436, time:7.2217, lr:0.001\u001b[0m\n",
            "2019-11-26 16:55:34 \u001b[32mINFO     \u001b[0m train.py: [43/50], [70/107], step: 4671, 5.679 samples/sec, batch_loss: 0.0968, batch_loss_c: 0.0865, batch_loss_s: 0.1209, time:7.0439, lr:0.001\u001b[0m\n",
            "2019-11-26 16:55:44 \u001b[32mINFO     \u001b[0m train.py: [43/50], [80/107], step: 4681, 4.310 samples/sec, batch_loss: 0.0526, batch_loss_c: 0.0492, batch_loss_s: 0.0606, time:9.2818, lr:0.001\u001b[0m\n",
            "tcmalloc: large alloc 1432182784 bytes == 0xa9034000 @  0x7fb8703161e7 0x7fb8659e9f71 0x7fb865a4d55d 0x7fb865a50e28 0x7fb865a513e5 0x7fb865ae7fc2 0x50abc5 0x50d320 0x5081d5 0x50a020 0x50aa1d 0x50d320 0x5093e5 0x5951c1 0x5a04ce 0x557878 0x50c81e 0x5081d5 0x50a020 0x50aa1d 0x50c549 0x5081d5 0x50a020 0x50aa1d 0x50c549 0x5081d5 0x5895e1 0x5a04ce 0x50d8f5 0x509ce8 0x50aa1d\n",
            "2019-11-26 16:55:52 \u001b[32mINFO     \u001b[0m train.py: [43/50], [90/107], step: 4691, 4.948 samples/sec, batch_loss: 0.0594, batch_loss_c: 0.0474, batch_loss_s: 0.0874, time:8.0836, lr:0.001\u001b[0m\n",
            "2019-11-26 16:55:57 \u001b[32mINFO     \u001b[0m train.py: [43/50], [100/107], step: 4701, 7.481 samples/sec, batch_loss: 0.0558, batch_loss_c: 0.0473, batch_loss_s: 0.0757, time:5.3468, lr:0.001\u001b[0m\n",
            "2019-11-26 16:56:00 \u001b[32mINFO     \u001b[0m train.py: [43/50], train_loss: 0.1468, time: 92.6672, lr: 0.001\u001b[0m\n",
            "2019-11-26 16:56:12 \u001b[32mINFO     \u001b[0m train.py: [44/50], [0/107], step: 4708, 3.568 samples/sec, batch_loss: 0.2606, batch_loss_c: 0.2493, batch_loss_s: 0.2869, time:11.2097, lr:0.001\u001b[0m\n",
            "2019-11-26 16:56:18 \u001b[32mINFO     \u001b[0m train.py: [44/50], [10/107], step: 4718, 6.163 samples/sec, batch_loss: 0.1051, batch_loss_c: 0.0992, batch_loss_s: 0.1189, time:6.4908, lr:0.001\u001b[0m\n",
            "2019-11-26 16:56:24 \u001b[32mINFO     \u001b[0m train.py: [44/50], [20/107], step: 4728, 6.628 samples/sec, batch_loss: 0.0689, batch_loss_c: 0.0612, batch_loss_s: 0.0869, time:6.0348, lr:0.001\u001b[0m\n",
            "2019-11-26 16:56:31 \u001b[32mINFO     \u001b[0m train.py: [44/50], [30/107], step: 4738, 5.988 samples/sec, batch_loss: 0.1070, batch_loss_c: 0.0977, batch_loss_s: 0.1287, time:6.6799, lr:0.001\u001b[0m\n",
            "2019-11-26 16:56:38 \u001b[32mINFO     \u001b[0m train.py: [44/50], [40/107], step: 4748, 5.454 samples/sec, batch_loss: 0.0606, batch_loss_c: 0.0588, batch_loss_s: 0.0649, time:7.3343, lr:0.001\u001b[0m\n",
            "2019-11-26 16:56:47 \u001b[32mINFO     \u001b[0m train.py: [44/50], [50/107], step: 4758, 4.780 samples/sec, batch_loss: 0.0734, batch_loss_c: 0.0568, batch_loss_s: 0.1121, time:8.3687, lr:0.001\u001b[0m\n",
            "2019-11-26 16:56:55 \u001b[32mINFO     \u001b[0m train.py: [44/50], [60/107], step: 4768, 5.149 samples/sec, batch_loss: 0.0920, batch_loss_c: 0.0799, batch_loss_s: 0.1203, time:7.7686, lr:0.001\u001b[0m\n",
            "tcmalloc: large alloc 1482350592 bytes == 0xa532a000 @  0x7fb8703161e7 0x7fb8659e9f71 0x7fb865a4d55d 0x7fb865a50e28 0x7fb865a513e5 0x7fb865ae7fc2 0x50abc5 0x50d320 0x5081d5 0x50a020 0x50aa1d 0x50d320 0x5093e5 0x5951c1 0x5a04ce 0x557878 0x50c81e 0x5081d5 0x50a020 0x50aa1d 0x50c549 0x5081d5 0x50a020 0x50aa1d 0x50c549 0x5081d5 0x5895e1 0x5a04ce 0x50d8f5 0x509ce8 0x50aa1d\n",
            "2019-11-26 16:57:03 \u001b[32mINFO     \u001b[0m train.py: [44/50], [70/107], step: 4778, 4.817 samples/sec, batch_loss: 0.0709, batch_loss_c: 0.0611, batch_loss_s: 0.0940, time:8.3044, lr:0.001\u001b[0m\n",
            "2019-11-26 16:57:13 \u001b[32mINFO     \u001b[0m train.py: [44/50], [80/107], step: 4788, 3.903 samples/sec, batch_loss: 0.0946, batch_loss_c: 0.0987, batch_loss_s: 0.0851, time:10.2489, lr:0.001\u001b[0m\n",
            "2019-11-26 16:57:20 \u001b[32mINFO     \u001b[0m train.py: [44/50], [90/107], step: 4798, 6.058 samples/sec, batch_loss: 0.0634, batch_loss_c: 0.0509, batch_loss_s: 0.0927, time:6.6031, lr:0.001\u001b[0m\n",
            "2019-11-26 16:57:25 \u001b[32mINFO     \u001b[0m train.py: [44/50], [100/107], step: 4808, 7.867 samples/sec, batch_loss: 0.1321, batch_loss_c: 0.1370, batch_loss_s: 0.1205, time:5.0845, lr:0.001\u001b[0m\n",
            "2019-11-26 16:57:28 \u001b[32mINFO     \u001b[0m train.py: [44/50], train_loss: 0.1198, time: 87.5364, lr: 0.001\u001b[0m\n",
            "2019-11-26 16:57:39 \u001b[32mINFO     \u001b[0m train.py: [45/50], [0/107], step: 4815, 3.787 samples/sec, batch_loss: 0.1240, batch_loss_c: 0.1047, batch_loss_s: 0.1688, time:10.5622, lr:0.001\u001b[0m\n",
            "2019-11-26 16:57:47 \u001b[32mINFO     \u001b[0m train.py: [45/50], [10/107], step: 4825, 5.053 samples/sec, batch_loss: 0.0545, batch_loss_c: 0.0350, batch_loss_s: 0.1000, time:7.9163, lr:0.001\u001b[0m\n",
            "2019-11-26 16:57:55 \u001b[32mINFO     \u001b[0m train.py: [45/50], [20/107], step: 4835, 5.338 samples/sec, batch_loss: 0.0766, batch_loss_c: 0.0607, batch_loss_s: 0.1134, time:7.4939, lr:0.001\u001b[0m\n",
            "2019-11-26 16:58:02 \u001b[32mINFO     \u001b[0m train.py: [45/50], [30/107], step: 4845, 5.769 samples/sec, batch_loss: 0.2812, batch_loss_c: 0.2568, batch_loss_s: 0.3382, time:6.9331, lr:0.001\u001b[0m\n",
            "2019-11-26 16:58:09 \u001b[32mINFO     \u001b[0m train.py: [45/50], [40/107], step: 4855, 5.435 samples/sec, batch_loss: 0.4168, batch_loss_c: 0.4068, batch_loss_s: 0.4402, time:7.3597, lr:0.001\u001b[0m\n",
            "2019-11-26 16:58:17 \u001b[32mINFO     \u001b[0m train.py: [45/50], [50/107], step: 4865, 5.007 samples/sec, batch_loss: 0.0679, batch_loss_c: 0.0628, batch_loss_s: 0.0798, time:7.9884, lr:0.001\u001b[0m\n",
            "2019-11-26 16:58:23 \u001b[32mINFO     \u001b[0m train.py: [45/50], [60/107], step: 4875, 6.517 samples/sec, batch_loss: 0.1311, batch_loss_c: 0.1293, batch_loss_s: 0.1352, time:6.1379, lr:0.001\u001b[0m\n",
            "2019-11-26 16:58:31 \u001b[32mINFO     \u001b[0m train.py: [45/50], [70/107], step: 4885, 5.026 samples/sec, batch_loss: 0.3414, batch_loss_c: 0.3185, batch_loss_s: 0.3947, time:7.9582, lr:0.001\u001b[0m\n",
            "2019-11-26 16:58:41 \u001b[32mINFO     \u001b[0m train.py: [45/50], [80/107], step: 4895, 3.913 samples/sec, batch_loss: 0.2980, batch_loss_c: 0.2951, batch_loss_s: 0.3049, time:10.2225, lr:0.001\u001b[0m\n",
            "2019-11-26 16:58:47 \u001b[32mINFO     \u001b[0m train.py: [45/50], [90/107], step: 4905, 6.648 samples/sec, batch_loss: 0.0703, batch_loss_c: 0.0698, batch_loss_s: 0.0713, time:6.0167, lr:0.001\u001b[0m\n",
            "2019-11-26 16:58:52 \u001b[32mINFO     \u001b[0m train.py: [45/50], [100/107], step: 4915, 7.921 samples/sec, batch_loss: 0.0603, batch_loss_c: 0.0575, batch_loss_s: 0.0670, time:5.0498, lr:0.001\u001b[0m\n",
            "2019-11-26 16:58:56 \u001b[32mINFO     \u001b[0m train.py: [45/50], train_loss: 0.1310, time: 87.0156, lr: 0.001\u001b[0m\n",
            "2019-11-26 16:59:06 \u001b[32mINFO     \u001b[0m train.py: [46/50], [0/107], step: 4922, 3.974 samples/sec, batch_loss: 0.1033, batch_loss_c: 0.0916, batch_loss_s: 0.1304, time:10.0666, lr:0.001\u001b[0m\n",
            "2019-11-26 16:59:15 \u001b[32mINFO     \u001b[0m train.py: [46/50], [10/107], step: 4932, 4.530 samples/sec, batch_loss: 0.1107, batch_loss_c: 0.0951, batch_loss_s: 0.1470, time:8.8305, lr:0.001\u001b[0m\n",
            "2019-11-26 16:59:25 \u001b[32mINFO     \u001b[0m train.py: [46/50], [20/107], step: 4942, 4.154 samples/sec, batch_loss: 0.3060, batch_loss_c: 0.3024, batch_loss_s: 0.3146, time:9.6284, lr:0.001\u001b[0m\n",
            "tcmalloc: large alloc 1215422464 bytes == 0xa6596000 @  0x7fb8703161e7 0x7fb8659e9f71 0x7fb865a4d55d 0x7fb865a50e28 0x7fb865a513e5 0x7fb865ae7fc2 0x50abc5 0x50d320 0x5081d5 0x50a020 0x50aa1d 0x50d320 0x5093e5 0x5951c1 0x5a04ce 0x557878 0x50c81e 0x5081d5 0x50a020 0x50aa1d 0x50c549 0x5081d5 0x50a020 0x50aa1d 0x50c549 0x5081d5 0x5895e1 0x5a04ce 0x50d8f5 0x509ce8 0x50aa1d\n",
            "2019-11-26 16:59:33 \u001b[32mINFO     \u001b[0m train.py: [46/50], [30/107], step: 4952, 4.948 samples/sec, batch_loss: 0.1503, batch_loss_c: 0.1445, batch_loss_s: 0.1638, time:8.0835, lr:0.001\u001b[0m\n",
            "2019-11-26 16:59:42 \u001b[32mINFO     \u001b[0m train.py: [46/50], [40/107], step: 4962, 4.423 samples/sec, batch_loss: 0.1457, batch_loss_c: 0.1450, batch_loss_s: 0.1473, time:9.0445, lr:0.001\u001b[0m\n",
            "2019-11-26 16:59:51 \u001b[32mINFO     \u001b[0m train.py: [46/50], [50/107], step: 4972, 4.511 samples/sec, batch_loss: 0.2019, batch_loss_c: 0.1827, batch_loss_s: 0.2468, time:8.8679, lr:0.001\u001b[0m\n",
            "2019-11-26 16:59:57 \u001b[32mINFO     \u001b[0m train.py: [46/50], [60/107], step: 4982, 6.046 samples/sec, batch_loss: 0.0872, batch_loss_c: 0.0802, batch_loss_s: 0.1034, time:6.6159, lr:0.001\u001b[0m\n",
            "2019-11-26 17:00:04 \u001b[32mINFO     \u001b[0m train.py: [46/50], [70/107], step: 4992, 6.172 samples/sec, batch_loss: 0.0734, batch_loss_c: 0.0669, batch_loss_s: 0.0888, time:6.4813, lr:0.001\u001b[0m\n",
            "2019-11-26 17:00:11 \u001b[32mINFO     \u001b[0m train.py: [46/50], [80/107], step: 5002, 5.332 samples/sec, batch_loss: 0.0899, batch_loss_c: 0.0902, batch_loss_s: 0.0892, time:7.5023, lr:0.001\u001b[0m\n",
            "2019-11-26 17:00:17 \u001b[32mINFO     \u001b[0m train.py: [46/50], [90/107], step: 5012, 7.433 samples/sec, batch_loss: 0.2954, batch_loss_c: 0.2846, batch_loss_s: 0.3206, time:5.3813, lr:0.001\u001b[0m\n",
            "2019-11-26 17:00:22 \u001b[32mINFO     \u001b[0m train.py: [46/50], [100/107], step: 5022, 7.848 samples/sec, batch_loss: 0.0996, batch_loss_c: 0.0748, batch_loss_s: 0.1575, time:5.0966, lr:0.001\u001b[0m\n",
            "2019-11-26 17:00:25 \u001b[32mINFO     \u001b[0m train.py: [46/50], train_loss: 0.1275, time: 88.9522, lr: 0.001\u001b[0m\n",
            "2019-11-26 17:00:32 \u001b[32mINFO     \u001b[0m train.py: [47/50], [0/107], step: 5029, 5.845 samples/sec, batch_loss: 0.0631, batch_loss_c: 0.0572, batch_loss_s: 0.0768, time:6.8431, lr:0.001\u001b[0m\n",
            "2019-11-26 17:00:43 \u001b[32mINFO     \u001b[0m train.py: [47/50], [10/107], step: 5039, 3.647 samples/sec, batch_loss: 0.0633, batch_loss_c: 0.0565, batch_loss_s: 0.0791, time:10.9672, lr:0.001\u001b[0m\n",
            "2019-11-26 17:00:50 \u001b[32mINFO     \u001b[0m train.py: [47/50], [20/107], step: 5049, 5.749 samples/sec, batch_loss: 0.0658, batch_loss_c: 0.0582, batch_loss_s: 0.0836, time:6.9572, lr:0.001\u001b[0m\n",
            "2019-11-26 17:00:57 \u001b[32mINFO     \u001b[0m train.py: [47/50], [30/107], step: 5059, 5.619 samples/sec, batch_loss: 0.0688, batch_loss_c: 0.0584, batch_loss_s: 0.0929, time:7.1183, lr:0.001\u001b[0m\n",
            "2019-11-26 17:01:06 \u001b[32mINFO     \u001b[0m train.py: [47/50], [40/107], step: 5069, 4.722 samples/sec, batch_loss: 0.0786, batch_loss_c: 0.0665, batch_loss_s: 0.1071, time:8.4714, lr:0.001\u001b[0m\n",
            "2019-11-26 17:01:16 \u001b[32mINFO     \u001b[0m train.py: [47/50], [50/107], step: 5079, 4.024 samples/sec, batch_loss: 0.1092, batch_loss_c: 0.1092, batch_loss_s: 0.1094, time:9.9393, lr:0.001\u001b[0m\n",
            "2019-11-26 17:01:22 \u001b[32mINFO     \u001b[0m train.py: [47/50], [60/107], step: 5089, 6.977 samples/sec, batch_loss: 0.0546, batch_loss_c: 0.0418, batch_loss_s: 0.0847, time:5.7330, lr:0.001\u001b[0m\n",
            "2019-11-26 17:01:28 \u001b[32mINFO     \u001b[0m train.py: [47/50], [70/107], step: 5099, 6.473 samples/sec, batch_loss: 0.2900, batch_loss_c: 0.2838, batch_loss_s: 0.3045, time:6.1798, lr:0.001\u001b[0m\n",
            "2019-11-26 17:01:35 \u001b[32mINFO     \u001b[0m train.py: [47/50], [80/107], step: 5109, 5.502 samples/sec, batch_loss: 0.0675, batch_loss_c: 0.0675, batch_loss_s: 0.0675, time:7.2707, lr:0.001\u001b[0m\n",
            "2019-11-26 17:01:41 \u001b[32mINFO     \u001b[0m train.py: [47/50], [90/107], step: 5119, 6.755 samples/sec, batch_loss: 0.1556, batch_loss_c: 0.1574, batch_loss_s: 0.1513, time:5.9216, lr:0.001\u001b[0m\n",
            "2019-11-26 17:01:46 \u001b[32mINFO     \u001b[0m train.py: [47/50], [100/107], step: 5129, 7.777 samples/sec, batch_loss: 0.0677, batch_loss_c: 0.0639, batch_loss_s: 0.0768, time:5.1435, lr:0.001\u001b[0m\n",
            "2019-11-26 17:01:49 \u001b[32mINFO     \u001b[0m train.py: [47/50], train_loss: 0.1231, time: 83.8877, lr: 0.001\u001b[0m\n",
            "2019-11-26 17:01:57 \u001b[32mINFO     \u001b[0m train.py: [48/50], [0/107], step: 5136, 5.740 samples/sec, batch_loss: 0.0783, batch_loss_c: 0.0663, batch_loss_s: 0.1063, time:6.9683, lr:0.001\u001b[0m\n",
            "2019-11-26 17:02:10 \u001b[32mINFO     \u001b[0m train.py: [48/50], [10/107], step: 5146, 3.087 samples/sec, batch_loss: 0.0624, batch_loss_c: 0.0537, batch_loss_s: 0.0826, time:12.9591, lr:0.001\u001b[0m\n",
            "2019-11-26 17:02:20 \u001b[32mINFO     \u001b[0m train.py: [48/50], [20/107], step: 5156, 4.106 samples/sec, batch_loss: 0.0798, batch_loss_c: 0.0702, batch_loss_s: 0.1020, time:9.7416, lr:0.001\u001b[0m\n",
            "2019-11-26 17:02:27 \u001b[32mINFO     \u001b[0m train.py: [48/50], [30/107], step: 5166, 5.630 samples/sec, batch_loss: 0.0870, batch_loss_c: 0.0763, batch_loss_s: 0.1119, time:7.1052, lr:0.001\u001b[0m\n",
            "2019-11-26 17:02:37 \u001b[32mINFO     \u001b[0m train.py: [48/50], [40/107], step: 5176, 4.060 samples/sec, batch_loss: 0.0696, batch_loss_c: 0.0546, batch_loss_s: 0.1047, time:9.8511, lr:0.001\u001b[0m\n",
            "2019-11-26 17:02:44 \u001b[32mINFO     \u001b[0m train.py: [48/50], [50/107], step: 5186, 5.676 samples/sec, batch_loss: 0.0848, batch_loss_c: 0.0795, batch_loss_s: 0.0972, time:7.0475, lr:0.001\u001b[0m\n",
            "2019-11-26 17:02:49 \u001b[32mINFO     \u001b[0m train.py: [48/50], [60/107], step: 5196, 6.918 samples/sec, batch_loss: 0.0577, batch_loss_c: 0.0513, batch_loss_s: 0.0726, time:5.7820, lr:0.001\u001b[0m\n",
            "2019-11-26 17:02:56 \u001b[32mINFO     \u001b[0m train.py: [48/50], [70/107], step: 5206, 5.943 samples/sec, batch_loss: 0.3126, batch_loss_c: 0.2910, batch_loss_s: 0.3631, time:6.7303, lr:0.001\u001b[0m\n",
            "2019-11-26 17:03:04 \u001b[32mINFO     \u001b[0m train.py: [48/50], [80/107], step: 5216, 5.203 samples/sec, batch_loss: 0.1269, batch_loss_c: 0.1203, batch_loss_s: 0.1423, time:7.6886, lr:0.001\u001b[0m\n",
            "2019-11-26 17:03:10 \u001b[32mINFO     \u001b[0m train.py: [48/50], [90/107], step: 5226, 6.341 samples/sec, batch_loss: 0.1128, batch_loss_c: 0.1197, batch_loss_s: 0.0967, time:6.3085, lr:0.001\u001b[0m\n",
            "2019-11-26 17:03:15 \u001b[32mINFO     \u001b[0m train.py: [48/50], [100/107], step: 5236, 7.966 samples/sec, batch_loss: 0.1605, batch_loss_c: 0.1494, batch_loss_s: 0.1865, time:5.0210, lr:0.001\u001b[0m\n",
            "2019-11-26 17:03:19 \u001b[32mINFO     \u001b[0m train.py: [48/50], train_loss: 0.1298, time: 88.5985, lr: 0.001\u001b[0m\n",
            "2019-11-26 17:03:28 \u001b[32mINFO     \u001b[0m train.py: [49/50], [0/107], step: 5243, 4.470 samples/sec, batch_loss: 0.1666, batch_loss_c: 0.1515, batch_loss_s: 0.2021, time:8.9476, lr:0.001\u001b[0m\n",
            "2019-11-26 17:03:36 \u001b[32mINFO     \u001b[0m train.py: [49/50], [10/107], step: 5253, 5.010 samples/sec, batch_loss: 0.1031, batch_loss_c: 0.0970, batch_loss_s: 0.1171, time:7.9835, lr:0.001\u001b[0m\n",
            "2019-11-26 17:03:46 \u001b[32mINFO     \u001b[0m train.py: [49/50], [20/107], step: 5263, 4.061 samples/sec, batch_loss: 0.1240, batch_loss_c: 0.1375, batch_loss_s: 0.0925, time:9.8507, lr:0.001\u001b[0m\n",
            "2019-11-26 17:03:55 \u001b[32mINFO     \u001b[0m train.py: [49/50], [30/107], step: 5273, 4.560 samples/sec, batch_loss: 0.0887, batch_loss_c: 0.0824, batch_loss_s: 0.1034, time:8.7711, lr:0.001\u001b[0m\n",
            "2019-11-26 17:04:05 \u001b[32mINFO     \u001b[0m train.py: [49/50], [40/107], step: 5283, 4.001 samples/sec, batch_loss: 0.1145, batch_loss_c: 0.1136, batch_loss_s: 0.1167, time:9.9969, lr:0.001\u001b[0m\n",
            "2019-11-26 17:04:11 \u001b[32mINFO     \u001b[0m train.py: [49/50], [50/107], step: 5293, 5.963 samples/sec, batch_loss: 0.6634, batch_loss_c: 0.6556, batch_loss_s: 0.6816, time:6.7078, lr:0.001\u001b[0m\n",
            "2019-11-26 17:04:17 \u001b[32mINFO     \u001b[0m train.py: [49/50], [60/107], step: 5303, 7.036 samples/sec, batch_loss: 0.0563, batch_loss_c: 0.0460, batch_loss_s: 0.0801, time:5.6852, lr:0.001\u001b[0m\n",
            "2019-11-26 17:04:24 \u001b[32mINFO     \u001b[0m train.py: [49/50], [70/107], step: 5313, 5.866 samples/sec, batch_loss: 0.0864, batch_loss_c: 0.0762, batch_loss_s: 0.1104, time:6.8193, lr:0.001\u001b[0m\n",
            "2019-11-26 17:04:32 \u001b[32mINFO     \u001b[0m train.py: [49/50], [80/107], step: 5323, 4.964 samples/sec, batch_loss: 0.0712, batch_loss_c: 0.0566, batch_loss_s: 0.1050, time:8.0573, lr:0.001\u001b[0m\n",
            "2019-11-26 17:04:38 \u001b[32mINFO     \u001b[0m train.py: [49/50], [90/107], step: 5333, 6.438 samples/sec, batch_loss: 0.2991, batch_loss_c: 0.3254, batch_loss_s: 0.2378, time:6.2130, lr:0.001\u001b[0m\n",
            "2019-11-26 17:04:43 \u001b[32mINFO     \u001b[0m train.py: [49/50], [100/107], step: 5343, 7.877 samples/sec, batch_loss: 0.3156, batch_loss_c: 0.3048, batch_loss_s: 0.3408, time:5.0782, lr:0.001\u001b[0m\n",
            "2019-11-26 17:04:47 \u001b[32mINFO     \u001b[0m train.py: [49/50], train_loss: 0.1259, time: 87.5048, lr: 0.001\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P47Jx7_DPeO7",
        "colab_type": "text"
      },
      "source": [
        "[The IIIT Scene Text Retrieval (STR) Dataset](https://cvit.iiit.ac.in/research/projects/cvit-projects/the-iiit-scene-text-retrieval-str-dataset)"
      ]
    }
  ]
}