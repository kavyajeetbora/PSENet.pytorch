{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PSENet_trial_run.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kavyajeetbora/PSENet.pytorch/blob/master/PSENet_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UByxwp0F3QUp",
        "colab_type": "text"
      },
      "source": [
        "## Installing softwares and other dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GSEHnmCxK_o9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, Dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y99kqKlHoYkz",
        "colab_type": "code",
        "outputId": "c07483b7-5d78-4c5a-f604-8c0e26ab53b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "!pip install pyclipper"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyclipper\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e1/40/57a0d54a1c696d58253c88a95677e50ab2b305a15af0ac64b70db4320562/pyclipper-1.1.0.post3-cp36-cp36m-manylinux1_x86_64.whl (131kB)\n",
            "\r\u001b[K     |██▌                             | 10kB 26.6MB/s eta 0:00:01\r\u001b[K     |█████                           | 20kB 6.6MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 30kB 9.3MB/s eta 0:00:01\r\u001b[K     |██████████                      | 40kB 11.8MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 51kB 7.5MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 61kB 8.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 71kB 10.0MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 81kB 11.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 92kB 12.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 102kB 9.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 112kB 9.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 122kB 9.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 133kB 9.8MB/s \n",
            "\u001b[?25hInstalling collected packages: pyclipper\n",
            "Successfully installed pyclipper-1.1.0.post3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ncoynOlfnIW1",
        "colab_type": "code",
        "outputId": "94570a2b-804d-4984-f705-801110065691",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hhJ-6yyXcnRx",
        "colab_type": "code",
        "outputId": "26701c8e-2efd-4591-bdfb-a02820c467fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        }
      },
      "source": [
        "import shutil, os\n",
        "os.chdir('/content')\n",
        "directory = '/content/cloned-repo'\n",
        "if os.path.exists(directory):\n",
        "  shutil.rmtree(directory)\n",
        "\n",
        "!git clone https://github.com/kavyajeetbora/PSENet.pytorch.git /content/cloned-repo\n",
        "print(\"Cloned the repository\")\n",
        "os.chdir('/content/cloned-repo')\n",
        "!ls"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into '/content/cloned-repo'...\n",
            "remote: Enumerating objects: 33, done.\u001b[K\n",
            "remote: Counting objects:   3% (1/33)\u001b[K\rremote: Counting objects:   6% (2/33)\u001b[K\rremote: Counting objects:   9% (3/33)\u001b[K\rremote: Counting objects:  12% (4/33)\u001b[K\rremote: Counting objects:  15% (5/33)\u001b[K\rremote: Counting objects:  18% (6/33)\u001b[K\rremote: Counting objects:  21% (7/33)\u001b[K\rremote: Counting objects:  24% (8/33)\u001b[K\rremote: Counting objects:  27% (9/33)\u001b[K\rremote: Counting objects:  30% (10/33)\u001b[K\rremote: Counting objects:  33% (11/33)\u001b[K\rremote: Counting objects:  36% (12/33)\u001b[K\rremote: Counting objects:  39% (13/33)\u001b[K\rremote: Counting objects:  42% (14/33)\u001b[K\rremote: Counting objects:  45% (15/33)\u001b[K\rremote: Counting objects:  48% (16/33)\u001b[K\rremote: Counting objects:  51% (17/33)\u001b[K\rremote: Counting objects:  54% (18/33)\u001b[K\rremote: Counting objects:  57% (19/33)\u001b[K\rremote: Counting objects:  60% (20/33)\u001b[K\rremote: Counting objects:  63% (21/33)\u001b[K\rremote: Counting objects:  66% (22/33)\u001b[K\rremote: Counting objects:  69% (23/33)\u001b[K\rremote: Counting objects:  72% (24/33)\u001b[K\rremote: Counting objects:  75% (25/33)\u001b[K\rremote: Counting objects:  78% (26/33)\u001b[K\rremote: Counting objects:  81% (27/33)\u001b[K\rremote: Counting objects:  84% (28/33)\u001b[K\rremote: Counting objects:  87% (29/33)\u001b[K\rremote: Counting objects:  90% (30/33)\u001b[K\rremote: Counting objects:  93% (31/33)\u001b[K\rremote: Counting objects:  96% (32/33)\u001b[K\rremote: Counting objects: 100% (33/33)\u001b[K\rremote: Counting objects: 100% (33/33), done.\u001b[K\n",
            "remote: Compressing objects: 100% (33/33), done.\u001b[K\n",
            "remote: Total 596 (delta 14), reused 0 (delta 0), pack-reused 563\u001b[K\n",
            "Receiving objects: 100% (596/596), 33.29 MiB | 17.96 MiB/s, done.\n",
            "Resolving deltas: 100% (311/311), done.\n",
            "Cloned the repository\n",
            "cal_recall  install_dependencies.sh  PSENet.ipynb\t     train.py\n",
            "config.py   LICENSE\t\t     PSENet_predict.ipynb    utils\n",
            "dataset     models\t\t     PSENet_training.ipynb\n",
            "eval.py     predict.py\t\t     PSENet_trial_run.ipynb\n",
            "imgs\t    pse\t\t\t     README.md\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PrOmfo6_3X2I",
        "colab_type": "text"
      },
      "source": [
        "## Extracting the data and setting up the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dGLMxTujlm0w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## unzipping the files\n",
        "from zipfile import ZipFile\n",
        "\n",
        "def unzip_files(file,output_dir):\n",
        "  with ZipFile(file, 'r') as zipObj:\n",
        "    # Extract all the contents of zip file in current directory\n",
        "    zipObj.extractall(output_dir)\n",
        "  print('Extracted to',output_dir)\n",
        "\n",
        "def make_directory(directory):\n",
        "  if os.path.isdir(directory):\n",
        "    shutil.rmtree(directory)\n",
        "  \n",
        "  os.mkdir(directory)\n",
        "  print('Created a new directory')\n",
        "\n",
        "training_data_zip = '/content/drive/My Drive/Colab Notebooks/padh.ai.notebooks/15. Object Detection/Scene Text Detection Dataset/English and Hindi MLT 2019.zip'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FqJ6xb5u2dgg",
        "colab_type": "code",
        "outputId": "acab83c8-af31-4d01-b54b-5233fde1f315",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# make directories\n",
        "make_directory('Training Set')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Created a new directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FFSaSXr-m-K4",
        "colab_type": "code",
        "outputId": "6cc15197-b774-4c68-cbd3-10104e3c950d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "%%time\n",
        "unzip_files(training_data_zip,'Training Set')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracted to Training Set\n",
            "CPU times: user 2.26 s, sys: 755 ms, total: 3.02 s\n",
            "Wall time: 15.4 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YRSeyluknVH4",
        "colab_type": "code",
        "outputId": "ccd0e41c-5a34-471c-fb33-382a4be55907",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(len(os.listdir('Training Set/Images')))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MOp4JsZ6nXvn",
        "colab_type": "code",
        "outputId": "2e3a0fa6-ceac-404d-b82d-0efa79d991f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(len(os.listdir('Training Set/Annotations')))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mhvg89mXn8zG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from dataset.data_utils import *\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader, Dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4JeHTz3G6Lyj",
        "colab_type": "code",
        "outputId": "8d5f5747-a0e9-4e92-f790-8a521a3d9602",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_data = MyDataset('Training Set',transform=transforms.ToTensor())\n",
        "len(train_data)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1938"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "odLKMn-5SBAz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device(\"cuda:0\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9a9PsyF3dqo",
        "colab_type": "text"
      },
      "source": [
        "## Training "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9x221Ya0RBGO",
        "outputId": "bce151ac-f42f-4704-ded5-ac387c384069",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        }
      },
      "source": [
        "!chmod +x install_dependencies.sh # make shell script executable\n",
        "!./install_dependencies.sh # run the shell script"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyclipper in /usr/local/lib/python3.6/dist-packages (1.1.0.post3)\n",
            "Collecting Polygon3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/73/a0/d07a4f3e80ed7020a33f3111db217f54ac44a485ff45da3c21ce49f65041/Polygon3-3.0.8.tar.gz (71kB)\n",
            "\r\u001b[K     |████▋                           | 10kB 29.9MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 20kB 6.3MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 30kB 8.5MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 40kB 5.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 51kB 7.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 61kB 8.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 71kB 5.4MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: Polygon3\n",
            "  Building wheel for Polygon3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for Polygon3: filename=Polygon3-3.0.8-cp36-cp36m-linux_x86_64.whl size=101495 sha256=030400a6cbd2b633ceb881f7976d6ba42d59e2524a75f3c0ca33803716b5d5a0\n",
            "  Stored in directory: /root/.cache/pip/wheels/95/32/f1/5525b233996d9d99cbce2f0a8da60d137ddddc555d3e8b0e2a\n",
            "Successfully built Polygon3\n",
            "Installing collected packages: Polygon3\n",
            "Successfully installed Polygon3-3.0.8\n",
            "Collecting colorlog\n",
            "  Downloading https://files.pythonhosted.org/packages/68/4d/892728b0c14547224f0ac40884e722a3d00cb54e7a146aea0b3186806c9e/colorlog-4.0.2-py2.py3-none-any.whl\n",
            "Installing collected packages: colorlog\n",
            "Successfully installed colorlog-4.0.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xD4GuTKC3wXY",
        "colab_type": "code",
        "outputId": "6946c0b6-49f1-4190-970f-26f98b61486e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!python3 train.py"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "make: Entering directory '/content/cloned-repo/pse'\n",
            "make: 'pse.so' is up to date.\n",
            "make: Leaving directory '/content/cloned-repo/pse'\n",
            "2019-12-08 08:52:53 \u001b[32mINFO     \u001b[0m utils.py: logger init finished\u001b[0m\n",
            "2019-12-08 08:52:53 \u001b[32mINFO     \u001b[0m train.py: {'Lambda': 0.7,\n",
            " 'OHEM_ratio': 3,\n",
            " 'backbone': 'resnet50',\n",
            " 'checkpoint': '',\n",
            " 'data_shape': 640,\n",
            " 'display_input_images': False,\n",
            " 'display_interval': 10,\n",
            " 'display_output_images': False,\n",
            " 'end_lr': 1e-06,\n",
            " 'epochs': 300,\n",
            " 'gpu_id': '0',\n",
            " 'lr': 1e-05,\n",
            " 'lr_decay_step': [100, 200],\n",
            " 'lr_gamma': 0.1,\n",
            " 'm': 0.5,\n",
            " 'n': 6,\n",
            " 'output_dir': '/content/drive/My Drive/PSENet_2',\n",
            " 'pretrained': False,\n",
            " 'pretrained_path': '/content/drive/My Drive/PSENet_2/PSENet_resnet50.pth',\n",
            " 'restart_training': False,\n",
            " 'scale': 1,\n",
            " 'seed': 2,\n",
            " 'show_images_interval': 50,\n",
            " 'start_epoch': 0,\n",
            " 'testroot': 'Test Set',\n",
            " 'train_batch_size': 4,\n",
            " 'trainroot': 'Training Set',\n",
            " 'warm_up_epoch': 6,\n",
            " 'warm_up_lr': 1.0000000000000002e-06,\n",
            " 'weight_decay': 0.0005,\n",
            " 'workers': 0}\u001b[0m\n",
            "2019-12-08 08:52:53 \u001b[32mINFO     \u001b[0m train.py: train with gpu 0 and pytorch 1.3.1\u001b[0m\n",
            "2019-12-08 08:53:05 \u001b[32mINFO     \u001b[0m train.py: train dataset has 1938 samples,484 in dataloader\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:100: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule.See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "2019-12-08 08:53:13 \u001b[32mINFO     \u001b[0m train.py: [0/300], [0/484], step: 0, 4.880 samples/sec, batch_loss: 0.0458, batch_loss_c: 0.0419, batch_loss_s: 0.0550, time:8.1970, lr:1e-05\u001b[0m\n",
            "tcmalloc: large alloc 1728004096 bytes == 0xb3762000 @  0x7f203bedd1e7 0x7f20304c1f71 0x7f203052555d 0x7f2030528e28 0x7f20305293e5 0x7f20305bffc2 0x50ac25 0x50d390 0x508245 0x50a080 0x50aa7d 0x50d390 0x509455 0x595311 0x5a067e 0x557d28 0x50c88b 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x509455 0x595311 0x5a522c 0x557f7e 0x4b5eff 0x50c810\n",
            "2019-12-08 08:53:31 \u001b[32mINFO     \u001b[0m train.py: [0/300], [10/484], step: 10, 2.218 samples/sec, batch_loss: 0.0933, batch_loss_c: 0.0884, batch_loss_s: 0.1047, time:18.0353, lr:1e-05\u001b[0m\n",
            "tcmalloc: large alloc 1226956800 bytes == 0xa61e4000 @  0x7f203bedd1e7 0x7f20304c1f71 0x7f203052555d 0x7f2030528e28 0x7f20305293e5 0x7f20305bffc2 0x50ac25 0x50d390 0x508245 0x50a080 0x50aa7d 0x50d390 0x509455 0x595311 0x5a067e 0x557d28 0x50c88b 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x509455 0x595311 0x5a522c 0x557f7e 0x4b5eff 0x50c810\n",
            "tcmalloc: large alloc 2960089088 bytes == 0x11bd54000 @  0x7f203bedd1e7 0x7f20304c1f71 0x7f203052555d 0x7f2030528e28 0x7f20305293e5 0x7f20305bffc2 0x50ac25 0x50d390 0x508245 0x50a080 0x50aa7d 0x50d390 0x509455 0x595311 0x5a067e 0x557d28 0x50c88b 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x509455 0x595311 0x5a522c 0x557f7e 0x4b5eff 0x50c810\n",
            "2019-12-08 08:54:07 \u001b[32mINFO     \u001b[0m train.py: [0/300], [20/484], step: 20, 1.098 samples/sec, batch_loss: 0.2960, batch_loss_c: 0.2902, batch_loss_s: 0.3095, time:36.4282, lr:1e-05\u001b[0m\n",
            "tcmalloc: large alloc 1725898752 bytes == 0x11bd54000 @  0x7f203bedd1e7 0x7f20304c1f71 0x7f203052555d 0x7f2030528e28 0x7f20305293e5 0x7f20305bffc2 0x50ac25 0x50d390 0x508245 0x50a080 0x50aa7d 0x50d390 0x509455 0x595311 0x5a067e 0x557d28 0x50c88b 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x509455 0x595311 0x5a522c 0x557f7e 0x4b5eff 0x50c810\n",
            "tcmalloc: large alloc 3428409344 bytes == 0x7f1cb9a6a000 @  0x7f203bedd1e7 0x7f20304c1f71 0x7f203052555d 0x7f2030528e28 0x7f20305293e5 0x7f20305bffc2 0x50ac25 0x50d390 0x508245 0x50a080 0x50aa7d 0x50d390 0x509455 0x595311 0x5a067e 0x557d28 0x50c88b 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x509455 0x595311 0x5a522c 0x557f7e 0x4b5eff 0x50c810\n",
            "2019-12-08 08:54:29 \u001b[32mINFO     \u001b[0m train.py: [0/300], [30/484], step: 30, 1.823 samples/sec, batch_loss: 0.2891, batch_loss_c: 0.2852, batch_loss_s: 0.2982, time:21.9369, lr:1e-05\u001b[0m\n",
            "2019-12-08 08:54:48 \u001b[32mINFO     \u001b[0m train.py: [0/300], [40/484], step: 40, 2.182 samples/sec, batch_loss: 0.0427, batch_loss_c: 0.0289, batch_loss_s: 0.0750, time:18.3355, lr:1e-05\u001b[0m\n",
            "tcmalloc: large alloc 2868396032 bytes == 0x11bd54000 @  0x7f203bedd1e7 0x7f20304c1f71 0x7f203052555d 0x7f2030528e28 0x7f20305293e5 0x7f20305bffc2 0x50ac25 0x50d390 0x508245 0x50a080 0x50aa7d 0x50d390 0x509455 0x595311 0x5a067e 0x557d28 0x50c88b 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x509455 0x595311 0x5a522c 0x557f7e 0x4b5eff 0x50c810\n",
            "2019-12-08 08:55:18 \u001b[32mINFO     \u001b[0m train.py: [0/300], [50/484], step: 50, 1.330 samples/sec, batch_loss: 0.0752, batch_loss_c: 0.0745, batch_loss_s: 0.0769, time:30.0664, lr:1e-05\u001b[0m\n",
            "2019-12-08 08:55:41 \u001b[32mINFO     \u001b[0m train.py: [0/300], [60/484], step: 60, 1.721 samples/sec, batch_loss: 0.3076, batch_loss_c: 0.2975, batch_loss_s: 0.3312, time:23.2484, lr:1e-05\u001b[0m\n",
            "2019-12-08 08:55:59 \u001b[32mINFO     \u001b[0m train.py: [0/300], [70/484], step: 70, 2.215 samples/sec, batch_loss: 0.0710, batch_loss_c: 0.0675, batch_loss_s: 0.0792, time:18.0566, lr:1e-05\u001b[0m\n",
            "tcmalloc: large alloc 2633637888 bytes == 0x11bd54000 @  0x7f203bedd1e7 0x7f20304c1f71 0x7f203052555d 0x7f2030528e28 0x7f20305293e5 0x7f20305bffc2 0x50ac25 0x50d390 0x508245 0x50a080 0x50aa7d 0x50d390 0x509455 0x595311 0x5a067e 0x557d28 0x50c88b 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x509455 0x595311 0x5a522c 0x557f7e 0x4b5eff 0x50c810\n",
            "2019-12-08 08:56:18 \u001b[32mINFO     \u001b[0m train.py: [0/300], [80/484], step: 80, 2.075 samples/sec, batch_loss: 0.1350, batch_loss_c: 0.1250, batch_loss_s: 0.1581, time:19.2798, lr:1e-05\u001b[0m\n",
            "2019-12-08 08:56:33 \u001b[32mINFO     \u001b[0m train.py: [0/300], [90/484], step: 90, 2.697 samples/sec, batch_loss: 0.0910, batch_loss_c: 0.0862, batch_loss_s: 0.1021, time:14.8329, lr:1e-05\u001b[0m\n",
            "2019-12-08 08:56:56 \u001b[32mINFO     \u001b[0m train.py: [0/300], [100/484], step: 100, 1.781 samples/sec, batch_loss: 0.4046, batch_loss_c: 0.4156, batch_loss_s: 0.3789, time:22.4559, lr:1e-05\u001b[0m\n",
            "2019-12-08 08:57:11 \u001b[32mINFO     \u001b[0m train.py: [0/300], [110/484], step: 110, 2.654 samples/sec, batch_loss: 0.0641, batch_loss_c: 0.0612, batch_loss_s: 0.0710, time:15.0735, lr:1e-05\u001b[0m\n",
            "tcmalloc: large alloc 2638168064 bytes == 0x11bd54000 @  0x7f203bedd1e7 0x7f20304c1f71 0x7f203052555d 0x7f2030528e28 0x7f20305293e5 0x7f20305bffc2 0x50ac25 0x50d390 0x508245 0x50a080 0x50aa7d 0x50d390 0x509455 0x595311 0x5a067e 0x557d28 0x50c88b 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x509455 0x595311 0x5a522c 0x557f7e 0x4b5eff 0x50c810\n",
            "2019-12-08 08:57:45 \u001b[32mINFO     \u001b[0m train.py: [0/300], [120/484], step: 120, 1.174 samples/sec, batch_loss: 0.2286, batch_loss_c: 0.2551, batch_loss_s: 0.1669, time:34.0699, lr:1e-05\u001b[0m\n",
            "2019-12-08 08:58:09 \u001b[32mINFO     \u001b[0m train.py: [0/300], [130/484], step: 130, 1.623 samples/sec, batch_loss: 0.0553, batch_loss_c: 0.0482, batch_loss_s: 0.0717, time:24.6531, lr:1e-05\u001b[0m\n",
            "2019-12-08 08:58:26 \u001b[32mINFO     \u001b[0m train.py: [0/300], [140/484], step: 140, 2.347 samples/sec, batch_loss: 0.0463, batch_loss_c: 0.0405, batch_loss_s: 0.0600, time:17.0413, lr:1e-05\u001b[0m\n",
            "2019-12-08 08:58:43 \u001b[32mINFO     \u001b[0m train.py: [0/300], [150/484], step: 150, 2.463 samples/sec, batch_loss: 0.0740, batch_loss_c: 0.0668, batch_loss_s: 0.0908, time:16.2391, lr:1e-05\u001b[0m\n",
            "2019-12-08 08:59:15 \u001b[32mINFO     \u001b[0m train.py: [0/300], [160/484], step: 160, 1.234 samples/sec, batch_loss: 0.3319, batch_loss_c: 0.3337, batch_loss_s: 0.3278, time:32.4204, lr:1e-05\u001b[0m\n",
            "2019-12-08 08:59:31 \u001b[32mINFO     \u001b[0m train.py: [0/300], [170/484], step: 170, 2.584 samples/sec, batch_loss: 0.2946, batch_loss_c: 0.2810, batch_loss_s: 0.3263, time:15.4822, lr:1e-05\u001b[0m\n",
            "2019-12-08 08:59:46 \u001b[32mINFO     \u001b[0m train.py: [0/300], [180/484], step: 180, 2.556 samples/sec, batch_loss: 0.2971, batch_loss_c: 0.3009, batch_loss_s: 0.2881, time:15.6469, lr:1e-05\u001b[0m\n",
            "2019-12-08 09:00:02 \u001b[32mINFO     \u001b[0m train.py: [0/300], [190/484], step: 190, 2.594 samples/sec, batch_loss: 0.0481, batch_loss_c: 0.0430, batch_loss_s: 0.0601, time:15.4225, lr:1e-05\u001b[0m\n",
            "2019-12-08 09:00:24 \u001b[32mINFO     \u001b[0m train.py: [0/300], [200/484], step: 200, 1.762 samples/sec, batch_loss: 0.4374, batch_loss_c: 0.4207, batch_loss_s: 0.4766, time:22.7068, lr:1e-05\u001b[0m\n",
            "tcmalloc: large alloc 3033710592 bytes == 0x7f1cb9a6a000 @  0x7f203bedd1e7 0x7f20304c1f71 0x7f203052555d 0x7f2030528e28 0x7f20305293e5 0x7f20305bffc2 0x50ac25 0x50d390 0x508245 0x50a080 0x50aa7d 0x50d390 0x509455 0x595311 0x5a067e 0x557d28 0x50c88b 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x509455 0x595311 0x5a522c 0x557f7e 0x4b5eff 0x50c810\n",
            "2019-12-08 09:00:52 \u001b[32mINFO     \u001b[0m train.py: [0/300], [210/484], step: 210, 1.457 samples/sec, batch_loss: 0.1316, batch_loss_c: 0.1145, batch_loss_s: 0.1717, time:27.4527, lr:1e-05\u001b[0m\n",
            "2019-12-08 09:01:08 \u001b[32mINFO     \u001b[0m train.py: [0/300], [220/484], step: 220, 2.481 samples/sec, batch_loss: 0.0831, batch_loss_c: 0.0756, batch_loss_s: 0.1006, time:16.1221, lr:1e-05\u001b[0m\n",
            "2019-12-08 09:01:26 \u001b[32mINFO     \u001b[0m train.py: [0/300], [230/484], step: 230, 2.197 samples/sec, batch_loss: 0.0637, batch_loss_c: 0.0589, batch_loss_s: 0.0749, time:18.2066, lr:1e-05\u001b[0m\n",
            "2019-12-08 09:01:44 \u001b[32mINFO     \u001b[0m train.py: [0/300], [240/484], step: 240, 2.212 samples/sec, batch_loss: 0.0916, batch_loss_c: 0.0853, batch_loss_s: 0.1064, time:18.0829, lr:1e-05\u001b[0m\n",
            "2019-12-08 09:02:13 \u001b[32mINFO     \u001b[0m train.py: [0/300], [250/484], step: 250, 1.371 samples/sec, batch_loss: 0.3641, batch_loss_c: 0.3628, batch_loss_s: 0.3672, time:29.1786, lr:1e-05\u001b[0m\n",
            "2019-12-08 09:02:29 \u001b[32mINFO     \u001b[0m train.py: [0/300], [260/484], step: 260, 2.495 samples/sec, batch_loss: 0.1062, batch_loss_c: 0.1011, batch_loss_s: 0.1182, time:16.0301, lr:1e-05\u001b[0m\n",
            "2019-12-08 09:02:52 \u001b[32mINFO     \u001b[0m train.py: [0/300], [270/484], step: 270, 1.789 samples/sec, batch_loss: 0.0744, batch_loss_c: 0.0592, batch_loss_s: 0.1098, time:22.3574, lr:1e-05\u001b[0m\n",
            "2019-12-08 09:03:07 \u001b[32mINFO     \u001b[0m train.py: [0/300], [280/484], step: 280, 2.645 samples/sec, batch_loss: 0.0580, batch_loss_c: 0.0573, batch_loss_s: 0.0596, time:15.1242, lr:1e-05\u001b[0m\n",
            "2019-12-08 09:03:31 \u001b[32mINFO     \u001b[0m train.py: [0/300], [290/484], step: 290, 1.691 samples/sec, batch_loss: 0.3479, batch_loss_c: 0.3561, batch_loss_s: 0.3287, time:23.6480, lr:1e-05\u001b[0m\n",
            "2019-12-08 09:03:49 \u001b[32mINFO     \u001b[0m train.py: [0/300], [300/484], step: 300, 2.130 samples/sec, batch_loss: 0.0872, batch_loss_c: 0.0716, batch_loss_s: 0.1235, time:18.7832, lr:1e-05\u001b[0m\n",
            "2019-12-08 09:04:13 \u001b[32mINFO     \u001b[0m train.py: [0/300], [310/484], step: 310, 1.707 samples/sec, batch_loss: 0.0660, batch_loss_c: 0.0592, batch_loss_s: 0.0817, time:23.4362, lr:1e-05\u001b[0m\n",
            "2019-12-08 09:04:31 \u001b[32mINFO     \u001b[0m train.py: [0/300], [320/484], step: 320, 2.177 samples/sec, batch_loss: 0.0472, batch_loss_c: 0.0369, batch_loss_s: 0.0712, time:18.3743, lr:1e-05\u001b[0m\n",
            "2019-12-08 09:04:49 \u001b[32mINFO     \u001b[0m train.py: [0/300], [330/484], step: 330, 2.224 samples/sec, batch_loss: 0.1027, batch_loss_c: 0.0926, batch_loss_s: 0.1262, time:17.9817, lr:1e-05\u001b[0m\n",
            "2019-12-08 09:05:09 \u001b[32mINFO     \u001b[0m train.py: [0/300], [340/484], step: 340, 1.974 samples/sec, batch_loss: 0.0709, batch_loss_c: 0.0605, batch_loss_s: 0.0954, time:20.2644, lr:1e-05\u001b[0m\n",
            "2019-12-08 09:05:26 \u001b[32mINFO     \u001b[0m train.py: [0/300], [350/484], step: 350, 2.410 samples/sec, batch_loss: 0.0575, batch_loss_c: 0.0402, batch_loss_s: 0.0978, time:16.6002, lr:1e-05\u001b[0m\n",
            "2019-12-08 09:05:47 \u001b[32mINFO     \u001b[0m train.py: [0/300], [360/484], step: 360, 1.958 samples/sec, batch_loss: 0.4068, batch_loss_c: 0.4226, batch_loss_s: 0.3699, time:20.4331, lr:1e-05\u001b[0m\n",
            "2019-12-08 09:06:04 \u001b[32mINFO     \u001b[0m train.py: [0/300], [370/484], step: 370, 2.238 samples/sec, batch_loss: 0.1439, batch_loss_c: 0.1438, batch_loss_s: 0.1441, time:17.8737, lr:1e-05\u001b[0m\n",
            "2019-12-08 09:06:19 \u001b[32mINFO     \u001b[0m train.py: [0/300], [380/484], step: 380, 2.649 samples/sec, batch_loss: 0.0646, batch_loss_c: 0.0591, batch_loss_s: 0.0776, time:15.0993, lr:1e-05\u001b[0m\n",
            "2019-12-08 09:06:43 \u001b[32mINFO     \u001b[0m train.py: [0/300], [390/484], step: 390, 1.717 samples/sec, batch_loss: 0.3916, batch_loss_c: 0.3856, batch_loss_s: 0.4057, time:23.3031, lr:1e-05\u001b[0m\n",
            "2019-12-08 09:07:02 \u001b[32mINFO     \u001b[0m train.py: [0/300], [400/484], step: 400, 2.107 samples/sec, batch_loss: 0.0635, batch_loss_c: 0.0513, batch_loss_s: 0.0921, time:18.9855, lr:1e-05\u001b[0m\n",
            "tcmalloc: large alloc 5197824000 bytes == 0x7f1b83d62000 @  0x7f203bedd1e7 0x7f20304c1f71 0x7f203052555d 0x7f2030528e28 0x7f20305293e5 0x7f20305bffc2 0x50ac25 0x50d390 0x508245 0x50a080 0x50aa7d 0x50d390 0x509455 0x595311 0x5a067e 0x557d28 0x50c88b 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x509455 0x595311 0x5a522c 0x557f7e 0x4b5eff 0x50c810\n",
            "2019-12-08 09:07:26 \u001b[32mINFO     \u001b[0m train.py: [0/300], [410/484], step: 410, 1.656 samples/sec, batch_loss: 0.0786, batch_loss_c: 0.0725, batch_loss_s: 0.0928, time:24.1558, lr:1e-05\u001b[0m\n",
            "2019-12-08 09:07:42 \u001b[32mINFO     \u001b[0m train.py: [0/300], [420/484], step: 420, 2.493 samples/sec, batch_loss: 0.0517, batch_loss_c: 0.0422, batch_loss_s: 0.0740, time:16.0418, lr:1e-05\u001b[0m\n",
            "2019-12-08 09:07:57 \u001b[32mINFO     \u001b[0m train.py: [0/300], [430/484], step: 430, 2.737 samples/sec, batch_loss: 0.0392, batch_loss_c: 0.0329, batch_loss_s: 0.0540, time:14.6162, lr:1e-05\u001b[0m\n",
            "2019-12-08 09:08:20 \u001b[32mINFO     \u001b[0m train.py: [0/300], [440/484], step: 440, 1.691 samples/sec, batch_loss: 0.0594, batch_loss_c: 0.0484, batch_loss_s: 0.0851, time:23.6493, lr:1e-05\u001b[0m\n",
            "2019-12-08 09:08:41 \u001b[32mINFO     \u001b[0m train.py: [0/300], [450/484], step: 450, 1.911 samples/sec, batch_loss: 0.5267, batch_loss_c: 0.5238, batch_loss_s: 0.5335, time:20.9307, lr:1e-05\u001b[0m\n",
            "2019-12-08 09:09:09 \u001b[32mINFO     \u001b[0m train.py: [0/300], [460/484], step: 460, 1.419 samples/sec, batch_loss: 0.0520, batch_loss_c: 0.0506, batch_loss_s: 0.0553, time:28.1868, lr:1e-05\u001b[0m\n",
            "2019-12-08 09:09:38 \u001b[32mINFO     \u001b[0m train.py: [0/300], [470/484], step: 470, 1.412 samples/sec, batch_loss: 0.0441, batch_loss_c: 0.0380, batch_loss_s: 0.0585, time:28.3229, lr:1e-05\u001b[0m\n",
            "2019-12-08 09:10:05 \u001b[32mINFO     \u001b[0m train.py: [0/300], [480/484], step: 480, 1.481 samples/sec, batch_loss: 0.0995, batch_loss_c: 0.0911, batch_loss_s: 0.1193, time:27.0104, lr:1e-05\u001b[0m\n",
            "2019-12-08 09:10:19 \u001b[32mINFO     \u001b[0m train.py: [0/300], train_loss: 0.1411, time: 1033.9521, lr: 1e-05\u001b[0m\n",
            "2019-12-08 09:10:23 \u001b[32mINFO     \u001b[0m train.py: [1/300], [0/484], step: 484, 11.082 samples/sec, batch_loss: 0.3670, batch_loss_c: 0.3581, batch_loss_s: 0.3878, time:3.6094, lr:1e-05\u001b[0m\n",
            "tcmalloc: large alloc 3986677760 bytes == 0x7f1b83d62000 @  0x7f203bedd1e7 0x7f20304c1f71 0x7f203052555d 0x7f2030528e28 0x7f20305293e5 0x7f20305bffc2 0x50ac25 0x50d390 0x508245 0x50a080 0x50aa7d 0x50d390 0x509455 0x595311 0x5a067e 0x557d28 0x50c88b 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x509455 0x595311 0x5a522c 0x557f7e 0x4b5eff 0x50c810\n",
            "2019-12-08 09:11:00 \u001b[32mINFO     \u001b[0m train.py: [1/300], [10/484], step: 494, 1.063 samples/sec, batch_loss: 0.1630, batch_loss_c: 0.1342, batch_loss_s: 0.2302, time:37.6160, lr:1e-05\u001b[0m\n",
            "2019-12-08 09:11:17 \u001b[32mINFO     \u001b[0m train.py: [1/300], [20/484], step: 504, 2.460 samples/sec, batch_loss: 0.0372, batch_loss_c: 0.0300, batch_loss_s: 0.0539, time:16.2607, lr:1e-05\u001b[0m\n",
            "2019-12-08 09:11:32 \u001b[32mINFO     \u001b[0m train.py: [1/300], [30/484], step: 514, 2.616 samples/sec, batch_loss: 0.0682, batch_loss_c: 0.0579, batch_loss_s: 0.0921, time:15.2893, lr:1e-05\u001b[0m\n",
            "2019-12-08 09:12:02 \u001b[32mINFO     \u001b[0m train.py: [1/300], [40/484], step: 524, 1.348 samples/sec, batch_loss: 0.0807, batch_loss_c: 0.0840, batch_loss_s: 0.0729, time:29.6737, lr:1e-05\u001b[0m\n",
            "2019-12-08 09:12:17 \u001b[32mINFO     \u001b[0m train.py: [1/300], [50/484], step: 534, 2.674 samples/sec, batch_loss: 0.0620, batch_loss_c: 0.0552, batch_loss_s: 0.0780, time:14.9599, lr:1e-05\u001b[0m\n",
            "2019-12-08 09:12:35 \u001b[32mINFO     \u001b[0m train.py: [1/300], [60/484], step: 544, 2.197 samples/sec, batch_loss: 0.3696, batch_loss_c: 0.3807, batch_loss_s: 0.3439, time:18.2036, lr:1e-05\u001b[0m\n",
            "2019-12-08 09:12:56 \u001b[32mINFO     \u001b[0m train.py: [1/300], [70/484], step: 554, 1.881 samples/sec, batch_loss: 0.1314, batch_loss_c: 0.1293, batch_loss_s: 0.1363, time:21.2680, lr:1e-05\u001b[0m\n",
            "2019-12-08 09:13:12 \u001b[32mINFO     \u001b[0m train.py: [1/300], [80/484], step: 564, 2.497 samples/sec, batch_loss: 0.0875, batch_loss_c: 0.0865, batch_loss_s: 0.0899, time:16.0218, lr:1e-05\u001b[0m\n",
            "2019-12-08 09:13:27 \u001b[32mINFO     \u001b[0m train.py: [1/300], [90/484], step: 574, 2.615 samples/sec, batch_loss: 0.0506, batch_loss_c: 0.0430, batch_loss_s: 0.0685, time:15.2966, lr:1e-05\u001b[0m\n",
            "2019-12-08 09:13:45 \u001b[32mINFO     \u001b[0m train.py: [1/300], [100/484], step: 584, 2.322 samples/sec, batch_loss: 0.1114, batch_loss_c: 0.1089, batch_loss_s: 0.1171, time:17.2253, lr:1e-05\u001b[0m\n",
            "2019-12-08 09:14:01 \u001b[32mINFO     \u001b[0m train.py: [1/300], [110/484], step: 594, 2.465 samples/sec, batch_loss: 0.0500, batch_loss_c: 0.0421, batch_loss_s: 0.0686, time:16.2271, lr:1e-05\u001b[0m\n",
            "2019-12-08 09:14:18 \u001b[32mINFO     \u001b[0m train.py: [1/300], [120/484], step: 604, 2.267 samples/sec, batch_loss: 0.0381, batch_loss_c: 0.0338, batch_loss_s: 0.0482, time:17.6478, lr:1e-05\u001b[0m\n",
            "2019-12-08 09:14:37 \u001b[32mINFO     \u001b[0m train.py: [1/300], [130/484], step: 614, 2.132 samples/sec, batch_loss: 0.0720, batch_loss_c: 0.0618, batch_loss_s: 0.0961, time:18.7581, lr:1e-05\u001b[0m\n",
            "2019-12-08 09:14:53 \u001b[32mINFO     \u001b[0m train.py: [1/300], [140/484], step: 624, 2.537 samples/sec, batch_loss: 0.0843, batch_loss_c: 0.0770, batch_loss_s: 0.1015, time:15.7676, lr:1e-05\u001b[0m\n",
            "2019-12-08 09:15:26 \u001b[32mINFO     \u001b[0m train.py: [1/300], [150/484], step: 634, 1.201 samples/sec, batch_loss: 0.0800, batch_loss_c: 0.0794, batch_loss_s: 0.0813, time:33.3040, lr:1e-05\u001b[0m\n",
            "2019-12-08 09:15:43 \u001b[32mINFO     \u001b[0m train.py: [1/300], [160/484], step: 644, 2.419 samples/sec, batch_loss: 0.0579, batch_loss_c: 0.0501, batch_loss_s: 0.0760, time:16.5357, lr:1e-05\u001b[0m\n",
            "2019-12-08 09:16:03 \u001b[32mINFO     \u001b[0m train.py: [1/300], [170/484], step: 654, 1.973 samples/sec, batch_loss: 0.0968, batch_loss_c: 0.0994, batch_loss_s: 0.0907, time:20.2761, lr:1e-05\u001b[0m\n",
            "2019-12-08 09:16:19 \u001b[32mINFO     \u001b[0m train.py: [1/300], [180/484], step: 664, 2.505 samples/sec, batch_loss: 0.1461, batch_loss_c: 0.1251, batch_loss_s: 0.1952, time:15.9699, lr:1e-05\u001b[0m\n",
            "2019-12-08 09:16:50 \u001b[32mINFO     \u001b[0m train.py: [1/300], [190/484], step: 674, 1.278 samples/sec, batch_loss: 0.3245, batch_loss_c: 0.3190, batch_loss_s: 0.3373, time:31.2887, lr:1e-05\u001b[0m\n",
            "2019-12-08 09:17:08 \u001b[32mINFO     \u001b[0m train.py: [1/300], [200/484], step: 684, 2.251 samples/sec, batch_loss: 0.0380, batch_loss_c: 0.0322, batch_loss_s: 0.0516, time:17.7738, lr:1e-05\u001b[0m\n",
            "2019-12-08 09:17:25 \u001b[32mINFO     \u001b[0m train.py: [1/300], [210/484], step: 694, 2.434 samples/sec, batch_loss: 0.1032, batch_loss_c: 0.0899, batch_loss_s: 0.1342, time:16.4325, lr:1e-05\u001b[0m\n",
            "2019-12-08 09:17:44 \u001b[32mINFO     \u001b[0m train.py: [1/300], [220/484], step: 704, 2.083 samples/sec, batch_loss: 0.0612, batch_loss_c: 0.0546, batch_loss_s: 0.0763, time:19.2015, lr:1e-05\u001b[0m\n",
            "2019-12-08 09:18:01 \u001b[32mINFO     \u001b[0m train.py: [1/300], [230/484], step: 714, 2.302 samples/sec, batch_loss: 0.1662, batch_loss_c: 0.1948, batch_loss_s: 0.0993, time:17.3726, lr:1e-05\u001b[0m\n",
            "2019-12-08 09:18:20 \u001b[32mINFO     \u001b[0m train.py: [1/300], [240/484], step: 724, 2.154 samples/sec, batch_loss: 0.1300, batch_loss_c: 0.1251, batch_loss_s: 0.1416, time:18.5717, lr:1e-05\u001b[0m\n",
            "2019-12-08 09:18:37 \u001b[32mINFO     \u001b[0m train.py: [1/300], [250/484], step: 734, 2.341 samples/sec, batch_loss: 0.2847, batch_loss_c: 0.2782, batch_loss_s: 0.2999, time:17.0866, lr:1e-05\u001b[0m\n",
            "2019-12-08 09:18:52 \u001b[32mINFO     \u001b[0m train.py: [1/300], [260/484], step: 744, 2.645 samples/sec, batch_loss: 0.0542, batch_loss_c: 0.0511, batch_loss_s: 0.0616, time:15.1235, lr:1e-05\u001b[0m\n",
            "2019-12-08 09:19:07 \u001b[32mINFO     \u001b[0m train.py: [1/300], [270/484], step: 754, 2.651 samples/sec, batch_loss: 0.0667, batch_loss_c: 0.0616, batch_loss_s: 0.0787, time:15.0900, lr:1e-05\u001b[0m\n",
            "2019-12-08 09:19:38 \u001b[32mINFO     \u001b[0m train.py: [1/300], [280/484], step: 764, 1.304 samples/sec, batch_loss: 0.1009, batch_loss_c: 0.1010, batch_loss_s: 0.1008, time:30.6801, lr:1e-05\u001b[0m\n",
            "2019-12-08 09:19:56 \u001b[32mINFO     \u001b[0m train.py: [1/300], [290/484], step: 774, 2.141 samples/sec, batch_loss: 0.0913, batch_loss_c: 0.0838, batch_loss_s: 0.1090, time:18.6802, lr:1e-05\u001b[0m\n",
            "2019-12-08 09:20:22 \u001b[32mINFO     \u001b[0m train.py: [1/300], [300/484], step: 784, 1.549 samples/sec, batch_loss: 0.0548, batch_loss_c: 0.0478, batch_loss_s: 0.0709, time:25.8229, lr:1e-05\u001b[0m\n",
            "2019-12-08 09:20:38 \u001b[32mINFO     \u001b[0m train.py: [1/300], [310/484], step: 794, 2.491 samples/sec, batch_loss: 0.0801, batch_loss_c: 0.0829, batch_loss_s: 0.0736, time:16.0579, lr:1e-05\u001b[0m\n",
            "2019-12-08 09:21:00 \u001b[32mINFO     \u001b[0m train.py: [1/300], [320/484], step: 804, 1.834 samples/sec, batch_loss: 0.2365, batch_loss_c: 0.2091, batch_loss_s: 0.3002, time:21.8090, lr:1e-05\u001b[0m\n",
            "2019-12-08 09:21:25 \u001b[32mINFO     \u001b[0m train.py: [1/300], [330/484], step: 814, 1.627 samples/sec, batch_loss: 0.3072, batch_loss_c: 0.2964, batch_loss_s: 0.3324, time:24.5813, lr:1e-05\u001b[0m\n",
            "2019-12-08 09:21:42 \u001b[32mINFO     \u001b[0m train.py: [1/300], [340/484], step: 824, 2.375 samples/sec, batch_loss: 0.1820, batch_loss_c: 0.1836, batch_loss_s: 0.1783, time:16.8388, lr:1e-05\u001b[0m\n",
            "2019-12-08 09:21:56 \u001b[32mINFO     \u001b[0m train.py: [1/300], [350/484], step: 834, 2.669 samples/sec, batch_loss: 0.0923, batch_loss_c: 0.0848, batch_loss_s: 0.1098, time:14.9849, lr:1e-05\u001b[0m\n",
            "2019-12-08 09:22:19 \u001b[32mINFO     \u001b[0m train.py: [1/300], [360/484], step: 844, 1.789 samples/sec, batch_loss: 0.2769, batch_loss_c: 0.2731, batch_loss_s: 0.2857, time:22.3636, lr:1e-05\u001b[0m\n",
            "2019-12-08 09:22:42 \u001b[32mINFO     \u001b[0m train.py: [1/300], [370/484], step: 854, 1.746 samples/sec, batch_loss: 0.1208, batch_loss_c: 0.1284, batch_loss_s: 0.1029, time:22.9075, lr:1e-05\u001b[0m\n",
            "2019-12-08 09:22:56 \u001b[32mINFO     \u001b[0m train.py: [1/300], [380/484], step: 864, 2.793 samples/sec, batch_loss: 0.0882, batch_loss_c: 0.0917, batch_loss_s: 0.0800, time:14.3199, lr:1e-05\u001b[0m\n",
            "2019-12-08 09:23:12 \u001b[32mINFO     \u001b[0m train.py: [1/300], [390/484], step: 874, 2.541 samples/sec, batch_loss: 0.0800, batch_loss_c: 0.0811, batch_loss_s: 0.0772, time:15.7394, lr:1e-05\u001b[0m\n",
            "2019-12-08 09:23:39 \u001b[32mINFO     \u001b[0m train.py: [1/300], [400/484], step: 884, 1.470 samples/sec, batch_loss: 0.0638, batch_loss_c: 0.0513, batch_loss_s: 0.0931, time:27.2037, lr:1e-05\u001b[0m\n",
            "2019-12-08 09:24:08 \u001b[32mINFO     \u001b[0m train.py: [1/300], [410/484], step: 894, 1.389 samples/sec, batch_loss: 0.1080, batch_loss_c: 0.1162, batch_loss_s: 0.0889, time:28.7961, lr:1e-05\u001b[0m\n",
            "2019-12-08 09:24:27 \u001b[32mINFO     \u001b[0m train.py: [1/300], [420/484], step: 904, 2.079 samples/sec, batch_loss: 0.3256, batch_loss_c: 0.3224, batch_loss_s: 0.3331, time:19.2385, lr:1e-05\u001b[0m\n",
            "2019-12-08 09:24:43 \u001b[32mINFO     \u001b[0m train.py: [1/300], [430/484], step: 914, 2.472 samples/sec, batch_loss: 0.0916, batch_loss_c: 0.0863, batch_loss_s: 0.1038, time:16.1793, lr:1e-05\u001b[0m\n",
            "2019-12-08 09:24:59 \u001b[32mINFO     \u001b[0m train.py: [1/300], [440/484], step: 924, 2.534 samples/sec, batch_loss: 0.0901, batch_loss_c: 0.0995, batch_loss_s: 0.0681, time:15.7855, lr:1e-05\u001b[0m\n",
            "2019-12-08 09:25:21 \u001b[32mINFO     \u001b[0m train.py: [1/300], [450/484], step: 934, 1.829 samples/sec, batch_loss: 0.0666, batch_loss_c: 0.0588, batch_loss_s: 0.0849, time:21.8672, lr:1e-05\u001b[0m\n",
            "2019-12-08 09:25:44 \u001b[32mINFO     \u001b[0m train.py: [1/300], [460/484], step: 944, 1.753 samples/sec, batch_loss: 0.4536, batch_loss_c: 0.4334, batch_loss_s: 0.5006, time:22.8147, lr:1e-05\u001b[0m\n",
            "2019-12-08 09:26:08 \u001b[32mINFO     \u001b[0m train.py: [1/300], [470/484], step: 954, 1.647 samples/sec, batch_loss: 0.1211, batch_loss_c: 0.1156, batch_loss_s: 0.1338, time:24.2863, lr:1e-05\u001b[0m\n",
            "2019-12-08 09:26:24 \u001b[32mINFO     \u001b[0m train.py: [1/300], [480/484], step: 964, 2.510 samples/sec, batch_loss: 0.3187, batch_loss_c: 0.3029, batch_loss_s: 0.3556, time:15.9367, lr:1e-05\u001b[0m\n",
            "2019-12-08 09:26:29 \u001b[32mINFO     \u001b[0m train.py: [1/300], train_loss: 0.1382, time: 970.0654, lr: 1e-05\u001b[0m\n",
            "2019-12-08 09:26:32 \u001b[32mINFO     \u001b[0m train.py: [2/300], [0/484], step: 968, 26.693 samples/sec, batch_loss: 0.0799, batch_loss_c: 0.0811, batch_loss_s: 0.0772, time:1.4985, lr:1e-05\u001b[0m\n",
            "2019-12-08 09:26:55 \u001b[32mINFO     \u001b[0m train.py: [2/300], [10/484], step: 978, 1.726 samples/sec, batch_loss: 0.1314, batch_loss_c: 0.0880, batch_loss_s: 0.2326, time:23.1772, lr:1e-05\u001b[0m\n",
            "2019-12-08 09:27:16 \u001b[32mINFO     \u001b[0m train.py: [2/300], [20/484], step: 988, 1.877 samples/sec, batch_loss: 0.0657, batch_loss_c: 0.0533, batch_loss_s: 0.0948, time:21.3146, lr:1e-05\u001b[0m\n",
            "2019-12-08 09:27:43 \u001b[32mINFO     \u001b[0m train.py: [2/300], [30/484], step: 998, 1.504 samples/sec, batch_loss: 0.2849, batch_loss_c: 0.2798, batch_loss_s: 0.2968, time:26.5938, lr:1e-05\u001b[0m\n",
            "2019-12-08 09:28:07 \u001b[32mINFO     \u001b[0m train.py: [2/300], [40/484], step: 1008, 1.641 samples/sec, batch_loss: 0.2488, batch_loss_c: 0.2606, batch_loss_s: 0.2212, time:24.3732, lr:1e-05\u001b[0m\n",
            "2019-12-08 09:28:24 \u001b[32mINFO     \u001b[0m train.py: [2/300], [50/484], step: 1018, 2.347 samples/sec, batch_loss: 0.5216, batch_loss_c: 0.5163, batch_loss_s: 0.5339, time:17.0410, lr:1e-05\u001b[0m\n",
            "2019-12-08 09:28:41 \u001b[32mINFO     \u001b[0m train.py: [2/300], [60/484], step: 1028, 2.432 samples/sec, batch_loss: 0.2917, batch_loss_c: 0.2861, batch_loss_s: 0.3047, time:16.4500, lr:1e-05\u001b[0m\n",
            "2019-12-08 09:28:58 \u001b[32mINFO     \u001b[0m train.py: [2/300], [70/484], step: 1038, 2.313 samples/sec, batch_loss: 0.5684, batch_loss_c: 0.5635, batch_loss_s: 0.5797, time:17.2935, lr:1e-05\u001b[0m\n",
            "2019-12-08 09:29:21 \u001b[32mINFO     \u001b[0m train.py: [2/300], [80/484], step: 1048, 1.734 samples/sec, batch_loss: 0.0542, batch_loss_c: 0.0522, batch_loss_s: 0.0589, time:23.0651, lr:1e-05\u001b[0m\n",
            "2019-12-08 09:29:51 \u001b[32mINFO     \u001b[0m train.py: [2/300], [90/484], step: 1058, 1.350 samples/sec, batch_loss: 0.2874, batch_loss_c: 0.2828, batch_loss_s: 0.2982, time:29.6197, lr:1e-05\u001b[0m\n",
            "2019-12-08 09:30:06 \u001b[32mINFO     \u001b[0m train.py: [2/300], [100/484], step: 1068, 2.624 samples/sec, batch_loss: 0.3184, batch_loss_c: 0.3226, batch_loss_s: 0.3088, time:15.2454, lr:1e-05\u001b[0m\n",
            "2019-12-08 09:30:32 \u001b[32mINFO     \u001b[0m train.py: [2/300], [110/484], step: 1078, 1.514 samples/sec, batch_loss: 0.2162, batch_loss_c: 0.2478, batch_loss_s: 0.1423, time:26.4253, lr:1e-05\u001b[0m\n",
            "2019-12-08 09:30:49 \u001b[32mINFO     \u001b[0m train.py: [2/300], [120/484], step: 1088, 2.394 samples/sec, batch_loss: 0.0527, batch_loss_c: 0.0409, batch_loss_s: 0.0803, time:16.7064, lr:1e-05\u001b[0m\n",
            "2019-12-08 09:31:08 \u001b[32mINFO     \u001b[0m train.py: [2/300], [130/484], step: 1098, 2.126 samples/sec, batch_loss: 0.0761, batch_loss_c: 0.0704, batch_loss_s: 0.0894, time:18.8156, lr:1e-05\u001b[0m\n",
            "2019-12-08 09:31:59 \u001b[32mINFO     \u001b[0m train.py: [2/300], [140/484], step: 1108, 0.782 samples/sec, batch_loss: 0.0776, batch_loss_c: 0.0819, batch_loss_s: 0.0675, time:51.1225, lr:1e-05\u001b[0m\n",
            "2019-12-08 09:32:14 \u001b[32mINFO     \u001b[0m train.py: [2/300], [150/484], step: 1118, 2.665 samples/sec, batch_loss: 0.0611, batch_loss_c: 0.0451, batch_loss_s: 0.0985, time:15.0103, lr:1e-05\u001b[0m\n",
            "2019-12-08 09:32:41 \u001b[32mINFO     \u001b[0m train.py: [2/300], [160/484], step: 1128, 1.473 samples/sec, batch_loss: 0.2589, batch_loss_c: 0.2319, batch_loss_s: 0.3219, time:27.1478, lr:1e-05\u001b[0m\n",
            "2019-12-08 09:33:04 \u001b[32mINFO     \u001b[0m train.py: [2/300], [170/484], step: 1138, 1.725 samples/sec, batch_loss: 0.0513, batch_loss_c: 0.0510, batch_loss_s: 0.0519, time:23.1905, lr:1e-05\u001b[0m\n",
            "2019-12-08 09:33:24 \u001b[32mINFO     \u001b[0m train.py: [2/300], [180/484], step: 1148, 2.053 samples/sec, batch_loss: 0.0578, batch_loss_c: 0.0511, batch_loss_s: 0.0733, time:19.4840, lr:1e-05\u001b[0m\n",
            "2019-12-08 09:33:41 \u001b[32mINFO     \u001b[0m train.py: [2/300], [190/484], step: 1158, 2.351 samples/sec, batch_loss: 0.0616, batch_loss_c: 0.0494, batch_loss_s: 0.0903, time:17.0127, lr:1e-05\u001b[0m\n",
            "2019-12-08 09:33:59 \u001b[32mINFO     \u001b[0m train.py: [2/300], [200/484], step: 1168, 2.226 samples/sec, batch_loss: 0.5226, batch_loss_c: 0.5194, batch_loss_s: 0.5301, time:17.9701, lr:1e-05\u001b[0m\n",
            "2019-12-08 09:34:16 \u001b[32mINFO     \u001b[0m train.py: [2/300], [210/484], step: 1178, 2.338 samples/sec, batch_loss: 0.0636, batch_loss_c: 0.0616, batch_loss_s: 0.0681, time:17.1114, lr:1e-05\u001b[0m\n",
            "2019-12-08 09:34:59 \u001b[32mINFO     \u001b[0m train.py: [2/300], [220/484], step: 1188, 0.933 samples/sec, batch_loss: 0.0930, batch_loss_c: 0.0936, batch_loss_s: 0.0914, time:42.8696, lr:1e-05\u001b[0m\n",
            "2019-12-08 09:35:27 \u001b[32mINFO     \u001b[0m train.py: [2/300], [230/484], step: 1198, 1.398 samples/sec, batch_loss: 0.2662, batch_loss_c: 0.2551, batch_loss_s: 0.2920, time:28.6072, lr:1e-05\u001b[0m\n",
            "2019-12-08 09:35:55 \u001b[32mINFO     \u001b[0m train.py: [2/300], [240/484], step: 1208, 1.440 samples/sec, batch_loss: 0.0592, batch_loss_c: 0.0489, batch_loss_s: 0.0835, time:27.7802, lr:1e-05\u001b[0m\n",
            "2019-12-08 09:36:10 \u001b[32mINFO     \u001b[0m train.py: [2/300], [250/484], step: 1218, 2.720 samples/sec, batch_loss: 0.1189, batch_loss_c: 0.1061, batch_loss_s: 0.1488, time:14.7050, lr:1e-05\u001b[0m\n",
            "2019-12-08 09:36:24 \u001b[32mINFO     \u001b[0m train.py: [2/300], [260/484], step: 1228, 2.884 samples/sec, batch_loss: 0.0803, batch_loss_c: 0.0739, batch_loss_s: 0.0952, time:13.8708, lr:1e-05\u001b[0m\n",
            "2019-12-08 09:36:42 \u001b[32mINFO     \u001b[0m train.py: [2/300], [270/484], step: 1238, 2.232 samples/sec, batch_loss: 0.0876, batch_loss_c: 0.0813, batch_loss_s: 0.1023, time:17.9190, lr:1e-05\u001b[0m\n",
            "2019-12-08 09:37:09 \u001b[32mINFO     \u001b[0m train.py: [2/300], [280/484], step: 1248, 1.447 samples/sec, batch_loss: 0.0588, batch_loss_c: 0.0465, batch_loss_s: 0.0876, time:27.6434, lr:1e-05\u001b[0m\n",
            "2019-12-08 09:37:35 \u001b[32mINFO     \u001b[0m train.py: [2/300], [290/484], step: 1258, 1.574 samples/sec, batch_loss: 0.0869, batch_loss_c: 0.0733, batch_loss_s: 0.1187, time:25.4176, lr:1e-05\u001b[0m\n",
            "2019-12-08 09:38:02 \u001b[32mINFO     \u001b[0m train.py: [2/300], [300/484], step: 1268, 1.484 samples/sec, batch_loss: 0.2290, batch_loss_c: 0.2786, batch_loss_s: 0.1134, time:26.9454, lr:1e-05\u001b[0m\n",
            "2019-12-08 09:38:24 \u001b[32mINFO     \u001b[0m train.py: [2/300], [310/484], step: 1278, 1.766 samples/sec, batch_loss: 0.0447, batch_loss_c: 0.0408, batch_loss_s: 0.0537, time:22.6501, lr:1e-05\u001b[0m\n",
            "2019-12-08 09:38:41 \u001b[32mINFO     \u001b[0m train.py: [2/300], [320/484], step: 1288, 2.361 samples/sec, batch_loss: 0.3049, batch_loss_c: 0.3033, batch_loss_s: 0.3087, time:16.9398, lr:1e-05\u001b[0m\n",
            "2019-12-08 09:38:59 \u001b[32mINFO     \u001b[0m train.py: [2/300], [330/484], step: 1298, 2.254 samples/sec, batch_loss: 0.0517, batch_loss_c: 0.0451, batch_loss_s: 0.0670, time:17.7501, lr:1e-05\u001b[0m\n",
            "2019-12-08 09:39:18 \u001b[32mINFO     \u001b[0m train.py: [2/300], [340/484], step: 1308, 2.054 samples/sec, batch_loss: 0.0633, batch_loss_c: 0.0482, batch_loss_s: 0.0984, time:19.4762, lr:1e-05\u001b[0m\n",
            "2019-12-08 09:39:39 \u001b[32mINFO     \u001b[0m train.py: [2/300], [350/484], step: 1318, 1.939 samples/sec, batch_loss: 0.0793, batch_loss_c: 0.0754, batch_loss_s: 0.0883, time:20.6271, lr:1e-05\u001b[0m\n",
            "2019-12-08 09:39:58 \u001b[32mINFO     \u001b[0m train.py: [2/300], [360/484], step: 1328, 2.159 samples/sec, batch_loss: 0.0566, batch_loss_c: 0.0499, batch_loss_s: 0.0723, time:18.5258, lr:1e-05\u001b[0m\n",
            "2019-12-08 09:40:22 \u001b[32mINFO     \u001b[0m train.py: [2/300], [370/484], step: 1338, 1.632 samples/sec, batch_loss: 0.0595, batch_loss_c: 0.0486, batch_loss_s: 0.0848, time:24.5036, lr:1e-05\u001b[0m\n",
            "2019-12-08 09:40:38 \u001b[32mINFO     \u001b[0m train.py: [2/300], [380/484], step: 1348, 2.559 samples/sec, batch_loss: 0.0814, batch_loss_c: 0.0766, batch_loss_s: 0.0926, time:15.6281, lr:1e-05\u001b[0m\n",
            "2019-12-08 09:40:53 \u001b[32mINFO     \u001b[0m train.py: [2/300], [390/484], step: 1358, 2.587 samples/sec, batch_loss: 0.0640, batch_loss_c: 0.0557, batch_loss_s: 0.0832, time:15.4613, lr:1e-05\u001b[0m\n",
            "2019-12-08 09:41:11 \u001b[32mINFO     \u001b[0m train.py: [2/300], [400/484], step: 1368, 2.223 samples/sec, batch_loss: 0.0499, batch_loss_c: 0.0419, batch_loss_s: 0.0687, time:17.9897, lr:1e-05\u001b[0m\n",
            "2019-12-08 09:41:37 \u001b[32mINFO     \u001b[0m train.py: [2/300], [410/484], step: 1378, 1.558 samples/sec, batch_loss: 0.0724, batch_loss_c: 0.0622, batch_loss_s: 0.0961, time:25.6716, lr:1e-05\u001b[0m\n",
            "2019-12-08 09:41:52 \u001b[32mINFO     \u001b[0m train.py: [2/300], [420/484], step: 1388, 2.660 samples/sec, batch_loss: 0.0673, batch_loss_c: 0.0624, batch_loss_s: 0.0788, time:15.0382, lr:1e-05\u001b[0m\n",
            "2019-12-08 09:42:16 \u001b[32mINFO     \u001b[0m train.py: [2/300], [430/484], step: 1398, 1.641 samples/sec, batch_loss: 0.2898, batch_loss_c: 0.2826, batch_loss_s: 0.3067, time:24.3701, lr:1e-05\u001b[0m\n",
            "2019-12-08 09:42:32 \u001b[32mINFO     \u001b[0m train.py: [2/300], [440/484], step: 1408, 2.600 samples/sec, batch_loss: 0.0689, batch_loss_c: 0.0648, batch_loss_s: 0.0783, time:15.3857, lr:1e-05\u001b[0m\n",
            "2019-12-08 09:42:54 \u001b[32mINFO     \u001b[0m train.py: [2/300], [450/484], step: 1418, 1.787 samples/sec, batch_loss: 0.2232, batch_loss_c: 0.1891, batch_loss_s: 0.3028, time:22.3858, lr:1e-05\u001b[0m\n",
            "tcmalloc: large alloc 4326277120 bytes == 0x7f1b83d62000 @  0x7f203bedd1e7 0x7f20304c1f71 0x7f203052555d 0x7f2030528e28 0x7f20305293e5 0x7f20305bffc2 0x50ac25 0x50d390 0x508245 0x50a080 0x50aa7d 0x50d390 0x509455 0x595311 0x5a067e 0x557d28 0x50c88b 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x509455 0x595311 0x5a522c 0x557f7e 0x4b5eff 0x50c810\n",
            "2019-12-08 09:43:27 \u001b[32mINFO     \u001b[0m train.py: [2/300], [460/484], step: 1428, 1.200 samples/sec, batch_loss: 0.0928, batch_loss_c: 0.0816, batch_loss_s: 0.1191, time:33.3420, lr:1e-05\u001b[0m\n",
            "2019-12-08 09:43:45 \u001b[32mINFO     \u001b[0m train.py: [2/300], [470/484], step: 1438, 2.319 samples/sec, batch_loss: 0.1004, batch_loss_c: 0.1047, batch_loss_s: 0.0903, time:17.2464, lr:1e-05\u001b[0m\n",
            "2019-12-08 09:44:21 \u001b[32mINFO     \u001b[0m train.py: [2/300], [480/484], step: 1448, 1.087 samples/sec, batch_loss: 0.0819, batch_loss_c: 0.0828, batch_loss_s: 0.0797, time:36.8061, lr:1e-05\u001b[0m\n",
            "2019-12-08 09:44:26 \u001b[32mINFO     \u001b[0m train.py: [2/300], train_loss: 0.1466, time: 1076.1354, lr: 1e-05\u001b[0m\n",
            "2019-12-08 09:44:28 \u001b[32mINFO     \u001b[0m train.py: [3/300], [0/484], step: 1452, 23.872 samples/sec, batch_loss: 0.0629, batch_loss_c: 0.0475, batch_loss_s: 0.0990, time:1.6756, lr:1e-05\u001b[0m\n",
            "2019-12-08 09:44:44 \u001b[32mINFO     \u001b[0m train.py: [3/300], [10/484], step: 1462, 2.684 samples/sec, batch_loss: 0.0607, batch_loss_c: 0.0459, batch_loss_s: 0.0951, time:14.9013, lr:1e-05\u001b[0m\n",
            "2019-12-08 09:45:06 \u001b[32mINFO     \u001b[0m train.py: [3/300], [20/484], step: 1472, 1.794 samples/sec, batch_loss: 0.3358, batch_loss_c: 0.3303, batch_loss_s: 0.3487, time:22.2923, lr:1e-05\u001b[0m\n",
            "2019-12-08 09:45:27 \u001b[32mINFO     \u001b[0m train.py: [3/300], [30/484], step: 1482, 1.873 samples/sec, batch_loss: 0.3160, batch_loss_c: 0.3100, batch_loss_s: 0.3300, time:21.3532, lr:1e-05\u001b[0m\n",
            "2019-12-08 09:45:48 \u001b[32mINFO     \u001b[0m train.py: [3/300], [40/484], step: 1492, 1.952 samples/sec, batch_loss: 0.0856, batch_loss_c: 0.0821, batch_loss_s: 0.0939, time:20.4934, lr:1e-05\u001b[0m\n",
            "2019-12-08 09:46:03 \u001b[32mINFO     \u001b[0m train.py: [3/300], [50/484], step: 1502, 2.559 samples/sec, batch_loss: 0.1565, batch_loss_c: 0.1450, batch_loss_s: 0.1834, time:15.6292, lr:1e-05\u001b[0m\n",
            "2019-12-08 09:46:27 \u001b[32mINFO     \u001b[0m train.py: [3/300], [60/484], step: 1512, 1.664 samples/sec, batch_loss: 0.0988, batch_loss_c: 0.0913, batch_loss_s: 0.1162, time:24.0318, lr:1e-05\u001b[0m\n",
            "2019-12-08 09:46:42 \u001b[32mINFO     \u001b[0m train.py: [3/300], [70/484], step: 1522, 2.751 samples/sec, batch_loss: 0.1046, batch_loss_c: 0.0982, batch_loss_s: 0.1196, time:14.5409, lr:1e-05\u001b[0m\n",
            "2019-12-08 09:47:12 \u001b[32mINFO     \u001b[0m train.py: [3/300], [80/484], step: 1532, 1.335 samples/sec, batch_loss: 0.2985, batch_loss_c: 0.2899, batch_loss_s: 0.3185, time:29.9691, lr:1e-05\u001b[0m\n",
            "2019-12-08 09:47:30 \u001b[32mINFO     \u001b[0m train.py: [3/300], [90/484], step: 1542, 2.167 samples/sec, batch_loss: 0.1549, batch_loss_c: 0.1126, batch_loss_s: 0.2534, time:18.4595, lr:1e-05\u001b[0m\n",
            "2019-12-08 09:47:50 \u001b[32mINFO     \u001b[0m train.py: [3/300], [100/484], step: 1552, 2.076 samples/sec, batch_loss: 0.0440, batch_loss_c: 0.0358, batch_loss_s: 0.0630, time:19.2699, lr:1e-05\u001b[0m\n",
            "2019-12-08 09:48:04 \u001b[32mINFO     \u001b[0m train.py: [3/300], [110/484], step: 1562, 2.852 samples/sec, batch_loss: 0.1011, batch_loss_c: 0.0995, batch_loss_s: 0.1047, time:14.0272, lr:1e-05\u001b[0m\n",
            "2019-12-08 09:48:26 \u001b[32mINFO     \u001b[0m train.py: [3/300], [120/484], step: 1572, 1.822 samples/sec, batch_loss: 0.0647, batch_loss_c: 0.0653, batch_loss_s: 0.0632, time:21.9559, lr:1e-05\u001b[0m\n",
            "2019-12-08 09:48:42 \u001b[32mINFO     \u001b[0m train.py: [3/300], [130/484], step: 1582, 2.459 samples/sec, batch_loss: 0.0835, batch_loss_c: 0.0825, batch_loss_s: 0.0858, time:16.2646, lr:1e-05\u001b[0m\n",
            "2019-12-08 09:48:56 \u001b[32mINFO     \u001b[0m train.py: [3/300], [140/484], step: 1592, 2.786 samples/sec, batch_loss: 0.0920, batch_loss_c: 0.0627, batch_loss_s: 0.1605, time:14.3578, lr:1e-05\u001b[0m\n",
            "2019-12-08 09:49:21 \u001b[32mINFO     \u001b[0m train.py: [3/300], [150/484], step: 1602, 1.603 samples/sec, batch_loss: 0.1067, batch_loss_c: 0.1204, batch_loss_s: 0.0749, time:24.9532, lr:1e-05\u001b[0m\n",
            "2019-12-08 09:49:38 \u001b[32mINFO     \u001b[0m train.py: [3/300], [160/484], step: 1612, 2.322 samples/sec, batch_loss: 0.0682, batch_loss_c: 0.0562, batch_loss_s: 0.0960, time:17.2240, lr:1e-05\u001b[0m\n",
            "2019-12-08 09:49:54 \u001b[32mINFO     \u001b[0m train.py: [3/300], [170/484], step: 1622, 2.585 samples/sec, batch_loss: 0.1326, batch_loss_c: 0.1224, batch_loss_s: 0.1564, time:15.4727, lr:1e-05\u001b[0m\n",
            "2019-12-08 09:50:11 \u001b[32mINFO     \u001b[0m train.py: [3/300], [180/484], step: 1632, 2.360 samples/sec, batch_loss: 0.1144, batch_loss_c: 0.1184, batch_loss_s: 0.1050, time:16.9497, lr:1e-05\u001b[0m\n",
            "2019-12-08 09:50:28 \u001b[32mINFO     \u001b[0m train.py: [3/300], [190/484], step: 1642, 2.371 samples/sec, batch_loss: 0.0911, batch_loss_c: 0.0876, batch_loss_s: 0.0992, time:16.8724, lr:1e-05\u001b[0m\n",
            "2019-12-08 09:50:45 \u001b[32mINFO     \u001b[0m train.py: [3/300], [200/484], step: 1652, 2.289 samples/sec, batch_loss: 0.1627, batch_loss_c: 0.1489, batch_loss_s: 0.1948, time:17.4760, lr:1e-05\u001b[0m\n",
            "2019-12-08 09:51:02 \u001b[32mINFO     \u001b[0m train.py: [3/300], [210/484], step: 1662, 2.346 samples/sec, batch_loss: 0.0876, batch_loss_c: 0.0908, batch_loss_s: 0.0801, time:17.0477, lr:1e-05\u001b[0m\n",
            "2019-12-08 09:51:29 \u001b[32mINFO     \u001b[0m train.py: [3/300], [220/484], step: 1672, 1.504 samples/sec, batch_loss: 0.0717, batch_loss_c: 0.0612, batch_loss_s: 0.0961, time:26.5920, lr:1e-05\u001b[0m\n",
            "2019-12-08 09:51:50 \u001b[32mINFO     \u001b[0m train.py: [3/300], [230/484], step: 1682, 1.921 samples/sec, batch_loss: 0.3139, batch_loss_c: 0.3112, batch_loss_s: 0.3202, time:20.8213, lr:1e-05\u001b[0m\n",
            "2019-12-08 09:52:32 \u001b[32mINFO     \u001b[0m train.py: [3/300], [240/484], step: 1692, 0.942 samples/sec, batch_loss: 0.1029, batch_loss_c: 0.1149, batch_loss_s: 0.0749, time:42.4622, lr:1e-05\u001b[0m\n",
            "2019-12-08 09:53:05 \u001b[32mINFO     \u001b[0m train.py: [3/300], [250/484], step: 1702, 1.214 samples/sec, batch_loss: 0.1118, batch_loss_c: 0.1101, batch_loss_s: 0.1159, time:32.9617, lr:1e-05\u001b[0m\n",
            "2019-12-08 09:53:23 \u001b[32mINFO     \u001b[0m train.py: [3/300], [260/484], step: 1712, 2.178 samples/sec, batch_loss: 0.0876, batch_loss_c: 0.0861, batch_loss_s: 0.0911, time:18.3619, lr:1e-05\u001b[0m\n",
            "2019-12-08 09:54:16 \u001b[32mINFO     \u001b[0m train.py: [3/300], [270/484], step: 1722, 0.755 samples/sec, batch_loss: 0.2997, batch_loss_c: 0.2987, batch_loss_s: 0.3020, time:52.9648, lr:1e-05\u001b[0m\n",
            "2019-12-08 09:54:35 \u001b[32mINFO     \u001b[0m train.py: [3/300], [280/484], step: 1732, 2.132 samples/sec, batch_loss: 0.0736, batch_loss_c: 0.0685, batch_loss_s: 0.0855, time:18.7579, lr:1e-05\u001b[0m\n",
            "2019-12-08 09:54:55 \u001b[32mINFO     \u001b[0m train.py: [3/300], [290/484], step: 1742, 2.021 samples/sec, batch_loss: 0.0697, batch_loss_c: 0.0644, batch_loss_s: 0.0823, time:19.7925, lr:1e-05\u001b[0m\n",
            "2019-12-08 09:55:20 \u001b[32mINFO     \u001b[0m train.py: [3/300], [300/484], step: 1752, 1.587 samples/sec, batch_loss: 0.0505, batch_loss_c: 0.0456, batch_loss_s: 0.0618, time:25.2016, lr:1e-05\u001b[0m\n",
            "2019-12-08 09:55:39 \u001b[32mINFO     \u001b[0m train.py: [3/300], [310/484], step: 1762, 2.085 samples/sec, batch_loss: 0.0569, batch_loss_c: 0.0512, batch_loss_s: 0.0701, time:19.1850, lr:1e-05\u001b[0m\n",
            "2019-12-08 09:55:57 \u001b[32mINFO     \u001b[0m train.py: [3/300], [320/484], step: 1772, 2.233 samples/sec, batch_loss: 0.2842, batch_loss_c: 0.2810, batch_loss_s: 0.2917, time:17.9137, lr:1e-05\u001b[0m\n",
            "2019-12-08 09:56:12 \u001b[32mINFO     \u001b[0m train.py: [3/300], [330/484], step: 1782, 2.704 samples/sec, batch_loss: 0.0549, batch_loss_c: 0.0449, batch_loss_s: 0.0782, time:14.7932, lr:1e-05\u001b[0m\n",
            "2019-12-08 09:56:29 \u001b[32mINFO     \u001b[0m train.py: [3/300], [340/484], step: 1792, 2.360 samples/sec, batch_loss: 0.0550, batch_loss_c: 0.0463, batch_loss_s: 0.0752, time:16.9465, lr:1e-05\u001b[0m\n",
            "2019-12-08 09:56:51 \u001b[32mINFO     \u001b[0m train.py: [3/300], [350/484], step: 1802, 1.785 samples/sec, batch_loss: 0.0811, batch_loss_c: 0.0695, batch_loss_s: 0.1080, time:22.4146, lr:1e-05\u001b[0m\n",
            "2019-12-08 09:57:06 \u001b[32mINFO     \u001b[0m train.py: [3/300], [360/484], step: 1812, 2.669 samples/sec, batch_loss: 0.3106, batch_loss_c: 0.3082, batch_loss_s: 0.3161, time:14.9850, lr:1e-05\u001b[0m\n",
            "2019-12-08 09:57:28 \u001b[32mINFO     \u001b[0m train.py: [3/300], [370/484], step: 1822, 1.823 samples/sec, batch_loss: 0.3139, batch_loss_c: 0.3128, batch_loss_s: 0.3165, time:21.9417, lr:1e-05\u001b[0m\n",
            "2019-12-08 09:57:49 \u001b[32mINFO     \u001b[0m train.py: [3/300], [380/484], step: 1832, 1.950 samples/sec, batch_loss: 0.1135, batch_loss_c: 0.0857, batch_loss_s: 0.1783, time:20.5098, lr:1e-05\u001b[0m\n",
            "2019-12-08 09:58:17 \u001b[32mINFO     \u001b[0m train.py: [3/300], [390/484], step: 1842, 1.413 samples/sec, batch_loss: 0.0418, batch_loss_c: 0.0387, batch_loss_s: 0.0490, time:28.3039, lr:1e-05\u001b[0m\n",
            "2019-12-08 09:58:42 \u001b[32mINFO     \u001b[0m train.py: [3/300], [400/484], step: 1852, 1.620 samples/sec, batch_loss: 0.5029, batch_loss_c: 0.4946, batch_loss_s: 0.5224, time:24.6896, lr:1e-05\u001b[0m\n",
            "2019-12-08 09:59:06 \u001b[32mINFO     \u001b[0m train.py: [3/300], [410/484], step: 1862, 1.623 samples/sec, batch_loss: 0.0794, batch_loss_c: 0.0712, batch_loss_s: 0.0986, time:24.6483, lr:1e-05\u001b[0m\n",
            "2019-12-08 09:59:25 \u001b[32mINFO     \u001b[0m train.py: [3/300], [420/484], step: 1872, 2.117 samples/sec, batch_loss: 0.0686, batch_loss_c: 0.0615, batch_loss_s: 0.0853, time:18.8939, lr:1e-05\u001b[0m\n",
            "2019-12-08 09:59:43 \u001b[32mINFO     \u001b[0m train.py: [3/300], [430/484], step: 1882, 2.307 samples/sec, batch_loss: 0.0833, batch_loss_c: 0.0763, batch_loss_s: 0.0997, time:17.3370, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:00:03 \u001b[32mINFO     \u001b[0m train.py: [3/300], [440/484], step: 1892, 1.989 samples/sec, batch_loss: 0.2919, batch_loss_c: 0.2856, batch_loss_s: 0.3065, time:20.1074, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:00:22 \u001b[32mINFO     \u001b[0m train.py: [3/300], [450/484], step: 1902, 2.040 samples/sec, batch_loss: 0.0578, batch_loss_c: 0.0445, batch_loss_s: 0.0887, time:19.6060, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:00:38 \u001b[32mINFO     \u001b[0m train.py: [3/300], [460/484], step: 1912, 2.494 samples/sec, batch_loss: 0.2996, batch_loss_c: 0.2961, batch_loss_s: 0.3076, time:16.0372, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:00:53 \u001b[32mINFO     \u001b[0m train.py: [3/300], [470/484], step: 1922, 2.693 samples/sec, batch_loss: 0.0667, batch_loss_c: 0.0531, batch_loss_s: 0.0985, time:14.8520, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:01:17 \u001b[32mINFO     \u001b[0m train.py: [3/300], [480/484], step: 1932, 1.711 samples/sec, batch_loss: 0.0695, batch_loss_c: 0.0659, batch_loss_s: 0.0779, time:23.3745, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:01:21 \u001b[32mINFO     \u001b[0m train.py: [3/300], train_loss: 0.1490, time: 1014.7040, lr: 1e-05\u001b[0m\n",
            "2019-12-08 10:01:25 \u001b[32mINFO     \u001b[0m train.py: [4/300], [0/484], step: 1936, 13.396 samples/sec, batch_loss: 0.2882, batch_loss_c: 0.2724, batch_loss_s: 0.3248, time:2.9860, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:01:43 \u001b[32mINFO     \u001b[0m train.py: [4/300], [10/484], step: 1946, 2.154 samples/sec, batch_loss: 0.1414, batch_loss_c: 0.1450, batch_loss_s: 0.1329, time:18.5685, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:02:12 \u001b[32mINFO     \u001b[0m train.py: [4/300], [20/484], step: 1956, 1.380 samples/sec, batch_loss: 0.3187, batch_loss_c: 0.3015, batch_loss_s: 0.3587, time:28.9867, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:02:44 \u001b[32mINFO     \u001b[0m train.py: [4/300], [30/484], step: 1966, 1.263 samples/sec, batch_loss: 0.3326, batch_loss_c: 0.3300, batch_loss_s: 0.3388, time:31.6666, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:02:59 \u001b[32mINFO     \u001b[0m train.py: [4/300], [40/484], step: 1976, 2.760 samples/sec, batch_loss: 0.3025, batch_loss_c: 0.2998, batch_loss_s: 0.3088, time:14.4927, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:03:16 \u001b[32mINFO     \u001b[0m train.py: [4/300], [50/484], step: 1986, 2.266 samples/sec, batch_loss: 0.1647, batch_loss_c: 0.1812, batch_loss_s: 0.1261, time:17.6550, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:03:42 \u001b[32mINFO     \u001b[0m train.py: [4/300], [60/484], step: 1996, 1.558 samples/sec, batch_loss: 0.0777, batch_loss_c: 0.0726, batch_loss_s: 0.0896, time:25.6676, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:04:05 \u001b[32mINFO     \u001b[0m train.py: [4/300], [70/484], step: 2006, 1.726 samples/sec, batch_loss: 0.0892, batch_loss_c: 0.0746, batch_loss_s: 0.1234, time:23.1814, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:04:21 \u001b[32mINFO     \u001b[0m train.py: [4/300], [80/484], step: 2016, 2.580 samples/sec, batch_loss: 0.0797, batch_loss_c: 0.0739, batch_loss_s: 0.0931, time:15.5023, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:04:44 \u001b[32mINFO     \u001b[0m train.py: [4/300], [90/484], step: 2026, 1.738 samples/sec, batch_loss: 0.1176, batch_loss_c: 0.1186, batch_loss_s: 0.1153, time:23.0206, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:04:59 \u001b[32mINFO     \u001b[0m train.py: [4/300], [100/484], step: 2036, 2.605 samples/sec, batch_loss: 0.1029, batch_loss_c: 0.0967, batch_loss_s: 0.1173, time:15.3567, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:05:17 \u001b[32mINFO     \u001b[0m train.py: [4/300], [110/484], step: 2046, 2.242 samples/sec, batch_loss: 0.1019, batch_loss_c: 0.0916, batch_loss_s: 0.1260, time:17.8411, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:05:36 \u001b[32mINFO     \u001b[0m train.py: [4/300], [120/484], step: 2056, 2.050 samples/sec, batch_loss: 0.0943, batch_loss_c: 0.0915, batch_loss_s: 0.1009, time:19.5099, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:05:51 \u001b[32mINFO     \u001b[0m train.py: [4/300], [130/484], step: 2066, 2.697 samples/sec, batch_loss: 0.0399, batch_loss_c: 0.0307, batch_loss_s: 0.0613, time:14.8286, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:06:06 \u001b[32mINFO     \u001b[0m train.py: [4/300], [140/484], step: 2076, 2.610 samples/sec, batch_loss: 0.1880, batch_loss_c: 0.1872, batch_loss_s: 0.1896, time:15.3240, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:06:29 \u001b[32mINFO     \u001b[0m train.py: [4/300], [150/484], step: 2086, 1.757 samples/sec, batch_loss: 0.2840, batch_loss_c: 0.2808, batch_loss_s: 0.2916, time:22.7684, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:06:47 \u001b[32mINFO     \u001b[0m train.py: [4/300], [160/484], step: 2096, 2.264 samples/sec, batch_loss: 0.0602, batch_loss_c: 0.0490, batch_loss_s: 0.0864, time:17.6669, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:07:04 \u001b[32mINFO     \u001b[0m train.py: [4/300], [170/484], step: 2106, 2.405 samples/sec, batch_loss: 0.3555, batch_loss_c: 0.3559, batch_loss_s: 0.3545, time:16.6312, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:07:33 \u001b[32mINFO     \u001b[0m train.py: [4/300], [180/484], step: 2116, 1.365 samples/sec, batch_loss: 0.0782, batch_loss_c: 0.0621, batch_loss_s: 0.1156, time:29.3099, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:07:56 \u001b[32mINFO     \u001b[0m train.py: [4/300], [190/484], step: 2126, 1.763 samples/sec, batch_loss: 0.3084, batch_loss_c: 0.2971, batch_loss_s: 0.3349, time:22.6915, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:08:12 \u001b[32mINFO     \u001b[0m train.py: [4/300], [200/484], step: 2136, 2.492 samples/sec, batch_loss: 0.0956, batch_loss_c: 0.0775, batch_loss_s: 0.1378, time:16.0539, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:08:26 \u001b[32mINFO     \u001b[0m train.py: [4/300], [210/484], step: 2146, 2.868 samples/sec, batch_loss: 0.1341, batch_loss_c: 0.1213, batch_loss_s: 0.1641, time:13.9465, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:08:50 \u001b[32mINFO     \u001b[0m train.py: [4/300], [220/484], step: 2156, 1.657 samples/sec, batch_loss: 0.1328, batch_loss_c: 0.1082, batch_loss_s: 0.1901, time:24.1461, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:09:12 \u001b[32mINFO     \u001b[0m train.py: [4/300], [230/484], step: 2166, 1.773 samples/sec, batch_loss: 0.4715, batch_loss_c: 0.4518, batch_loss_s: 0.5175, time:22.5595, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:09:47 \u001b[32mINFO     \u001b[0m train.py: [4/300], [240/484], step: 2176, 1.163 samples/sec, batch_loss: 0.0481, batch_loss_c: 0.0409, batch_loss_s: 0.0647, time:34.3940, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:10:17 \u001b[32mINFO     \u001b[0m train.py: [4/300], [250/484], step: 2186, 1.317 samples/sec, batch_loss: 0.0817, batch_loss_c: 0.0630, batch_loss_s: 0.1252, time:30.3774, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:10:35 \u001b[32mINFO     \u001b[0m train.py: [4/300], [260/484], step: 2196, 2.195 samples/sec, batch_loss: 0.1177, batch_loss_c: 0.1181, batch_loss_s: 0.1168, time:18.2224, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:11:13 \u001b[32mINFO     \u001b[0m train.py: [4/300], [270/484], step: 2206, 1.068 samples/sec, batch_loss: 0.3173, batch_loss_c: 0.3158, batch_loss_s: 0.3207, time:37.4704, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:11:38 \u001b[32mINFO     \u001b[0m train.py: [4/300], [280/484], step: 2216, 1.590 samples/sec, batch_loss: 0.0634, batch_loss_c: 0.0570, batch_loss_s: 0.0784, time:25.1562, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:11:54 \u001b[32mINFO     \u001b[0m train.py: [4/300], [290/484], step: 2226, 2.556 samples/sec, batch_loss: 0.0500, batch_loss_c: 0.0450, batch_loss_s: 0.0618, time:15.6475, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:12:12 \u001b[32mINFO     \u001b[0m train.py: [4/300], [300/484], step: 2236, 2.184 samples/sec, batch_loss: 0.2830, batch_loss_c: 0.2797, batch_loss_s: 0.2908, time:18.3181, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:12:30 \u001b[32mINFO     \u001b[0m train.py: [4/300], [310/484], step: 2246, 2.255 samples/sec, batch_loss: 0.0804, batch_loss_c: 0.0715, batch_loss_s: 0.1011, time:17.7422, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:12:45 \u001b[32mINFO     \u001b[0m train.py: [4/300], [320/484], step: 2256, 2.585 samples/sec, batch_loss: 0.1262, batch_loss_c: 0.1195, batch_loss_s: 0.1419, time:15.4717, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:13:12 \u001b[32mINFO     \u001b[0m train.py: [4/300], [330/484], step: 2266, 1.492 samples/sec, batch_loss: 0.1694, batch_loss_c: 0.1745, batch_loss_s: 0.1576, time:26.8032, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:13:28 \u001b[32mINFO     \u001b[0m train.py: [4/300], [340/484], step: 2276, 2.473 samples/sec, batch_loss: 0.1462, batch_loss_c: 0.1017, batch_loss_s: 0.2501, time:16.1758, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:13:56 \u001b[32mINFO     \u001b[0m train.py: [4/300], [350/484], step: 2286, 1.436 samples/sec, batch_loss: 0.0869, batch_loss_c: 0.0803, batch_loss_s: 0.1022, time:27.8466, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:14:13 \u001b[32mINFO     \u001b[0m train.py: [4/300], [360/484], step: 2296, 2.291 samples/sec, batch_loss: 0.2773, batch_loss_c: 0.2741, batch_loss_s: 0.2848, time:17.4622, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:14:31 \u001b[32mINFO     \u001b[0m train.py: [4/300], [370/484], step: 2306, 2.243 samples/sec, batch_loss: 0.0950, batch_loss_c: 0.1028, batch_loss_s: 0.0769, time:17.8352, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:14:55 \u001b[32mINFO     \u001b[0m train.py: [4/300], [380/484], step: 2316, 1.716 samples/sec, batch_loss: 0.2895, batch_loss_c: 0.2856, batch_loss_s: 0.2985, time:23.3070, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:15:26 \u001b[32mINFO     \u001b[0m train.py: [4/300], [390/484], step: 2326, 1.259 samples/sec, batch_loss: 0.5584, batch_loss_c: 0.5578, batch_loss_s: 0.5598, time:31.7823, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:15:43 \u001b[32mINFO     \u001b[0m train.py: [4/300], [400/484], step: 2336, 2.356 samples/sec, batch_loss: 0.0524, batch_loss_c: 0.0384, batch_loss_s: 0.0850, time:16.9804, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:16:06 \u001b[32mINFO     \u001b[0m train.py: [4/300], [410/484], step: 2346, 1.767 samples/sec, batch_loss: 0.0764, batch_loss_c: 0.0712, batch_loss_s: 0.0883, time:22.6374, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:16:23 \u001b[32mINFO     \u001b[0m train.py: [4/300], [420/484], step: 2356, 2.410 samples/sec, batch_loss: 0.1366, batch_loss_c: 0.1535, batch_loss_s: 0.0971, time:16.5960, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:16:45 \u001b[32mINFO     \u001b[0m train.py: [4/300], [430/484], step: 2366, 1.817 samples/sec, batch_loss: 0.0788, batch_loss_c: 0.0776, batch_loss_s: 0.0815, time:22.0168, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:17:13 \u001b[32mINFO     \u001b[0m train.py: [4/300], [440/484], step: 2376, 1.391 samples/sec, batch_loss: 0.2913, batch_loss_c: 0.2786, batch_loss_s: 0.3208, time:28.7596, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:17:30 \u001b[32mINFO     \u001b[0m train.py: [4/300], [450/484], step: 2386, 2.439 samples/sec, batch_loss: 0.0773, batch_loss_c: 0.0681, batch_loss_s: 0.0989, time:16.3996, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:17:45 \u001b[32mINFO     \u001b[0m train.py: [4/300], [460/484], step: 2396, 2.542 samples/sec, batch_loss: 0.0831, batch_loss_c: 0.0739, batch_loss_s: 0.1045, time:15.7339, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:18:14 \u001b[32mINFO     \u001b[0m train.py: [4/300], [470/484], step: 2406, 1.412 samples/sec, batch_loss: 0.0557, batch_loss_c: 0.0377, batch_loss_s: 0.0977, time:28.3279, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:18:31 \u001b[32mINFO     \u001b[0m train.py: [4/300], [480/484], step: 2416, 2.312 samples/sec, batch_loss: 0.0577, batch_loss_c: 0.0523, batch_loss_s: 0.0704, time:17.2978, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:18:35 \u001b[32mINFO     \u001b[0m train.py: [4/300], train_loss: 0.1470, time: 1033.1768, lr: 1e-05\u001b[0m\n",
            "2019-12-08 10:18:37 \u001b[32mINFO     \u001b[0m train.py: [5/300], [0/484], step: 2420, 26.676 samples/sec, batch_loss: 0.2911, batch_loss_c: 0.2910, batch_loss_s: 0.2913, time:1.4995, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:19:20 \u001b[32mINFO     \u001b[0m train.py: [5/300], [10/484], step: 2430, 0.934 samples/sec, batch_loss: 0.2131, batch_loss_c: 0.1711, batch_loss_s: 0.3112, time:42.8408, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:19:58 \u001b[32mINFO     \u001b[0m train.py: [5/300], [20/484], step: 2440, 1.056 samples/sec, batch_loss: 0.3508, batch_loss_c: 0.3470, batch_loss_s: 0.3596, time:37.8936, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:20:18 \u001b[32mINFO     \u001b[0m train.py: [5/300], [30/484], step: 2450, 1.987 samples/sec, batch_loss: 0.1044, batch_loss_c: 0.0928, batch_loss_s: 0.1314, time:20.1304, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:20:35 \u001b[32mINFO     \u001b[0m train.py: [5/300], [40/484], step: 2460, 2.359 samples/sec, batch_loss: 0.0648, batch_loss_c: 0.0575, batch_loss_s: 0.0817, time:16.9528, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:21:01 \u001b[32mINFO     \u001b[0m train.py: [5/300], [50/484], step: 2470, 1.532 samples/sec, batch_loss: 0.0588, batch_loss_c: 0.0539, batch_loss_s: 0.0704, time:26.1125, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:21:20 \u001b[32mINFO     \u001b[0m train.py: [5/300], [60/484], step: 2480, 2.142 samples/sec, batch_loss: 0.0839, batch_loss_c: 0.0776, batch_loss_s: 0.0986, time:18.6703, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:21:56 \u001b[32mINFO     \u001b[0m train.py: [5/300], [70/484], step: 2490, 1.110 samples/sec, batch_loss: 0.1332, batch_loss_c: 0.1155, batch_loss_s: 0.1746, time:36.0521, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:22:15 \u001b[32mINFO     \u001b[0m train.py: [5/300], [80/484], step: 2500, 2.040 samples/sec, batch_loss: 0.0738, batch_loss_c: 0.0714, batch_loss_s: 0.0793, time:19.6052, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:22:29 \u001b[32mINFO     \u001b[0m train.py: [5/300], [90/484], step: 2510, 2.808 samples/sec, batch_loss: 0.0677, batch_loss_c: 0.0692, batch_loss_s: 0.0643, time:14.2460, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:22:53 \u001b[32mINFO     \u001b[0m train.py: [5/300], [100/484], step: 2520, 1.693 samples/sec, batch_loss: 0.3145, batch_loss_c: 0.3094, batch_loss_s: 0.3264, time:23.6317, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:23:21 \u001b[32mINFO     \u001b[0m train.py: [5/300], [110/484], step: 2530, 1.432 samples/sec, batch_loss: 0.3073, batch_loss_c: 0.3020, batch_loss_s: 0.3194, time:27.9390, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:23:52 \u001b[32mINFO     \u001b[0m train.py: [5/300], [120/484], step: 2540, 1.297 samples/sec, batch_loss: 0.0412, batch_loss_c: 0.0294, batch_loss_s: 0.0687, time:30.8419, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:24:11 \u001b[32mINFO     \u001b[0m train.py: [5/300], [130/484], step: 2550, 2.084 samples/sec, batch_loss: 0.0746, batch_loss_c: 0.0647, batch_loss_s: 0.0977, time:19.1940, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:24:41 \u001b[32mINFO     \u001b[0m train.py: [5/300], [140/484], step: 2560, 1.327 samples/sec, batch_loss: 0.0879, batch_loss_c: 0.0764, batch_loss_s: 0.1145, time:30.1340, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:25:11 \u001b[32mINFO     \u001b[0m train.py: [5/300], [150/484], step: 2570, 1.363 samples/sec, batch_loss: 0.0825, batch_loss_c: 0.0733, batch_loss_s: 0.1039, time:29.3407, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:25:38 \u001b[32mINFO     \u001b[0m train.py: [5/300], [160/484], step: 2580, 1.456 samples/sec, batch_loss: 0.0625, batch_loss_c: 0.0566, batch_loss_s: 0.0762, time:27.4743, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:25:52 \u001b[32mINFO     \u001b[0m train.py: [5/300], [170/484], step: 2590, 2.795 samples/sec, batch_loss: 0.0590, batch_loss_c: 0.0518, batch_loss_s: 0.0757, time:14.3093, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:26:16 \u001b[32mINFO     \u001b[0m train.py: [5/300], [180/484], step: 2600, 1.684 samples/sec, batch_loss: 0.3160, batch_loss_c: 0.3119, batch_loss_s: 0.3255, time:23.7511, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:26:36 \u001b[32mINFO     \u001b[0m train.py: [5/300], [190/484], step: 2610, 1.997 samples/sec, batch_loss: 0.2857, batch_loss_c: 0.2810, batch_loss_s: 0.2968, time:20.0340, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:26:50 \u001b[32mINFO     \u001b[0m train.py: [5/300], [200/484], step: 2620, 2.921 samples/sec, batch_loss: 0.0995, batch_loss_c: 0.0862, batch_loss_s: 0.1304, time:13.6950, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:27:04 \u001b[32mINFO     \u001b[0m train.py: [5/300], [210/484], step: 2630, 2.902 samples/sec, batch_loss: 0.0770, batch_loss_c: 0.0632, batch_loss_s: 0.1091, time:13.7834, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:27:18 \u001b[32mINFO     \u001b[0m train.py: [5/300], [220/484], step: 2640, 2.778 samples/sec, batch_loss: 0.0470, batch_loss_c: 0.0412, batch_loss_s: 0.0607, time:14.4006, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:27:39 \u001b[32mINFO     \u001b[0m train.py: [5/300], [230/484], step: 2650, 1.919 samples/sec, batch_loss: 0.0769, batch_loss_c: 0.0802, batch_loss_s: 0.0692, time:20.8440, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:27:57 \u001b[32mINFO     \u001b[0m train.py: [5/300], [240/484], step: 2660, 2.163 samples/sec, batch_loss: 0.0657, batch_loss_c: 0.0540, batch_loss_s: 0.0931, time:18.4944, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:28:13 \u001b[32mINFO     \u001b[0m train.py: [5/300], [250/484], step: 2670, 2.506 samples/sec, batch_loss: 0.0717, batch_loss_c: 0.0674, batch_loss_s: 0.0818, time:15.9595, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:28:28 \u001b[32mINFO     \u001b[0m train.py: [5/300], [260/484], step: 2680, 2.764 samples/sec, batch_loss: 0.2013, batch_loss_c: 0.2173, batch_loss_s: 0.1642, time:14.4701, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:28:52 \u001b[32mINFO     \u001b[0m train.py: [5/300], [270/484], step: 2690, 1.686 samples/sec, batch_loss: 0.2336, batch_loss_c: 0.2070, batch_loss_s: 0.2957, time:23.7241, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:29:08 \u001b[32mINFO     \u001b[0m train.py: [5/300], [280/484], step: 2700, 2.360 samples/sec, batch_loss: 0.1096, batch_loss_c: 0.1072, batch_loss_s: 0.1150, time:16.9467, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:29:28 \u001b[32mINFO     \u001b[0m train.py: [5/300], [290/484], step: 2710, 2.019 samples/sec, batch_loss: 0.0909, batch_loss_c: 0.0804, batch_loss_s: 0.1153, time:19.8116, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:29:57 \u001b[32mINFO     \u001b[0m train.py: [5/300], [300/484], step: 2720, 1.380 samples/sec, batch_loss: 0.0582, batch_loss_c: 0.0526, batch_loss_s: 0.0713, time:28.9881, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:30:29 \u001b[32mINFO     \u001b[0m train.py: [5/300], [310/484], step: 2730, 1.251 samples/sec, batch_loss: 0.0643, batch_loss_c: 0.0545, batch_loss_s: 0.0870, time:31.9866, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:30:45 \u001b[32mINFO     \u001b[0m train.py: [5/300], [320/484], step: 2740, 2.581 samples/sec, batch_loss: 0.3488, batch_loss_c: 0.3505, batch_loss_s: 0.3446, time:15.4971, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:31:01 \u001b[32mINFO     \u001b[0m train.py: [5/300], [330/484], step: 2750, 2.534 samples/sec, batch_loss: 0.1186, batch_loss_c: 0.1224, batch_loss_s: 0.1098, time:15.7844, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:31:22 \u001b[32mINFO     \u001b[0m train.py: [5/300], [340/484], step: 2760, 1.839 samples/sec, batch_loss: 0.3111, batch_loss_c: 0.3096, batch_loss_s: 0.3146, time:21.7525, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:31:39 \u001b[32mINFO     \u001b[0m train.py: [5/300], [350/484], step: 2770, 2.395 samples/sec, batch_loss: 0.1610, batch_loss_c: 0.1575, batch_loss_s: 0.1692, time:16.6980, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:32:01 \u001b[32mINFO     \u001b[0m train.py: [5/300], [360/484], step: 2780, 1.849 samples/sec, batch_loss: 0.0572, batch_loss_c: 0.0500, batch_loss_s: 0.0742, time:21.6283, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:32:34 \u001b[32mINFO     \u001b[0m train.py: [5/300], [370/484], step: 2790, 1.192 samples/sec, batch_loss: 0.0541, batch_loss_c: 0.0450, batch_loss_s: 0.0752, time:33.5579, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:32:52 \u001b[32mINFO     \u001b[0m train.py: [5/300], [380/484], step: 2800, 2.274 samples/sec, batch_loss: 0.1085, batch_loss_c: 0.0921, batch_loss_s: 0.1467, time:17.5882, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:33:12 \u001b[32mINFO     \u001b[0m train.py: [5/300], [390/484], step: 2810, 1.979 samples/sec, batch_loss: 0.1050, batch_loss_c: 0.1024, batch_loss_s: 0.1111, time:20.2116, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:33:33 \u001b[32mINFO     \u001b[0m train.py: [5/300], [400/484], step: 2820, 1.926 samples/sec, batch_loss: 0.1212, batch_loss_c: 0.1073, batch_loss_s: 0.1536, time:20.7657, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:33:56 \u001b[32mINFO     \u001b[0m train.py: [5/300], [410/484], step: 2830, 1.688 samples/sec, batch_loss: 0.2489, batch_loss_c: 0.2686, batch_loss_s: 0.2029, time:23.6943, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:34:17 \u001b[32mINFO     \u001b[0m train.py: [5/300], [420/484], step: 2840, 1.963 samples/sec, batch_loss: 0.0524, batch_loss_c: 0.0431, batch_loss_s: 0.0739, time:20.3751, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:34:33 \u001b[32mINFO     \u001b[0m train.py: [5/300], [430/484], step: 2850, 2.448 samples/sec, batch_loss: 0.0595, batch_loss_c: 0.0489, batch_loss_s: 0.0840, time:16.3378, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:34:49 \u001b[32mINFO     \u001b[0m train.py: [5/300], [440/484], step: 2860, 2.460 samples/sec, batch_loss: 0.0493, batch_loss_c: 0.0424, batch_loss_s: 0.0654, time:16.2619, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:35:05 \u001b[32mINFO     \u001b[0m train.py: [5/300], [450/484], step: 2870, 2.518 samples/sec, batch_loss: 0.1018, batch_loss_c: 0.1102, batch_loss_s: 0.0821, time:15.8840, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:35:33 \u001b[32mINFO     \u001b[0m train.py: [5/300], [460/484], step: 2880, 1.468 samples/sec, batch_loss: 0.0723, batch_loss_c: 0.0659, batch_loss_s: 0.0872, time:27.2486, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:35:53 \u001b[32mINFO     \u001b[0m train.py: [5/300], [470/484], step: 2890, 1.961 samples/sec, batch_loss: 0.0547, batch_loss_c: 0.0486, batch_loss_s: 0.0691, time:20.3952, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:36:09 \u001b[32mINFO     \u001b[0m train.py: [5/300], [480/484], step: 2900, 2.500 samples/sec, batch_loss: 0.0536, batch_loss_c: 0.0470, batch_loss_s: 0.0689, time:15.9971, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:36:24 \u001b[32mINFO     \u001b[0m train.py: [5/300], train_loss: 0.1397, time: 1068.4624, lr: 1e-05\u001b[0m\n",
            "2019-12-08 10:36:26 \u001b[32mINFO     \u001b[0m train.py: [6/300], [0/484], step: 2904, 29.054 samples/sec, batch_loss: 0.0517, batch_loss_c: 0.0475, batch_loss_s: 0.0616, time:1.3768, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:36:43 \u001b[32mINFO     \u001b[0m train.py: [6/300], [10/484], step: 2914, 2.272 samples/sec, batch_loss: 0.0733, batch_loss_c: 0.0623, batch_loss_s: 0.0990, time:17.6066, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:37:00 \u001b[32mINFO     \u001b[0m train.py: [6/300], [20/484], step: 2924, 2.412 samples/sec, batch_loss: 0.0543, batch_loss_c: 0.0482, batch_loss_s: 0.0685, time:16.5815, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:37:17 \u001b[32mINFO     \u001b[0m train.py: [6/300], [30/484], step: 2934, 2.397 samples/sec, batch_loss: 0.3277, batch_loss_c: 0.3247, batch_loss_s: 0.3345, time:16.6841, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:37:31 \u001b[32mINFO     \u001b[0m train.py: [6/300], [40/484], step: 2944, 2.787 samples/sec, batch_loss: 0.2922, batch_loss_c: 0.2815, batch_loss_s: 0.3174, time:14.3546, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:37:52 \u001b[32mINFO     \u001b[0m train.py: [6/300], [50/484], step: 2954, 1.925 samples/sec, batch_loss: 0.1958, batch_loss_c: 0.1955, batch_loss_s: 0.1967, time:20.7805, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:38:08 \u001b[32mINFO     \u001b[0m train.py: [6/300], [60/484], step: 2964, 2.431 samples/sec, batch_loss: 0.0706, batch_loss_c: 0.0642, batch_loss_s: 0.0853, time:16.4570, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:38:31 \u001b[32mINFO     \u001b[0m train.py: [6/300], [70/484], step: 2974, 1.754 samples/sec, batch_loss: 0.6874, batch_loss_c: 0.6680, batch_loss_s: 0.7326, time:22.8087, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:38:46 \u001b[32mINFO     \u001b[0m train.py: [6/300], [80/484], step: 2984, 2.684 samples/sec, batch_loss: 0.0917, batch_loss_c: 0.0704, batch_loss_s: 0.1413, time:14.9005, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:39:07 \u001b[32mINFO     \u001b[0m train.py: [6/300], [90/484], step: 2994, 1.899 samples/sec, batch_loss: 0.1242, batch_loss_c: 0.1232, batch_loss_s: 0.1267, time:21.0585, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:39:23 \u001b[32mINFO     \u001b[0m train.py: [6/300], [100/484], step: 3004, 2.515 samples/sec, batch_loss: 0.0723, batch_loss_c: 0.0646, batch_loss_s: 0.0903, time:15.9036, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:39:38 \u001b[32mINFO     \u001b[0m train.py: [6/300], [110/484], step: 3014, 2.667 samples/sec, batch_loss: 0.0470, batch_loss_c: 0.0420, batch_loss_s: 0.0588, time:14.9994, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:40:00 \u001b[32mINFO     \u001b[0m train.py: [6/300], [120/484], step: 3024, 1.845 samples/sec, batch_loss: 0.2098, batch_loss_c: 0.1854, batch_loss_s: 0.2668, time:21.6812, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:40:14 \u001b[32mINFO     \u001b[0m train.py: [6/300], [130/484], step: 3034, 2.816 samples/sec, batch_loss: 0.2148, batch_loss_c: 0.1790, batch_loss_s: 0.2981, time:14.2065, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:40:31 \u001b[32mINFO     \u001b[0m train.py: [6/300], [140/484], step: 3044, 2.306 samples/sec, batch_loss: 0.0663, batch_loss_c: 0.0557, batch_loss_s: 0.0909, time:17.3440, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:40:54 \u001b[32mINFO     \u001b[0m train.py: [6/300], [150/484], step: 3054, 1.721 samples/sec, batch_loss: 0.1449, batch_loss_c: 0.1246, batch_loss_s: 0.1921, time:23.2379, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:41:13 \u001b[32mINFO     \u001b[0m train.py: [6/300], [160/484], step: 3064, 2.135 samples/sec, batch_loss: 0.0711, batch_loss_c: 0.0698, batch_loss_s: 0.0739, time:18.7332, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:41:30 \u001b[32mINFO     \u001b[0m train.py: [6/300], [170/484], step: 3074, 2.387 samples/sec, batch_loss: 0.0515, batch_loss_c: 0.0449, batch_loss_s: 0.0668, time:16.7579, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:41:46 \u001b[32mINFO     \u001b[0m train.py: [6/300], [180/484], step: 3084, 2.485 samples/sec, batch_loss: 0.0639, batch_loss_c: 0.0560, batch_loss_s: 0.0824, time:16.0955, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:42:06 \u001b[32mINFO     \u001b[0m train.py: [6/300], [190/484], step: 3094, 1.948 samples/sec, batch_loss: 0.0982, batch_loss_c: 0.0917, batch_loss_s: 0.1136, time:20.5315, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:42:34 \u001b[32mINFO     \u001b[0m train.py: [6/300], [200/484], step: 3104, 1.469 samples/sec, batch_loss: 0.0894, batch_loss_c: 0.0817, batch_loss_s: 0.1074, time:27.2293, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:42:49 \u001b[32mINFO     \u001b[0m train.py: [6/300], [210/484], step: 3114, 2.547 samples/sec, batch_loss: 0.2833, batch_loss_c: 0.2693, batch_loss_s: 0.3158, time:15.7035, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:43:15 \u001b[32mINFO     \u001b[0m train.py: [6/300], [220/484], step: 3124, 1.576 samples/sec, batch_loss: 0.0472, batch_loss_c: 0.0413, batch_loss_s: 0.0608, time:25.3809, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:43:31 \u001b[32mINFO     \u001b[0m train.py: [6/300], [230/484], step: 3134, 2.493 samples/sec, batch_loss: 0.0468, batch_loss_c: 0.0383, batch_loss_s: 0.0664, time:16.0425, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:43:52 \u001b[32mINFO     \u001b[0m train.py: [6/300], [240/484], step: 3144, 1.917 samples/sec, batch_loss: 0.5415, batch_loss_c: 0.5367, batch_loss_s: 0.5527, time:20.8713, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:44:17 \u001b[32mINFO     \u001b[0m train.py: [6/300], [250/484], step: 3154, 1.611 samples/sec, batch_loss: 0.2811, batch_loss_c: 0.2761, batch_loss_s: 0.2928, time:24.8244, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:44:32 \u001b[32mINFO     \u001b[0m train.py: [6/300], [260/484], step: 3164, 2.546 samples/sec, batch_loss: 0.2908, batch_loss_c: 0.2843, batch_loss_s: 0.3058, time:15.7107, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:45:02 \u001b[32mINFO     \u001b[0m train.py: [6/300], [270/484], step: 3174, 1.334 samples/sec, batch_loss: 0.2109, batch_loss_c: 0.2041, batch_loss_s: 0.2266, time:29.9767, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:45:20 \u001b[32mINFO     \u001b[0m train.py: [6/300], [280/484], step: 3184, 2.240 samples/sec, batch_loss: 0.3066, batch_loss_c: 0.3010, batch_loss_s: 0.3195, time:17.8582, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:45:42 \u001b[32mINFO     \u001b[0m train.py: [6/300], [290/484], step: 3194, 1.803 samples/sec, batch_loss: 0.0641, batch_loss_c: 0.0550, batch_loss_s: 0.0855, time:22.1848, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:45:59 \u001b[32mINFO     \u001b[0m train.py: [6/300], [300/484], step: 3204, 2.359 samples/sec, batch_loss: 0.0951, batch_loss_c: 0.0964, batch_loss_s: 0.0921, time:16.9539, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:46:30 \u001b[32mINFO     \u001b[0m train.py: [6/300], [310/484], step: 3214, 1.309 samples/sec, batch_loss: 0.1076, batch_loss_c: 0.0965, batch_loss_s: 0.1337, time:30.5509, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:46:56 \u001b[32mINFO     \u001b[0m train.py: [6/300], [320/484], step: 3224, 1.500 samples/sec, batch_loss: 0.2987, batch_loss_c: 0.2951, batch_loss_s: 0.3072, time:26.6621, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:47:18 \u001b[32mINFO     \u001b[0m train.py: [6/300], [330/484], step: 3234, 1.887 samples/sec, batch_loss: 0.0676, batch_loss_c: 0.0598, batch_loss_s: 0.0859, time:21.1951, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:47:57 \u001b[32mINFO     \u001b[0m train.py: [6/300], [340/484], step: 3244, 1.020 samples/sec, batch_loss: 0.1315, batch_loss_c: 0.1277, batch_loss_s: 0.1404, time:39.1976, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:48:23 \u001b[32mINFO     \u001b[0m train.py: [6/300], [350/484], step: 3254, 1.500 samples/sec, batch_loss: 0.0564, batch_loss_c: 0.0461, batch_loss_s: 0.0805, time:26.6666, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:48:40 \u001b[32mINFO     \u001b[0m train.py: [6/300], [360/484], step: 3264, 2.392 samples/sec, batch_loss: 0.1074, batch_loss_c: 0.1102, batch_loss_s: 0.1008, time:16.7239, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:48:59 \u001b[32mINFO     \u001b[0m train.py: [6/300], [370/484], step: 3274, 2.157 samples/sec, batch_loss: 0.1814, batch_loss_c: 0.1725, batch_loss_s: 0.2022, time:18.5402, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:49:22 \u001b[32mINFO     \u001b[0m train.py: [6/300], [380/484], step: 3284, 1.743 samples/sec, batch_loss: 0.1504, batch_loss_c: 0.1734, batch_loss_s: 0.0966, time:22.9519, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:49:44 \u001b[32mINFO     \u001b[0m train.py: [6/300], [390/484], step: 3294, 1.760 samples/sec, batch_loss: 0.5577, batch_loss_c: 0.5477, batch_loss_s: 0.5809, time:22.7272, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:50:02 \u001b[32mINFO     \u001b[0m train.py: [6/300], [400/484], step: 3304, 2.325 samples/sec, batch_loss: 0.2905, batch_loss_c: 0.2858, batch_loss_s: 0.3014, time:17.2042, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:50:17 \u001b[32mINFO     \u001b[0m train.py: [6/300], [410/484], step: 3314, 2.609 samples/sec, batch_loss: 0.0591, batch_loss_c: 0.0485, batch_loss_s: 0.0838, time:15.3333, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:50:35 \u001b[32mINFO     \u001b[0m train.py: [6/300], [420/484], step: 3324, 2.269 samples/sec, batch_loss: 0.2921, batch_loss_c: 0.2881, batch_loss_s: 0.3014, time:17.6323, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:50:49 \u001b[32mINFO     \u001b[0m train.py: [6/300], [430/484], step: 3334, 2.725 samples/sec, batch_loss: 0.0496, batch_loss_c: 0.0432, batch_loss_s: 0.0647, time:14.6781, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:51:14 \u001b[32mINFO     \u001b[0m train.py: [6/300], [440/484], step: 3344, 1.599 samples/sec, batch_loss: 0.2955, batch_loss_c: 0.2915, batch_loss_s: 0.3050, time:25.0227, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:51:42 \u001b[32mINFO     \u001b[0m train.py: [6/300], [450/484], step: 3354, 1.466 samples/sec, batch_loss: 0.1603, batch_loss_c: 0.1578, batch_loss_s: 0.1662, time:27.2934, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:51:57 \u001b[32mINFO     \u001b[0m train.py: [6/300], [460/484], step: 3364, 2.578 samples/sec, batch_loss: 0.1498, batch_loss_c: 0.1301, batch_loss_s: 0.1958, time:15.5158, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:52:12 \u001b[32mINFO     \u001b[0m train.py: [6/300], [470/484], step: 3374, 2.779 samples/sec, batch_loss: 0.0698, batch_loss_c: 0.0607, batch_loss_s: 0.0909, time:14.3944, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:52:31 \u001b[32mINFO     \u001b[0m train.py: [6/300], [480/484], step: 3384, 2.071 samples/sec, batch_loss: 0.0573, batch_loss_c: 0.0437, batch_loss_s: 0.0890, time:19.3110, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:52:35 \u001b[32mINFO     \u001b[0m train.py: [6/300], train_loss: 0.1485, time: 970.8883, lr: 1e-05\u001b[0m\n",
            "2019-12-08 10:52:38 \u001b[32mINFO     \u001b[0m train.py: [7/300], [0/484], step: 3388, 18.790 samples/sec, batch_loss: 0.2853, batch_loss_c: 0.2803, batch_loss_s: 0.2968, time:2.1288, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:53:00 \u001b[32mINFO     \u001b[0m train.py: [7/300], [10/484], step: 3398, 1.824 samples/sec, batch_loss: 0.0419, batch_loss_c: 0.0294, batch_loss_s: 0.0711, time:21.9316, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:53:19 \u001b[32mINFO     \u001b[0m train.py: [7/300], [20/484], step: 3408, 2.105 samples/sec, batch_loss: 0.0436, batch_loss_c: 0.0354, batch_loss_s: 0.0628, time:18.9993, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:53:45 \u001b[32mINFO     \u001b[0m train.py: [7/300], [30/484], step: 3418, 1.525 samples/sec, batch_loss: 0.2873, batch_loss_c: 0.2841, batch_loss_s: 0.2948, time:26.2284, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:54:07 \u001b[32mINFO     \u001b[0m train.py: [7/300], [40/484], step: 3428, 1.837 samples/sec, batch_loss: 0.0945, batch_loss_c: 0.0872, batch_loss_s: 0.1114, time:21.7689, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:54:22 \u001b[32mINFO     \u001b[0m train.py: [7/300], [50/484], step: 3438, 2.589 samples/sec, batch_loss: 0.1201, batch_loss_c: 0.1284, batch_loss_s: 0.1005, time:15.4471, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:54:38 \u001b[32mINFO     \u001b[0m train.py: [7/300], [60/484], step: 3448, 2.549 samples/sec, batch_loss: 0.1292, batch_loss_c: 0.0949, batch_loss_s: 0.2092, time:15.6898, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:54:52 \u001b[32mINFO     \u001b[0m train.py: [7/300], [70/484], step: 3458, 2.914 samples/sec, batch_loss: 0.0766, batch_loss_c: 0.0661, batch_loss_s: 0.1011, time:13.7267, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:55:11 \u001b[32mINFO     \u001b[0m train.py: [7/300], [80/484], step: 3468, 2.089 samples/sec, batch_loss: 0.1553, batch_loss_c: 0.1482, batch_loss_s: 0.1720, time:19.1447, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:55:32 \u001b[32mINFO     \u001b[0m train.py: [7/300], [90/484], step: 3478, 1.842 samples/sec, batch_loss: 0.1330, batch_loss_c: 0.1328, batch_loss_s: 0.1335, time:21.7112, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:55:52 \u001b[32mINFO     \u001b[0m train.py: [7/300], [100/484], step: 3488, 2.044 samples/sec, batch_loss: 0.2870, batch_loss_c: 0.2748, batch_loss_s: 0.3155, time:19.5690, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:56:11 \u001b[32mINFO     \u001b[0m train.py: [7/300], [110/484], step: 3498, 2.110 samples/sec, batch_loss: 0.0776, batch_loss_c: 0.0695, batch_loss_s: 0.0965, time:18.9614, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:56:26 \u001b[32mINFO     \u001b[0m train.py: [7/300], [120/484], step: 3508, 2.605 samples/sec, batch_loss: 0.3341, batch_loss_c: 0.3352, batch_loss_s: 0.3316, time:15.3567, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:56:56 \u001b[32mINFO     \u001b[0m train.py: [7/300], [130/484], step: 3518, 1.356 samples/sec, batch_loss: 0.0568, batch_loss_c: 0.0517, batch_loss_s: 0.0687, time:29.4941, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:57:11 \u001b[32mINFO     \u001b[0m train.py: [7/300], [140/484], step: 3528, 2.552 samples/sec, batch_loss: 0.0453, batch_loss_c: 0.0360, batch_loss_s: 0.0667, time:15.6712, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:57:30 \u001b[32mINFO     \u001b[0m train.py: [7/300], [150/484], step: 3538, 2.099 samples/sec, batch_loss: 0.0853, batch_loss_c: 0.0725, batch_loss_s: 0.1153, time:19.0527, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:57:48 \u001b[32mINFO     \u001b[0m train.py: [7/300], [160/484], step: 3548, 2.318 samples/sec, batch_loss: 0.0785, batch_loss_c: 0.0743, batch_loss_s: 0.0886, time:17.2586, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:58:09 \u001b[32mINFO     \u001b[0m train.py: [7/300], [170/484], step: 3558, 1.858 samples/sec, batch_loss: 0.0627, batch_loss_c: 0.0523, batch_loss_s: 0.0868, time:21.5233, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:58:37 \u001b[32mINFO     \u001b[0m train.py: [7/300], [180/484], step: 3568, 1.449 samples/sec, batch_loss: 0.1715, batch_loss_c: 0.1501, batch_loss_s: 0.2214, time:27.6029, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:58:53 \u001b[32mINFO     \u001b[0m train.py: [7/300], [190/484], step: 3578, 2.541 samples/sec, batch_loss: 0.1803, batch_loss_c: 0.1724, batch_loss_s: 0.1985, time:15.7409, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:59:19 \u001b[32mINFO     \u001b[0m train.py: [7/300], [200/484], step: 3588, 1.521 samples/sec, batch_loss: 0.1233, batch_loss_c: 0.1024, batch_loss_s: 0.1719, time:26.2988, lr:1e-05\u001b[0m\n",
            "2019-12-08 10:59:47 \u001b[32mINFO     \u001b[0m train.py: [7/300], [210/484], step: 3598, 1.429 samples/sec, batch_loss: 0.3994, batch_loss_c: 0.3990, batch_loss_s: 0.4003, time:27.9941, lr:1e-05\u001b[0m\n",
            "2019-12-08 11:00:10 \u001b[32mINFO     \u001b[0m train.py: [7/300], [220/484], step: 3608, 1.728 samples/sec, batch_loss: 0.0946, batch_loss_c: 0.0881, batch_loss_s: 0.1097, time:23.1481, lr:1e-05\u001b[0m\n",
            "2019-12-08 11:00:39 \u001b[32mINFO     \u001b[0m train.py: [7/300], [230/484], step: 3618, 1.382 samples/sec, batch_loss: 0.0771, batch_loss_c: 0.0701, batch_loss_s: 0.0935, time:28.9374, lr:1e-05\u001b[0m\n",
            "2019-12-08 11:00:56 \u001b[32mINFO     \u001b[0m train.py: [7/300], [240/484], step: 3628, 2.395 samples/sec, batch_loss: 0.1683, batch_loss_c: 0.1665, batch_loss_s: 0.1725, time:16.7031, lr:1e-05\u001b[0m\n",
            "2019-12-08 11:01:12 \u001b[32mINFO     \u001b[0m train.py: [7/300], [250/484], step: 3638, 2.508 samples/sec, batch_loss: 0.1515, batch_loss_c: 0.1475, batch_loss_s: 0.1609, time:15.9487, lr:1e-05\u001b[0m\n",
            "2019-12-08 11:01:29 \u001b[32mINFO     \u001b[0m train.py: [7/300], [260/484], step: 3648, 2.349 samples/sec, batch_loss: 0.0771, batch_loss_c: 0.0645, batch_loss_s: 0.1068, time:17.0256, lr:1e-05\u001b[0m\n",
            "2019-12-08 11:01:49 \u001b[32mINFO     \u001b[0m train.py: [7/300], [270/484], step: 3658, 1.965 samples/sec, batch_loss: 0.2892, batch_loss_c: 0.2841, batch_loss_s: 0.3010, time:20.3582, lr:1e-05\u001b[0m\n",
            "2019-12-08 11:02:07 \u001b[32mINFO     \u001b[0m train.py: [7/300], [280/484], step: 3668, 2.270 samples/sec, batch_loss: 0.0789, batch_loss_c: 0.0712, batch_loss_s: 0.0968, time:17.6227, lr:1e-05\u001b[0m\n",
            "2019-12-08 11:02:30 \u001b[32mINFO     \u001b[0m train.py: [7/300], [290/484], step: 3678, 1.714 samples/sec, batch_loss: 0.0914, batch_loss_c: 0.0866, batch_loss_s: 0.1026, time:23.3437, lr:1e-05\u001b[0m\n",
            "2019-12-08 11:02:49 \u001b[32mINFO     \u001b[0m train.py: [7/300], [300/484], step: 3688, 2.073 samples/sec, batch_loss: 0.0766, batch_loss_c: 0.0675, batch_loss_s: 0.0978, time:19.2943, lr:1e-05\u001b[0m\n",
            "2019-12-08 11:03:06 \u001b[32mINFO     \u001b[0m train.py: [7/300], [310/484], step: 3698, 2.450 samples/sec, batch_loss: 0.0671, batch_loss_c: 0.0602, batch_loss_s: 0.0832, time:16.3293, lr:1e-05\u001b[0m\n",
            "2019-12-08 11:03:22 \u001b[32mINFO     \u001b[0m train.py: [7/300], [320/484], step: 3708, 2.372 samples/sec, batch_loss: 0.0744, batch_loss_c: 0.0637, batch_loss_s: 0.0995, time:16.8661, lr:1e-05\u001b[0m\n",
            "2019-12-08 11:03:38 \u001b[32mINFO     \u001b[0m train.py: [7/300], [330/484], step: 3718, 2.620 samples/sec, batch_loss: 0.0567, batch_loss_c: 0.0469, batch_loss_s: 0.0793, time:15.2661, lr:1e-05\u001b[0m\n",
            "2019-12-08 11:03:56 \u001b[32mINFO     \u001b[0m train.py: [7/300], [340/484], step: 3728, 2.223 samples/sec, batch_loss: 0.1149, batch_loss_c: 0.1186, batch_loss_s: 0.1064, time:17.9939, lr:1e-05\u001b[0m\n",
            "2019-12-08 11:04:15 \u001b[32mINFO     \u001b[0m train.py: [7/300], [350/484], step: 3738, 2.133 samples/sec, batch_loss: 0.0397, batch_loss_c: 0.0328, batch_loss_s: 0.0559, time:18.7504, lr:1e-05\u001b[0m\n",
            "2019-12-08 11:04:41 \u001b[32mINFO     \u001b[0m train.py: [7/300], [360/484], step: 3748, 1.483 samples/sec, batch_loss: 0.2029, batch_loss_c: 0.1894, batch_loss_s: 0.2345, time:26.9742, lr:1e-05\u001b[0m\n",
            "2019-12-08 11:04:58 \u001b[32mINFO     \u001b[0m train.py: [7/300], [370/484], step: 3758, 2.401 samples/sec, batch_loss: 0.0762, batch_loss_c: 0.0727, batch_loss_s: 0.0844, time:16.6607, lr:1e-05\u001b[0m\n",
            "2019-12-08 11:05:18 \u001b[32mINFO     \u001b[0m train.py: [7/300], [380/484], step: 3768, 1.981 samples/sec, batch_loss: 0.5163, batch_loss_c: 0.5124, batch_loss_s: 0.5255, time:20.1883, lr:1e-05\u001b[0m\n",
            "2019-12-08 11:05:39 \u001b[32mINFO     \u001b[0m train.py: [7/300], [390/484], step: 3778, 1.907 samples/sec, batch_loss: 0.2903, batch_loss_c: 0.2847, batch_loss_s: 0.3035, time:20.9727, lr:1e-05\u001b[0m\n",
            "2019-12-08 11:06:02 \u001b[32mINFO     \u001b[0m train.py: [7/300], [400/484], step: 3788, 1.764 samples/sec, batch_loss: 0.2940, batch_loss_c: 0.2926, batch_loss_s: 0.2971, time:22.6804, lr:1e-05\u001b[0m\n",
            "2019-12-08 11:06:20 \u001b[32mINFO     \u001b[0m train.py: [7/300], [410/484], step: 3798, 2.241 samples/sec, batch_loss: 0.1089, batch_loss_c: 0.1188, batch_loss_s: 0.0860, time:17.8505, lr:1e-05\u001b[0m\n",
            "2019-12-08 11:06:39 \u001b[32mINFO     \u001b[0m train.py: [7/300], [420/484], step: 3808, 2.037 samples/sec, batch_loss: 0.2451, batch_loss_c: 0.2356, batch_loss_s: 0.2671, time:19.6405, lr:1e-05\u001b[0m\n",
            "2019-12-08 11:06:58 \u001b[32mINFO     \u001b[0m train.py: [7/300], [430/484], step: 3818, 2.196 samples/sec, batch_loss: 0.0594, batch_loss_c: 0.0574, batch_loss_s: 0.0643, time:18.2118, lr:1e-05\u001b[0m\n",
            "2019-12-08 11:07:18 \u001b[32mINFO     \u001b[0m train.py: [7/300], [440/484], step: 3828, 2.017 samples/sec, batch_loss: 0.0940, batch_loss_c: 0.0802, batch_loss_s: 0.1263, time:19.8278, lr:1e-05\u001b[0m\n",
            "2019-12-08 11:07:37 \u001b[32mINFO     \u001b[0m train.py: [7/300], [450/484], step: 3838, 2.064 samples/sec, batch_loss: 0.2900, batch_loss_c: 0.2861, batch_loss_s: 0.2991, time:19.3817, lr:1e-05\u001b[0m\n",
            "2019-12-08 11:07:55 \u001b[32mINFO     \u001b[0m train.py: [7/300], [460/484], step: 3848, 2.159 samples/sec, batch_loss: 0.0860, batch_loss_c: 0.0835, batch_loss_s: 0.0918, time:18.5303, lr:1e-05\u001b[0m\n",
            "2019-12-08 11:08:25 \u001b[32mINFO     \u001b[0m train.py: [7/300], [470/484], step: 3858, 1.333 samples/sec, batch_loss: 0.0527, batch_loss_c: 0.0526, batch_loss_s: 0.0530, time:30.0063, lr:1e-05\u001b[0m\n",
            "2019-12-08 11:08:42 \u001b[32mINFO     \u001b[0m train.py: [7/300], [480/484], step: 3868, 2.411 samples/sec, batch_loss: 0.2960, batch_loss_c: 0.2933, batch_loss_s: 0.3023, time:16.5939, lr:1e-05\u001b[0m\n",
            "2019-12-08 11:08:48 \u001b[32mINFO     \u001b[0m train.py: [7/300], train_loss: 0.1405, time: 972.2518, lr: 1e-05\u001b[0m\n",
            "2019-12-08 11:08:49 \u001b[32mINFO     \u001b[0m train.py: [8/300], [0/484], step: 3872, 34.736 samples/sec, batch_loss: 0.0859, batch_loss_c: 0.0664, batch_loss_s: 0.1313, time:1.1515, lr:1e-05\u001b[0m\n",
            "2019-12-08 11:09:08 \u001b[32mINFO     \u001b[0m train.py: [8/300], [10/484], step: 3882, 2.163 samples/sec, batch_loss: 0.0833, batch_loss_c: 0.0784, batch_loss_s: 0.0948, time:18.4943, lr:1e-05\u001b[0m\n",
            "2019-12-08 11:09:25 \u001b[32mINFO     \u001b[0m train.py: [8/300], [20/484], step: 3892, 2.272 samples/sec, batch_loss: 0.0481, batch_loss_c: 0.0397, batch_loss_s: 0.0676, time:17.6089, lr:1e-05\u001b[0m\n",
            "2019-12-08 11:09:45 \u001b[32mINFO     \u001b[0m train.py: [8/300], [30/484], step: 3902, 2.015 samples/sec, batch_loss: 0.0504, batch_loss_c: 0.0394, batch_loss_s: 0.0760, time:19.8526, lr:1e-05\u001b[0m\n",
            "2019-12-08 11:10:19 \u001b[32mINFO     \u001b[0m train.py: [8/300], [40/484], step: 3912, 1.199 samples/sec, batch_loss: 0.0693, batch_loss_c: 0.0639, batch_loss_s: 0.0818, time:33.3734, lr:1e-05\u001b[0m\n",
            "2019-12-08 11:10:39 \u001b[32mINFO     \u001b[0m train.py: [8/300], [50/484], step: 3922, 1.997 samples/sec, batch_loss: 0.3044, batch_loss_c: 0.2959, batch_loss_s: 0.3243, time:20.0334, lr:1e-05\u001b[0m\n",
            "2019-12-08 11:10:54 \u001b[32mINFO     \u001b[0m train.py: [8/300], [60/484], step: 3932, 2.626 samples/sec, batch_loss: 0.0698, batch_loss_c: 0.0584, batch_loss_s: 0.0964, time:15.2334, lr:1e-05\u001b[0m\n",
            "2019-12-08 11:11:09 \u001b[32mINFO     \u001b[0m train.py: [8/300], [70/484], step: 3942, 2.663 samples/sec, batch_loss: 0.0725, batch_loss_c: 0.0698, batch_loss_s: 0.0789, time:15.0194, lr:1e-05\u001b[0m\n",
            "2019-12-08 11:11:33 \u001b[32mINFO     \u001b[0m train.py: [8/300], [80/484], step: 3952, 1.676 samples/sec, batch_loss: 0.2946, batch_loss_c: 0.2881, batch_loss_s: 0.3099, time:23.8689, lr:1e-05\u001b[0m\n",
            "2019-12-08 11:11:55 \u001b[32mINFO     \u001b[0m train.py: [8/300], [90/484], step: 3962, 1.794 samples/sec, batch_loss: 0.1239, batch_loss_c: 0.1234, batch_loss_s: 0.1248, time:22.3004, lr:1e-05\u001b[0m\n",
            "2019-12-08 11:12:13 \u001b[32mINFO     \u001b[0m train.py: [8/300], [100/484], step: 3972, 2.271 samples/sec, batch_loss: 0.0731, batch_loss_c: 0.0684, batch_loss_s: 0.0839, time:17.6121, lr:1e-05\u001b[0m\n",
            "2019-12-08 11:12:27 \u001b[32mINFO     \u001b[0m train.py: [8/300], [110/484], step: 3982, 2.722 samples/sec, batch_loss: 0.2875, batch_loss_c: 0.2772, batch_loss_s: 0.3117, time:14.6946, lr:1e-05\u001b[0m\n",
            "2019-12-08 11:12:42 \u001b[32mINFO     \u001b[0m train.py: [8/300], [120/484], step: 3992, 2.700 samples/sec, batch_loss: 0.0797, batch_loss_c: 0.0792, batch_loss_s: 0.0807, time:14.8155, lr:1e-05\u001b[0m\n",
            "2019-12-08 11:13:01 \u001b[32mINFO     \u001b[0m train.py: [8/300], [130/484], step: 4002, 2.195 samples/sec, batch_loss: 0.1138, batch_loss_c: 0.1122, batch_loss_s: 0.1174, time:18.2192, lr:1e-05\u001b[0m\n",
            "2019-12-08 11:13:18 \u001b[32mINFO     \u001b[0m train.py: [8/300], [140/484], step: 4012, 2.272 samples/sec, batch_loss: 0.0555, batch_loss_c: 0.0417, batch_loss_s: 0.0876, time:17.6058, lr:1e-05\u001b[0m\n",
            "2019-12-08 11:13:52 \u001b[32mINFO     \u001b[0m train.py: [8/300], [150/484], step: 4022, 1.194 samples/sec, batch_loss: 0.0525, batch_loss_c: 0.0458, batch_loss_s: 0.0679, time:33.5125, lr:1e-05\u001b[0m\n",
            "2019-12-08 11:14:07 \u001b[32mINFO     \u001b[0m train.py: [8/300], [160/484], step: 4032, 2.630 samples/sec, batch_loss: 0.1038, batch_loss_c: 0.0968, batch_loss_s: 0.1201, time:15.2072, lr:1e-05\u001b[0m\n",
            "2019-12-08 11:14:24 \u001b[32mINFO     \u001b[0m train.py: [8/300], [170/484], step: 4042, 2.320 samples/sec, batch_loss: 0.0655, batch_loss_c: 0.0579, batch_loss_s: 0.0832, time:17.2426, lr:1e-05\u001b[0m\n",
            "2019-12-08 11:14:56 \u001b[32mINFO     \u001b[0m train.py: [8/300], [180/484], step: 4052, 1.240 samples/sec, batch_loss: 0.0811, batch_loss_c: 0.0679, batch_loss_s: 0.1120, time:32.2617, lr:1e-05\u001b[0m\n",
            "2019-12-08 11:15:13 \u001b[32mINFO     \u001b[0m train.py: [8/300], [190/484], step: 4062, 2.335 samples/sec, batch_loss: 0.2974, batch_loss_c: 0.2933, batch_loss_s: 0.3070, time:17.1339, lr:1e-05\u001b[0m\n",
            "2019-12-08 11:15:31 \u001b[32mINFO     \u001b[0m train.py: [8/300], [200/484], step: 4072, 2.229 samples/sec, batch_loss: 0.0624, batch_loss_c: 0.0591, batch_loss_s: 0.0699, time:17.9471, lr:1e-05\u001b[0m\n",
            "2019-12-08 11:15:50 \u001b[32mINFO     \u001b[0m train.py: [8/300], [210/484], step: 4082, 2.211 samples/sec, batch_loss: 0.0975, batch_loss_c: 0.1000, batch_loss_s: 0.0918, time:18.0945, lr:1e-05\u001b[0m\n",
            "2019-12-08 11:16:06 \u001b[32mINFO     \u001b[0m train.py: [8/300], [220/484], step: 4092, 2.468 samples/sec, batch_loss: 0.0660, batch_loss_c: 0.0648, batch_loss_s: 0.0687, time:16.2063, lr:1e-05\u001b[0m\n",
            "2019-12-08 11:16:21 \u001b[32mINFO     \u001b[0m train.py: [8/300], [230/484], step: 4102, 2.704 samples/sec, batch_loss: 0.0358, batch_loss_c: 0.0287, batch_loss_s: 0.0524, time:14.7909, lr:1e-05\u001b[0m\n",
            "2019-12-08 11:16:42 \u001b[32mINFO     \u001b[0m train.py: [8/300], [240/484], step: 4112, 1.835 samples/sec, batch_loss: 0.0888, batch_loss_c: 0.0686, batch_loss_s: 0.1357, time:21.7978, lr:1e-05\u001b[0m\n",
            "2019-12-08 11:16:58 \u001b[32mINFO     \u001b[0m train.py: [8/300], [250/484], step: 4122, 2.552 samples/sec, batch_loss: 0.2728, batch_loss_c: 0.2645, batch_loss_s: 0.2923, time:15.6731, lr:1e-05\u001b[0m\n",
            "2019-12-08 11:17:23 \u001b[32mINFO     \u001b[0m train.py: [8/300], [260/484], step: 4132, 1.573 samples/sec, batch_loss: 0.0762, batch_loss_c: 0.0656, batch_loss_s: 0.1008, time:25.4282, lr:1e-05\u001b[0m\n",
            "2019-12-08 11:17:39 \u001b[32mINFO     \u001b[0m train.py: [8/300], [270/484], step: 4142, 2.618 samples/sec, batch_loss: 0.0685, batch_loss_c: 0.0580, batch_loss_s: 0.0930, time:15.2790, lr:1e-05\u001b[0m\n",
            "2019-12-08 11:17:59 \u001b[32mINFO     \u001b[0m train.py: [8/300], [280/484], step: 4152, 2.001 samples/sec, batch_loss: 0.2649, batch_loss_c: 0.2480, batch_loss_s: 0.3043, time:19.9866, lr:1e-05\u001b[0m\n",
            "2019-12-08 11:18:22 \u001b[32mINFO     \u001b[0m train.py: [8/300], [290/484], step: 4162, 1.697 samples/sec, batch_loss: 0.0601, batch_loss_c: 0.0521, batch_loss_s: 0.0788, time:23.5751, lr:1e-05\u001b[0m\n",
            "2019-12-08 11:18:41 \u001b[32mINFO     \u001b[0m train.py: [8/300], [300/484], step: 4172, 2.105 samples/sec, batch_loss: 0.0777, batch_loss_c: 0.0706, batch_loss_s: 0.0941, time:19.0023, lr:1e-05\u001b[0m\n",
            "2019-12-08 11:18:59 \u001b[32mINFO     \u001b[0m train.py: [8/300], [310/484], step: 4182, 2.229 samples/sec, batch_loss: 0.0596, batch_loss_c: 0.0534, batch_loss_s: 0.0739, time:17.9417, lr:1e-05\u001b[0m\n",
            "2019-12-08 11:19:21 \u001b[32mINFO     \u001b[0m train.py: [8/300], [320/484], step: 4192, 1.816 samples/sec, batch_loss: 0.0819, batch_loss_c: 0.0775, batch_loss_s: 0.0923, time:22.0264, lr:1e-05\u001b[0m\n",
            "2019-12-08 11:19:46 \u001b[32mINFO     \u001b[0m train.py: [8/300], [330/484], step: 4202, 1.609 samples/sec, batch_loss: 0.0949, batch_loss_c: 0.0828, batch_loss_s: 0.1231, time:24.8599, lr:1e-05\u001b[0m\n",
            "2019-12-08 11:20:01 \u001b[32mINFO     \u001b[0m train.py: [8/300], [340/484], step: 4212, 2.627 samples/sec, batch_loss: 0.1503, batch_loss_c: 0.1625, batch_loss_s: 0.1217, time:15.2291, lr:1e-05\u001b[0m\n",
            "2019-12-08 11:20:17 \u001b[32mINFO     \u001b[0m train.py: [8/300], [350/484], step: 4222, 2.477 samples/sec, batch_loss: 0.0998, batch_loss_c: 0.1042, batch_loss_s: 0.0894, time:16.1498, lr:1e-05\u001b[0m\n",
            "2019-12-08 11:20:36 \u001b[32mINFO     \u001b[0m train.py: [8/300], [360/484], step: 4232, 2.170 samples/sec, batch_loss: 0.1417, batch_loss_c: 0.0920, batch_loss_s: 0.2575, time:18.4348, lr:1e-05\u001b[0m\n",
            "2019-12-08 11:20:53 \u001b[32mINFO     \u001b[0m train.py: [8/300], [370/484], step: 4242, 2.358 samples/sec, batch_loss: 0.1071, batch_loss_c: 0.0984, batch_loss_s: 0.1274, time:16.9666, lr:1e-05\u001b[0m\n",
            "2019-12-08 11:21:14 \u001b[32mINFO     \u001b[0m train.py: [8/300], [380/484], step: 4252, 1.911 samples/sec, batch_loss: 0.0619, batch_loss_c: 0.0579, batch_loss_s: 0.0712, time:20.9317, lr:1e-05\u001b[0m\n",
            "2019-12-08 11:21:52 \u001b[32mINFO     \u001b[0m train.py: [8/300], [390/484], step: 4262, 1.042 samples/sec, batch_loss: 0.0533, batch_loss_c: 0.0484, batch_loss_s: 0.0648, time:38.3702, lr:1e-05\u001b[0m\n",
            "2019-12-08 11:22:06 \u001b[32mINFO     \u001b[0m train.py: [8/300], [400/484], step: 4272, 2.843 samples/sec, batch_loss: 0.0630, batch_loss_c: 0.0539, batch_loss_s: 0.0844, time:14.0717, lr:1e-05\u001b[0m\n",
            "2019-12-08 11:22:27 \u001b[32mINFO     \u001b[0m train.py: [8/300], [410/484], step: 4282, 1.885 samples/sec, batch_loss: 0.3009, batch_loss_c: 0.2975, batch_loss_s: 0.3091, time:21.2204, lr:1e-05\u001b[0m\n",
            "2019-12-08 11:22:45 \u001b[32mINFO     \u001b[0m train.py: [8/300], [420/484], step: 4292, 2.252 samples/sec, batch_loss: 0.0725, batch_loss_c: 0.0652, batch_loss_s: 0.0895, time:17.7613, lr:1e-05\u001b[0m\n",
            "2019-12-08 11:23:10 \u001b[32mINFO     \u001b[0m train.py: [8/300], [430/484], step: 4302, 1.603 samples/sec, batch_loss: 0.0596, batch_loss_c: 0.0516, batch_loss_s: 0.0784, time:24.9456, lr:1e-05\u001b[0m\n",
            "2019-12-08 11:23:27 \u001b[32mINFO     \u001b[0m train.py: [8/300], [440/484], step: 4312, 2.423 samples/sec, batch_loss: 0.2975, batch_loss_c: 0.2946, batch_loss_s: 0.3043, time:16.5111, lr:1e-05\u001b[0m\n",
            "2019-12-08 11:23:44 \u001b[32mINFO     \u001b[0m train.py: [8/300], [450/484], step: 4322, 2.277 samples/sec, batch_loss: 0.1107, batch_loss_c: 0.1037, batch_loss_s: 0.1269, time:17.5642, lr:1e-05\u001b[0m\n",
            "2019-12-08 11:24:09 \u001b[32mINFO     \u001b[0m train.py: [8/300], [460/484], step: 4332, 1.619 samples/sec, batch_loss: 0.0864, batch_loss_c: 0.0778, batch_loss_s: 0.1065, time:24.7071, lr:1e-05\u001b[0m\n",
            "2019-12-08 11:24:34 \u001b[32mINFO     \u001b[0m train.py: [8/300], [470/484], step: 4342, 1.577 samples/sec, batch_loss: 0.0512, batch_loss_c: 0.0460, batch_loss_s: 0.0633, time:25.3709, lr:1e-05\u001b[0m\n",
            "2019-12-08 11:24:51 \u001b[32mINFO     \u001b[0m train.py: [8/300], [480/484], step: 4352, 2.437 samples/sec, batch_loss: 0.0779, batch_loss_c: 0.0734, batch_loss_s: 0.0882, time:16.4167, lr:1e-05\u001b[0m\n",
            "2019-12-08 11:25:01 \u001b[32mINFO     \u001b[0m train.py: [8/300], train_loss: 0.1425, time: 972.5110, lr: 1e-05\u001b[0m\n",
            "2019-12-08 11:25:03 \u001b[32mINFO     \u001b[0m train.py: [9/300], [0/484], step: 4356, 26.836 samples/sec, batch_loss: 0.3001, batch_loss_c: 0.2899, batch_loss_s: 0.3241, time:1.4905, lr:1e-05\u001b[0m\n",
            "2019-12-08 11:25:24 \u001b[32mINFO     \u001b[0m train.py: [9/300], [10/484], step: 4366, 1.870 samples/sec, batch_loss: 0.0471, batch_loss_c: 0.0393, batch_loss_s: 0.0652, time:21.3879, lr:1e-05\u001b[0m\n",
            "2019-12-08 11:25:43 \u001b[32mINFO     \u001b[0m train.py: [9/300], [20/484], step: 4376, 2.080 samples/sec, batch_loss: 0.0553, batch_loss_c: 0.0518, batch_loss_s: 0.0635, time:19.2307, lr:1e-05\u001b[0m\n",
            "2019-12-08 11:26:02 \u001b[32mINFO     \u001b[0m train.py: [9/300], [30/484], step: 4386, 2.150 samples/sec, batch_loss: 0.5178, batch_loss_c: 0.5121, batch_loss_s: 0.5311, time:18.6063, lr:1e-05\u001b[0m\n",
            "2019-12-08 11:26:17 \u001b[32mINFO     \u001b[0m train.py: [9/300], [40/484], step: 4396, 2.627 samples/sec, batch_loss: 0.0668, batch_loss_c: 0.0583, batch_loss_s: 0.0868, time:15.2259, lr:1e-05\u001b[0m\n",
            "2019-12-08 11:26:54 \u001b[32mINFO     \u001b[0m train.py: [9/300], [50/484], step: 4406, 1.074 samples/sec, batch_loss: 0.3273, batch_loss_c: 0.3165, batch_loss_s: 0.3526, time:37.2369, lr:1e-05\u001b[0m\n",
            "2019-12-08 11:27:13 \u001b[32mINFO     \u001b[0m train.py: [9/300], [60/484], step: 4416, 2.149 samples/sec, batch_loss: 0.3101, batch_loss_c: 0.3122, batch_loss_s: 0.3052, time:18.6151, lr:1e-05\u001b[0m\n",
            "2019-12-08 11:27:34 \u001b[32mINFO     \u001b[0m train.py: [9/300], [70/484], step: 4426, 1.955 samples/sec, batch_loss: 0.0816, batch_loss_c: 0.0701, batch_loss_s: 0.1086, time:20.4576, lr:1e-05\u001b[0m\n",
            "2019-12-08 11:27:58 \u001b[32mINFO     \u001b[0m train.py: [9/300], [80/484], step: 4436, 1.620 samples/sec, batch_loss: 0.3396, batch_loss_c: 0.3269, batch_loss_s: 0.3692, time:24.6860, lr:1e-05\u001b[0m\n",
            "2019-12-08 11:28:17 \u001b[32mINFO     \u001b[0m train.py: [9/300], [90/484], step: 4446, 2.133 samples/sec, batch_loss: 0.2602, batch_loss_c: 0.2510, batch_loss_s: 0.2817, time:18.7550, lr:1e-05\u001b[0m\n",
            "2019-12-08 11:28:35 \u001b[32mINFO     \u001b[0m train.py: [9/300], [100/484], step: 4456, 2.278 samples/sec, batch_loss: 0.3828, batch_loss_c: 0.3469, batch_loss_s: 0.4664, time:17.5560, lr:1e-05\u001b[0m\n",
            "2019-12-08 11:28:52 \u001b[32mINFO     \u001b[0m train.py: [9/300], [110/484], step: 4466, 2.338 samples/sec, batch_loss: 0.2887, batch_loss_c: 0.2751, batch_loss_s: 0.3203, time:17.1057, lr:1e-05\u001b[0m\n",
            "2019-12-08 11:29:27 \u001b[32mINFO     \u001b[0m train.py: [9/300], [120/484], step: 4476, 1.124 samples/sec, batch_loss: 0.1490, batch_loss_c: 0.1720, batch_loss_s: 0.0953, time:35.5843, lr:1e-05\u001b[0m\n",
            "2019-12-08 11:29:57 \u001b[32mINFO     \u001b[0m train.py: [9/300], [130/484], step: 4486, 1.323 samples/sec, batch_loss: 0.0634, batch_loss_c: 0.0517, batch_loss_s: 0.0907, time:30.2382, lr:1e-05\u001b[0m\n",
            "2019-12-08 11:30:26 \u001b[32mINFO     \u001b[0m train.py: [9/300], [140/484], step: 4496, 1.416 samples/sec, batch_loss: 0.0498, batch_loss_c: 0.0408, batch_loss_s: 0.0706, time:28.2395, lr:1e-05\u001b[0m\n",
            "2019-12-08 11:30:46 \u001b[32mINFO     \u001b[0m train.py: [9/300], [150/484], step: 4506, 1.995 samples/sec, batch_loss: 0.0680, batch_loss_c: 0.0588, batch_loss_s: 0.0892, time:20.0497, lr:1e-05\u001b[0m\n",
            "2019-12-08 11:31:19 \u001b[32mINFO     \u001b[0m train.py: [9/300], [160/484], step: 4516, 1.220 samples/sec, batch_loss: 0.1212, batch_loss_c: 0.1021, batch_loss_s: 0.1659, time:32.7945, lr:1e-05\u001b[0m\n",
            "2019-12-08 11:31:37 \u001b[32mINFO     \u001b[0m train.py: [9/300], [170/484], step: 4526, 2.155 samples/sec, batch_loss: 0.0802, batch_loss_c: 0.0674, batch_loss_s: 0.1099, time:18.5645, lr:1e-05\u001b[0m\n",
            "2019-12-08 11:31:54 \u001b[32mINFO     \u001b[0m train.py: [9/300], [180/484], step: 4536, 2.308 samples/sec, batch_loss: 0.1605, batch_loss_c: 0.1519, batch_loss_s: 0.1805, time:17.3275, lr:1e-05\u001b[0m\n",
            "2019-12-08 11:32:12 \u001b[32mINFO     \u001b[0m train.py: [9/300], [190/484], step: 4546, 2.242 samples/sec, batch_loss: 0.0464, batch_loss_c: 0.0373, batch_loss_s: 0.0674, time:17.8451, lr:1e-05\u001b[0m\n",
            "2019-12-08 11:32:29 \u001b[32mINFO     \u001b[0m train.py: [9/300], [200/484], step: 4556, 2.338 samples/sec, batch_loss: 0.0621, batch_loss_c: 0.0533, batch_loss_s: 0.0826, time:17.1084, lr:1e-05\u001b[0m\n",
            "2019-12-08 11:32:52 \u001b[32mINFO     \u001b[0m train.py: [9/300], [210/484], step: 4566, 1.790 samples/sec, batch_loss: 0.1383, batch_loss_c: 0.1585, batch_loss_s: 0.0912, time:22.3424, lr:1e-05\u001b[0m\n",
            "2019-12-08 11:33:19 \u001b[32mINFO     \u001b[0m train.py: [9/300], [220/484], step: 4576, 1.469 samples/sec, batch_loss: 0.0505, batch_loss_c: 0.0433, batch_loss_s: 0.0673, time:27.2340, lr:1e-05\u001b[0m\n",
            "2019-12-08 11:33:39 \u001b[32mINFO     \u001b[0m train.py: [9/300], [230/484], step: 4586, 1.990 samples/sec, batch_loss: 0.0718, batch_loss_c: 0.0681, batch_loss_s: 0.0804, time:20.1043, lr:1e-05\u001b[0m\n",
            "2019-12-08 11:33:53 \u001b[32mINFO     \u001b[0m train.py: [9/300], [240/484], step: 4596, 2.934 samples/sec, batch_loss: 0.0837, batch_loss_c: 0.0729, batch_loss_s: 0.1089, time:13.6311, lr:1e-05\u001b[0m\n",
            "2019-12-08 11:34:08 \u001b[32mINFO     \u001b[0m train.py: [9/300], [250/484], step: 4606, 2.690 samples/sec, batch_loss: 0.2919, batch_loss_c: 0.2787, batch_loss_s: 0.3226, time:14.8679, lr:1e-05\u001b[0m\n",
            "2019-12-08 11:34:25 \u001b[32mINFO     \u001b[0m train.py: [9/300], [260/484], step: 4616, 2.359 samples/sec, batch_loss: 0.3638, batch_loss_c: 0.3596, batch_loss_s: 0.3735, time:16.9594, lr:1e-05\u001b[0m\n",
            "2019-12-08 11:35:06 \u001b[32mINFO     \u001b[0m train.py: [9/300], [270/484], step: 4626, 0.970 samples/sec, batch_loss: 0.1685, batch_loss_c: 0.1577, batch_loss_s: 0.1938, time:41.2179, lr:1e-05\u001b[0m\n",
            "2019-12-08 11:35:32 \u001b[32mINFO     \u001b[0m train.py: [9/300], [280/484], step: 4636, 1.514 samples/sec, batch_loss: 0.0443, batch_loss_c: 0.0360, batch_loss_s: 0.0638, time:26.4258, lr:1e-05\u001b[0m\n",
            "2019-12-08 11:35:55 \u001b[32mINFO     \u001b[0m train.py: [9/300], [290/484], step: 4646, 1.752 samples/sec, batch_loss: 0.0728, batch_loss_c: 0.0664, batch_loss_s: 0.0877, time:22.8349, lr:1e-05\u001b[0m\n",
            "2019-12-08 11:36:22 \u001b[32mINFO     \u001b[0m train.py: [9/300], [300/484], step: 4656, 1.497 samples/sec, batch_loss: 0.0987, batch_loss_c: 0.0864, batch_loss_s: 0.1272, time:26.7241, lr:1e-05\u001b[0m\n",
            "2019-12-08 11:36:37 \u001b[32mINFO     \u001b[0m train.py: [9/300], [310/484], step: 4666, 2.694 samples/sec, batch_loss: 0.0704, batch_loss_c: 0.0651, batch_loss_s: 0.0828, time:14.8488, lr:1e-05\u001b[0m\n",
            "tcmalloc: large alloc 6722592768 bytes == 0x7f1b83d62000 @  0x7f203bedd1e7 0x7f20304c1f71 0x7f203052555d 0x7f2030528e28 0x7f20305293e5 0x7f20305bffc2 0x50ac25 0x50d390 0x508245 0x50a080 0x50aa7d 0x50d390 0x509455 0x595311 0x5a067e 0x557d28 0x50c88b 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x509455 0x595311 0x5a522c 0x557f7e 0x4b5eff 0x50c810\n",
            "2019-12-08 11:37:31 \u001b[32mINFO     \u001b[0m train.py: [9/300], [320/484], step: 4676, 0.740 samples/sec, batch_loss: 0.0795, batch_loss_c: 0.0708, batch_loss_s: 0.0998, time:54.0823, lr:1e-05\u001b[0m\n",
            "2019-12-08 11:37:46 \u001b[32mINFO     \u001b[0m train.py: [9/300], [330/484], step: 4686, 2.616 samples/sec, batch_loss: 0.2826, batch_loss_c: 0.2753, batch_loss_s: 0.2995, time:15.2885, lr:1e-05\u001b[0m\n",
            "2019-12-08 11:38:21 \u001b[32mINFO     \u001b[0m train.py: [9/300], [340/484], step: 4696, 1.127 samples/sec, batch_loss: 0.0862, batch_loss_c: 0.0795, batch_loss_s: 0.1020, time:35.4806, lr:1e-05\u001b[0m\n",
            "2019-12-08 11:38:43 \u001b[32mINFO     \u001b[0m train.py: [9/300], [350/484], step: 4706, 1.855 samples/sec, batch_loss: 0.0665, batch_loss_c: 0.0679, batch_loss_s: 0.0633, time:21.5672, lr:1e-05\u001b[0m\n",
            "2019-12-08 11:39:05 \u001b[32mINFO     \u001b[0m train.py: [9/300], [360/484], step: 4716, 1.801 samples/sec, batch_loss: 0.5417, batch_loss_c: 0.5391, batch_loss_s: 0.5478, time:22.2151, lr:1e-05\u001b[0m\n",
            "2019-12-08 11:39:34 \u001b[32mINFO     \u001b[0m train.py: [9/300], [370/484], step: 4726, 1.408 samples/sec, batch_loss: 0.1039, batch_loss_c: 0.1131, batch_loss_s: 0.0822, time:28.4088, lr:1e-05\u001b[0m\n",
            "2019-12-08 11:40:05 \u001b[32mINFO     \u001b[0m train.py: [9/300], [380/484], step: 4736, 1.287 samples/sec, batch_loss: 0.2811, batch_loss_c: 0.2762, batch_loss_s: 0.2924, time:31.0911, lr:1e-05\u001b[0m\n",
            "2019-12-08 11:40:29 \u001b[32mINFO     \u001b[0m train.py: [9/300], [390/484], step: 4746, 1.654 samples/sec, batch_loss: 0.3055, batch_loss_c: 0.3041, batch_loss_s: 0.3087, time:24.1775, lr:1e-05\u001b[0m\n",
            "2019-12-08 11:40:44 \u001b[32mINFO     \u001b[0m train.py: [9/300], [400/484], step: 4756, 2.676 samples/sec, batch_loss: 0.0844, batch_loss_c: 0.0689, batch_loss_s: 0.1207, time:14.9504, lr:1e-05\u001b[0m\n",
            "2019-12-08 11:40:59 \u001b[32mINFO     \u001b[0m train.py: [9/300], [410/484], step: 4766, 2.698 samples/sec, batch_loss: 0.2909, batch_loss_c: 0.2854, batch_loss_s: 0.3037, time:14.8278, lr:1e-05\u001b[0m\n",
            "2019-12-08 11:41:17 \u001b[32mINFO     \u001b[0m train.py: [9/300], [420/484], step: 4776, 2.199 samples/sec, batch_loss: 0.2902, batch_loss_c: 0.2895, batch_loss_s: 0.2917, time:18.1883, lr:1e-05\u001b[0m\n",
            "2019-12-08 11:41:33 \u001b[32mINFO     \u001b[0m train.py: [9/300], [430/484], step: 4786, 2.523 samples/sec, batch_loss: 0.1540, batch_loss_c: 0.1425, batch_loss_s: 0.1808, time:15.8525, lr:1e-05\u001b[0m\n",
            "2019-12-08 11:42:00 \u001b[32mINFO     \u001b[0m train.py: [9/300], [440/484], step: 4796, 1.481 samples/sec, batch_loss: 0.0564, batch_loss_c: 0.0514, batch_loss_s: 0.0680, time:27.0040, lr:1e-05\u001b[0m\n",
            "2019-12-08 11:42:25 \u001b[32mINFO     \u001b[0m train.py: [9/300], [450/484], step: 4806, 1.553 samples/sec, batch_loss: 0.3542, batch_loss_c: 0.3461, batch_loss_s: 0.3729, time:25.7511, lr:1e-05\u001b[0m\n",
            "2019-12-08 11:42:42 \u001b[32mINFO     \u001b[0m train.py: [9/300], [460/484], step: 4816, 2.362 samples/sec, batch_loss: 0.0633, batch_loss_c: 0.0543, batch_loss_s: 0.0843, time:16.9384, lr:1e-05\u001b[0m\n",
            "2019-12-08 11:43:17 \u001b[32mINFO     \u001b[0m train.py: [9/300], [470/484], step: 4826, 1.150 samples/sec, batch_loss: 0.3489, batch_loss_c: 0.3384, batch_loss_s: 0.3733, time:34.7818, lr:1e-05\u001b[0m\n",
            "2019-12-08 11:43:41 \u001b[32mINFO     \u001b[0m train.py: [9/300], [480/484], step: 4836, 1.690 samples/sec, batch_loss: 0.0759, batch_loss_c: 0.0674, batch_loss_s: 0.0957, time:23.6655, lr:1e-05\u001b[0m\n",
            "2019-12-08 11:43:45 \u001b[32mINFO     \u001b[0m train.py: [9/300], train_loss: 0.1465, time: 1123.3879, lr: 1e-05\u001b[0m\n",
            "2019-12-08 11:43:46 \u001b[32mINFO     \u001b[0m train.py: [10/300], [0/484], step: 4840, 31.525 samples/sec, batch_loss: 0.1831, batch_loss_c: 0.2098, batch_loss_s: 0.1207, time:1.2688, lr:1e-05\u001b[0m\n",
            "2019-12-08 11:44:00 \u001b[32mINFO     \u001b[0m train.py: [10/300], [10/484], step: 4850, 2.857 samples/sec, batch_loss: 0.1300, batch_loss_c: 0.1182, batch_loss_s: 0.1577, time:14.0001, lr:1e-05\u001b[0m\n",
            "2019-12-08 11:44:24 \u001b[32mINFO     \u001b[0m train.py: [10/300], [20/484], step: 4860, 1.671 samples/sec, batch_loss: 0.0660, batch_loss_c: 0.0642, batch_loss_s: 0.0703, time:23.9434, lr:1e-05\u001b[0m\n",
            "2019-12-08 11:45:06 \u001b[32mINFO     \u001b[0m train.py: [10/300], [30/484], step: 4870, 0.952 samples/sec, batch_loss: 0.1574, batch_loss_c: 0.1610, batch_loss_s: 0.1491, time:42.0226, lr:1e-05\u001b[0m\n",
            "2019-12-08 11:45:27 \u001b[32mINFO     \u001b[0m train.py: [10/300], [40/484], step: 4880, 1.940 samples/sec, batch_loss: 0.0822, batch_loss_c: 0.0821, batch_loss_s: 0.0825, time:20.6179, lr:1e-05\u001b[0m\n",
            "2019-12-08 11:45:42 \u001b[32mINFO     \u001b[0m train.py: [10/300], [50/484], step: 4890, 2.655 samples/sec, batch_loss: 0.1039, batch_loss_c: 0.1016, batch_loss_s: 0.1092, time:15.0643, lr:1e-05\u001b[0m\n",
            "2019-12-08 11:46:12 \u001b[32mINFO     \u001b[0m train.py: [10/300], [60/484], step: 4900, 1.315 samples/sec, batch_loss: 0.3934, batch_loss_c: 0.3718, batch_loss_s: 0.4439, time:30.4290, lr:1e-05\u001b[0m\n",
            "2019-12-08 11:46:48 \u001b[32mINFO     \u001b[0m train.py: [10/300], [70/484], step: 4910, 1.131 samples/sec, batch_loss: 0.3144, batch_loss_c: 0.3016, batch_loss_s: 0.3443, time:35.3629, lr:1e-05\u001b[0m\n",
            "2019-12-08 11:47:04 \u001b[32mINFO     \u001b[0m train.py: [10/300], [80/484], step: 4920, 2.518 samples/sec, batch_loss: 0.0500, batch_loss_c: 0.0474, batch_loss_s: 0.0562, time:15.8852, lr:1e-05\u001b[0m\n",
            "2019-12-08 11:47:21 \u001b[32mINFO     \u001b[0m train.py: [10/300], [90/484], step: 4930, 2.307 samples/sec, batch_loss: 0.0865, batch_loss_c: 0.0683, batch_loss_s: 0.1291, time:17.3385, lr:1e-05\u001b[0m\n",
            "2019-12-08 11:47:37 \u001b[32mINFO     \u001b[0m train.py: [10/300], [100/484], step: 4940, 2.558 samples/sec, batch_loss: 0.0472, batch_loss_c: 0.0443, batch_loss_s: 0.0539, time:15.6343, lr:1e-05\u001b[0m\n",
            "2019-12-08 11:48:00 \u001b[32mINFO     \u001b[0m train.py: [10/300], [110/484], step: 4950, 1.702 samples/sec, batch_loss: 0.3085, batch_loss_c: 0.3066, batch_loss_s: 0.3130, time:23.4997, lr:1e-05\u001b[0m\n",
            "2019-12-08 11:48:15 \u001b[32mINFO     \u001b[0m train.py: [10/300], [120/484], step: 4960, 2.696 samples/sec, batch_loss: 0.1399, batch_loss_c: 0.1367, batch_loss_s: 0.1472, time:14.8384, lr:1e-05\u001b[0m\n",
            "2019-12-08 11:48:43 \u001b[32mINFO     \u001b[0m train.py: [10/300], [130/484], step: 4970, 1.427 samples/sec, batch_loss: 0.1908, batch_loss_c: 0.2241, batch_loss_s: 0.1130, time:28.0327, lr:1e-05\u001b[0m\n",
            "2019-12-08 11:49:04 \u001b[32mINFO     \u001b[0m train.py: [10/300], [140/484], step: 4980, 1.903 samples/sec, batch_loss: 0.2997, batch_loss_c: 0.2909, batch_loss_s: 0.3203, time:21.0162, lr:1e-05\u001b[0m\n",
            "2019-12-08 11:49:21 \u001b[32mINFO     \u001b[0m train.py: [10/300], [150/484], step: 4990, 2.316 samples/sec, batch_loss: 0.3626, batch_loss_c: 0.3392, batch_loss_s: 0.4172, time:17.2701, lr:1e-05\u001b[0m\n",
            "2019-12-08 11:49:39 \u001b[32mINFO     \u001b[0m train.py: [10/300], [160/484], step: 5000, 2.234 samples/sec, batch_loss: 0.2890, batch_loss_c: 0.2816, batch_loss_s: 0.3061, time:17.9077, lr:1e-05\u001b[0m\n",
            "2019-12-08 11:50:05 \u001b[32mINFO     \u001b[0m train.py: [10/300], [170/484], step: 5010, 1.545 samples/sec, batch_loss: 0.3678, batch_loss_c: 0.3785, batch_loss_s: 0.3427, time:25.8827, lr:1e-05\u001b[0m\n",
            "2019-12-08 11:50:25 \u001b[32mINFO     \u001b[0m train.py: [10/300], [180/484], step: 5020, 2.012 samples/sec, batch_loss: 0.3212, batch_loss_c: 0.2866, batch_loss_s: 0.4022, time:19.8781, lr:1e-05\u001b[0m\n",
            "2019-12-08 11:50:44 \u001b[32mINFO     \u001b[0m train.py: [10/300], [190/484], step: 5030, 2.100 samples/sec, batch_loss: 0.0623, batch_loss_c: 0.0648, batch_loss_s: 0.0566, time:19.0466, lr:1e-05\u001b[0m\n",
            "2019-12-08 11:51:16 \u001b[32mINFO     \u001b[0m train.py: [10/300], [200/484], step: 5040, 1.262 samples/sec, batch_loss: 0.0403, batch_loss_c: 0.0386, batch_loss_s: 0.0441, time:31.6973, lr:1e-05\u001b[0m\n",
            "2019-12-08 11:51:35 \u001b[32mINFO     \u001b[0m train.py: [10/300], [210/484], step: 5050, 2.123 samples/sec, batch_loss: 0.3018, batch_loss_c: 0.2852, batch_loss_s: 0.3403, time:18.8372, lr:1e-05\u001b[0m\n",
            "2019-12-08 11:51:57 \u001b[32mINFO     \u001b[0m train.py: [10/300], [220/484], step: 5060, 1.747 samples/sec, batch_loss: 0.1429, batch_loss_c: 0.1485, batch_loss_s: 0.1296, time:22.8971, lr:1e-05\u001b[0m\n",
            "2019-12-08 11:52:16 \u001b[32mINFO     \u001b[0m train.py: [10/300], [230/484], step: 5070, 2.105 samples/sec, batch_loss: 0.0782, batch_loss_c: 0.0746, batch_loss_s: 0.0865, time:18.9982, lr:1e-05\u001b[0m\n",
            "2019-12-08 11:52:42 \u001b[32mINFO     \u001b[0m train.py: [10/300], [240/484], step: 5080, 1.555 samples/sec, batch_loss: 0.1152, batch_loss_c: 0.1059, batch_loss_s: 0.1369, time:25.7281, lr:1e-05\u001b[0m\n",
            "2019-12-08 11:53:03 \u001b[32mINFO     \u001b[0m train.py: [10/300], [250/484], step: 5090, 1.931 samples/sec, batch_loss: 0.1770, batch_loss_c: 0.1712, batch_loss_s: 0.1904, time:20.7113, lr:1e-05\u001b[0m\n",
            "2019-12-08 11:53:23 \u001b[32mINFO     \u001b[0m train.py: [10/300], [260/484], step: 5100, 1.988 samples/sec, batch_loss: 0.1330, batch_loss_c: 0.1149, batch_loss_s: 0.1751, time:20.1243, lr:1e-05\u001b[0m\n",
            "2019-12-08 11:53:38 \u001b[32mINFO     \u001b[0m train.py: [10/300], [270/484], step: 5110, 2.640 samples/sec, batch_loss: 0.0701, batch_loss_c: 0.0552, batch_loss_s: 0.1050, time:15.1522, lr:1e-05\u001b[0m\n",
            "2019-12-08 11:53:53 \u001b[32mINFO     \u001b[0m train.py: [10/300], [280/484], step: 5120, 2.753 samples/sec, batch_loss: 0.0931, batch_loss_c: 0.0865, batch_loss_s: 0.1085, time:14.5297, lr:1e-05\u001b[0m\n",
            "2019-12-08 11:54:09 \u001b[32mINFO     \u001b[0m train.py: [10/300], [290/484], step: 5130, 2.465 samples/sec, batch_loss: 0.2753, batch_loss_c: 0.2707, batch_loss_s: 0.2860, time:16.2267, lr:1e-05\u001b[0m\n",
            "2019-12-08 11:54:29 \u001b[32mINFO     \u001b[0m train.py: [10/300], [300/484], step: 5140, 2.029 samples/sec, batch_loss: 0.5388, batch_loss_c: 0.5318, batch_loss_s: 0.5552, time:19.7139, lr:1e-05\u001b[0m\n",
            "2019-12-08 11:54:47 \u001b[32mINFO     \u001b[0m train.py: [10/300], [310/484], step: 5150, 2.185 samples/sec, batch_loss: 0.2425, batch_loss_c: 0.2267, batch_loss_s: 0.2794, time:18.3093, lr:1e-05\u001b[0m\n",
            "2019-12-08 11:55:05 \u001b[32mINFO     \u001b[0m train.py: [10/300], [320/484], step: 5160, 2.243 samples/sec, batch_loss: 0.1205, batch_loss_c: 0.1251, batch_loss_s: 0.1096, time:17.8313, lr:1e-05\u001b[0m\n",
            "2019-12-08 11:55:19 \u001b[32mINFO     \u001b[0m train.py: [10/300], [330/484], step: 5170, 2.717 samples/sec, batch_loss: 0.0601, batch_loss_c: 0.0481, batch_loss_s: 0.0882, time:14.7234, lr:1e-05\u001b[0m\n",
            "2019-12-08 11:55:33 \u001b[32mINFO     \u001b[0m train.py: [10/300], [340/484], step: 5180, 2.904 samples/sec, batch_loss: 0.0717, batch_loss_c: 0.0645, batch_loss_s: 0.0885, time:13.7736, lr:1e-05\u001b[0m\n",
            "2019-12-08 11:55:54 \u001b[32mINFO     \u001b[0m train.py: [10/300], [350/484], step: 5190, 1.905 samples/sec, batch_loss: 0.0825, batch_loss_c: 0.0669, batch_loss_s: 0.1189, time:20.9972, lr:1e-05\u001b[0m\n",
            "2019-12-08 11:56:11 \u001b[32mINFO     \u001b[0m train.py: [10/300], [360/484], step: 5200, 2.324 samples/sec, batch_loss: 0.0830, batch_loss_c: 0.0741, batch_loss_s: 0.1036, time:17.2108, lr:1e-05\u001b[0m\n",
            "2019-12-08 11:56:37 \u001b[32mINFO     \u001b[0m train.py: [10/300], [370/484], step: 5210, 1.583 samples/sec, batch_loss: 0.0914, batch_loss_c: 0.0877, batch_loss_s: 0.1001, time:25.2719, lr:1e-05\u001b[0m\n",
            "2019-12-08 11:56:53 \u001b[32mINFO     \u001b[0m train.py: [10/300], [380/484], step: 5220, 2.504 samples/sec, batch_loss: 0.2934, batch_loss_c: 0.2902, batch_loss_s: 0.3009, time:15.9758, lr:1e-05\u001b[0m\n",
            "2019-12-08 11:57:17 \u001b[32mINFO     \u001b[0m train.py: [10/300], [390/484], step: 5230, 1.678 samples/sec, batch_loss: 0.1655, batch_loss_c: 0.1594, batch_loss_s: 0.1797, time:23.8364, lr:1e-05\u001b[0m\n",
            "2019-12-08 11:57:31 \u001b[32mINFO     \u001b[0m train.py: [10/300], [400/484], step: 5240, 2.850 samples/sec, batch_loss: 0.0690, batch_loss_c: 0.0616, batch_loss_s: 0.0860, time:14.0372, lr:1e-05\u001b[0m\n",
            "2019-12-08 11:57:48 \u001b[32mINFO     \u001b[0m train.py: [10/300], [410/484], step: 5250, 2.329 samples/sec, batch_loss: 0.0973, batch_loss_c: 0.0792, batch_loss_s: 0.1396, time:17.1733, lr:1e-05\u001b[0m\n",
            "2019-12-08 11:58:06 \u001b[32mINFO     \u001b[0m train.py: [10/300], [420/484], step: 5260, 2.181 samples/sec, batch_loss: 0.2947, batch_loss_c: 0.2876, batch_loss_s: 0.3114, time:18.3367, lr:1e-05\u001b[0m\n",
            "2019-12-08 11:58:25 \u001b[32mINFO     \u001b[0m train.py: [10/300], [430/484], step: 5270, 2.086 samples/sec, batch_loss: 0.0624, batch_loss_c: 0.0571, batch_loss_s: 0.0749, time:19.1758, lr:1e-05\u001b[0m\n",
            "2019-12-08 11:58:42 \u001b[32mINFO     \u001b[0m train.py: [10/300], [440/484], step: 5280, 2.329 samples/sec, batch_loss: 0.1305, batch_loss_c: 0.1177, batch_loss_s: 0.1604, time:17.1740, lr:1e-05\u001b[0m\n",
            "2019-12-08 11:59:07 \u001b[32mINFO     \u001b[0m train.py: [10/300], [450/484], step: 5290, 1.636 samples/sec, batch_loss: 0.1331, batch_loss_c: 0.0782, batch_loss_s: 0.2613, time:24.4505, lr:1e-05\u001b[0m\n",
            "2019-12-08 11:59:25 \u001b[32mINFO     \u001b[0m train.py: [10/300], [460/484], step: 5300, 2.258 samples/sec, batch_loss: 0.0560, batch_loss_c: 0.0482, batch_loss_s: 0.0742, time:17.7115, lr:1e-05\u001b[0m\n",
            "2019-12-08 11:59:59 \u001b[32mINFO     \u001b[0m train.py: [10/300], [470/484], step: 5310, 1.180 samples/sec, batch_loss: 0.0655, batch_loss_c: 0.0590, batch_loss_s: 0.0807, time:33.9120, lr:1e-05\u001b[0m\n",
            "2019-12-08 12:00:17 \u001b[32mINFO     \u001b[0m train.py: [10/300], [480/484], step: 5320, 2.113 samples/sec, batch_loss: 0.0524, batch_loss_c: 0.0430, batch_loss_s: 0.0743, time:18.9334, lr:1e-05\u001b[0m\n",
            "2019-12-08 12:00:47 \u001b[32mINFO     \u001b[0m train.py: [10/300], train_loss: 0.1424, time: 1021.4658, lr: 1e-05\u001b[0m\n",
            "2019-12-08 12:00:49 \u001b[32mINFO     \u001b[0m train.py: [11/300], [0/484], step: 5324, 24.484 samples/sec, batch_loss: 0.0845, batch_loss_c: 0.0741, batch_loss_s: 0.1086, time:1.6337, lr:1e-05\u001b[0m\n",
            "2019-12-08 12:01:05 \u001b[32mINFO     \u001b[0m train.py: [11/300], [10/484], step: 5334, 2.497 samples/sec, batch_loss: 0.1257, batch_loss_c: 0.1169, batch_loss_s: 0.1462, time:16.0204, lr:1e-05\u001b[0m\n",
            "2019-12-08 12:01:21 \u001b[32mINFO     \u001b[0m train.py: [11/300], [20/484], step: 5344, 2.425 samples/sec, batch_loss: 0.0523, batch_loss_c: 0.0507, batch_loss_s: 0.0559, time:16.4958, lr:1e-05\u001b[0m\n",
            "2019-12-08 12:01:39 \u001b[32mINFO     \u001b[0m train.py: [11/300], [30/484], step: 5354, 2.271 samples/sec, batch_loss: 0.0526, batch_loss_c: 0.0453, batch_loss_s: 0.0694, time:17.6099, lr:1e-05\u001b[0m\n",
            "2019-12-08 12:01:54 \u001b[32mINFO     \u001b[0m train.py: [11/300], [40/484], step: 5364, 2.702 samples/sec, batch_loss: 0.0682, batch_loss_c: 0.0635, batch_loss_s: 0.0789, time:14.8031, lr:1e-05\u001b[0m\n",
            "2019-12-08 12:02:13 \u001b[32mINFO     \u001b[0m train.py: [11/300], [50/484], step: 5374, 2.095 samples/sec, batch_loss: 0.0831, batch_loss_c: 0.0678, batch_loss_s: 0.1186, time:19.0925, lr:1e-05\u001b[0m\n",
            "2019-12-08 12:02:30 \u001b[32mINFO     \u001b[0m train.py: [11/300], [60/484], step: 5384, 2.298 samples/sec, batch_loss: 0.0645, batch_loss_c: 0.0644, batch_loss_s: 0.0648, time:17.4080, lr:1e-05\u001b[0m\n",
            "2019-12-08 12:02:47 \u001b[32mINFO     \u001b[0m train.py: [11/300], [70/484], step: 5394, 2.305 samples/sec, batch_loss: 0.0607, batch_loss_c: 0.0523, batch_loss_s: 0.0804, time:17.3552, lr:1e-05\u001b[0m\n",
            "2019-12-08 12:03:05 \u001b[32mINFO     \u001b[0m train.py: [11/300], [80/484], step: 5404, 2.318 samples/sec, batch_loss: 0.0862, batch_loss_c: 0.0776, batch_loss_s: 0.1063, time:17.2545, lr:1e-05\u001b[0m\n",
            "2019-12-08 12:03:27 \u001b[32mINFO     \u001b[0m train.py: [11/300], [90/484], step: 5414, 1.811 samples/sec, batch_loss: 0.0903, batch_loss_c: 0.0939, batch_loss_s: 0.0818, time:22.0922, lr:1e-05\u001b[0m\n",
            "2019-12-08 12:03:54 \u001b[32mINFO     \u001b[0m train.py: [11/300], [100/484], step: 5424, 1.458 samples/sec, batch_loss: 0.0894, batch_loss_c: 0.0831, batch_loss_s: 0.1042, time:27.4350, lr:1e-05\u001b[0m\n",
            "2019-12-08 12:04:23 \u001b[32mINFO     \u001b[0m train.py: [11/300], [110/484], step: 5434, 1.390 samples/sec, batch_loss: 0.0758, batch_loss_c: 0.0691, batch_loss_s: 0.0915, time:28.7828, lr:1e-05\u001b[0m\n",
            "2019-12-08 12:04:37 \u001b[32mINFO     \u001b[0m train.py: [11/300], [120/484], step: 5444, 2.763 samples/sec, batch_loss: 0.0873, batch_loss_c: 0.0832, batch_loss_s: 0.0968, time:14.4782, lr:1e-05\u001b[0m\n",
            "2019-12-08 12:04:54 \u001b[32mINFO     \u001b[0m train.py: [11/300], [130/484], step: 5454, 2.481 samples/sec, batch_loss: 0.1148, batch_loss_c: 0.0878, batch_loss_s: 0.1779, time:16.1228, lr:1e-05\u001b[0m\n",
            "2019-12-08 12:05:11 \u001b[32mINFO     \u001b[0m train.py: [11/300], [140/484], step: 5464, 2.235 samples/sec, batch_loss: 0.0963, batch_loss_c: 0.0837, batch_loss_s: 0.1257, time:17.8943, lr:1e-05\u001b[0m\n",
            "2019-12-08 12:05:28 \u001b[32mINFO     \u001b[0m train.py: [11/300], [150/484], step: 5474, 2.395 samples/sec, batch_loss: 0.1114, batch_loss_c: 0.1135, batch_loss_s: 0.1065, time:16.7043, lr:1e-05\u001b[0m\n",
            "2019-12-08 12:05:45 \u001b[32mINFO     \u001b[0m train.py: [11/300], [160/484], step: 5484, 2.372 samples/sec, batch_loss: 0.0838, batch_loss_c: 0.0692, batch_loss_s: 0.1179, time:16.8657, lr:1e-05\u001b[0m\n",
            "2019-12-08 12:06:04 \u001b[32mINFO     \u001b[0m train.py: [11/300], [170/484], step: 5494, 2.096 samples/sec, batch_loss: 0.0886, batch_loss_c: 0.0896, batch_loss_s: 0.0864, time:19.0841, lr:1e-05\u001b[0m\n",
            "2019-12-08 12:06:22 \u001b[32mINFO     \u001b[0m train.py: [11/300], [180/484], step: 5504, 2.218 samples/sec, batch_loss: 0.1299, batch_loss_c: 0.1038, batch_loss_s: 0.1911, time:18.0341, lr:1e-05\u001b[0m\n",
            "2019-12-08 12:06:39 \u001b[32mINFO     \u001b[0m train.py: [11/300], [190/484], step: 5514, 2.360 samples/sec, batch_loss: 0.3796, batch_loss_c: 0.3742, batch_loss_s: 0.3921, time:16.9456, lr:1e-05\u001b[0m\n",
            "2019-12-08 12:06:57 \u001b[32mINFO     \u001b[0m train.py: [11/300], [200/484], step: 5524, 2.222 samples/sec, batch_loss: 0.1195, batch_loss_c: 0.1065, batch_loss_s: 0.1499, time:17.9988, lr:1e-05\u001b[0m\n",
            "2019-12-08 12:07:13 \u001b[32mINFO     \u001b[0m train.py: [11/300], [210/484], step: 5534, 2.474 samples/sec, batch_loss: 0.0403, batch_loss_c: 0.0345, batch_loss_s: 0.0540, time:16.1655, lr:1e-05\u001b[0m\n",
            "2019-12-08 12:07:33 \u001b[32mINFO     \u001b[0m train.py: [11/300], [220/484], step: 5544, 2.047 samples/sec, batch_loss: 0.0423, batch_loss_c: 0.0371, batch_loss_s: 0.0544, time:19.5372, lr:1e-05\u001b[0m\n",
            "2019-12-08 12:07:50 \u001b[32mINFO     \u001b[0m train.py: [11/300], [230/484], step: 5554, 2.302 samples/sec, batch_loss: 0.1256, batch_loss_c: 0.1061, batch_loss_s: 0.1710, time:17.3736, lr:1e-05\u001b[0m\n",
            "2019-12-08 12:08:10 \u001b[32mINFO     \u001b[0m train.py: [11/300], [240/484], step: 5564, 1.983 samples/sec, batch_loss: 0.0683, batch_loss_c: 0.0666, batch_loss_s: 0.0721, time:20.1714, lr:1e-05\u001b[0m\n",
            "2019-12-08 12:08:25 \u001b[32mINFO     \u001b[0m train.py: [11/300], [250/484], step: 5574, 2.651 samples/sec, batch_loss: 0.0383, batch_loss_c: 0.0286, batch_loss_s: 0.0607, time:15.0910, lr:1e-05\u001b[0m\n",
            "2019-12-08 12:08:41 \u001b[32mINFO     \u001b[0m train.py: [11/300], [260/484], step: 5584, 2.607 samples/sec, batch_loss: 0.0735, batch_loss_c: 0.0632, batch_loss_s: 0.0975, time:15.3439, lr:1e-05\u001b[0m\n",
            "2019-12-08 12:08:58 \u001b[32mINFO     \u001b[0m train.py: [11/300], [270/484], step: 5594, 2.259 samples/sec, batch_loss: 0.1239, batch_loss_c: 0.1240, batch_loss_s: 0.1238, time:17.7053, lr:1e-05\u001b[0m\n",
            "2019-12-08 12:09:19 \u001b[32mINFO     \u001b[0m train.py: [11/300], [280/484], step: 5604, 1.964 samples/sec, batch_loss: 0.0935, batch_loss_c: 0.0874, batch_loss_s: 0.1076, time:20.3714, lr:1e-05\u001b[0m\n",
            "2019-12-08 12:09:43 \u001b[32mINFO     \u001b[0m train.py: [11/300], [290/484], step: 5614, 1.668 samples/sec, batch_loss: 0.1485, batch_loss_c: 0.1482, batch_loss_s: 0.1492, time:23.9780, lr:1e-05\u001b[0m\n",
            "2019-12-08 12:10:00 \u001b[32mINFO     \u001b[0m train.py: [11/300], [300/484], step: 5624, 2.262 samples/sec, batch_loss: 0.0613, batch_loss_c: 0.0570, batch_loss_s: 0.0715, time:17.6796, lr:1e-05\u001b[0m\n",
            "2019-12-08 12:10:22 \u001b[32mINFO     \u001b[0m train.py: [11/300], [310/484], step: 5634, 1.838 samples/sec, batch_loss: 0.0516, batch_loss_c: 0.0454, batch_loss_s: 0.0660, time:21.7621, lr:1e-05\u001b[0m\n",
            "2019-12-08 12:10:46 \u001b[32mINFO     \u001b[0m train.py: [11/300], [320/484], step: 5644, 1.708 samples/sec, batch_loss: 0.3473, batch_loss_c: 0.3494, batch_loss_s: 0.3425, time:23.4182, lr:1e-05\u001b[0m\n",
            "2019-12-08 12:11:11 \u001b[32mINFO     \u001b[0m train.py: [11/300], [330/484], step: 5654, 1.595 samples/sec, batch_loss: 0.0558, batch_loss_c: 0.0473, batch_loss_s: 0.0755, time:25.0824, lr:1e-05\u001b[0m\n",
            "2019-12-08 12:11:27 \u001b[32mINFO     \u001b[0m train.py: [11/300], [340/484], step: 5664, 2.525 samples/sec, batch_loss: 0.0444, batch_loss_c: 0.0371, batch_loss_s: 0.0614, time:15.8393, lr:1e-05\u001b[0m\n",
            "2019-12-08 12:11:43 \u001b[32mINFO     \u001b[0m train.py: [11/300], [350/484], step: 5674, 2.424 samples/sec, batch_loss: 0.0894, batch_loss_c: 0.0855, batch_loss_s: 0.0984, time:16.5020, lr:1e-05\u001b[0m\n",
            "2019-12-08 12:12:27 \u001b[32mINFO     \u001b[0m train.py: [11/300], [360/484], step: 5684, 0.920 samples/sec, batch_loss: 0.1453, batch_loss_c: 0.1198, batch_loss_s: 0.2050, time:43.4629, lr:1e-05\u001b[0m\n",
            "2019-12-08 12:12:52 \u001b[32mINFO     \u001b[0m train.py: [11/300], [370/484], step: 5694, 1.603 samples/sec, batch_loss: 0.3630, batch_loss_c: 0.3729, batch_loss_s: 0.3398, time:24.9483, lr:1e-05\u001b[0m\n",
            "2019-12-08 12:13:09 \u001b[32mINFO     \u001b[0m train.py: [11/300], [380/484], step: 5704, 2.225 samples/sec, batch_loss: 0.0646, batch_loss_c: 0.0580, batch_loss_s: 0.0800, time:17.9806, lr:1e-05\u001b[0m\n",
            "2019-12-08 12:13:26 \u001b[32mINFO     \u001b[0m train.py: [11/300], [390/484], step: 5714, 2.437 samples/sec, batch_loss: 0.3048, batch_loss_c: 0.2984, batch_loss_s: 0.3197, time:16.4131, lr:1e-05\u001b[0m\n",
            "2019-12-08 12:13:43 \u001b[32mINFO     \u001b[0m train.py: [11/300], [400/484], step: 5724, 2.401 samples/sec, batch_loss: 0.0649, batch_loss_c: 0.0583, batch_loss_s: 0.0804, time:16.6579, lr:1e-05\u001b[0m\n",
            "2019-12-08 12:14:01 \u001b[32mINFO     \u001b[0m train.py: [11/300], [410/484], step: 5734, 2.178 samples/sec, batch_loss: 0.0689, batch_loss_c: 0.0584, batch_loss_s: 0.0932, time:18.3671, lr:1e-05\u001b[0m\n",
            "2019-12-08 12:14:29 \u001b[32mINFO     \u001b[0m train.py: [11/300], [420/484], step: 5744, 1.433 samples/sec, batch_loss: 0.0495, batch_loss_c: 0.0403, batch_loss_s: 0.0711, time:27.9180, lr:1e-05\u001b[0m\n",
            "2019-12-08 12:14:47 \u001b[32mINFO     \u001b[0m train.py: [11/300], [430/484], step: 5754, 2.191 samples/sec, batch_loss: 0.0660, batch_loss_c: 0.0588, batch_loss_s: 0.0829, time:18.2534, lr:1e-05\u001b[0m\n",
            "2019-12-08 12:15:02 \u001b[32mINFO     \u001b[0m train.py: [11/300], [440/484], step: 5764, 2.603 samples/sec, batch_loss: 0.2799, batch_loss_c: 0.2715, batch_loss_s: 0.2995, time:15.3693, lr:1e-05\u001b[0m\n",
            "2019-12-08 12:15:21 \u001b[32mINFO     \u001b[0m train.py: [11/300], [450/484], step: 5774, 2.146 samples/sec, batch_loss: 0.0681, batch_loss_c: 0.0594, batch_loss_s: 0.0883, time:18.6388, lr:1e-05\u001b[0m\n",
            "2019-12-08 12:16:17 \u001b[32mINFO     \u001b[0m train.py: [11/300], [460/484], step: 5784, 0.715 samples/sec, batch_loss: 0.2874, batch_loss_c: 0.2817, batch_loss_s: 0.3007, time:55.9324, lr:1e-05\u001b[0m\n",
            "2019-12-08 12:16:46 \u001b[32mINFO     \u001b[0m train.py: [11/300], [470/484], step: 5794, 1.370 samples/sec, batch_loss: 0.2833, batch_loss_c: 0.2774, batch_loss_s: 0.2969, time:29.1908, lr:1e-05\u001b[0m\n",
            "2019-12-08 12:17:11 \u001b[32mINFO     \u001b[0m train.py: [11/300], [480/484], step: 5804, 1.637 samples/sec, batch_loss: 0.2126, batch_loss_c: 0.1963, batch_loss_s: 0.2506, time:24.4359, lr:1e-05\u001b[0m\n",
            "2019-12-08 12:17:15 \u001b[32mINFO     \u001b[0m train.py: [11/300], train_loss: 0.1415, time: 988.0430, lr: 1e-05\u001b[0m\n",
            "2019-12-08 12:17:17 \u001b[32mINFO     \u001b[0m train.py: [12/300], [0/484], step: 5808, 26.062 samples/sec, batch_loss: 0.0677, batch_loss_c: 0.0588, batch_loss_s: 0.0885, time:1.5348, lr:1e-05\u001b[0m\n",
            "2019-12-08 12:17:33 \u001b[32mINFO     \u001b[0m train.py: [12/300], [10/484], step: 5818, 2.468 samples/sec, batch_loss: 0.3345, batch_loss_c: 0.3219, batch_loss_s: 0.3641, time:16.2089, lr:1e-05\u001b[0m\n",
            "2019-12-08 12:18:00 \u001b[32mINFO     \u001b[0m train.py: [12/300], [20/484], step: 5828, 1.483 samples/sec, batch_loss: 0.1332, batch_loss_c: 0.1222, batch_loss_s: 0.1589, time:26.9793, lr:1e-05\u001b[0m\n",
            "2019-12-08 12:18:18 \u001b[32mINFO     \u001b[0m train.py: [12/300], [30/484], step: 5838, 2.241 samples/sec, batch_loss: 0.0794, batch_loss_c: 0.0691, batch_loss_s: 0.1034, time:17.8526, lr:1e-05\u001b[0m\n",
            "2019-12-08 12:18:37 \u001b[32mINFO     \u001b[0m train.py: [12/300], [40/484], step: 5848, 2.101 samples/sec, batch_loss: 0.2617, batch_loss_c: 0.2778, batch_loss_s: 0.2240, time:19.0391, lr:1e-05\u001b[0m\n",
            "2019-12-08 12:18:57 \u001b[32mINFO     \u001b[0m train.py: [12/300], [50/484], step: 5858, 2.031 samples/sec, batch_loss: 0.0565, batch_loss_c: 0.0513, batch_loss_s: 0.0685, time:19.6939, lr:1e-05\u001b[0m\n",
            "2019-12-08 12:19:31 \u001b[32mINFO     \u001b[0m train.py: [12/300], [60/484], step: 5868, 1.167 samples/sec, batch_loss: 0.0529, batch_loss_c: 0.0410, batch_loss_s: 0.0804, time:34.2644, lr:1e-05\u001b[0m\n",
            "2019-12-08 12:19:55 \u001b[32mINFO     \u001b[0m train.py: [12/300], [70/484], step: 5878, 1.689 samples/sec, batch_loss: 0.3292, batch_loss_c: 0.3097, batch_loss_s: 0.3749, time:23.6840, lr:1e-05\u001b[0m\n",
            "2019-12-08 12:20:21 \u001b[32mINFO     \u001b[0m train.py: [12/300], [80/484], step: 5888, 1.503 samples/sec, batch_loss: 0.1661, batch_loss_c: 0.1833, batch_loss_s: 0.1262, time:26.6206, lr:1e-05\u001b[0m\n",
            "2019-12-08 12:20:40 \u001b[32mINFO     \u001b[0m train.py: [12/300], [90/484], step: 5898, 2.142 samples/sec, batch_loss: 0.0635, batch_loss_c: 0.0596, batch_loss_s: 0.0726, time:18.6713, lr:1e-05\u001b[0m\n",
            "2019-12-08 12:21:14 \u001b[32mINFO     \u001b[0m train.py: [12/300], [100/484], step: 5908, 1.188 samples/sec, batch_loss: 0.0682, batch_loss_c: 0.0608, batch_loss_s: 0.0854, time:33.6595, lr:1e-05\u001b[0m\n",
            "2019-12-08 12:21:28 \u001b[32mINFO     \u001b[0m train.py: [12/300], [110/484], step: 5918, 2.844 samples/sec, batch_loss: 0.0608, batch_loss_c: 0.0551, batch_loss_s: 0.0739, time:14.0656, lr:1e-05\u001b[0m\n",
            "2019-12-08 12:21:49 \u001b[32mINFO     \u001b[0m train.py: [12/300], [120/484], step: 5928, 1.884 samples/sec, batch_loss: 0.0696, batch_loss_c: 0.0602, batch_loss_s: 0.0916, time:21.2327, lr:1e-05\u001b[0m\n",
            "2019-12-08 12:22:12 \u001b[32mINFO     \u001b[0m train.py: [12/300], [130/484], step: 5938, 1.693 samples/sec, batch_loss: 0.4998, batch_loss_c: 0.4886, batch_loss_s: 0.5259, time:23.6206, lr:1e-05\u001b[0m\n",
            "2019-12-08 12:22:28 \u001b[32mINFO     \u001b[0m train.py: [12/300], [140/484], step: 5948, 2.656 samples/sec, batch_loss: 0.0845, batch_loss_c: 0.0877, batch_loss_s: 0.0772, time:15.0618, lr:1e-05\u001b[0m\n",
            "2019-12-08 12:22:46 \u001b[32mINFO     \u001b[0m train.py: [12/300], [150/484], step: 5958, 2.193 samples/sec, batch_loss: 0.0701, batch_loss_c: 0.0622, batch_loss_s: 0.0885, time:18.2380, lr:1e-05\u001b[0m\n",
            "2019-12-08 12:23:01 \u001b[32mINFO     \u001b[0m train.py: [12/300], [160/484], step: 5968, 2.556 samples/sec, batch_loss: 0.1477, batch_loss_c: 0.1400, batch_loss_s: 0.1657, time:15.6467, lr:1e-05\u001b[0m\n",
            "2019-12-08 12:23:21 \u001b[32mINFO     \u001b[0m train.py: [12/300], [170/484], step: 5978, 2.079 samples/sec, batch_loss: 0.2839, batch_loss_c: 0.2795, batch_loss_s: 0.2944, time:19.2354, lr:1e-05\u001b[0m\n",
            "2019-12-08 12:23:38 \u001b[32mINFO     \u001b[0m train.py: [12/300], [180/484], step: 5988, 2.302 samples/sec, batch_loss: 0.0697, batch_loss_c: 0.0516, batch_loss_s: 0.1122, time:17.3761, lr:1e-05\u001b[0m\n",
            "2019-12-08 12:24:07 \u001b[32mINFO     \u001b[0m train.py: [12/300], [190/484], step: 5998, 1.382 samples/sec, batch_loss: 0.0539, batch_loss_c: 0.0483, batch_loss_s: 0.0671, time:28.9422, lr:1e-05\u001b[0m\n",
            "2019-12-08 12:24:27 \u001b[32mINFO     \u001b[0m train.py: [12/300], [200/484], step: 6008, 2.047 samples/sec, batch_loss: 0.0503, batch_loss_c: 0.0392, batch_loss_s: 0.0763, time:19.5441, lr:1e-05\u001b[0m\n",
            "2019-12-08 12:24:42 \u001b[32mINFO     \u001b[0m train.py: [12/300], [210/484], step: 6018, 2.640 samples/sec, batch_loss: 0.1353, batch_loss_c: 0.1439, batch_loss_s: 0.1154, time:15.1528, lr:1e-05\u001b[0m\n",
            "2019-12-08 12:25:07 \u001b[32mINFO     \u001b[0m train.py: [12/300], [220/484], step: 6028, 1.590 samples/sec, batch_loss: 0.0794, batch_loss_c: 0.0719, batch_loss_s: 0.0970, time:25.1564, lr:1e-05\u001b[0m\n",
            "2019-12-08 12:25:27 \u001b[32mINFO     \u001b[0m train.py: [12/300], [230/484], step: 6038, 1.957 samples/sec, batch_loss: 0.5468, batch_loss_c: 0.5778, batch_loss_s: 0.4745, time:20.4368, lr:1e-05\u001b[0m\n",
            "2019-12-08 12:25:57 \u001b[32mINFO     \u001b[0m train.py: [12/300], [240/484], step: 6048, 1.329 samples/sec, batch_loss: 0.1183, batch_loss_c: 0.1271, batch_loss_s: 0.0975, time:30.1031, lr:1e-05\u001b[0m\n",
            "2019-12-08 12:26:13 \u001b[32mINFO     \u001b[0m train.py: [12/300], [250/484], step: 6058, 2.525 samples/sec, batch_loss: 0.0712, batch_loss_c: 0.0673, batch_loss_s: 0.0803, time:15.8396, lr:1e-05\u001b[0m\n",
            "2019-12-08 12:26:48 \u001b[32mINFO     \u001b[0m train.py: [12/300], [260/484], step: 6068, 1.148 samples/sec, batch_loss: 0.0451, batch_loss_c: 0.0383, batch_loss_s: 0.0608, time:34.8414, lr:1e-05\u001b[0m\n",
            "2019-12-08 12:27:05 \u001b[32mINFO     \u001b[0m train.py: [12/300], [270/484], step: 6078, 2.421 samples/sec, batch_loss: 0.1911, batch_loss_c: 0.1573, batch_loss_s: 0.2701, time:16.5239, lr:1e-05\u001b[0m\n",
            "2019-12-08 12:27:19 \u001b[32mINFO     \u001b[0m train.py: [12/300], [280/484], step: 6088, 2.737 samples/sec, batch_loss: 0.0689, batch_loss_c: 0.0505, batch_loss_s: 0.1119, time:14.6131, lr:1e-05\u001b[0m\n",
            "tcmalloc: large alloc 5197824000 bytes == 0x7f1b83d62000 @  0x7f203bedd1e7 0x7f20304c1f71 0x7f203052555d 0x7f2030528e28 0x7f20305293e5 0x7f20305bffc2 0x50ac25 0x50d390 0x508245 0x50a080 0x50aa7d 0x50d390 0x509455 0x595311 0x5a067e 0x557d28 0x50c88b 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x509455 0x595311 0x5a522c 0x557f7e 0x4b5eff 0x50c810\n",
            "2019-12-08 12:27:50 \u001b[32mINFO     \u001b[0m train.py: [12/300], [290/484], step: 6098, 1.281 samples/sec, batch_loss: 0.2830, batch_loss_c: 0.2779, batch_loss_s: 0.2949, time:31.2153, lr:1e-05\u001b[0m\n",
            "2019-12-08 12:28:10 \u001b[32mINFO     \u001b[0m train.py: [12/300], [300/484], step: 6108, 2.039 samples/sec, batch_loss: 0.3191, batch_loss_c: 0.2899, batch_loss_s: 0.3874, time:19.6211, lr:1e-05\u001b[0m\n",
            "2019-12-08 12:28:32 \u001b[32mINFO     \u001b[0m train.py: [12/300], [310/484], step: 6118, 1.842 samples/sec, batch_loss: 0.0751, batch_loss_c: 0.0625, batch_loss_s: 0.1046, time:21.7139, lr:1e-05\u001b[0m\n",
            "2019-12-08 12:28:50 \u001b[32mINFO     \u001b[0m train.py: [12/300], [320/484], step: 6128, 2.234 samples/sec, batch_loss: 0.1522, batch_loss_c: 0.1677, batch_loss_s: 0.1160, time:17.9078, lr:1e-05\u001b[0m\n",
            "2019-12-08 12:29:03 \u001b[32mINFO     \u001b[0m train.py: [12/300], [330/484], step: 6138, 2.899 samples/sec, batch_loss: 0.0715, batch_loss_c: 0.0649, batch_loss_s: 0.0867, time:13.8000, lr:1e-05\u001b[0m\n",
            "2019-12-08 12:29:18 \u001b[32mINFO     \u001b[0m train.py: [12/300], [340/484], step: 6148, 2.769 samples/sec, batch_loss: 0.0705, batch_loss_c: 0.0640, batch_loss_s: 0.0856, time:14.4472, lr:1e-05\u001b[0m\n",
            "2019-12-08 12:29:43 \u001b[32mINFO     \u001b[0m train.py: [12/300], [350/484], step: 6158, 1.622 samples/sec, batch_loss: 0.3091, batch_loss_c: 0.3020, batch_loss_s: 0.3256, time:24.6600, lr:1e-05\u001b[0m\n",
            "2019-12-08 12:30:03 \u001b[32mINFO     \u001b[0m train.py: [12/300], [360/484], step: 6168, 2.002 samples/sec, batch_loss: 0.0811, batch_loss_c: 0.0788, batch_loss_s: 0.0863, time:19.9790, lr:1e-05\u001b[0m\n",
            "2019-12-08 12:30:32 \u001b[32mINFO     \u001b[0m train.py: [12/300], [370/484], step: 6178, 1.339 samples/sec, batch_loss: 0.2635, batch_loss_c: 0.3092, batch_loss_s: 0.1569, time:29.8627, lr:1e-05\u001b[0m\n",
            "2019-12-08 12:30:52 \u001b[32mINFO     \u001b[0m train.py: [12/300], [380/484], step: 6188, 2.075 samples/sec, batch_loss: 0.2130, batch_loss_c: 0.2007, batch_loss_s: 0.2419, time:19.2744, lr:1e-05\u001b[0m\n",
            "2019-12-08 12:31:20 \u001b[32mINFO     \u001b[0m train.py: [12/300], [390/484], step: 6198, 1.417 samples/sec, batch_loss: 0.0664, batch_loss_c: 0.0633, batch_loss_s: 0.0736, time:28.2200, lr:1e-05\u001b[0m\n",
            "2019-12-08 12:31:40 \u001b[32mINFO     \u001b[0m train.py: [12/300], [400/484], step: 6208, 1.948 samples/sec, batch_loss: 0.0794, batch_loss_c: 0.0595, batch_loss_s: 0.1259, time:20.5380, lr:1e-05\u001b[0m\n",
            "2019-12-08 12:31:59 \u001b[32mINFO     \u001b[0m train.py: [12/300], [410/484], step: 6218, 2.195 samples/sec, batch_loss: 0.0810, batch_loss_c: 0.0580, batch_loss_s: 0.1344, time:18.2258, lr:1e-05\u001b[0m\n",
            "2019-12-08 12:32:26 \u001b[32mINFO     \u001b[0m train.py: [12/300], [420/484], step: 6228, 1.490 samples/sec, batch_loss: 0.3093, batch_loss_c: 0.3042, batch_loss_s: 0.3210, time:26.8422, lr:1e-05\u001b[0m\n",
            "2019-12-08 12:32:52 \u001b[32mINFO     \u001b[0m train.py: [12/300], [430/484], step: 6238, 1.493 samples/sec, batch_loss: 0.0606, batch_loss_c: 0.0582, batch_loss_s: 0.0661, time:26.7882, lr:1e-05\u001b[0m\n",
            "2019-12-08 12:33:11 \u001b[32mINFO     \u001b[0m train.py: [12/300], [440/484], step: 6248, 2.108 samples/sec, batch_loss: 0.1067, batch_loss_c: 0.1022, batch_loss_s: 0.1173, time:18.9763, lr:1e-05\u001b[0m\n",
            "2019-12-08 12:33:28 \u001b[32mINFO     \u001b[0m train.py: [12/300], [450/484], step: 6258, 2.375 samples/sec, batch_loss: 0.1184, batch_loss_c: 0.1256, batch_loss_s: 0.1016, time:16.8414, lr:1e-05\u001b[0m\n",
            "2019-12-08 12:33:42 \u001b[32mINFO     \u001b[0m train.py: [12/300], [460/484], step: 6268, 2.898 samples/sec, batch_loss: 0.1016, batch_loss_c: 0.0882, batch_loss_s: 0.1327, time:13.8026, lr:1e-05\u001b[0m\n",
            "2019-12-08 12:34:02 \u001b[32mINFO     \u001b[0m train.py: [12/300], [470/484], step: 6278, 2.026 samples/sec, batch_loss: 0.0719, batch_loss_c: 0.0610, batch_loss_s: 0.0972, time:19.7388, lr:1e-05\u001b[0m\n",
            "2019-12-08 12:34:18 \u001b[32mINFO     \u001b[0m train.py: [12/300], [480/484], step: 6288, 2.443 samples/sec, batch_loss: 0.1692, batch_loss_c: 0.1022, batch_loss_s: 0.3255, time:16.3752, lr:1e-05\u001b[0m\n",
            "2019-12-08 12:34:23 \u001b[32mINFO     \u001b[0m train.py: [12/300], train_loss: 0.1381, time: 1027.3198, lr: 1e-05\u001b[0m\n",
            "2019-12-08 12:34:25 \u001b[32mINFO     \u001b[0m train.py: [13/300], [0/484], step: 6292, 24.187 samples/sec, batch_loss: 0.1438, batch_loss_c: 0.1154, batch_loss_s: 0.2099, time:1.6538, lr:1e-05\u001b[0m\n",
            "2019-12-08 12:34:41 \u001b[32mINFO     \u001b[0m train.py: [13/300], [10/484], step: 6302, 2.496 samples/sec, batch_loss: 0.1202, batch_loss_c: 0.1271, batch_loss_s: 0.1041, time:16.0254, lr:1e-05\u001b[0m\n",
            "2019-12-08 12:34:55 \u001b[32mINFO     \u001b[0m train.py: [13/300], [20/484], step: 6312, 2.909 samples/sec, batch_loss: 0.0794, batch_loss_c: 0.0691, batch_loss_s: 0.1034, time:13.7508, lr:1e-05\u001b[0m\n",
            "2019-12-08 12:35:13 \u001b[32mINFO     \u001b[0m train.py: [13/300], [30/484], step: 6322, 2.143 samples/sec, batch_loss: 0.0425, batch_loss_c: 0.0401, batch_loss_s: 0.0479, time:18.6679, lr:1e-05\u001b[0m\n",
            "2019-12-08 12:35:30 \u001b[32mINFO     \u001b[0m train.py: [13/300], [40/484], step: 6332, 2.402 samples/sec, batch_loss: 0.1352, batch_loss_c: 0.1237, batch_loss_s: 0.1622, time:16.6526, lr:1e-05\u001b[0m\n",
            "2019-12-08 12:35:53 \u001b[32mINFO     \u001b[0m train.py: [13/300], [50/484], step: 6342, 1.749 samples/sec, batch_loss: 0.1384, batch_loss_c: 0.1342, batch_loss_s: 0.1484, time:22.8639, lr:1e-05\u001b[0m\n",
            "2019-12-08 12:36:08 \u001b[32mINFO     \u001b[0m train.py: [13/300], [60/484], step: 6352, 2.608 samples/sec, batch_loss: 0.0811, batch_loss_c: 0.0659, batch_loss_s: 0.1165, time:15.3345, lr:1e-05\u001b[0m\n",
            "2019-12-08 12:36:23 \u001b[32mINFO     \u001b[0m train.py: [13/300], [70/484], step: 6362, 2.658 samples/sec, batch_loss: 0.1579, batch_loss_c: 0.1683, batch_loss_s: 0.1337, time:15.0495, lr:1e-05\u001b[0m\n",
            "2019-12-08 12:36:42 \u001b[32mINFO     \u001b[0m train.py: [13/300], [80/484], step: 6372, 2.080 samples/sec, batch_loss: 0.0639, batch_loss_c: 0.0613, batch_loss_s: 0.0699, time:19.2287, lr:1e-05\u001b[0m\n",
            "2019-12-08 12:37:10 \u001b[32mINFO     \u001b[0m train.py: [13/300], [90/484], step: 6382, 1.456 samples/sec, batch_loss: 0.0517, batch_loss_c: 0.0435, batch_loss_s: 0.0709, time:27.4635, lr:1e-05\u001b[0m\n",
            "2019-12-08 12:37:24 \u001b[32mINFO     \u001b[0m train.py: [13/300], [100/484], step: 6392, 2.916 samples/sec, batch_loss: 0.0825, batch_loss_c: 0.0867, batch_loss_s: 0.0726, time:13.7161, lr:1e-05\u001b[0m\n",
            "2019-12-08 12:37:38 \u001b[32mINFO     \u001b[0m train.py: [13/300], [110/484], step: 6402, 2.795 samples/sec, batch_loss: 0.0446, batch_loss_c: 0.0386, batch_loss_s: 0.0585, time:14.3095, lr:1e-05\u001b[0m\n",
            "2019-12-08 12:37:56 \u001b[32mINFO     \u001b[0m train.py: [13/300], [120/484], step: 6412, 2.258 samples/sec, batch_loss: 0.1381, batch_loss_c: 0.1476, batch_loss_s: 0.1159, time:17.7131, lr:1e-05\u001b[0m\n",
            "2019-12-08 12:38:10 \u001b[32mINFO     \u001b[0m train.py: [13/300], [130/484], step: 6422, 2.740 samples/sec, batch_loss: 0.0594, batch_loss_c: 0.0485, batch_loss_s: 0.0848, time:14.5970, lr:1e-05\u001b[0m\n",
            "2019-12-08 12:38:27 \u001b[32mINFO     \u001b[0m train.py: [13/300], [140/484], step: 6432, 2.356 samples/sec, batch_loss: 0.0905, batch_loss_c: 0.0821, batch_loss_s: 0.1102, time:16.9774, lr:1e-05\u001b[0m\n",
            "2019-12-08 12:38:47 \u001b[32mINFO     \u001b[0m train.py: [13/300], [150/484], step: 6442, 2.039 samples/sec, batch_loss: 0.4343, batch_loss_c: 0.4639, batch_loss_s: 0.3653, time:19.6197, lr:1e-05\u001b[0m\n",
            "2019-12-08 12:39:09 \u001b[32mINFO     \u001b[0m train.py: [13/300], [160/484], step: 6452, 1.778 samples/sec, batch_loss: 0.1741, batch_loss_c: 0.1819, batch_loss_s: 0.1559, time:22.5011, lr:1e-05\u001b[0m\n",
            "2019-12-08 12:39:31 \u001b[32mINFO     \u001b[0m train.py: [13/300], [170/484], step: 6462, 1.875 samples/sec, batch_loss: 0.3293, batch_loss_c: 0.3167, batch_loss_s: 0.3587, time:21.3358, lr:1e-05\u001b[0m\n",
            "2019-12-08 12:39:46 \u001b[32mINFO     \u001b[0m train.py: [13/300], [180/484], step: 6472, 2.605 samples/sec, batch_loss: 0.5338, batch_loss_c: 0.5290, batch_loss_s: 0.5449, time:15.3525, lr:1e-05\u001b[0m\n",
            "2019-12-08 12:40:13 \u001b[32mINFO     \u001b[0m train.py: [13/300], [190/484], step: 6482, 1.485 samples/sec, batch_loss: 0.0960, batch_loss_c: 0.1046, batch_loss_s: 0.0762, time:26.9360, lr:1e-05\u001b[0m\n",
            "2019-12-08 12:40:36 \u001b[32mINFO     \u001b[0m train.py: [13/300], [200/484], step: 6492, 1.736 samples/sec, batch_loss: 0.3382, batch_loss_c: 0.3304, batch_loss_s: 0.3564, time:23.0450, lr:1e-05\u001b[0m\n",
            "2019-12-08 12:41:12 \u001b[32mINFO     \u001b[0m train.py: [13/300], [210/484], step: 6502, 1.117 samples/sec, batch_loss: 0.1023, batch_loss_c: 0.1055, batch_loss_s: 0.0949, time:35.8217, lr:1e-05\u001b[0m\n",
            "2019-12-08 12:41:45 \u001b[32mINFO     \u001b[0m train.py: [13/300], [220/484], step: 6512, 1.218 samples/sec, batch_loss: 0.1502, batch_loss_c: 0.1639, batch_loss_s: 0.1184, time:32.8455, lr:1e-05\u001b[0m\n",
            "2019-12-08 12:42:13 \u001b[32mINFO     \u001b[0m train.py: [13/300], [230/484], step: 6522, 1.388 samples/sec, batch_loss: 0.0377, batch_loss_c: 0.0323, batch_loss_s: 0.0501, time:28.8163, lr:1e-05\u001b[0m\n",
            "2019-12-08 12:42:36 \u001b[32mINFO     \u001b[0m train.py: [13/300], [240/484], step: 6532, 1.786 samples/sec, batch_loss: 0.0602, batch_loss_c: 0.0499, batch_loss_s: 0.0842, time:22.3916, lr:1e-05\u001b[0m\n",
            "2019-12-08 12:42:55 \u001b[32mINFO     \u001b[0m train.py: [13/300], [250/484], step: 6542, 2.117 samples/sec, batch_loss: 0.1676, batch_loss_c: 0.1591, batch_loss_s: 0.1876, time:18.8965, lr:1e-05\u001b[0m\n",
            "2019-12-08 12:43:26 \u001b[32mINFO     \u001b[0m train.py: [13/300], [260/484], step: 6552, 1.263 samples/sec, batch_loss: 0.0976, batch_loss_c: 0.0984, batch_loss_s: 0.0959, time:31.6616, lr:1e-05\u001b[0m\n",
            "2019-12-08 12:43:47 \u001b[32mINFO     \u001b[0m train.py: [13/300], [270/484], step: 6562, 1.930 samples/sec, batch_loss: 0.3265, batch_loss_c: 0.3184, batch_loss_s: 0.3453, time:20.7208, lr:1e-05\u001b[0m\n",
            "2019-12-08 12:44:06 \u001b[32mINFO     \u001b[0m train.py: [13/300], [280/484], step: 6572, 2.107 samples/sec, batch_loss: 0.1040, batch_loss_c: 0.1024, batch_loss_s: 0.1077, time:18.9856, lr:1e-05\u001b[0m\n",
            "2019-12-08 12:44:30 \u001b[32mINFO     \u001b[0m train.py: [13/300], [290/484], step: 6582, 1.673 samples/sec, batch_loss: 0.0551, batch_loss_c: 0.0494, batch_loss_s: 0.0683, time:23.9074, lr:1e-05\u001b[0m\n",
            "2019-12-08 12:44:50 \u001b[32mINFO     \u001b[0m train.py: [13/300], [300/484], step: 6592, 2.046 samples/sec, batch_loss: 0.0891, batch_loss_c: 0.0699, batch_loss_s: 0.1342, time:19.5550, lr:1e-05\u001b[0m\n",
            "2019-12-08 12:45:11 \u001b[32mINFO     \u001b[0m train.py: [13/300], [310/484], step: 6602, 1.868 samples/sec, batch_loss: 0.0780, batch_loss_c: 0.0645, batch_loss_s: 0.1097, time:21.4083, lr:1e-05\u001b[0m\n",
            "2019-12-08 12:45:38 \u001b[32mINFO     \u001b[0m train.py: [13/300], [320/484], step: 6612, 1.495 samples/sec, batch_loss: 0.1478, batch_loss_c: 0.1431, batch_loss_s: 0.1587, time:26.7615, lr:1e-05\u001b[0m\n",
            "2019-12-08 12:45:53 \u001b[32mINFO     \u001b[0m train.py: [13/300], [330/484], step: 6622, 2.608 samples/sec, batch_loss: 0.2867, batch_loss_c: 0.2839, batch_loss_s: 0.2932, time:15.3354, lr:1e-05\u001b[0m\n",
            "2019-12-08 12:46:11 \u001b[32mINFO     \u001b[0m train.py: [13/300], [340/484], step: 6632, 2.259 samples/sec, batch_loss: 0.1465, batch_loss_c: 0.1426, batch_loss_s: 0.1558, time:17.7083, lr:1e-05\u001b[0m\n",
            "2019-12-08 12:46:37 \u001b[32mINFO     \u001b[0m train.py: [13/300], [350/484], step: 6642, 1.500 samples/sec, batch_loss: 0.2923, batch_loss_c: 0.2872, batch_loss_s: 0.3042, time:26.6657, lr:1e-05\u001b[0m\n",
            "2019-12-08 12:46:58 \u001b[32mINFO     \u001b[0m train.py: [13/300], [360/484], step: 6652, 1.977 samples/sec, batch_loss: 0.0946, batch_loss_c: 0.0934, batch_loss_s: 0.0973, time:20.2365, lr:1e-05\u001b[0m\n",
            "2019-12-08 12:47:16 \u001b[32mINFO     \u001b[0m train.py: [13/300], [370/484], step: 6662, 2.222 samples/sec, batch_loss: 0.1734, batch_loss_c: 0.1216, batch_loss_s: 0.2943, time:18.0038, lr:1e-05\u001b[0m\n",
            "2019-12-08 12:47:36 \u001b[32mINFO     \u001b[0m train.py: [13/300], [380/484], step: 6672, 1.930 samples/sec, batch_loss: 0.3223, batch_loss_c: 0.3225, batch_loss_s: 0.3220, time:20.7211, lr:1e-05\u001b[0m\n",
            "2019-12-08 12:47:52 \u001b[32mINFO     \u001b[0m train.py: [13/300], [390/484], step: 6682, 2.539 samples/sec, batch_loss: 0.0763, batch_loss_c: 0.0756, batch_loss_s: 0.0780, time:15.7545, lr:1e-05\u001b[0m\n",
            "2019-12-08 12:48:18 \u001b[32mINFO     \u001b[0m train.py: [13/300], [400/484], step: 6692, 1.533 samples/sec, batch_loss: 0.1281, batch_loss_c: 0.1266, batch_loss_s: 0.1314, time:26.0917, lr:1e-05\u001b[0m\n",
            "2019-12-08 12:48:42 \u001b[32mINFO     \u001b[0m train.py: [13/300], [410/484], step: 6702, 1.689 samples/sec, batch_loss: 0.2904, batch_loss_c: 0.2845, batch_loss_s: 0.3039, time:23.6757, lr:1e-05\u001b[0m\n",
            "2019-12-08 12:49:05 \u001b[32mINFO     \u001b[0m train.py: [13/300], [420/484], step: 6712, 1.752 samples/sec, batch_loss: 0.0473, batch_loss_c: 0.0406, batch_loss_s: 0.0628, time:22.8322, lr:1e-05\u001b[0m\n",
            "2019-12-08 12:49:27 \u001b[32mINFO     \u001b[0m train.py: [13/300], [430/484], step: 6722, 1.782 samples/sec, batch_loss: 0.1069, batch_loss_c: 0.1163, batch_loss_s: 0.0849, time:22.4444, lr:1e-05\u001b[0m\n",
            "2019-12-08 12:49:45 \u001b[32mINFO     \u001b[0m train.py: [13/300], [440/484], step: 6732, 2.200 samples/sec, batch_loss: 0.0637, batch_loss_c: 0.0524, batch_loss_s: 0.0899, time:18.1808, lr:1e-05\u001b[0m\n",
            "2019-12-08 12:50:12 \u001b[32mINFO     \u001b[0m train.py: [13/300], [450/484], step: 6742, 1.513 samples/sec, batch_loss: 0.0461, batch_loss_c: 0.0371, batch_loss_s: 0.0670, time:26.4419, lr:1e-05\u001b[0m\n",
            "2019-12-08 12:50:34 \u001b[32mINFO     \u001b[0m train.py: [13/300], [460/484], step: 6752, 1.780 samples/sec, batch_loss: 0.0601, batch_loss_c: 0.0441, batch_loss_s: 0.0974, time:22.4752, lr:1e-05\u001b[0m\n",
            "2019-12-08 12:50:57 \u001b[32mINFO     \u001b[0m train.py: [13/300], [470/484], step: 6762, 1.750 samples/sec, batch_loss: 0.0629, batch_loss_c: 0.0568, batch_loss_s: 0.0772, time:22.8617, lr:1e-05\u001b[0m\n",
            "2019-12-08 12:51:13 \u001b[32mINFO     \u001b[0m train.py: [13/300], [480/484], step: 6772, 2.532 samples/sec, batch_loss: 0.0625, batch_loss_c: 0.0508, batch_loss_s: 0.0897, time:15.7970, lr:1e-05\u001b[0m\n",
            "2019-12-08 12:51:17 \u001b[32mINFO     \u001b[0m train.py: [13/300], train_loss: 0.1470, time: 1014.1825, lr: 1e-05\u001b[0m\n",
            "2019-12-08 12:51:19 \u001b[32mINFO     \u001b[0m train.py: [14/300], [0/484], step: 6776, 32.180 samples/sec, batch_loss: 0.0567, batch_loss_c: 0.0478, batch_loss_s: 0.0776, time:1.2430, lr:1e-05\u001b[0m\n",
            "2019-12-08 12:51:40 \u001b[32mINFO     \u001b[0m train.py: [14/300], [10/484], step: 6786, 1.929 samples/sec, batch_loss: 0.0788, batch_loss_c: 0.0632, batch_loss_s: 0.1150, time:20.7339, lr:1e-05\u001b[0m\n",
            "2019-12-08 12:51:56 \u001b[32mINFO     \u001b[0m train.py: [14/300], [20/484], step: 6796, 2.471 samples/sec, batch_loss: 0.1202, batch_loss_c: 0.1084, batch_loss_s: 0.1477, time:16.1888, lr:1e-05\u001b[0m\n",
            "2019-12-08 12:52:31 \u001b[32mINFO     \u001b[0m train.py: [14/300], [30/484], step: 6806, 1.134 samples/sec, batch_loss: 0.0460, batch_loss_c: 0.0390, batch_loss_s: 0.0621, time:35.2589, lr:1e-05\u001b[0m\n",
            "2019-12-08 12:52:57 \u001b[32mINFO     \u001b[0m train.py: [14/300], [40/484], step: 6816, 1.534 samples/sec, batch_loss: 0.3497, batch_loss_c: 0.3433, batch_loss_s: 0.3646, time:26.0770, lr:1e-05\u001b[0m\n",
            "2019-12-08 12:53:29 \u001b[32mINFO     \u001b[0m train.py: [14/300], [50/484], step: 6826, 1.257 samples/sec, batch_loss: 0.0935, batch_loss_c: 0.0741, batch_loss_s: 0.1388, time:31.8302, lr:1e-05\u001b[0m\n",
            "2019-12-08 12:53:46 \u001b[32mINFO     \u001b[0m train.py: [14/300], [60/484], step: 6836, 2.306 samples/sec, batch_loss: 0.0540, batch_loss_c: 0.0460, batch_loss_s: 0.0727, time:17.3432, lr:1e-05\u001b[0m\n",
            "2019-12-08 12:54:20 \u001b[32mINFO     \u001b[0m train.py: [14/300], [70/484], step: 6846, 1.193 samples/sec, batch_loss: 0.1780, batch_loss_c: 0.2039, batch_loss_s: 0.1175, time:33.5344, lr:1e-05\u001b[0m\n",
            "2019-12-08 12:54:36 \u001b[32mINFO     \u001b[0m train.py: [14/300], [80/484], step: 6856, 2.430 samples/sec, batch_loss: 0.0515, batch_loss_c: 0.0439, batch_loss_s: 0.0693, time:16.4591, lr:1e-05\u001b[0m\n",
            "2019-12-08 12:54:58 \u001b[32mINFO     \u001b[0m train.py: [14/300], [90/484], step: 6866, 1.852 samples/sec, batch_loss: 0.2705, batch_loss_c: 0.2500, batch_loss_s: 0.3183, time:21.5952, lr:1e-05\u001b[0m\n",
            "2019-12-08 12:55:16 \u001b[32mINFO     \u001b[0m train.py: [14/300], [100/484], step: 6876, 2.277 samples/sec, batch_loss: 0.0554, batch_loss_c: 0.0500, batch_loss_s: 0.0682, time:17.5656, lr:1e-05\u001b[0m\n",
            "2019-12-08 12:55:44 \u001b[32mINFO     \u001b[0m train.py: [14/300], [110/484], step: 6886, 1.394 samples/sec, batch_loss: 0.0716, batch_loss_c: 0.0674, batch_loss_s: 0.0815, time:28.6949, lr:1e-05\u001b[0m\n",
            "2019-12-08 12:55:59 \u001b[32mINFO     \u001b[0m train.py: [14/300], [120/484], step: 6896, 2.783 samples/sec, batch_loss: 0.2858, batch_loss_c: 0.2846, batch_loss_s: 0.2886, time:14.3755, lr:1e-05\u001b[0m\n",
            "2019-12-08 12:56:15 \u001b[32mINFO     \u001b[0m train.py: [14/300], [130/484], step: 6906, 2.417 samples/sec, batch_loss: 0.3148, batch_loss_c: 0.3109, batch_loss_s: 0.3239, time:16.5467, lr:1e-05\u001b[0m\n",
            "2019-12-08 12:56:39 \u001b[32mINFO     \u001b[0m train.py: [14/300], [140/484], step: 6916, 1.684 samples/sec, batch_loss: 0.0510, batch_loss_c: 0.0403, batch_loss_s: 0.0760, time:23.7565, lr:1e-05\u001b[0m\n",
            "2019-12-08 12:56:53 \u001b[32mINFO     \u001b[0m train.py: [14/300], [150/484], step: 6926, 2.821 samples/sec, batch_loss: 0.0633, batch_loss_c: 0.0572, batch_loss_s: 0.0775, time:14.1771, lr:1e-05\u001b[0m\n",
            "2019-12-08 12:57:20 \u001b[32mINFO     \u001b[0m train.py: [14/300], [160/484], step: 6936, 1.501 samples/sec, batch_loss: 0.0838, batch_loss_c: 0.0625, batch_loss_s: 0.1335, time:26.6442, lr:1e-05\u001b[0m\n",
            "2019-12-08 12:57:36 \u001b[32mINFO     \u001b[0m train.py: [14/300], [170/484], step: 6946, 2.420 samples/sec, batch_loss: 0.3275, batch_loss_c: 0.3231, batch_loss_s: 0.3378, time:16.5271, lr:1e-05\u001b[0m\n",
            "2019-12-08 12:57:59 \u001b[32mINFO     \u001b[0m train.py: [14/300], [180/484], step: 6956, 1.775 samples/sec, batch_loss: 0.3317, batch_loss_c: 0.3349, batch_loss_s: 0.3241, time:22.5305, lr:1e-05\u001b[0m\n",
            "2019-12-08 12:58:14 \u001b[32mINFO     \u001b[0m train.py: [14/300], [190/484], step: 6966, 2.701 samples/sec, batch_loss: 0.0798, batch_loss_c: 0.0783, batch_loss_s: 0.0832, time:14.8074, lr:1e-05\u001b[0m\n",
            "2019-12-08 12:58:30 \u001b[32mINFO     \u001b[0m train.py: [14/300], [200/484], step: 6976, 2.411 samples/sec, batch_loss: 0.0557, batch_loss_c: 0.0452, batch_loss_s: 0.0800, time:16.5938, lr:1e-05\u001b[0m\n",
            "2019-12-08 12:58:58 \u001b[32mINFO     \u001b[0m train.py: [14/300], [210/484], step: 6986, 1.446 samples/sec, batch_loss: 0.0559, batch_loss_c: 0.0493, batch_loss_s: 0.0713, time:27.6670, lr:1e-05\u001b[0m\n",
            "2019-12-08 12:59:17 \u001b[32mINFO     \u001b[0m train.py: [14/300], [220/484], step: 6996, 2.065 samples/sec, batch_loss: 0.0623, batch_loss_c: 0.0616, batch_loss_s: 0.0640, time:19.3724, lr:1e-05\u001b[0m\n",
            "2019-12-08 12:59:36 \u001b[32mINFO     \u001b[0m train.py: [14/300], [230/484], step: 7006, 2.117 samples/sec, batch_loss: 0.0503, batch_loss_c: 0.0434, batch_loss_s: 0.0666, time:18.8927, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:00:00 \u001b[32mINFO     \u001b[0m train.py: [14/300], [240/484], step: 7016, 1.655 samples/sec, batch_loss: 0.1721, batch_loss_c: 0.1958, batch_loss_s: 0.1167, time:24.1676, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:00:18 \u001b[32mINFO     \u001b[0m train.py: [14/300], [250/484], step: 7026, 2.306 samples/sec, batch_loss: 0.0718, batch_loss_c: 0.0587, batch_loss_s: 0.1023, time:17.3462, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:00:49 \u001b[32mINFO     \u001b[0m train.py: [14/300], [260/484], step: 7036, 1.282 samples/sec, batch_loss: 0.1006, batch_loss_c: 0.0914, batch_loss_s: 0.1221, time:31.1928, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:01:10 \u001b[32mINFO     \u001b[0m train.py: [14/300], [270/484], step: 7046, 1.903 samples/sec, batch_loss: 0.1034, batch_loss_c: 0.0984, batch_loss_s: 0.1150, time:21.0202, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:01:29 \u001b[32mINFO     \u001b[0m train.py: [14/300], [280/484], step: 7056, 2.148 samples/sec, batch_loss: 0.3047, batch_loss_c: 0.2865, batch_loss_s: 0.3470, time:18.6192, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:01:44 \u001b[32mINFO     \u001b[0m train.py: [14/300], [290/484], step: 7066, 2.581 samples/sec, batch_loss: 0.1135, batch_loss_c: 0.1269, batch_loss_s: 0.0823, time:15.4998, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:02:05 \u001b[32mINFO     \u001b[0m train.py: [14/300], [300/484], step: 7076, 1.887 samples/sec, batch_loss: 0.0690, batch_loss_c: 0.0641, batch_loss_s: 0.0804, time:21.1997, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:02:42 \u001b[32mINFO     \u001b[0m train.py: [14/300], [310/484], step: 7086, 1.093 samples/sec, batch_loss: 0.0462, batch_loss_c: 0.0404, batch_loss_s: 0.0597, time:36.6113, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:02:58 \u001b[32mINFO     \u001b[0m train.py: [14/300], [320/484], step: 7096, 2.541 samples/sec, batch_loss: 0.2911, batch_loss_c: 0.2788, batch_loss_s: 0.3199, time:15.7429, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:03:20 \u001b[32mINFO     \u001b[0m train.py: [14/300], [330/484], step: 7106, 1.820 samples/sec, batch_loss: 0.2973, batch_loss_c: 0.2937, batch_loss_s: 0.3058, time:21.9722, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:03:33 \u001b[32mINFO     \u001b[0m train.py: [14/300], [340/484], step: 7116, 3.088 samples/sec, batch_loss: 0.4705, batch_loss_c: 0.4652, batch_loss_s: 0.4828, time:12.9527, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:04:02 \u001b[32mINFO     \u001b[0m train.py: [14/300], [350/484], step: 7126, 1.378 samples/sec, batch_loss: 0.2950, batch_loss_c: 0.2863, batch_loss_s: 0.3154, time:29.0276, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:04:19 \u001b[32mINFO     \u001b[0m train.py: [14/300], [360/484], step: 7136, 2.297 samples/sec, batch_loss: 0.0631, batch_loss_c: 0.0475, batch_loss_s: 0.0994, time:17.4140, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:04:47 \u001b[32mINFO     \u001b[0m train.py: [14/300], [370/484], step: 7146, 1.446 samples/sec, batch_loss: 0.1341, batch_loss_c: 0.1221, batch_loss_s: 0.1620, time:27.6615, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:05:15 \u001b[32mINFO     \u001b[0m train.py: [14/300], [380/484], step: 7156, 1.435 samples/sec, batch_loss: 0.0934, batch_loss_c: 0.0818, batch_loss_s: 0.1203, time:27.8793, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:05:38 \u001b[32mINFO     \u001b[0m train.py: [14/300], [390/484], step: 7166, 1.675 samples/sec, batch_loss: 0.1264, batch_loss_c: 0.1148, batch_loss_s: 0.1536, time:23.8814, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:06:00 \u001b[32mINFO     \u001b[0m train.py: [14/300], [400/484], step: 7176, 1.865 samples/sec, batch_loss: 0.0571, batch_loss_c: 0.0503, batch_loss_s: 0.0730, time:21.4465, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:06:35 \u001b[32mINFO     \u001b[0m train.py: [14/300], [410/484], step: 7186, 1.123 samples/sec, batch_loss: 0.0519, batch_loss_c: 0.0466, batch_loss_s: 0.0644, time:35.6196, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:06:51 \u001b[32mINFO     \u001b[0m train.py: [14/300], [420/484], step: 7196, 2.549 samples/sec, batch_loss: 0.0572, batch_loss_c: 0.0561, batch_loss_s: 0.0599, time:15.6943, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:07:14 \u001b[32mINFO     \u001b[0m train.py: [14/300], [430/484], step: 7206, 1.783 samples/sec, batch_loss: 0.5330, batch_loss_c: 0.5279, batch_loss_s: 0.5449, time:22.4369, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:07:36 \u001b[32mINFO     \u001b[0m train.py: [14/300], [440/484], step: 7216, 1.807 samples/sec, batch_loss: 0.2807, batch_loss_c: 0.2576, batch_loss_s: 0.3348, time:22.1330, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:07:57 \u001b[32mINFO     \u001b[0m train.py: [14/300], [450/484], step: 7226, 1.844 samples/sec, batch_loss: 0.2840, batch_loss_c: 0.2801, batch_loss_s: 0.2930, time:21.6914, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:08:14 \u001b[32mINFO     \u001b[0m train.py: [14/300], [460/484], step: 7236, 2.405 samples/sec, batch_loss: 0.0585, batch_loss_c: 0.0590, batch_loss_s: 0.0575, time:16.6304, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:08:30 \u001b[32mINFO     \u001b[0m train.py: [14/300], [470/484], step: 7246, 2.467 samples/sec, batch_loss: 0.2922, batch_loss_c: 0.2932, batch_loss_s: 0.2899, time:16.2162, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:08:59 \u001b[32mINFO     \u001b[0m train.py: [14/300], [480/484], step: 7256, 1.411 samples/sec, batch_loss: 0.0701, batch_loss_c: 0.0555, batch_loss_s: 0.1044, time:28.3502, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:09:03 \u001b[32mINFO     \u001b[0m train.py: [14/300], train_loss: 0.1392, time: 1065.2499, lr: 1e-05\u001b[0m\n",
            "2019-12-08 13:09:05 \u001b[32mINFO     \u001b[0m train.py: [15/300], [0/484], step: 7260, 22.584 samples/sec, batch_loss: 0.0563, batch_loss_c: 0.0479, batch_loss_s: 0.0760, time:1.7712, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:09:21 \u001b[32mINFO     \u001b[0m train.py: [15/300], [10/484], step: 7270, 2.502 samples/sec, batch_loss: 0.0990, batch_loss_c: 0.0769, batch_loss_s: 0.1505, time:15.9850, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:09:39 \u001b[32mINFO     \u001b[0m train.py: [15/300], [20/484], step: 7280, 2.263 samples/sec, batch_loss: 0.0592, batch_loss_c: 0.0533, batch_loss_s: 0.0731, time:17.6728, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:10:00 \u001b[32mINFO     \u001b[0m train.py: [15/300], [30/484], step: 7290, 1.859 samples/sec, batch_loss: 0.1948, batch_loss_c: 0.2360, batch_loss_s: 0.0986, time:21.5131, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:10:17 \u001b[32mINFO     \u001b[0m train.py: [15/300], [40/484], step: 7300, 2.395 samples/sec, batch_loss: 0.0724, batch_loss_c: 0.0591, batch_loss_s: 0.1034, time:16.7023, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:10:33 \u001b[32mINFO     \u001b[0m train.py: [15/300], [50/484], step: 7310, 2.485 samples/sec, batch_loss: 0.1494, batch_loss_c: 0.1559, batch_loss_s: 0.1343, time:16.0989, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:10:52 \u001b[32mINFO     \u001b[0m train.py: [15/300], [60/484], step: 7320, 2.096 samples/sec, batch_loss: 0.0639, batch_loss_c: 0.0586, batch_loss_s: 0.0765, time:19.0877, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:11:09 \u001b[32mINFO     \u001b[0m train.py: [15/300], [70/484], step: 7330, 2.469 samples/sec, batch_loss: 0.0905, batch_loss_c: 0.0904, batch_loss_s: 0.0905, time:16.2035, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:11:22 \u001b[32mINFO     \u001b[0m train.py: [15/300], [80/484], step: 7340, 2.883 samples/sec, batch_loss: 0.2145, batch_loss_c: 0.2365, batch_loss_s: 0.1633, time:13.8749, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:11:43 \u001b[32mINFO     \u001b[0m train.py: [15/300], [90/484], step: 7350, 1.907 samples/sec, batch_loss: 0.3163, batch_loss_c: 0.3037, batch_loss_s: 0.3457, time:20.9722, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:12:04 \u001b[32mINFO     \u001b[0m train.py: [15/300], [100/484], step: 7360, 1.966 samples/sec, batch_loss: 0.3576, batch_loss_c: 0.3502, batch_loss_s: 0.3750, time:20.3484, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:12:30 \u001b[32mINFO     \u001b[0m train.py: [15/300], [110/484], step: 7370, 1.550 samples/sec, batch_loss: 0.0663, batch_loss_c: 0.0621, batch_loss_s: 0.0761, time:25.7994, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:13:13 \u001b[32mINFO     \u001b[0m train.py: [15/300], [120/484], step: 7380, 0.913 samples/sec, batch_loss: 0.2903, batch_loss_c: 0.2816, batch_loss_s: 0.3107, time:43.8094, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:13:32 \u001b[32mINFO     \u001b[0m train.py: [15/300], [130/484], step: 7390, 2.095 samples/sec, batch_loss: 0.0644, batch_loss_c: 0.0616, batch_loss_s: 0.0710, time:19.0912, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:14:01 \u001b[32mINFO     \u001b[0m train.py: [15/300], [140/484], step: 7400, 1.422 samples/sec, batch_loss: 0.0740, batch_loss_c: 0.0673, batch_loss_s: 0.0896, time:28.1308, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:14:23 \u001b[32mINFO     \u001b[0m train.py: [15/300], [150/484], step: 7410, 1.799 samples/sec, batch_loss: 0.0798, batch_loss_c: 0.0651, batch_loss_s: 0.1139, time:22.2289, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:14:53 \u001b[32mINFO     \u001b[0m train.py: [15/300], [160/484], step: 7420, 1.331 samples/sec, batch_loss: 0.0856, batch_loss_c: 0.0708, batch_loss_s: 0.1203, time:30.0434, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:15:10 \u001b[32mINFO     \u001b[0m train.py: [15/300], [170/484], step: 7430, 2.324 samples/sec, batch_loss: 0.0495, batch_loss_c: 0.0430, batch_loss_s: 0.0646, time:17.2124, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:15:30 \u001b[32mINFO     \u001b[0m train.py: [15/300], [180/484], step: 7440, 1.986 samples/sec, batch_loss: 0.0388, batch_loss_c: 0.0326, batch_loss_s: 0.0533, time:20.1378, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:15:58 \u001b[32mINFO     \u001b[0m train.py: [15/300], [190/484], step: 7450, 1.437 samples/sec, batch_loss: 0.0987, batch_loss_c: 0.0892, batch_loss_s: 0.1210, time:27.8270, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:16:17 \u001b[32mINFO     \u001b[0m train.py: [15/300], [200/484], step: 7460, 2.122 samples/sec, batch_loss: 0.3082, batch_loss_c: 0.3061, batch_loss_s: 0.3133, time:18.8533, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:16:38 \u001b[32mINFO     \u001b[0m train.py: [15/300], [210/484], step: 7470, 1.913 samples/sec, batch_loss: 0.0392, batch_loss_c: 0.0346, batch_loss_s: 0.0499, time:20.9083, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:16:54 \u001b[32mINFO     \u001b[0m train.py: [15/300], [220/484], step: 7480, 2.516 samples/sec, batch_loss: 0.0614, batch_loss_c: 0.0545, batch_loss_s: 0.0774, time:15.9002, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:17:09 \u001b[32mINFO     \u001b[0m train.py: [15/300], [230/484], step: 7490, 2.622 samples/sec, batch_loss: 0.1466, batch_loss_c: 0.1371, batch_loss_s: 0.1687, time:15.2534, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:17:27 \u001b[32mINFO     \u001b[0m train.py: [15/300], [240/484], step: 7500, 2.251 samples/sec, batch_loss: 0.0983, batch_loss_c: 0.1028, batch_loss_s: 0.0878, time:17.7678, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:17:50 \u001b[32mINFO     \u001b[0m train.py: [15/300], [250/484], step: 7510, 1.721 samples/sec, batch_loss: 0.1353, batch_loss_c: 0.1536, batch_loss_s: 0.0926, time:23.2460, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:18:12 \u001b[32mINFO     \u001b[0m train.py: [15/300], [260/484], step: 7520, 1.842 samples/sec, batch_loss: 0.1073, batch_loss_c: 0.1097, batch_loss_s: 0.1015, time:21.7178, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:18:27 \u001b[32mINFO     \u001b[0m train.py: [15/300], [270/484], step: 7530, 2.689 samples/sec, batch_loss: 0.0955, batch_loss_c: 0.0704, batch_loss_s: 0.1541, time:14.8769, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:18:45 \u001b[32mINFO     \u001b[0m train.py: [15/300], [280/484], step: 7540, 2.145 samples/sec, batch_loss: 0.0626, batch_loss_c: 0.0598, batch_loss_s: 0.0692, time:18.6467, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:19:01 \u001b[32mINFO     \u001b[0m train.py: [15/300], [290/484], step: 7550, 2.569 samples/sec, batch_loss: 0.1105, batch_loss_c: 0.1119, batch_loss_s: 0.1070, time:15.5712, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:19:15 \u001b[32mINFO     \u001b[0m train.py: [15/300], [300/484], step: 7560, 2.842 samples/sec, batch_loss: 0.0414, batch_loss_c: 0.0355, batch_loss_s: 0.0552, time:14.0746, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:19:31 \u001b[32mINFO     \u001b[0m train.py: [15/300], [310/484], step: 7570, 2.428 samples/sec, batch_loss: 0.1586, batch_loss_c: 0.1544, batch_loss_s: 0.1685, time:16.4769, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:19:51 \u001b[32mINFO     \u001b[0m train.py: [15/300], [320/484], step: 7580, 2.082 samples/sec, batch_loss: 0.0474, batch_loss_c: 0.0428, batch_loss_s: 0.0583, time:19.2080, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:20:12 \u001b[32mINFO     \u001b[0m train.py: [15/300], [330/484], step: 7590, 1.830 samples/sec, batch_loss: 0.3232, batch_loss_c: 0.3233, batch_loss_s: 0.3227, time:21.8542, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:20:29 \u001b[32mINFO     \u001b[0m train.py: [15/300], [340/484], step: 7600, 2.374 samples/sec, batch_loss: 0.0682, batch_loss_c: 0.0627, batch_loss_s: 0.0809, time:16.8498, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:20:45 \u001b[32mINFO     \u001b[0m train.py: [15/300], [350/484], step: 7610, 2.561 samples/sec, batch_loss: 0.2897, batch_loss_c: 0.2839, batch_loss_s: 0.3034, time:15.6177, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:21:11 \u001b[32mINFO     \u001b[0m train.py: [15/300], [360/484], step: 7620, 1.525 samples/sec, batch_loss: 0.3031, batch_loss_c: 0.2942, batch_loss_s: 0.3239, time:26.2212, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:21:44 \u001b[32mINFO     \u001b[0m train.py: [15/300], [370/484], step: 7630, 1.226 samples/sec, batch_loss: 0.0581, batch_loss_c: 0.0533, batch_loss_s: 0.0693, time:32.6385, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:22:03 \u001b[32mINFO     \u001b[0m train.py: [15/300], [380/484], step: 7640, 2.059 samples/sec, batch_loss: 0.2897, batch_loss_c: 0.2858, batch_loss_s: 0.2986, time:19.4265, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:22:19 \u001b[32mINFO     \u001b[0m train.py: [15/300], [390/484], step: 7650, 2.499 samples/sec, batch_loss: 0.0697, batch_loss_c: 0.0680, batch_loss_s: 0.0737, time:16.0057, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:22:35 \u001b[32mINFO     \u001b[0m train.py: [15/300], [400/484], step: 7660, 2.604 samples/sec, batch_loss: 0.1671, batch_loss_c: 0.1529, batch_loss_s: 0.2005, time:15.3585, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:22:50 \u001b[32mINFO     \u001b[0m train.py: [15/300], [410/484], step: 7670, 2.583 samples/sec, batch_loss: 0.3130, batch_loss_c: 0.3094, batch_loss_s: 0.3214, time:15.4850, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:23:06 \u001b[32mINFO     \u001b[0m train.py: [15/300], [420/484], step: 7680, 2.430 samples/sec, batch_loss: 0.0691, batch_loss_c: 0.0617, batch_loss_s: 0.0863, time:16.4602, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:23:23 \u001b[32mINFO     \u001b[0m train.py: [15/300], [430/484], step: 7690, 2.422 samples/sec, batch_loss: 0.1605, batch_loss_c: 0.1586, batch_loss_s: 0.1650, time:16.5122, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:24:01 \u001b[32mINFO     \u001b[0m train.py: [15/300], [440/484], step: 7700, 1.061 samples/sec, batch_loss: 0.1296, batch_loss_c: 0.1551, batch_loss_s: 0.0700, time:37.7040, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:24:24 \u001b[32mINFO     \u001b[0m train.py: [15/300], [450/484], step: 7710, 1.743 samples/sec, batch_loss: 0.2987, batch_loss_c: 0.2951, batch_loss_s: 0.3070, time:22.9522, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:25:04 \u001b[32mINFO     \u001b[0m train.py: [15/300], [460/484], step: 7720, 0.998 samples/sec, batch_loss: 0.3170, batch_loss_c: 0.3089, batch_loss_s: 0.3359, time:40.0988, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:25:19 \u001b[32mINFO     \u001b[0m train.py: [15/300], [470/484], step: 7730, 2.605 samples/sec, batch_loss: 0.0476, batch_loss_c: 0.0386, batch_loss_s: 0.0687, time:15.3530, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:25:35 \u001b[32mINFO     \u001b[0m train.py: [15/300], [480/484], step: 7740, 2.550 samples/sec, batch_loss: 0.0861, batch_loss_c: 0.0820, batch_loss_s: 0.0958, time:15.6839, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:25:40 \u001b[32mINFO     \u001b[0m train.py: [15/300], train_loss: 0.1368, time: 996.3434, lr: 1e-05\u001b[0m\n",
            "2019-12-08 13:25:41 \u001b[32mINFO     \u001b[0m train.py: [16/300], [0/484], step: 7744, 34.330 samples/sec, batch_loss: 0.0545, batch_loss_c: 0.0434, batch_loss_s: 0.0805, time:1.1652, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:25:58 \u001b[32mINFO     \u001b[0m train.py: [16/300], [10/484], step: 7754, 2.411 samples/sec, batch_loss: 0.0793, batch_loss_c: 0.0719, batch_loss_s: 0.0966, time:16.5940, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:26:30 \u001b[32mINFO     \u001b[0m train.py: [16/300], [20/484], step: 7764, 1.253 samples/sec, batch_loss: 0.1083, batch_loss_c: 0.1036, batch_loss_s: 0.1193, time:31.9236, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:26:46 \u001b[32mINFO     \u001b[0m train.py: [16/300], [30/484], step: 7774, 2.465 samples/sec, batch_loss: 0.1986, batch_loss_c: 0.1891, batch_loss_s: 0.2208, time:16.2296, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:27:02 \u001b[32mINFO     \u001b[0m train.py: [16/300], [40/484], step: 7784, 2.465 samples/sec, batch_loss: 0.0747, batch_loss_c: 0.0597, batch_loss_s: 0.1096, time:16.2264, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:27:30 \u001b[32mINFO     \u001b[0m train.py: [16/300], [50/484], step: 7794, 1.437 samples/sec, batch_loss: 0.0798, batch_loss_c: 0.0815, batch_loss_s: 0.0759, time:27.8436, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:27:46 \u001b[32mINFO     \u001b[0m train.py: [16/300], [60/484], step: 7804, 2.565 samples/sec, batch_loss: 0.1046, batch_loss_c: 0.0969, batch_loss_s: 0.1228, time:15.5924, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:28:08 \u001b[32mINFO     \u001b[0m train.py: [16/300], [70/484], step: 7814, 1.807 samples/sec, batch_loss: 0.0791, batch_loss_c: 0.0762, batch_loss_s: 0.0858, time:22.1383, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:28:37 \u001b[32mINFO     \u001b[0m train.py: [16/300], [80/484], step: 7824, 1.378 samples/sec, batch_loss: 0.1480, batch_loss_c: 0.1642, batch_loss_s: 0.1100, time:29.0173, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:28:53 \u001b[32mINFO     \u001b[0m train.py: [16/300], [90/484], step: 7834, 2.458 samples/sec, batch_loss: 0.0793, batch_loss_c: 0.0688, batch_loss_s: 0.1036, time:16.2747, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:29:23 \u001b[32mINFO     \u001b[0m train.py: [16/300], [100/484], step: 7844, 1.356 samples/sec, batch_loss: 0.0855, batch_loss_c: 0.0718, batch_loss_s: 0.1176, time:29.5019, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:30:04 \u001b[32mINFO     \u001b[0m train.py: [16/300], [110/484], step: 7854, 0.982 samples/sec, batch_loss: 0.5383, batch_loss_c: 0.5325, batch_loss_s: 0.5520, time:40.7405, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:30:39 \u001b[32mINFO     \u001b[0m train.py: [16/300], [120/484], step: 7864, 1.142 samples/sec, batch_loss: 0.5360, batch_loss_c: 0.5297, batch_loss_s: 0.5505, time:35.0304, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:30:53 \u001b[32mINFO     \u001b[0m train.py: [16/300], [130/484], step: 7874, 2.767 samples/sec, batch_loss: 0.0612, batch_loss_c: 0.0560, batch_loss_s: 0.0731, time:14.4566, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:31:16 \u001b[32mINFO     \u001b[0m train.py: [16/300], [140/484], step: 7884, 1.747 samples/sec, batch_loss: 0.2650, batch_loss_c: 0.2482, batch_loss_s: 0.3040, time:22.8987, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:31:49 \u001b[32mINFO     \u001b[0m train.py: [16/300], [150/484], step: 7894, 1.199 samples/sec, batch_loss: 0.3007, batch_loss_c: 0.2876, batch_loss_s: 0.3314, time:33.3565, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:32:16 \u001b[32mINFO     \u001b[0m train.py: [16/300], [160/484], step: 7904, 1.501 samples/sec, batch_loss: 0.0719, batch_loss_c: 0.0656, batch_loss_s: 0.0869, time:26.6431, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:32:40 \u001b[32mINFO     \u001b[0m train.py: [16/300], [170/484], step: 7914, 1.675 samples/sec, batch_loss: 0.1477, batch_loss_c: 0.1437, batch_loss_s: 0.1571, time:23.8828, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:32:59 \u001b[32mINFO     \u001b[0m train.py: [16/300], [180/484], step: 7924, 2.066 samples/sec, batch_loss: 0.0424, batch_loss_c: 0.0376, batch_loss_s: 0.0537, time:19.3643, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:33:21 \u001b[32mINFO     \u001b[0m train.py: [16/300], [190/484], step: 7934, 1.792 samples/sec, batch_loss: 0.0337, batch_loss_c: 0.0246, batch_loss_s: 0.0550, time:22.3250, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:33:37 \u001b[32mINFO     \u001b[0m train.py: [16/300], [200/484], step: 7944, 2.660 samples/sec, batch_loss: 0.0773, batch_loss_c: 0.0688, batch_loss_s: 0.0971, time:15.0361, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:33:52 \u001b[32mINFO     \u001b[0m train.py: [16/300], [210/484], step: 7954, 2.588 samples/sec, batch_loss: 0.0874, batch_loss_c: 0.0779, batch_loss_s: 0.1096, time:15.4537, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:34:09 \u001b[32mINFO     \u001b[0m train.py: [16/300], [220/484], step: 7964, 2.375 samples/sec, batch_loss: 0.3580, batch_loss_c: 0.3456, batch_loss_s: 0.3869, time:16.8441, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:34:32 \u001b[32mINFO     \u001b[0m train.py: [16/300], [230/484], step: 7974, 1.760 samples/sec, batch_loss: 0.0662, batch_loss_c: 0.0559, batch_loss_s: 0.0904, time:22.7324, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:34:45 \u001b[32mINFO     \u001b[0m train.py: [16/300], [240/484], step: 7984, 3.072 samples/sec, batch_loss: 0.0391, batch_loss_c: 0.0323, batch_loss_s: 0.0552, time:13.0224, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:35:03 \u001b[32mINFO     \u001b[0m train.py: [16/300], [250/484], step: 7994, 2.194 samples/sec, batch_loss: 0.0549, batch_loss_c: 0.0455, batch_loss_s: 0.0769, time:18.2306, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:35:20 \u001b[32mINFO     \u001b[0m train.py: [16/300], [260/484], step: 8004, 2.388 samples/sec, batch_loss: 0.0874, batch_loss_c: 0.0737, batch_loss_s: 0.1194, time:16.7480, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:35:44 \u001b[32mINFO     \u001b[0m train.py: [16/300], [270/484], step: 8014, 1.646 samples/sec, batch_loss: 0.2708, batch_loss_c: 0.2620, batch_loss_s: 0.2912, time:24.3041, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:36:11 \u001b[32mINFO     \u001b[0m train.py: [16/300], [280/484], step: 8024, 1.488 samples/sec, batch_loss: 0.3038, batch_loss_c: 0.2973, batch_loss_s: 0.3192, time:26.8879, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:36:26 \u001b[32mINFO     \u001b[0m train.py: [16/300], [290/484], step: 8034, 2.560 samples/sec, batch_loss: 0.0490, batch_loss_c: 0.0452, batch_loss_s: 0.0579, time:15.6241, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:36:44 \u001b[32mINFO     \u001b[0m train.py: [16/300], [300/484], step: 8044, 2.326 samples/sec, batch_loss: 0.0417, batch_loss_c: 0.0337, batch_loss_s: 0.0601, time:17.1966, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:37:05 \u001b[32mINFO     \u001b[0m train.py: [16/300], [310/484], step: 8054, 1.863 samples/sec, batch_loss: 0.2835, batch_loss_c: 0.2780, batch_loss_s: 0.2965, time:21.4700, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:37:23 \u001b[32mINFO     \u001b[0m train.py: [16/300], [320/484], step: 8064, 2.291 samples/sec, batch_loss: 0.1596, batch_loss_c: 0.1536, batch_loss_s: 0.1734, time:17.4612, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:37:45 \u001b[32mINFO     \u001b[0m train.py: [16/300], [330/484], step: 8074, 1.766 samples/sec, batch_loss: 0.0611, batch_loss_c: 0.0513, batch_loss_s: 0.0840, time:22.6547, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:38:03 \u001b[32mINFO     \u001b[0m train.py: [16/300], [340/484], step: 8084, 2.216 samples/sec, batch_loss: 0.3249, batch_loss_c: 0.3285, batch_loss_s: 0.3166, time:18.0532, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:38:17 \u001b[32mINFO     \u001b[0m train.py: [16/300], [350/484], step: 8094, 2.930 samples/sec, batch_loss: 0.2782, batch_loss_c: 0.2755, batch_loss_s: 0.2844, time:13.6539, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:38:42 \u001b[32mINFO     \u001b[0m train.py: [16/300], [360/484], step: 8104, 1.573 samples/sec, batch_loss: 0.0655, batch_loss_c: 0.0618, batch_loss_s: 0.0741, time:25.4255, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:38:58 \u001b[32mINFO     \u001b[0m train.py: [16/300], [370/484], step: 8114, 2.602 samples/sec, batch_loss: 0.3019, batch_loss_c: 0.2979, batch_loss_s: 0.3111, time:15.3751, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:39:22 \u001b[32mINFO     \u001b[0m train.py: [16/300], [380/484], step: 8124, 1.666 samples/sec, batch_loss: 0.0596, batch_loss_c: 0.0460, batch_loss_s: 0.0914, time:24.0033, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:39:38 \u001b[32mINFO     \u001b[0m train.py: [16/300], [390/484], step: 8134, 2.425 samples/sec, batch_loss: 0.2889, batch_loss_c: 0.2848, batch_loss_s: 0.2985, time:16.4934, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:39:54 \u001b[32mINFO     \u001b[0m train.py: [16/300], [400/484], step: 8144, 2.532 samples/sec, batch_loss: 0.0562, batch_loss_c: 0.0454, batch_loss_s: 0.0815, time:15.7984, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:40:11 \u001b[32mINFO     \u001b[0m train.py: [16/300], [410/484], step: 8154, 2.359 samples/sec, batch_loss: 0.1989, batch_loss_c: 0.1778, batch_loss_s: 0.2482, time:16.9567, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:40:32 \u001b[32mINFO     \u001b[0m train.py: [16/300], [420/484], step: 8164, 1.884 samples/sec, batch_loss: 0.2902, batch_loss_c: 0.2879, batch_loss_s: 0.2956, time:21.2282, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:40:50 \u001b[32mINFO     \u001b[0m train.py: [16/300], [430/484], step: 8174, 2.266 samples/sec, batch_loss: 0.3365, batch_loss_c: 0.3430, batch_loss_s: 0.3214, time:17.6507, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:41:06 \u001b[32mINFO     \u001b[0m train.py: [16/300], [440/484], step: 8184, 2.547 samples/sec, batch_loss: 0.1026, batch_loss_c: 0.0939, batch_loss_s: 0.1228, time:15.7075, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:41:22 \u001b[32mINFO     \u001b[0m train.py: [16/300], [450/484], step: 8194, 2.440 samples/sec, batch_loss: 0.0811, batch_loss_c: 0.0823, batch_loss_s: 0.0784, time:16.3915, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:41:41 \u001b[32mINFO     \u001b[0m train.py: [16/300], [460/484], step: 8204, 2.076 samples/sec, batch_loss: 0.0556, batch_loss_c: 0.0501, batch_loss_s: 0.0685, time:19.2707, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:41:59 \u001b[32mINFO     \u001b[0m train.py: [16/300], [470/484], step: 8214, 2.216 samples/sec, batch_loss: 0.0590, batch_loss_c: 0.0529, batch_loss_s: 0.0731, time:18.0513, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:42:16 \u001b[32mINFO     \u001b[0m train.py: [16/300], [480/484], step: 8224, 2.420 samples/sec, batch_loss: 0.0971, batch_loss_c: 0.0882, batch_loss_s: 0.1180, time:16.5280, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:42:21 \u001b[32mINFO     \u001b[0m train.py: [16/300], train_loss: 0.1395, time: 1000.7957, lr: 1e-05\u001b[0m\n",
            "2019-12-08 13:42:26 \u001b[32mINFO     \u001b[0m train.py: [17/300], [0/484], step: 8228, 8.481 samples/sec, batch_loss: 0.2926, batch_loss_c: 0.2882, batch_loss_s: 0.3027, time:4.7162, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:42:58 \u001b[32mINFO     \u001b[0m train.py: [17/300], [10/484], step: 8238, 1.268 samples/sec, batch_loss: 0.0625, batch_loss_c: 0.0538, batch_loss_s: 0.0826, time:31.5488, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:43:19 \u001b[32mINFO     \u001b[0m train.py: [17/300], [20/484], step: 8248, 1.847 samples/sec, batch_loss: 0.3153, batch_loss_c: 0.3113, batch_loss_s: 0.3248, time:21.6622, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:43:34 \u001b[32mINFO     \u001b[0m train.py: [17/300], [30/484], step: 8258, 2.703 samples/sec, batch_loss: 0.1306, batch_loss_c: 0.1159, batch_loss_s: 0.1651, time:14.7993, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:43:52 \u001b[32mINFO     \u001b[0m train.py: [17/300], [40/484], step: 8268, 2.269 samples/sec, batch_loss: 0.0585, batch_loss_c: 0.0531, batch_loss_s: 0.0711, time:17.6252, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:44:07 \u001b[32mINFO     \u001b[0m train.py: [17/300], [50/484], step: 8278, 2.653 samples/sec, batch_loss: 0.0718, batch_loss_c: 0.0612, batch_loss_s: 0.0964, time:15.0781, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:44:22 \u001b[32mINFO     \u001b[0m train.py: [17/300], [60/484], step: 8288, 2.661 samples/sec, batch_loss: 0.2012, batch_loss_c: 0.1812, batch_loss_s: 0.2479, time:15.0308, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:44:58 \u001b[32mINFO     \u001b[0m train.py: [17/300], [70/484], step: 8298, 1.112 samples/sec, batch_loss: 0.2906, batch_loss_c: 0.2815, batch_loss_s: 0.3120, time:35.9575, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:45:18 \u001b[32mINFO     \u001b[0m train.py: [17/300], [80/484], step: 8308, 1.986 samples/sec, batch_loss: 0.0479, batch_loss_c: 0.0432, batch_loss_s: 0.0588, time:20.1426, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:45:45 \u001b[32mINFO     \u001b[0m train.py: [17/300], [90/484], step: 8318, 1.483 samples/sec, batch_loss: 0.3027, batch_loss_c: 0.3008, batch_loss_s: 0.3073, time:26.9776, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:46:02 \u001b[32mINFO     \u001b[0m train.py: [17/300], [100/484], step: 8328, 2.435 samples/sec, batch_loss: 0.1074, batch_loss_c: 0.1210, batch_loss_s: 0.0758, time:16.4285, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:46:21 \u001b[32mINFO     \u001b[0m train.py: [17/300], [110/484], step: 8338, 2.042 samples/sec, batch_loss: 0.2839, batch_loss_c: 0.2706, batch_loss_s: 0.3150, time:19.5918, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:46:34 \u001b[32mINFO     \u001b[0m train.py: [17/300], [120/484], step: 8348, 3.030 samples/sec, batch_loss: 0.1057, batch_loss_c: 0.1017, batch_loss_s: 0.1152, time:13.2008, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:46:57 \u001b[32mINFO     \u001b[0m train.py: [17/300], [130/484], step: 8358, 1.745 samples/sec, batch_loss: 0.0404, batch_loss_c: 0.0315, batch_loss_s: 0.0610, time:22.9209, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:47:27 \u001b[32mINFO     \u001b[0m train.py: [17/300], [140/484], step: 8368, 1.330 samples/sec, batch_loss: 0.3030, batch_loss_c: 0.2942, batch_loss_s: 0.3234, time:30.0686, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:47:45 \u001b[32mINFO     \u001b[0m train.py: [17/300], [150/484], step: 8378, 2.276 samples/sec, batch_loss: 0.0538, batch_loss_c: 0.0496, batch_loss_s: 0.0636, time:17.5774, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:48:06 \u001b[32mINFO     \u001b[0m train.py: [17/300], [160/484], step: 8388, 1.855 samples/sec, batch_loss: 0.0554, batch_loss_c: 0.0525, batch_loss_s: 0.0623, time:21.5656, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:48:21 \u001b[32mINFO     \u001b[0m train.py: [17/300], [170/484], step: 8398, 2.718 samples/sec, batch_loss: 0.0773, batch_loss_c: 0.0697, batch_loss_s: 0.0950, time:14.7142, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:48:39 \u001b[32mINFO     \u001b[0m train.py: [17/300], [180/484], step: 8408, 2.286 samples/sec, batch_loss: 0.0617, batch_loss_c: 0.0465, batch_loss_s: 0.0974, time:17.4956, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:49:03 \u001b[32mINFO     \u001b[0m train.py: [17/300], [190/484], step: 8418, 1.632 samples/sec, batch_loss: 0.1785, batch_loss_c: 0.1718, batch_loss_s: 0.1943, time:24.5125, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:49:18 \u001b[32mINFO     \u001b[0m train.py: [17/300], [200/484], step: 8428, 2.668 samples/sec, batch_loss: 0.1156, batch_loss_c: 0.0925, batch_loss_s: 0.1695, time:14.9897, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:49:44 \u001b[32mINFO     \u001b[0m train.py: [17/300], [210/484], step: 8438, 1.569 samples/sec, batch_loss: 0.2445, batch_loss_c: 0.2387, batch_loss_s: 0.2581, time:25.4975, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:49:59 \u001b[32mINFO     \u001b[0m train.py: [17/300], [220/484], step: 8448, 2.567 samples/sec, batch_loss: 0.1809, batch_loss_c: 0.1679, batch_loss_s: 0.2111, time:15.5799, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:50:13 \u001b[32mINFO     \u001b[0m train.py: [17/300], [230/484], step: 8458, 2.841 samples/sec, batch_loss: 0.0859, batch_loss_c: 0.0789, batch_loss_s: 0.1024, time:14.0795, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:50:49 \u001b[32mINFO     \u001b[0m train.py: [17/300], [240/484], step: 8468, 1.115 samples/sec, batch_loss: 0.0575, batch_loss_c: 0.0472, batch_loss_s: 0.0814, time:35.8598, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:51:08 \u001b[32mINFO     \u001b[0m train.py: [17/300], [250/484], step: 8478, 2.157 samples/sec, batch_loss: 0.0520, batch_loss_c: 0.0474, batch_loss_s: 0.0629, time:18.5476, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:51:24 \u001b[32mINFO     \u001b[0m train.py: [17/300], [260/484], step: 8488, 2.480 samples/sec, batch_loss: 0.1817, batch_loss_c: 0.1915, batch_loss_s: 0.1591, time:16.1300, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:51:52 \u001b[32mINFO     \u001b[0m train.py: [17/300], [270/484], step: 8498, 1.437 samples/sec, batch_loss: 0.2806, batch_loss_c: 0.2706, batch_loss_s: 0.3040, time:27.8444, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:52:06 \u001b[32mINFO     \u001b[0m train.py: [17/300], [280/484], step: 8508, 2.731 samples/sec, batch_loss: 0.1104, batch_loss_c: 0.1245, batch_loss_s: 0.0777, time:14.6450, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:52:24 \u001b[32mINFO     \u001b[0m train.py: [17/300], [290/484], step: 8518, 2.269 samples/sec, batch_loss: 0.3257, batch_loss_c: 0.3228, batch_loss_s: 0.3327, time:17.6280, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:52:44 \u001b[32mINFO     \u001b[0m train.py: [17/300], [300/484], step: 8528, 1.975 samples/sec, batch_loss: 0.0632, batch_loss_c: 0.0509, batch_loss_s: 0.0918, time:20.2496, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:53:03 \u001b[32mINFO     \u001b[0m train.py: [17/300], [310/484], step: 8538, 2.185 samples/sec, batch_loss: 0.1080, batch_loss_c: 0.1126, batch_loss_s: 0.0973, time:18.3034, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:53:33 \u001b[32mINFO     \u001b[0m train.py: [17/300], [320/484], step: 8548, 1.298 samples/sec, batch_loss: 0.3347, batch_loss_c: 0.3333, batch_loss_s: 0.3380, time:30.8129, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:53:53 \u001b[32mINFO     \u001b[0m train.py: [17/300], [330/484], step: 8558, 2.027 samples/sec, batch_loss: 0.0891, batch_loss_c: 0.0787, batch_loss_s: 0.1133, time:19.7374, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:54:13 \u001b[32mINFO     \u001b[0m train.py: [17/300], [340/484], step: 8568, 2.049 samples/sec, batch_loss: 0.3013, batch_loss_c: 0.2963, batch_loss_s: 0.3129, time:19.5220, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:54:29 \u001b[32mINFO     \u001b[0m train.py: [17/300], [350/484], step: 8578, 2.400 samples/sec, batch_loss: 0.0552, batch_loss_c: 0.0490, batch_loss_s: 0.0699, time:16.6696, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:54:46 \u001b[32mINFO     \u001b[0m train.py: [17/300], [360/484], step: 8588, 2.373 samples/sec, batch_loss: 0.0765, batch_loss_c: 0.0740, batch_loss_s: 0.0823, time:16.8566, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:55:10 \u001b[32mINFO     \u001b[0m train.py: [17/300], [370/484], step: 8598, 1.642 samples/sec, batch_loss: 0.0713, batch_loss_c: 0.0681, batch_loss_s: 0.0786, time:24.3610, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:55:31 \u001b[32mINFO     \u001b[0m train.py: [17/300], [380/484], step: 8608, 1.967 samples/sec, batch_loss: 0.0799, batch_loss_c: 0.0833, batch_loss_s: 0.0720, time:20.3331, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:55:50 \u001b[32mINFO     \u001b[0m train.py: [17/300], [390/484], step: 8618, 2.062 samples/sec, batch_loss: 0.0676, batch_loss_c: 0.0549, batch_loss_s: 0.0971, time:19.4007, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:56:07 \u001b[32mINFO     \u001b[0m train.py: [17/300], [400/484], step: 8628, 2.328 samples/sec, batch_loss: 0.0797, batch_loss_c: 0.0763, batch_loss_s: 0.0875, time:17.1852, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:56:29 \u001b[32mINFO     \u001b[0m train.py: [17/300], [410/484], step: 8638, 1.818 samples/sec, batch_loss: 0.0684, batch_loss_c: 0.0578, batch_loss_s: 0.0931, time:22.0060, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:56:46 \u001b[32mINFO     \u001b[0m train.py: [17/300], [420/484], step: 8648, 2.470 samples/sec, batch_loss: 0.0639, batch_loss_c: 0.0462, batch_loss_s: 0.1053, time:16.1930, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:57:06 \u001b[32mINFO     \u001b[0m train.py: [17/300], [430/484], step: 8658, 1.941 samples/sec, batch_loss: 0.2822, batch_loss_c: 0.2763, batch_loss_s: 0.2960, time:20.6035, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:57:35 \u001b[32mINFO     \u001b[0m train.py: [17/300], [440/484], step: 8668, 1.378 samples/sec, batch_loss: 0.0647, batch_loss_c: 0.0509, batch_loss_s: 0.0969, time:29.0261, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:57:52 \u001b[32mINFO     \u001b[0m train.py: [17/300], [450/484], step: 8678, 2.460 samples/sec, batch_loss: 0.0730, batch_loss_c: 0.0654, batch_loss_s: 0.0909, time:16.2631, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:58:21 \u001b[32mINFO     \u001b[0m train.py: [17/300], [460/484], step: 8688, 1.365 samples/sec, batch_loss: 0.0951, batch_loss_c: 0.0923, batch_loss_s: 0.1018, time:29.3013, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:58:47 \u001b[32mINFO     \u001b[0m train.py: [17/300], [470/484], step: 8698, 1.519 samples/sec, batch_loss: 0.0483, batch_loss_c: 0.0433, batch_loss_s: 0.0602, time:26.3279, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:59:03 \u001b[32mINFO     \u001b[0m train.py: [17/300], [480/484], step: 8708, 2.519 samples/sec, batch_loss: 0.0455, batch_loss_c: 0.0394, batch_loss_s: 0.0595, time:15.8816, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:59:08 \u001b[32mINFO     \u001b[0m train.py: [17/300], train_loss: 0.1444, time: 1006.2732, lr: 1e-05\u001b[0m\n",
            "2019-12-08 13:59:09 \u001b[32mINFO     \u001b[0m train.py: [18/300], [0/484], step: 8712, 34.955 samples/sec, batch_loss: 0.0588, batch_loss_c: 0.0512, batch_loss_s: 0.0768, time:1.1443, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:59:29 \u001b[32mINFO     \u001b[0m train.py: [18/300], [10/484], step: 8722, 2.069 samples/sec, batch_loss: 0.1696, batch_loss_c: 0.1688, batch_loss_s: 0.1714, time:19.3308, lr:1e-05\u001b[0m\n",
            "2019-12-08 13:59:59 \u001b[32mINFO     \u001b[0m train.py: [18/300], [20/484], step: 8732, 1.339 samples/sec, batch_loss: 0.0509, batch_loss_c: 0.0430, batch_loss_s: 0.0691, time:29.8632, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:00:12 \u001b[32mINFO     \u001b[0m train.py: [18/300], [30/484], step: 8742, 3.046 samples/sec, batch_loss: 0.0470, batch_loss_c: 0.0408, batch_loss_s: 0.0616, time:13.1307, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:00:31 \u001b[32mINFO     \u001b[0m train.py: [18/300], [40/484], step: 8752, 2.027 samples/sec, batch_loss: 0.0453, batch_loss_c: 0.0359, batch_loss_s: 0.0674, time:19.7348, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:00:50 \u001b[32mINFO     \u001b[0m train.py: [18/300], [50/484], step: 8762, 2.131 samples/sec, batch_loss: 0.2960, batch_loss_c: 0.2932, batch_loss_s: 0.3028, time:18.7681, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:01:09 \u001b[32mINFO     \u001b[0m train.py: [18/300], [60/484], step: 8772, 2.113 samples/sec, batch_loss: 0.0783, batch_loss_c: 0.0648, batch_loss_s: 0.1097, time:18.9334, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:01:27 \u001b[32mINFO     \u001b[0m train.py: [18/300], [70/484], step: 8782, 2.229 samples/sec, batch_loss: 0.2001, batch_loss_c: 0.1854, batch_loss_s: 0.2346, time:17.9431, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:01:44 \u001b[32mINFO     \u001b[0m train.py: [18/300], [80/484], step: 8792, 2.353 samples/sec, batch_loss: 0.0887, batch_loss_c: 0.0792, batch_loss_s: 0.1108, time:16.9973, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:02:02 \u001b[32mINFO     \u001b[0m train.py: [18/300], [90/484], step: 8802, 2.274 samples/sec, batch_loss: 0.0805, batch_loss_c: 0.0721, batch_loss_s: 0.0999, time:17.5899, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:02:20 \u001b[32mINFO     \u001b[0m train.py: [18/300], [100/484], step: 8812, 2.172 samples/sec, batch_loss: 0.0581, batch_loss_c: 0.0450, batch_loss_s: 0.0885, time:18.4184, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:02:37 \u001b[32mINFO     \u001b[0m train.py: [18/300], [110/484], step: 8822, 2.350 samples/sec, batch_loss: 0.1094, batch_loss_c: 0.1081, batch_loss_s: 0.1126, time:17.0208, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:03:02 \u001b[32mINFO     \u001b[0m train.py: [18/300], [120/484], step: 8832, 1.593 samples/sec, batch_loss: 0.0956, batch_loss_c: 0.0896, batch_loss_s: 0.1095, time:25.1120, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:03:29 \u001b[32mINFO     \u001b[0m train.py: [18/300], [130/484], step: 8842, 1.478 samples/sec, batch_loss: 0.2768, batch_loss_c: 0.2677, batch_loss_s: 0.2979, time:27.0547, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:03:54 \u001b[32mINFO     \u001b[0m train.py: [18/300], [140/484], step: 8852, 1.651 samples/sec, batch_loss: 0.2836, batch_loss_c: 0.2781, batch_loss_s: 0.2963, time:24.2288, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:04:22 \u001b[32mINFO     \u001b[0m train.py: [18/300], [150/484], step: 8862, 1.395 samples/sec, batch_loss: 0.0614, batch_loss_c: 0.0652, batch_loss_s: 0.0523, time:28.6647, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:04:42 \u001b[32mINFO     \u001b[0m train.py: [18/300], [160/484], step: 8872, 1.992 samples/sec, batch_loss: 0.0618, batch_loss_c: 0.0569, batch_loss_s: 0.0732, time:20.0774, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:04:59 \u001b[32mINFO     \u001b[0m train.py: [18/300], [170/484], step: 8882, 2.420 samples/sec, batch_loss: 0.3919, batch_loss_c: 0.3919, batch_loss_s: 0.3919, time:16.5317, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:05:13 \u001b[32mINFO     \u001b[0m train.py: [18/300], [180/484], step: 8892, 2.832 samples/sec, batch_loss: 0.0696, batch_loss_c: 0.0629, batch_loss_s: 0.0851, time:14.1233, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:05:28 \u001b[32mINFO     \u001b[0m train.py: [18/300], [190/484], step: 8902, 2.609 samples/sec, batch_loss: 0.0908, batch_loss_c: 0.0785, batch_loss_s: 0.1196, time:15.3342, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:05:46 \u001b[32mINFO     \u001b[0m train.py: [18/300], [200/484], step: 8912, 2.224 samples/sec, batch_loss: 0.0533, batch_loss_c: 0.0440, batch_loss_s: 0.0752, time:17.9851, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:06:23 \u001b[32mINFO     \u001b[0m train.py: [18/300], [210/484], step: 8922, 1.082 samples/sec, batch_loss: 0.3518, batch_loss_c: 0.3442, batch_loss_s: 0.3695, time:36.9856, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:06:56 \u001b[32mINFO     \u001b[0m train.py: [18/300], [220/484], step: 8932, 1.233 samples/sec, batch_loss: 0.3208, batch_loss_c: 0.3130, batch_loss_s: 0.3390, time:32.4435, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:07:19 \u001b[32mINFO     \u001b[0m train.py: [18/300], [230/484], step: 8942, 1.684 samples/sec, batch_loss: 0.0839, batch_loss_c: 0.0764, batch_loss_s: 0.1014, time:23.7483, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:07:42 \u001b[32mINFO     \u001b[0m train.py: [18/300], [240/484], step: 8952, 1.772 samples/sec, batch_loss: 0.1658, batch_loss_c: 0.1050, batch_loss_s: 0.3077, time:22.5747, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:07:59 \u001b[32mINFO     \u001b[0m train.py: [18/300], [250/484], step: 8962, 2.421 samples/sec, batch_loss: 0.2805, batch_loss_c: 0.2750, batch_loss_s: 0.2932, time:16.5227, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:08:21 \u001b[32mINFO     \u001b[0m train.py: [18/300], [260/484], step: 8972, 1.803 samples/sec, batch_loss: 0.0523, batch_loss_c: 0.0331, batch_loss_s: 0.0971, time:22.1804, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:08:41 \u001b[32mINFO     \u001b[0m train.py: [18/300], [270/484], step: 8982, 1.933 samples/sec, batch_loss: 0.0779, batch_loss_c: 0.0699, batch_loss_s: 0.0964, time:20.6900, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:09:01 \u001b[32mINFO     \u001b[0m train.py: [18/300], [280/484], step: 8992, 2.040 samples/sec, batch_loss: 0.0497, batch_loss_c: 0.0444, batch_loss_s: 0.0620, time:19.6045, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:09:18 \u001b[32mINFO     \u001b[0m train.py: [18/300], [290/484], step: 9002, 2.417 samples/sec, batch_loss: 0.1741, batch_loss_c: 0.1676, batch_loss_s: 0.1893, time:16.5467, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:09:39 \u001b[32mINFO     \u001b[0m train.py: [18/300], [300/484], step: 9012, 1.890 samples/sec, batch_loss: 0.0843, batch_loss_c: 0.0821, batch_loss_s: 0.0894, time:21.1600, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:10:02 \u001b[32mINFO     \u001b[0m train.py: [18/300], [310/484], step: 9022, 1.691 samples/sec, batch_loss: 0.5579, batch_loss_c: 0.5603, batch_loss_s: 0.5521, time:23.6554, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:10:22 \u001b[32mINFO     \u001b[0m train.py: [18/300], [320/484], step: 9032, 2.086 samples/sec, batch_loss: 0.4486, batch_loss_c: 0.4130, batch_loss_s: 0.5316, time:19.1754, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:10:37 \u001b[32mINFO     \u001b[0m train.py: [18/300], [330/484], step: 9042, 2.641 samples/sec, batch_loss: 0.0840, batch_loss_c: 0.0843, batch_loss_s: 0.0832, time:15.1432, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:11:07 \u001b[32mINFO     \u001b[0m train.py: [18/300], [340/484], step: 9052, 1.305 samples/sec, batch_loss: 0.2716, batch_loss_c: 0.2687, batch_loss_s: 0.2784, time:30.6440, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:11:24 \u001b[32mINFO     \u001b[0m train.py: [18/300], [350/484], step: 9062, 2.345 samples/sec, batch_loss: 0.0517, batch_loss_c: 0.0350, batch_loss_s: 0.0907, time:17.0602, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:11:43 \u001b[32mINFO     \u001b[0m train.py: [18/300], [360/484], step: 9072, 2.109 samples/sec, batch_loss: 0.0569, batch_loss_c: 0.0497, batch_loss_s: 0.0738, time:18.9664, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:12:11 \u001b[32mINFO     \u001b[0m train.py: [18/300], [370/484], step: 9082, 1.463 samples/sec, batch_loss: 0.3403, batch_loss_c: 0.3154, batch_loss_s: 0.3986, time:27.3479, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:12:30 \u001b[32mINFO     \u001b[0m train.py: [18/300], [380/484], step: 9092, 2.065 samples/sec, batch_loss: 0.2808, batch_loss_c: 0.2762, batch_loss_s: 0.2914, time:19.3744, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:12:57 \u001b[32mINFO     \u001b[0m train.py: [18/300], [390/484], step: 9102, 1.515 samples/sec, batch_loss: 0.2173, batch_loss_c: 0.1812, batch_loss_s: 0.3014, time:26.4027, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:13:14 \u001b[32mINFO     \u001b[0m train.py: [18/300], [400/484], step: 9112, 2.293 samples/sec, batch_loss: 0.0715, batch_loss_c: 0.0725, batch_loss_s: 0.0691, time:17.4433, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:13:33 \u001b[32mINFO     \u001b[0m train.py: [18/300], [410/484], step: 9122, 2.088 samples/sec, batch_loss: 0.1925, batch_loss_c: 0.2282, batch_loss_s: 0.1091, time:19.1533, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:13:49 \u001b[32mINFO     \u001b[0m train.py: [18/300], [420/484], step: 9132, 2.506 samples/sec, batch_loss: 0.5306, batch_loss_c: 0.4751, batch_loss_s: 0.6602, time:15.9599, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:14:04 \u001b[32mINFO     \u001b[0m train.py: [18/300], [430/484], step: 9142, 2.601 samples/sec, batch_loss: 0.0520, batch_loss_c: 0.0426, batch_loss_s: 0.0740, time:15.3800, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:14:20 \u001b[32mINFO     \u001b[0m train.py: [18/300], [440/484], step: 9152, 2.656 samples/sec, batch_loss: 0.1151, batch_loss_c: 0.1203, batch_loss_s: 0.1029, time:15.0623, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:14:39 \u001b[32mINFO     \u001b[0m train.py: [18/300], [450/484], step: 9162, 2.015 samples/sec, batch_loss: 0.0662, batch_loss_c: 0.0569, batch_loss_s: 0.0880, time:19.8528, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:14:57 \u001b[32mINFO     \u001b[0m train.py: [18/300], [460/484], step: 9172, 2.263 samples/sec, batch_loss: 0.1611, batch_loss_c: 0.2056, batch_loss_s: 0.0573, time:17.6765, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:15:14 \u001b[32mINFO     \u001b[0m train.py: [18/300], [470/484], step: 9182, 2.373 samples/sec, batch_loss: 0.1760, batch_loss_c: 0.1519, batch_loss_s: 0.2324, time:16.8581, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:15:29 \u001b[32mINFO     \u001b[0m train.py: [18/300], [480/484], step: 9192, 2.738 samples/sec, batch_loss: 0.0586, batch_loss_c: 0.0533, batch_loss_s: 0.0710, time:14.6087, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:15:38 \u001b[32mINFO     \u001b[0m train.py: [18/300], train_loss: 0.1459, time: 989.7623, lr: 1e-05\u001b[0m\n",
            "2019-12-08 14:15:41 \u001b[32mINFO     \u001b[0m train.py: [19/300], [0/484], step: 9196, 13.961 samples/sec, batch_loss: 0.0760, batch_loss_c: 0.0680, batch_loss_s: 0.0948, time:2.8651, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:16:00 \u001b[32mINFO     \u001b[0m train.py: [19/300], [10/484], step: 9206, 2.202 samples/sec, batch_loss: 0.0776, batch_loss_c: 0.0702, batch_loss_s: 0.0950, time:18.1668, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:16:33 \u001b[32mINFO     \u001b[0m train.py: [19/300], [20/484], step: 9216, 1.182 samples/sec, batch_loss: 0.0781, batch_loss_c: 0.0679, batch_loss_s: 0.1021, time:33.8273, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:16:59 \u001b[32mINFO     \u001b[0m train.py: [19/300], [30/484], step: 9226, 1.551 samples/sec, batch_loss: 0.1868, batch_loss_c: 0.2149, batch_loss_s: 0.1213, time:25.7972, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:17:23 \u001b[32mINFO     \u001b[0m train.py: [19/300], [40/484], step: 9236, 1.648 samples/sec, batch_loss: 0.0409, batch_loss_c: 0.0352, batch_loss_s: 0.0541, time:24.2765, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:17:54 \u001b[32mINFO     \u001b[0m train.py: [19/300], [50/484], step: 9246, 1.306 samples/sec, batch_loss: 0.2130, batch_loss_c: 0.2099, batch_loss_s: 0.2201, time:30.6174, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:18:12 \u001b[32mINFO     \u001b[0m train.py: [19/300], [60/484], step: 9256, 2.255 samples/sec, batch_loss: 0.1130, batch_loss_c: 0.1071, batch_loss_s: 0.1268, time:17.7398, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:18:29 \u001b[32mINFO     \u001b[0m train.py: [19/300], [70/484], step: 9266, 2.364 samples/sec, batch_loss: 0.0820, batch_loss_c: 0.0718, batch_loss_s: 0.1059, time:16.9239, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:19:05 \u001b[32mINFO     \u001b[0m train.py: [19/300], [80/484], step: 9276, 1.103 samples/sec, batch_loss: 0.0897, batch_loss_c: 0.0873, batch_loss_s: 0.0953, time:36.2669, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:19:30 \u001b[32mINFO     \u001b[0m train.py: [19/300], [90/484], step: 9286, 1.630 samples/sec, batch_loss: 0.1212, batch_loss_c: 0.1247, batch_loss_s: 0.1131, time:24.5393, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:19:53 \u001b[32mINFO     \u001b[0m train.py: [19/300], [100/484], step: 9296, 1.707 samples/sec, batch_loss: 0.0467, batch_loss_c: 0.0379, batch_loss_s: 0.0675, time:23.4267, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:20:07 \u001b[32mINFO     \u001b[0m train.py: [19/300], [110/484], step: 9306, 2.905 samples/sec, batch_loss: 0.0479, batch_loss_c: 0.0435, batch_loss_s: 0.0581, time:13.7688, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:20:23 \u001b[32mINFO     \u001b[0m train.py: [19/300], [120/484], step: 9316, 2.500 samples/sec, batch_loss: 0.2990, batch_loss_c: 0.2953, batch_loss_s: 0.3076, time:16.0032, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:20:49 \u001b[32mINFO     \u001b[0m train.py: [19/300], [130/484], step: 9326, 1.521 samples/sec, batch_loss: 0.0708, batch_loss_c: 0.0593, batch_loss_s: 0.0977, time:26.2971, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:21:27 \u001b[32mINFO     \u001b[0m train.py: [19/300], [140/484], step: 9336, 1.043 samples/sec, batch_loss: 0.4648, batch_loss_c: 0.4293, batch_loss_s: 0.5476, time:38.3623, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:21:54 \u001b[32mINFO     \u001b[0m train.py: [19/300], [150/484], step: 9346, 1.486 samples/sec, batch_loss: 0.0609, batch_loss_c: 0.0578, batch_loss_s: 0.0679, time:26.9236, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:22:15 \u001b[32mINFO     \u001b[0m train.py: [19/300], [160/484], step: 9356, 1.942 samples/sec, batch_loss: 0.1238, batch_loss_c: 0.1243, batch_loss_s: 0.1227, time:20.5980, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:22:37 \u001b[32mINFO     \u001b[0m train.py: [19/300], [170/484], step: 9366, 1.825 samples/sec, batch_loss: 0.0494, batch_loss_c: 0.0426, batch_loss_s: 0.0654, time:21.9193, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:22:59 \u001b[32mINFO     \u001b[0m train.py: [19/300], [180/484], step: 9376, 1.787 samples/sec, batch_loss: 0.0700, batch_loss_c: 0.0681, batch_loss_s: 0.0743, time:22.3900, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:23:30 \u001b[32mINFO     \u001b[0m train.py: [19/300], [190/484], step: 9386, 1.287 samples/sec, batch_loss: 0.0636, batch_loss_c: 0.0584, batch_loss_s: 0.0756, time:31.0865, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:23:47 \u001b[32mINFO     \u001b[0m train.py: [19/300], [200/484], step: 9396, 2.474 samples/sec, batch_loss: 0.0572, batch_loss_c: 0.0488, batch_loss_s: 0.0766, time:16.1710, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:24:10 \u001b[32mINFO     \u001b[0m train.py: [19/300], [210/484], step: 9406, 1.668 samples/sec, batch_loss: 0.3112, batch_loss_c: 0.2997, batch_loss_s: 0.3381, time:23.9799, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:24:36 \u001b[32mINFO     \u001b[0m train.py: [19/300], [220/484], step: 9416, 1.571 samples/sec, batch_loss: 0.0924, batch_loss_c: 0.0743, batch_loss_s: 0.1346, time:25.4626, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:24:54 \u001b[32mINFO     \u001b[0m train.py: [19/300], [230/484], step: 9426, 2.277 samples/sec, batch_loss: 0.0565, batch_loss_c: 0.0457, batch_loss_s: 0.0817, time:17.5647, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:25:18 \u001b[32mINFO     \u001b[0m train.py: [19/300], [240/484], step: 9436, 1.646 samples/sec, batch_loss: 0.0403, batch_loss_c: 0.0336, batch_loss_s: 0.0557, time:24.3006, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:25:37 \u001b[32mINFO     \u001b[0m train.py: [19/300], [250/484], step: 9446, 2.066 samples/sec, batch_loss: 0.0642, batch_loss_c: 0.0566, batch_loss_s: 0.0817, time:19.3593, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:25:52 \u001b[32mINFO     \u001b[0m train.py: [19/300], [260/484], step: 9456, 2.631 samples/sec, batch_loss: 0.0827, batch_loss_c: 0.0593, batch_loss_s: 0.1374, time:15.2017, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:26:13 \u001b[32mINFO     \u001b[0m train.py: [19/300], [270/484], step: 9466, 1.942 samples/sec, batch_loss: 0.0700, batch_loss_c: 0.0557, batch_loss_s: 0.1034, time:20.5927, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:26:27 \u001b[32mINFO     \u001b[0m train.py: [19/300], [280/484], step: 9476, 2.819 samples/sec, batch_loss: 0.0783, batch_loss_c: 0.0727, batch_loss_s: 0.0914, time:14.1905, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:26:50 \u001b[32mINFO     \u001b[0m train.py: [19/300], [290/484], step: 9486, 1.715 samples/sec, batch_loss: 0.0592, batch_loss_c: 0.0582, batch_loss_s: 0.0616, time:23.3234, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:27:14 \u001b[32mINFO     \u001b[0m train.py: [19/300], [300/484], step: 9496, 1.686 samples/sec, batch_loss: 0.0919, batch_loss_c: 0.0780, batch_loss_s: 0.1241, time:23.7244, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:27:32 \u001b[32mINFO     \u001b[0m train.py: [19/300], [310/484], step: 9506, 2.274 samples/sec, batch_loss: 0.1757, batch_loss_c: 0.1634, batch_loss_s: 0.2044, time:17.5912, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:27:48 \u001b[32mINFO     \u001b[0m train.py: [19/300], [320/484], step: 9516, 2.515 samples/sec, batch_loss: 0.2825, batch_loss_c: 0.2796, batch_loss_s: 0.2892, time:15.9068, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:28:02 \u001b[32mINFO     \u001b[0m train.py: [19/300], [330/484], step: 9526, 2.830 samples/sec, batch_loss: 0.1423, batch_loss_c: 0.1387, batch_loss_s: 0.1507, time:14.1353, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:28:21 \u001b[32mINFO     \u001b[0m train.py: [19/300], [340/484], step: 9536, 2.072 samples/sec, batch_loss: 0.2911, batch_loss_c: 0.2841, batch_loss_s: 0.3073, time:19.3064, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:28:40 \u001b[32mINFO     \u001b[0m train.py: [19/300], [350/484], step: 9546, 2.074 samples/sec, batch_loss: 0.0499, batch_loss_c: 0.0421, batch_loss_s: 0.0682, time:19.2833, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:28:55 \u001b[32mINFO     \u001b[0m train.py: [19/300], [360/484], step: 9556, 2.732 samples/sec, batch_loss: 0.0820, batch_loss_c: 0.0663, batch_loss_s: 0.1188, time:14.6409, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:29:15 \u001b[32mINFO     \u001b[0m train.py: [19/300], [370/484], step: 9566, 2.020 samples/sec, batch_loss: 0.0430, batch_loss_c: 0.0351, batch_loss_s: 0.0613, time:19.8003, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:29:33 \u001b[32mINFO     \u001b[0m train.py: [19/300], [380/484], step: 9576, 2.237 samples/sec, batch_loss: 0.1244, batch_loss_c: 0.1152, batch_loss_s: 0.1459, time:17.8838, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:30:04 \u001b[32mINFO     \u001b[0m train.py: [19/300], [390/484], step: 9586, 1.262 samples/sec, batch_loss: 0.0558, batch_loss_c: 0.0490, batch_loss_s: 0.0714, time:31.6987, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:30:24 \u001b[32mINFO     \u001b[0m train.py: [19/300], [400/484], step: 9596, 2.060 samples/sec, batch_loss: 0.0936, batch_loss_c: 0.0846, batch_loss_s: 0.1148, time:19.4185, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:30:44 \u001b[32mINFO     \u001b[0m train.py: [19/300], [410/484], step: 9606, 1.965 samples/sec, batch_loss: 0.2078, batch_loss_c: 0.1676, batch_loss_s: 0.3016, time:20.3562, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:31:03 \u001b[32mINFO     \u001b[0m train.py: [19/300], [420/484], step: 9616, 2.144 samples/sec, batch_loss: 0.0853, batch_loss_c: 0.0692, batch_loss_s: 0.1229, time:18.6549, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:31:18 \u001b[32mINFO     \u001b[0m train.py: [19/300], [430/484], step: 9626, 2.600 samples/sec, batch_loss: 0.0668, batch_loss_c: 0.0651, batch_loss_s: 0.0707, time:15.3851, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:31:34 \u001b[32mINFO     \u001b[0m train.py: [19/300], [440/484], step: 9636, 2.515 samples/sec, batch_loss: 0.0657, batch_loss_c: 0.0536, batch_loss_s: 0.0940, time:15.9046, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:31:59 \u001b[32mINFO     \u001b[0m train.py: [19/300], [450/484], step: 9646, 1.596 samples/sec, batch_loss: 0.3055, batch_loss_c: 0.2958, batch_loss_s: 0.3282, time:25.0578, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:32:22 \u001b[32mINFO     \u001b[0m train.py: [19/300], [460/484], step: 9656, 1.772 samples/sec, batch_loss: 0.4479, batch_loss_c: 0.4611, batch_loss_s: 0.4171, time:22.5758, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:32:40 \u001b[32mINFO     \u001b[0m train.py: [19/300], [470/484], step: 9666, 2.250 samples/sec, batch_loss: 0.0487, batch_loss_c: 0.0437, batch_loss_s: 0.0602, time:17.7783, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:32:55 \u001b[32mINFO     \u001b[0m train.py: [19/300], [480/484], step: 9676, 2.672 samples/sec, batch_loss: 0.1010, batch_loss_c: 0.1063, batch_loss_s: 0.0887, time:14.9703, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:32:59 \u001b[32mINFO     \u001b[0m train.py: [19/300], train_loss: 0.1364, time: 1040.3715, lr: 1e-05\u001b[0m\n",
            "2019-12-08 14:33:01 \u001b[32mINFO     \u001b[0m train.py: [20/300], [0/484], step: 9680, 25.511 samples/sec, batch_loss: 0.0546, batch_loss_c: 0.0417, batch_loss_s: 0.0847, time:1.5679, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:33:20 \u001b[32mINFO     \u001b[0m train.py: [20/300], [10/484], step: 9690, 2.049 samples/sec, batch_loss: 0.0541, batch_loss_c: 0.0458, batch_loss_s: 0.0733, time:19.5212, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:33:38 \u001b[32mINFO     \u001b[0m train.py: [20/300], [20/484], step: 9700, 2.234 samples/sec, batch_loss: 0.3243, batch_loss_c: 0.3232, batch_loss_s: 0.3269, time:17.9043, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:33:55 \u001b[32mINFO     \u001b[0m train.py: [20/300], [30/484], step: 9710, 2.405 samples/sec, batch_loss: 0.0355, batch_loss_c: 0.0284, batch_loss_s: 0.0520, time:16.6286, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:34:13 \u001b[32mINFO     \u001b[0m train.py: [20/300], [40/484], step: 9720, 2.238 samples/sec, batch_loss: 0.2859, batch_loss_c: 0.2815, batch_loss_s: 0.2961, time:17.8708, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:34:28 \u001b[32mINFO     \u001b[0m train.py: [20/300], [50/484], step: 9730, 2.675 samples/sec, batch_loss: 0.0515, batch_loss_c: 0.0411, batch_loss_s: 0.0756, time:14.9506, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:34:50 \u001b[32mINFO     \u001b[0m train.py: [20/300], [60/484], step: 9740, 1.811 samples/sec, batch_loss: 0.1107, batch_loss_c: 0.1098, batch_loss_s: 0.1130, time:22.0913, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:35:14 \u001b[32mINFO     \u001b[0m train.py: [20/300], [70/484], step: 9750, 1.628 samples/sec, batch_loss: 0.2244, batch_loss_c: 0.2059, batch_loss_s: 0.2676, time:24.5713, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:35:55 \u001b[32mINFO     \u001b[0m train.py: [20/300], [80/484], step: 9760, 0.975 samples/sec, batch_loss: 0.1203, batch_loss_c: 0.1252, batch_loss_s: 0.1088, time:41.0433, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:36:11 \u001b[32mINFO     \u001b[0m train.py: [20/300], [90/484], step: 9770, 2.550 samples/sec, batch_loss: 0.2016, batch_loss_c: 0.1743, batch_loss_s: 0.2654, time:15.6859, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:36:33 \u001b[32mINFO     \u001b[0m train.py: [20/300], [100/484], step: 9780, 1.817 samples/sec, batch_loss: 0.2861, batch_loss_c: 0.2816, batch_loss_s: 0.2965, time:22.0146, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:37:02 \u001b[32mINFO     \u001b[0m train.py: [20/300], [110/484], step: 9790, 1.387 samples/sec, batch_loss: 0.1378, batch_loss_c: 0.1326, batch_loss_s: 0.1499, time:28.8369, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:37:16 \u001b[32mINFO     \u001b[0m train.py: [20/300], [120/484], step: 9800, 2.885 samples/sec, batch_loss: 0.0850, batch_loss_c: 0.0829, batch_loss_s: 0.0900, time:13.8652, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:37:37 \u001b[32mINFO     \u001b[0m train.py: [20/300], [130/484], step: 9810, 1.929 samples/sec, batch_loss: 0.2918, batch_loss_c: 0.2791, batch_loss_s: 0.3216, time:20.7412, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:37:55 \u001b[32mINFO     \u001b[0m train.py: [20/300], [140/484], step: 9820, 2.149 samples/sec, batch_loss: 0.3122, batch_loss_c: 0.3080, batch_loss_s: 0.3219, time:18.6119, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:38:10 \u001b[32mINFO     \u001b[0m train.py: [20/300], [150/484], step: 9830, 2.666 samples/sec, batch_loss: 0.0589, batch_loss_c: 0.0551, batch_loss_s: 0.0677, time:15.0051, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:38:29 \u001b[32mINFO     \u001b[0m train.py: [20/300], [160/484], step: 9840, 2.120 samples/sec, batch_loss: 0.0929, batch_loss_c: 0.0699, batch_loss_s: 0.1467, time:18.8688, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:38:54 \u001b[32mINFO     \u001b[0m train.py: [20/300], [170/484], step: 9850, 1.579 samples/sec, batch_loss: 0.0572, batch_loss_c: 0.0523, batch_loss_s: 0.0685, time:25.3257, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:39:09 \u001b[32mINFO     \u001b[0m train.py: [20/300], [180/484], step: 9860, 2.663 samples/sec, batch_loss: 0.0414, batch_loss_c: 0.0337, batch_loss_s: 0.0592, time:15.0227, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:39:31 \u001b[32mINFO     \u001b[0m train.py: [20/300], [190/484], step: 9870, 1.817 samples/sec, batch_loss: 0.1417, batch_loss_c: 0.1491, batch_loss_s: 0.1244, time:22.0169, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:39:52 \u001b[32mINFO     \u001b[0m train.py: [20/300], [200/484], step: 9880, 1.927 samples/sec, batch_loss: 0.1032, batch_loss_c: 0.1046, batch_loss_s: 0.0998, time:20.7557, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:40:24 \u001b[32mINFO     \u001b[0m train.py: [20/300], [210/484], step: 9890, 1.249 samples/sec, batch_loss: 0.2949, batch_loss_c: 0.2842, batch_loss_s: 0.3198, time:32.0202, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:40:53 \u001b[32mINFO     \u001b[0m train.py: [20/300], [220/484], step: 9900, 1.369 samples/sec, batch_loss: 0.1154, batch_loss_c: 0.1135, batch_loss_s: 0.1199, time:29.2234, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:41:14 \u001b[32mINFO     \u001b[0m train.py: [20/300], [230/484], step: 9910, 1.964 samples/sec, batch_loss: 0.0500, batch_loss_c: 0.0402, batch_loss_s: 0.0729, time:20.3708, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:41:33 \u001b[32mINFO     \u001b[0m train.py: [20/300], [240/484], step: 9920, 2.086 samples/sec, batch_loss: 0.2889, batch_loss_c: 0.2842, batch_loss_s: 0.3000, time:19.1769, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:41:48 \u001b[32mINFO     \u001b[0m train.py: [20/300], [250/484], step: 9930, 2.635 samples/sec, batch_loss: 0.0552, batch_loss_c: 0.0532, batch_loss_s: 0.0598, time:15.1799, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:42:09 \u001b[32mINFO     \u001b[0m train.py: [20/300], [260/484], step: 9940, 1.966 samples/sec, batch_loss: 0.0811, batch_loss_c: 0.0712, batch_loss_s: 0.1041, time:20.3499, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:42:27 \u001b[32mINFO     \u001b[0m train.py: [20/300], [270/484], step: 9950, 2.205 samples/sec, batch_loss: 0.2795, batch_loss_c: 0.2753, batch_loss_s: 0.2894, time:18.1390, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:42:43 \u001b[32mINFO     \u001b[0m train.py: [20/300], [280/484], step: 9960, 2.454 samples/sec, batch_loss: 0.3785, batch_loss_c: 0.3720, batch_loss_s: 0.3937, time:16.3004, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:43:00 \u001b[32mINFO     \u001b[0m train.py: [20/300], [290/484], step: 9970, 2.338 samples/sec, batch_loss: 0.1590, batch_loss_c: 0.1932, batch_loss_s: 0.0793, time:17.1056, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:43:19 \u001b[32mINFO     \u001b[0m train.py: [20/300], [300/484], step: 9980, 2.155 samples/sec, batch_loss: 0.0509, batch_loss_c: 0.0440, batch_loss_s: 0.0668, time:18.5644, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:43:36 \u001b[32mINFO     \u001b[0m train.py: [20/300], [310/484], step: 9990, 2.268 samples/sec, batch_loss: 0.0629, batch_loss_c: 0.0545, batch_loss_s: 0.0825, time:17.6373, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:43:54 \u001b[32mINFO     \u001b[0m train.py: [20/300], [320/484], step: 10000, 2.242 samples/sec, batch_loss: 0.3106, batch_loss_c: 0.3052, batch_loss_s: 0.3233, time:17.8427, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:44:10 \u001b[32mINFO     \u001b[0m train.py: [20/300], [330/484], step: 10010, 2.606 samples/sec, batch_loss: 0.0761, batch_loss_c: 0.0670, batch_loss_s: 0.0974, time:15.3515, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:44:51 \u001b[32mINFO     \u001b[0m train.py: [20/300], [340/484], step: 10020, 0.965 samples/sec, batch_loss: 0.0538, batch_loss_c: 0.0450, batch_loss_s: 0.0743, time:41.4492, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:45:08 \u001b[32mINFO     \u001b[0m train.py: [20/300], [350/484], step: 10030, 2.418 samples/sec, batch_loss: 0.0780, batch_loss_c: 0.0743, batch_loss_s: 0.0866, time:16.5417, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:45:30 \u001b[32mINFO     \u001b[0m train.py: [20/300], [360/484], step: 10040, 1.756 samples/sec, batch_loss: 0.0618, batch_loss_c: 0.0553, batch_loss_s: 0.0771, time:22.7840, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:45:50 \u001b[32mINFO     \u001b[0m train.py: [20/300], [370/484], step: 10050, 2.020 samples/sec, batch_loss: 0.2164, batch_loss_c: 0.2184, batch_loss_s: 0.2119, time:19.8010, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:46:09 \u001b[32mINFO     \u001b[0m train.py: [20/300], [380/484], step: 10060, 2.127 samples/sec, batch_loss: 0.3163, batch_loss_c: 0.3102, batch_loss_s: 0.3305, time:18.8086, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:46:27 \u001b[32mINFO     \u001b[0m train.py: [20/300], [390/484], step: 10070, 2.208 samples/sec, batch_loss: 0.3207, batch_loss_c: 0.3152, batch_loss_s: 0.3335, time:18.1191, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:46:46 \u001b[32mINFO     \u001b[0m train.py: [20/300], [400/484], step: 10080, 2.149 samples/sec, batch_loss: 0.1029, batch_loss_c: 0.0864, batch_loss_s: 0.1414, time:18.6151, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:47:01 \u001b[32mINFO     \u001b[0m train.py: [20/300], [410/484], step: 10090, 2.683 samples/sec, batch_loss: 0.1421, batch_loss_c: 0.1069, batch_loss_s: 0.2244, time:14.9106, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:47:30 \u001b[32mINFO     \u001b[0m train.py: [20/300], [420/484], step: 10100, 1.371 samples/sec, batch_loss: 0.0500, batch_loss_c: 0.0435, batch_loss_s: 0.0651, time:29.1697, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:47:50 \u001b[32mINFO     \u001b[0m train.py: [20/300], [430/484], step: 10110, 1.957 samples/sec, batch_loss: 0.0649, batch_loss_c: 0.0535, batch_loss_s: 0.0915, time:20.4345, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:48:08 \u001b[32mINFO     \u001b[0m train.py: [20/300], [440/484], step: 10120, 2.232 samples/sec, batch_loss: 0.0764, batch_loss_c: 0.0720, batch_loss_s: 0.0865, time:17.9233, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:48:27 \u001b[32mINFO     \u001b[0m train.py: [20/300], [450/484], step: 10130, 2.154 samples/sec, batch_loss: 0.3584, batch_loss_c: 0.3507, batch_loss_s: 0.3763, time:18.5707, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:48:49 \u001b[32mINFO     \u001b[0m train.py: [20/300], [460/484], step: 10140, 1.766 samples/sec, batch_loss: 0.1307, batch_loss_c: 0.1316, batch_loss_s: 0.1286, time:22.6488, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:49:03 \u001b[32mINFO     \u001b[0m train.py: [20/300], [470/484], step: 10150, 3.006 samples/sec, batch_loss: 0.0678, batch_loss_c: 0.0644, batch_loss_s: 0.0757, time:13.3084, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:49:18 \u001b[32mINFO     \u001b[0m train.py: [20/300], [480/484], step: 10160, 2.662 samples/sec, batch_loss: 0.1183, batch_loss_c: 0.1217, batch_loss_s: 0.1105, time:15.0280, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:49:22 \u001b[32mINFO     \u001b[0m train.py: [20/300], train_loss: 0.1440, time: 982.6970, lr: 1e-05\u001b[0m\n",
            "2019-12-08 14:49:24 \u001b[32mINFO     \u001b[0m train.py: [21/300], [0/484], step: 10164, 24.617 samples/sec, batch_loss: 0.0551, batch_loss_c: 0.0469, batch_loss_s: 0.0742, time:1.6249, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:49:42 \u001b[32mINFO     \u001b[0m train.py: [21/300], [10/484], step: 10174, 2.208 samples/sec, batch_loss: 0.1660, batch_loss_c: 0.1182, batch_loss_s: 0.2775, time:18.1185, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:50:16 \u001b[32mINFO     \u001b[0m train.py: [21/300], [20/484], step: 10184, 1.202 samples/sec, batch_loss: 0.3016, batch_loss_c: 0.3024, batch_loss_s: 0.2997, time:33.2698, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:50:36 \u001b[32mINFO     \u001b[0m train.py: [21/300], [30/484], step: 10194, 1.978 samples/sec, batch_loss: 0.2971, batch_loss_c: 0.2873, batch_loss_s: 0.3202, time:20.2263, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:50:59 \u001b[32mINFO     \u001b[0m train.py: [21/300], [40/484], step: 10204, 1.692 samples/sec, batch_loss: 0.1813, batch_loss_c: 0.1500, batch_loss_s: 0.2543, time:23.6338, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:51:18 \u001b[32mINFO     \u001b[0m train.py: [21/300], [50/484], step: 10214, 2.191 samples/sec, batch_loss: 0.0339, batch_loss_c: 0.0289, batch_loss_s: 0.0454, time:18.2544, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:51:33 \u001b[32mINFO     \u001b[0m train.py: [21/300], [60/484], step: 10224, 2.635 samples/sec, batch_loss: 0.1257, batch_loss_c: 0.1370, batch_loss_s: 0.0993, time:15.1795, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:51:50 \u001b[32mINFO     \u001b[0m train.py: [21/300], [70/484], step: 10234, 2.391 samples/sec, batch_loss: 0.0873, batch_loss_c: 0.0810, batch_loss_s: 0.1021, time:16.7322, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:52:15 \u001b[32mINFO     \u001b[0m train.py: [21/300], [80/484], step: 10244, 1.589 samples/sec, batch_loss: 0.0994, batch_loss_c: 0.0795, batch_loss_s: 0.1457, time:25.1738, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:52:31 \u001b[32mINFO     \u001b[0m train.py: [21/300], [90/484], step: 10254, 2.387 samples/sec, batch_loss: 0.0965, batch_loss_c: 0.0889, batch_loss_s: 0.1142, time:16.7591, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:53:10 \u001b[32mINFO     \u001b[0m train.py: [21/300], [100/484], step: 10264, 1.028 samples/sec, batch_loss: 0.0521, batch_loss_c: 0.0425, batch_loss_s: 0.0746, time:38.9132, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:53:24 \u001b[32mINFO     \u001b[0m train.py: [21/300], [110/484], step: 10274, 2.902 samples/sec, batch_loss: 0.0518, batch_loss_c: 0.0446, batch_loss_s: 0.0684, time:13.7838, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:53:41 \u001b[32mINFO     \u001b[0m train.py: [21/300], [120/484], step: 10284, 2.353 samples/sec, batch_loss: 0.1746, batch_loss_c: 0.1787, batch_loss_s: 0.1649, time:17.0018, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:54:08 \u001b[32mINFO     \u001b[0m train.py: [21/300], [130/484], step: 10294, 1.485 samples/sec, batch_loss: 0.0553, batch_loss_c: 0.0454, batch_loss_s: 0.0785, time:26.9441, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:54:25 \u001b[32mINFO     \u001b[0m train.py: [21/300], [140/484], step: 10304, 2.434 samples/sec, batch_loss: 0.2890, batch_loss_c: 0.2839, batch_loss_s: 0.3009, time:16.4344, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:54:44 \u001b[32mINFO     \u001b[0m train.py: [21/300], [150/484], step: 10314, 2.077 samples/sec, batch_loss: 0.0566, batch_loss_c: 0.0522, batch_loss_s: 0.0671, time:19.2565, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:55:02 \u001b[32mINFO     \u001b[0m train.py: [21/300], [160/484], step: 10324, 2.208 samples/sec, batch_loss: 0.0878, batch_loss_c: 0.0798, batch_loss_s: 0.1065, time:18.1130, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:55:16 \u001b[32mINFO     \u001b[0m train.py: [21/300], [170/484], step: 10334, 2.765 samples/sec, batch_loss: 0.1066, batch_loss_c: 0.0971, batch_loss_s: 0.1287, time:14.4679, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:55:42 \u001b[32mINFO     \u001b[0m train.py: [21/300], [180/484], step: 10344, 1.561 samples/sec, batch_loss: 0.0996, batch_loss_c: 0.0867, batch_loss_s: 0.1297, time:25.6169, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:56:10 \u001b[32mINFO     \u001b[0m train.py: [21/300], [190/484], step: 10354, 1.427 samples/sec, batch_loss: 0.0526, batch_loss_c: 0.0491, batch_loss_s: 0.0607, time:28.0380, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:56:24 \u001b[32mINFO     \u001b[0m train.py: [21/300], [200/484], step: 10364, 2.822 samples/sec, batch_loss: 0.0733, batch_loss_c: 0.0624, batch_loss_s: 0.0985, time:14.1761, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:56:55 \u001b[32mINFO     \u001b[0m train.py: [21/300], [210/484], step: 10374, 1.317 samples/sec, batch_loss: 0.0571, batch_loss_c: 0.0451, batch_loss_s: 0.0852, time:30.3771, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:57:13 \u001b[32mINFO     \u001b[0m train.py: [21/300], [220/484], step: 10384, 2.140 samples/sec, batch_loss: 0.0707, batch_loss_c: 0.0697, batch_loss_s: 0.0732, time:18.6888, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:57:31 \u001b[32mINFO     \u001b[0m train.py: [21/300], [230/484], step: 10394, 2.254 samples/sec, batch_loss: 0.3044, batch_loss_c: 0.2966, batch_loss_s: 0.3228, time:17.7445, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:58:03 \u001b[32mINFO     \u001b[0m train.py: [21/300], [240/484], step: 10404, 1.267 samples/sec, batch_loss: 0.3292, batch_loss_c: 0.3257, batch_loss_s: 0.3372, time:31.5728, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:58:22 \u001b[32mINFO     \u001b[0m train.py: [21/300], [250/484], step: 10414, 2.051 samples/sec, batch_loss: 0.0621, batch_loss_c: 0.0522, batch_loss_s: 0.0851, time:19.5046, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:58:39 \u001b[32mINFO     \u001b[0m train.py: [21/300], [260/484], step: 10424, 2.380 samples/sec, batch_loss: 0.0652, batch_loss_c: 0.0587, batch_loss_s: 0.0803, time:16.8074, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:59:06 \u001b[32mINFO     \u001b[0m train.py: [21/300], [270/484], step: 10434, 1.468 samples/sec, batch_loss: 0.3294, batch_loss_c: 0.3148, batch_loss_s: 0.3635, time:27.2526, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:59:20 \u001b[32mINFO     \u001b[0m train.py: [21/300], [280/484], step: 10444, 2.882 samples/sec, batch_loss: 0.0793, batch_loss_c: 0.0650, batch_loss_s: 0.1127, time:13.8810, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:59:35 \u001b[32mINFO     \u001b[0m train.py: [21/300], [290/484], step: 10454, 2.596 samples/sec, batch_loss: 0.0965, batch_loss_c: 0.0945, batch_loss_s: 0.1012, time:15.4057, lr:1e-05\u001b[0m\n",
            "2019-12-08 14:59:53 \u001b[32mINFO     \u001b[0m train.py: [21/300], [300/484], step: 10464, 2.246 samples/sec, batch_loss: 0.0504, batch_loss_c: 0.0468, batch_loss_s: 0.0588, time:17.8088, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:00:11 \u001b[32mINFO     \u001b[0m train.py: [21/300], [310/484], step: 10474, 2.317 samples/sec, batch_loss: 0.0955, batch_loss_c: 0.0918, batch_loss_s: 0.1041, time:17.2666, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:00:29 \u001b[32mINFO     \u001b[0m train.py: [21/300], [320/484], step: 10484, 2.195 samples/sec, batch_loss: 0.0485, batch_loss_c: 0.0408, batch_loss_s: 0.0665, time:18.2193, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:00:46 \u001b[32mINFO     \u001b[0m train.py: [21/300], [330/484], step: 10494, 2.322 samples/sec, batch_loss: 0.2872, batch_loss_c: 0.2815, batch_loss_s: 0.3005, time:17.2278, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:01:00 \u001b[32mINFO     \u001b[0m train.py: [21/300], [340/484], step: 10504, 2.859 samples/sec, batch_loss: 0.0986, batch_loss_c: 0.0796, batch_loss_s: 0.1430, time:13.9900, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:01:16 \u001b[32mINFO     \u001b[0m train.py: [21/300], [350/484], step: 10514, 2.562 samples/sec, batch_loss: 0.0574, batch_loss_c: 0.0453, batch_loss_s: 0.0858, time:15.6113, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:01:36 \u001b[32mINFO     \u001b[0m train.py: [21/300], [360/484], step: 10524, 1.922 samples/sec, batch_loss: 0.1230, batch_loss_c: 0.1244, batch_loss_s: 0.1198, time:20.8079, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:01:52 \u001b[32mINFO     \u001b[0m train.py: [21/300], [370/484], step: 10534, 2.487 samples/sec, batch_loss: 0.0757, batch_loss_c: 0.0679, batch_loss_s: 0.0939, time:16.0825, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:02:19 \u001b[32mINFO     \u001b[0m train.py: [21/300], [380/484], step: 10544, 1.515 samples/sec, batch_loss: 0.3076, batch_loss_c: 0.2664, batch_loss_s: 0.4037, time:26.4059, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:02:41 \u001b[32mINFO     \u001b[0m train.py: [21/300], [390/484], step: 10554, 1.790 samples/sec, batch_loss: 0.0797, batch_loss_c: 0.0691, batch_loss_s: 0.1046, time:22.3506, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:02:57 \u001b[32mINFO     \u001b[0m train.py: [21/300], [400/484], step: 10564, 2.464 samples/sec, batch_loss: 0.0855, batch_loss_c: 0.0870, batch_loss_s: 0.0817, time:16.2341, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:03:21 \u001b[32mINFO     \u001b[0m train.py: [21/300], [410/484], step: 10574, 1.720 samples/sec, batch_loss: 0.0841, batch_loss_c: 0.0799, batch_loss_s: 0.0938, time:23.2580, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:03:36 \u001b[32mINFO     \u001b[0m train.py: [21/300], [420/484], step: 10584, 2.647 samples/sec, batch_loss: 0.2723, batch_loss_c: 0.2538, batch_loss_s: 0.3156, time:15.1089, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:04:03 \u001b[32mINFO     \u001b[0m train.py: [21/300], [430/484], step: 10594, 1.447 samples/sec, batch_loss: 0.3282, batch_loss_c: 0.2999, batch_loss_s: 0.3942, time:27.6474, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:04:25 \u001b[32mINFO     \u001b[0m train.py: [21/300], [440/484], step: 10604, 1.821 samples/sec, batch_loss: 0.0770, batch_loss_c: 0.0570, batch_loss_s: 0.1236, time:21.9661, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:04:54 \u001b[32mINFO     \u001b[0m train.py: [21/300], [450/484], step: 10614, 1.391 samples/sec, batch_loss: 0.0520, batch_loss_c: 0.0432, batch_loss_s: 0.0727, time:28.7486, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:05:14 \u001b[32mINFO     \u001b[0m train.py: [21/300], [460/484], step: 10624, 2.064 samples/sec, batch_loss: 0.0585, batch_loss_c: 0.0511, batch_loss_s: 0.0757, time:19.3795, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:05:51 \u001b[32mINFO     \u001b[0m train.py: [21/300], [470/484], step: 10634, 1.066 samples/sec, batch_loss: 0.1614, batch_loss_c: 0.1256, batch_loss_s: 0.2448, time:37.5247, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:06:12 \u001b[32mINFO     \u001b[0m train.py: [21/300], [480/484], step: 10644, 1.872 samples/sec, batch_loss: 0.0932, batch_loss_c: 0.0558, batch_loss_s: 0.1805, time:21.3670, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:06:18 \u001b[32mINFO     \u001b[0m train.py: [21/300], train_loss: 0.1384, time: 1015.9055, lr: 1e-05\u001b[0m\n",
            "2019-12-08 15:06:20 \u001b[32mINFO     \u001b[0m train.py: [22/300], [0/484], step: 10648, 30.442 samples/sec, batch_loss: 0.3047, batch_loss_c: 0.2991, batch_loss_s: 0.3180, time:1.3140, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:06:38 \u001b[32mINFO     \u001b[0m train.py: [22/300], [10/484], step: 10658, 2.230 samples/sec, batch_loss: 0.0684, batch_loss_c: 0.0668, batch_loss_s: 0.0723, time:17.9396, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:07:04 \u001b[32mINFO     \u001b[0m train.py: [22/300], [20/484], step: 10668, 1.517 samples/sec, batch_loss: 0.1216, batch_loss_c: 0.1084, batch_loss_s: 0.1524, time:26.3649, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:07:23 \u001b[32mINFO     \u001b[0m train.py: [22/300], [30/484], step: 10678, 2.120 samples/sec, batch_loss: 0.0509, batch_loss_c: 0.0437, batch_loss_s: 0.0679, time:18.8685, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:07:47 \u001b[32mINFO     \u001b[0m train.py: [22/300], [40/484], step: 10688, 1.675 samples/sec, batch_loss: 0.0598, batch_loss_c: 0.0505, batch_loss_s: 0.0814, time:23.8843, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:08:02 \u001b[32mINFO     \u001b[0m train.py: [22/300], [50/484], step: 10698, 2.798 samples/sec, batch_loss: 0.1735, batch_loss_c: 0.1584, batch_loss_s: 0.2088, time:14.2963, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:08:33 \u001b[32mINFO     \u001b[0m train.py: [22/300], [60/484], step: 10708, 1.283 samples/sec, batch_loss: 0.0709, batch_loss_c: 0.0643, batch_loss_s: 0.0862, time:31.1819, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:08:50 \u001b[32mINFO     \u001b[0m train.py: [22/300], [70/484], step: 10718, 2.267 samples/sec, batch_loss: 0.0497, batch_loss_c: 0.0496, batch_loss_s: 0.0500, time:17.6417, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:09:05 \u001b[32mINFO     \u001b[0m train.py: [22/300], [80/484], step: 10728, 2.732 samples/sec, batch_loss: 0.0544, batch_loss_c: 0.0458, batch_loss_s: 0.0745, time:14.6426, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:09:37 \u001b[32mINFO     \u001b[0m train.py: [22/300], [90/484], step: 10738, 1.241 samples/sec, batch_loss: 0.5041, batch_loss_c: 0.5427, batch_loss_s: 0.4138, time:32.2231, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:09:56 \u001b[32mINFO     \u001b[0m train.py: [22/300], [100/484], step: 10748, 2.119 samples/sec, batch_loss: 0.0655, batch_loss_c: 0.0545, batch_loss_s: 0.0911, time:18.8760, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:10:21 \u001b[32mINFO     \u001b[0m train.py: [22/300], [110/484], step: 10758, 1.579 samples/sec, batch_loss: 0.0884, batch_loss_c: 0.0724, batch_loss_s: 0.1256, time:25.3392, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:10:41 \u001b[32mINFO     \u001b[0m train.py: [22/300], [120/484], step: 10768, 2.061 samples/sec, batch_loss: 0.2898, batch_loss_c: 0.2821, batch_loss_s: 0.3079, time:19.4092, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:11:05 \u001b[32mINFO     \u001b[0m train.py: [22/300], [130/484], step: 10778, 1.690 samples/sec, batch_loss: 0.0979, batch_loss_c: 0.1069, batch_loss_s: 0.0768, time:23.6744, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:11:28 \u001b[32mINFO     \u001b[0m train.py: [22/300], [140/484], step: 10788, 1.737 samples/sec, batch_loss: 0.0890, batch_loss_c: 0.0906, batch_loss_s: 0.0853, time:23.0217, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:11:43 \u001b[32mINFO     \u001b[0m train.py: [22/300], [150/484], step: 10798, 2.614 samples/sec, batch_loss: 0.0864, batch_loss_c: 0.0849, batch_loss_s: 0.0897, time:15.3024, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:12:05 \u001b[32mINFO     \u001b[0m train.py: [22/300], [160/484], step: 10808, 1.811 samples/sec, batch_loss: 0.2924, batch_loss_c: 0.2941, batch_loss_s: 0.2884, time:22.0841, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:12:21 \u001b[32mINFO     \u001b[0m train.py: [22/300], [170/484], step: 10818, 2.564 samples/sec, batch_loss: 0.0469, batch_loss_c: 0.0413, batch_loss_s: 0.0599, time:15.5979, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:12:41 \u001b[32mINFO     \u001b[0m train.py: [22/300], [180/484], step: 10828, 1.956 samples/sec, batch_loss: 0.1220, batch_loss_c: 0.0928, batch_loss_s: 0.1900, time:20.4549, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:12:58 \u001b[32mINFO     \u001b[0m train.py: [22/300], [190/484], step: 10838, 2.339 samples/sec, batch_loss: 0.0886, batch_loss_c: 0.0733, batch_loss_s: 0.1241, time:17.0992, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:13:13 \u001b[32mINFO     \u001b[0m train.py: [22/300], [200/484], step: 10848, 2.657 samples/sec, batch_loss: 0.2400, batch_loss_c: 0.2026, batch_loss_s: 0.3272, time:15.0534, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:13:32 \u001b[32mINFO     \u001b[0m train.py: [22/300], [210/484], step: 10858, 2.147 samples/sec, batch_loss: 0.2966, batch_loss_c: 0.2917, batch_loss_s: 0.3082, time:18.6321, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:13:46 \u001b[32mINFO     \u001b[0m train.py: [22/300], [220/484], step: 10868, 2.808 samples/sec, batch_loss: 0.0687, batch_loss_c: 0.0648, batch_loss_s: 0.0779, time:14.2463, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:14:04 \u001b[32mINFO     \u001b[0m train.py: [22/300], [230/484], step: 10878, 2.245 samples/sec, batch_loss: 0.0583, batch_loss_c: 0.0489, batch_loss_s: 0.0804, time:17.8172, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:14:27 \u001b[32mINFO     \u001b[0m train.py: [22/300], [240/484], step: 10888, 1.699 samples/sec, batch_loss: 0.1699, batch_loss_c: 0.1652, batch_loss_s: 0.1810, time:23.5377, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:14:44 \u001b[32mINFO     \u001b[0m train.py: [22/300], [250/484], step: 10898, 2.443 samples/sec, batch_loss: 0.2974, batch_loss_c: 0.3198, batch_loss_s: 0.2451, time:16.3710, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:15:03 \u001b[32mINFO     \u001b[0m train.py: [22/300], [260/484], step: 10908, 2.079 samples/sec, batch_loss: 0.1389, batch_loss_c: 0.1383, batch_loss_s: 0.1403, time:19.2408, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:15:18 \u001b[32mINFO     \u001b[0m train.py: [22/300], [270/484], step: 10918, 2.672 samples/sec, batch_loss: 0.0525, batch_loss_c: 0.0387, batch_loss_s: 0.0847, time:14.9728, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:15:38 \u001b[32mINFO     \u001b[0m train.py: [22/300], [280/484], step: 10928, 2.041 samples/sec, batch_loss: 0.0849, batch_loss_c: 0.0811, batch_loss_s: 0.0936, time:19.6014, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:15:58 \u001b[32mINFO     \u001b[0m train.py: [22/300], [290/484], step: 10938, 2.000 samples/sec, batch_loss: 0.3149, batch_loss_c: 0.3049, batch_loss_s: 0.3384, time:20.0047, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:16:23 \u001b[32mINFO     \u001b[0m train.py: [22/300], [300/484], step: 10948, 1.592 samples/sec, batch_loss: 0.0860, batch_loss_c: 0.0768, batch_loss_s: 0.1075, time:25.1278, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:16:38 \u001b[32mINFO     \u001b[0m train.py: [22/300], [310/484], step: 10958, 2.642 samples/sec, batch_loss: 0.1649, batch_loss_c: 0.1394, batch_loss_s: 0.2244, time:15.1420, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:16:56 \u001b[32mINFO     \u001b[0m train.py: [22/300], [320/484], step: 10968, 2.217 samples/sec, batch_loss: 0.1079, batch_loss_c: 0.0823, batch_loss_s: 0.1676, time:18.0450, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:17:29 \u001b[32mINFO     \u001b[0m train.py: [22/300], [330/484], step: 10978, 1.191 samples/sec, batch_loss: 0.1038, batch_loss_c: 0.0976, batch_loss_s: 0.1181, time:33.5900, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:17:48 \u001b[32mINFO     \u001b[0m train.py: [22/300], [340/484], step: 10988, 2.175 samples/sec, batch_loss: 0.4340, batch_loss_c: 0.4647, batch_loss_s: 0.3626, time:18.3903, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:18:04 \u001b[32mINFO     \u001b[0m train.py: [22/300], [350/484], step: 10998, 2.556 samples/sec, batch_loss: 0.0515, batch_loss_c: 0.0421, batch_loss_s: 0.0735, time:15.6500, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:18:18 \u001b[32mINFO     \u001b[0m train.py: [22/300], [360/484], step: 11008, 2.695 samples/sec, batch_loss: 0.0579, batch_loss_c: 0.0520, batch_loss_s: 0.0715, time:14.8402, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:18:35 \u001b[32mINFO     \u001b[0m train.py: [22/300], [370/484], step: 11018, 2.384 samples/sec, batch_loss: 0.0764, batch_loss_c: 0.0661, batch_loss_s: 0.1005, time:16.7786, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:18:57 \u001b[32mINFO     \u001b[0m train.py: [22/300], [380/484], step: 11028, 1.867 samples/sec, batch_loss: 0.1332, batch_loss_c: 0.1307, batch_loss_s: 0.1389, time:21.4218, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:19:14 \u001b[32mINFO     \u001b[0m train.py: [22/300], [390/484], step: 11038, 2.321 samples/sec, batch_loss: 0.2366, batch_loss_c: 0.2202, batch_loss_s: 0.2747, time:17.2359, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:19:32 \u001b[32mINFO     \u001b[0m train.py: [22/300], [400/484], step: 11048, 2.168 samples/sec, batch_loss: 0.2963, batch_loss_c: 0.2816, batch_loss_s: 0.3308, time:18.4540, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:19:53 \u001b[32mINFO     \u001b[0m train.py: [22/300], [410/484], step: 11058, 1.883 samples/sec, batch_loss: 0.3967, batch_loss_c: 0.4062, batch_loss_s: 0.3745, time:21.2441, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:20:08 \u001b[32mINFO     \u001b[0m train.py: [22/300], [420/484], step: 11068, 2.692 samples/sec, batch_loss: 0.0634, batch_loss_c: 0.0609, batch_loss_s: 0.0694, time:14.8601, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:20:23 \u001b[32mINFO     \u001b[0m train.py: [22/300], [430/484], step: 11078, 2.795 samples/sec, batch_loss: 0.0695, batch_loss_c: 0.0629, batch_loss_s: 0.0851, time:14.3121, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:20:41 \u001b[32mINFO     \u001b[0m train.py: [22/300], [440/484], step: 11088, 2.126 samples/sec, batch_loss: 0.0491, batch_loss_c: 0.0432, batch_loss_s: 0.0627, time:18.8125, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:20:55 \u001b[32mINFO     \u001b[0m train.py: [22/300], [450/484], step: 11098, 2.987 samples/sec, batch_loss: 0.0453, batch_loss_c: 0.0410, batch_loss_s: 0.0554, time:13.3925, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:21:20 \u001b[32mINFO     \u001b[0m train.py: [22/300], [460/484], step: 11108, 1.613 samples/sec, batch_loss: 0.1385, batch_loss_c: 0.1460, batch_loss_s: 0.1209, time:24.8036, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:21:39 \u001b[32mINFO     \u001b[0m train.py: [22/300], [470/484], step: 11118, 2.065 samples/sec, batch_loss: 0.5142, batch_loss_c: 0.4966, batch_loss_s: 0.5551, time:19.3664, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:21:54 \u001b[32mINFO     \u001b[0m train.py: [22/300], [480/484], step: 11128, 2.631 samples/sec, batch_loss: 0.0712, batch_loss_c: 0.0609, batch_loss_s: 0.0953, time:15.2015, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:22:00 \u001b[32mINFO     \u001b[0m train.py: [22/300], train_loss: 0.1406, time: 941.0389, lr: 1e-05\u001b[0m\n",
            "2019-12-08 15:22:02 \u001b[32mINFO     \u001b[0m train.py: [23/300], [0/484], step: 11132, 28.903 samples/sec, batch_loss: 0.0686, batch_loss_c: 0.0627, batch_loss_s: 0.0825, time:1.3840, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:22:24 \u001b[32mINFO     \u001b[0m train.py: [23/300], [10/484], step: 11142, 1.809 samples/sec, batch_loss: 0.1043, batch_loss_c: 0.0871, batch_loss_s: 0.1445, time:22.1068, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:22:38 \u001b[32mINFO     \u001b[0m train.py: [23/300], [20/484], step: 11152, 2.922 samples/sec, batch_loss: 0.1495, batch_loss_c: 0.1355, batch_loss_s: 0.1822, time:13.6895, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:23:06 \u001b[32mINFO     \u001b[0m train.py: [23/300], [30/484], step: 11162, 1.423 samples/sec, batch_loss: 0.3005, batch_loss_c: 0.3002, batch_loss_s: 0.3012, time:28.1094, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:23:25 \u001b[32mINFO     \u001b[0m train.py: [23/300], [40/484], step: 11172, 2.097 samples/sec, batch_loss: 0.2998, batch_loss_c: 0.2924, batch_loss_s: 0.3171, time:19.0734, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:23:45 \u001b[32mINFO     \u001b[0m train.py: [23/300], [50/484], step: 11182, 1.959 samples/sec, batch_loss: 0.0955, batch_loss_c: 0.1009, batch_loss_s: 0.0829, time:20.4181, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:24:01 \u001b[32mINFO     \u001b[0m train.py: [23/300], [60/484], step: 11192, 2.555 samples/sec, batch_loss: 0.0706, batch_loss_c: 0.0590, batch_loss_s: 0.0979, time:15.6577, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:24:29 \u001b[32mINFO     \u001b[0m train.py: [23/300], [70/484], step: 11202, 1.410 samples/sec, batch_loss: 0.0447, batch_loss_c: 0.0333, batch_loss_s: 0.0715, time:28.3738, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:24:53 \u001b[32mINFO     \u001b[0m train.py: [23/300], [80/484], step: 11212, 1.674 samples/sec, batch_loss: 0.1411, batch_loss_c: 0.1474, batch_loss_s: 0.1264, time:23.9000, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:25:16 \u001b[32mINFO     \u001b[0m train.py: [23/300], [90/484], step: 11222, 1.711 samples/sec, batch_loss: 0.0491, batch_loss_c: 0.0392, batch_loss_s: 0.0721, time:23.3737, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:25:32 \u001b[32mINFO     \u001b[0m train.py: [23/300], [100/484], step: 11232, 2.588 samples/sec, batch_loss: 0.2810, batch_loss_c: 0.2766, batch_loss_s: 0.2912, time:15.4563, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:25:48 \u001b[32mINFO     \u001b[0m train.py: [23/300], [110/484], step: 11242, 2.481 samples/sec, batch_loss: 0.0850, batch_loss_c: 0.0743, batch_loss_s: 0.1099, time:16.1244, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:26:07 \u001b[32mINFO     \u001b[0m train.py: [23/300], [120/484], step: 11252, 2.080 samples/sec, batch_loss: 0.3236, batch_loss_c: 0.3271, batch_loss_s: 0.3154, time:19.2308, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:26:28 \u001b[32mINFO     \u001b[0m train.py: [23/300], [130/484], step: 11262, 1.883 samples/sec, batch_loss: 0.3119, batch_loss_c: 0.3076, batch_loss_s: 0.3220, time:21.2417, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:26:54 \u001b[32mINFO     \u001b[0m train.py: [23/300], [140/484], step: 11272, 1.596 samples/sec, batch_loss: 0.3021, batch_loss_c: 0.2948, batch_loss_s: 0.3193, time:25.0690, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:27:25 \u001b[32mINFO     \u001b[0m train.py: [23/300], [150/484], step: 11282, 1.280 samples/sec, batch_loss: 0.0938, batch_loss_c: 0.0835, batch_loss_s: 0.1179, time:31.2399, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:27:57 \u001b[32mINFO     \u001b[0m train.py: [23/300], [160/484], step: 11292, 1.253 samples/sec, batch_loss: 0.3096, batch_loss_c: 0.3061, batch_loss_s: 0.3175, time:31.9270, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:28:26 \u001b[32mINFO     \u001b[0m train.py: [23/300], [170/484], step: 11302, 1.376 samples/sec, batch_loss: 0.0494, batch_loss_c: 0.0423, batch_loss_s: 0.0660, time:29.0648, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:28:48 \u001b[32mINFO     \u001b[0m train.py: [23/300], [180/484], step: 11312, 1.788 samples/sec, batch_loss: 0.2946, batch_loss_c: 0.2904, batch_loss_s: 0.3044, time:22.3687, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:29:08 \u001b[32mINFO     \u001b[0m train.py: [23/300], [190/484], step: 11322, 2.039 samples/sec, batch_loss: 0.0426, batch_loss_c: 0.0340, batch_loss_s: 0.0626, time:19.6209, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:29:39 \u001b[32mINFO     \u001b[0m train.py: [23/300], [200/484], step: 11332, 1.286 samples/sec, batch_loss: 0.2283, batch_loss_c: 0.2283, batch_loss_s: 0.2284, time:31.1068, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:30:05 \u001b[32mINFO     \u001b[0m train.py: [23/300], [210/484], step: 11342, 1.532 samples/sec, batch_loss: 0.0467, batch_loss_c: 0.0400, batch_loss_s: 0.0622, time:26.1042, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:30:26 \u001b[32mINFO     \u001b[0m train.py: [23/300], [220/484], step: 11352, 1.947 samples/sec, batch_loss: 0.1399, batch_loss_c: 0.1565, batch_loss_s: 0.1010, time:20.5430, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:30:45 \u001b[32mINFO     \u001b[0m train.py: [23/300], [230/484], step: 11362, 2.031 samples/sec, batch_loss: 0.1047, batch_loss_c: 0.0718, batch_loss_s: 0.1817, time:19.6980, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:31:05 \u001b[32mINFO     \u001b[0m train.py: [23/300], [240/484], step: 11372, 1.985 samples/sec, batch_loss: 0.2951, batch_loss_c: 0.2922, batch_loss_s: 0.3019, time:20.1508, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:31:30 \u001b[32mINFO     \u001b[0m train.py: [23/300], [250/484], step: 11382, 1.645 samples/sec, batch_loss: 0.2727, batch_loss_c: 0.2969, batch_loss_s: 0.2161, time:24.3235, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:31:46 \u001b[32mINFO     \u001b[0m train.py: [23/300], [260/484], step: 11392, 2.469 samples/sec, batch_loss: 0.1701, batch_loss_c: 0.1741, batch_loss_s: 0.1608, time:16.2005, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:32:05 \u001b[32mINFO     \u001b[0m train.py: [23/300], [270/484], step: 11402, 2.150 samples/sec, batch_loss: 0.0678, batch_loss_c: 0.0614, batch_loss_s: 0.0829, time:18.6046, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:32:21 \u001b[32mINFO     \u001b[0m train.py: [23/300], [280/484], step: 11412, 2.375 samples/sec, batch_loss: 0.0434, batch_loss_c: 0.0393, batch_loss_s: 0.0531, time:16.8454, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:32:39 \u001b[32mINFO     \u001b[0m train.py: [23/300], [290/484], step: 11422, 2.325 samples/sec, batch_loss: 0.0853, batch_loss_c: 0.0754, batch_loss_s: 0.1082, time:17.2064, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:33:22 \u001b[32mINFO     \u001b[0m train.py: [23/300], [300/484], step: 11432, 0.928 samples/sec, batch_loss: 0.2818, batch_loss_c: 0.2773, batch_loss_s: 0.2922, time:43.0952, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:33:37 \u001b[32mINFO     \u001b[0m train.py: [23/300], [310/484], step: 11442, 2.674 samples/sec, batch_loss: 0.0798, batch_loss_c: 0.0772, batch_loss_s: 0.0860, time:14.9592, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:34:05 \u001b[32mINFO     \u001b[0m train.py: [23/300], [320/484], step: 11452, 1.419 samples/sec, batch_loss: 0.0554, batch_loss_c: 0.0449, batch_loss_s: 0.0799, time:28.1814, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:34:21 \u001b[32mINFO     \u001b[0m train.py: [23/300], [330/484], step: 11462, 2.464 samples/sec, batch_loss: 0.1254, batch_loss_c: 0.1149, batch_loss_s: 0.1497, time:16.2361, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:34:38 \u001b[32mINFO     \u001b[0m train.py: [23/300], [340/484], step: 11472, 2.293 samples/sec, batch_loss: 0.2407, batch_loss_c: 0.2794, batch_loss_s: 0.1505, time:17.4448, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:34:56 \u001b[32mINFO     \u001b[0m train.py: [23/300], [350/484], step: 11482, 2.324 samples/sec, batch_loss: 0.0724, batch_loss_c: 0.0668, batch_loss_s: 0.0853, time:17.2141, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:35:12 \u001b[32mINFO     \u001b[0m train.py: [23/300], [360/484], step: 11492, 2.439 samples/sec, batch_loss: 0.0540, batch_loss_c: 0.0516, batch_loss_s: 0.0596, time:16.3976, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:35:45 \u001b[32mINFO     \u001b[0m train.py: [23/300], [370/484], step: 11502, 1.217 samples/sec, batch_loss: 0.3134, batch_loss_c: 0.3056, batch_loss_s: 0.3315, time:32.8599, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:36:19 \u001b[32mINFO     \u001b[0m train.py: [23/300], [380/484], step: 11512, 1.167 samples/sec, batch_loss: 0.1708, batch_loss_c: 0.1871, batch_loss_s: 0.1325, time:34.2664, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:36:37 \u001b[32mINFO     \u001b[0m train.py: [23/300], [390/484], step: 11522, 2.272 samples/sec, batch_loss: 0.3046, batch_loss_c: 0.2977, batch_loss_s: 0.3206, time:17.6031, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:36:51 \u001b[32mINFO     \u001b[0m train.py: [23/300], [400/484], step: 11532, 2.780 samples/sec, batch_loss: 0.0418, batch_loss_c: 0.0348, batch_loss_s: 0.0582, time:14.3861, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:37:12 \u001b[32mINFO     \u001b[0m train.py: [23/300], [410/484], step: 11542, 1.931 samples/sec, batch_loss: 0.0801, batch_loss_c: 0.0735, batch_loss_s: 0.0957, time:20.7183, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:37:31 \u001b[32mINFO     \u001b[0m train.py: [23/300], [420/484], step: 11552, 2.152 samples/sec, batch_loss: 0.2985, batch_loss_c: 0.2930, batch_loss_s: 0.3113, time:18.5831, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:37:46 \u001b[32mINFO     \u001b[0m train.py: [23/300], [430/484], step: 11562, 2.514 samples/sec, batch_loss: 0.0453, batch_loss_c: 0.0389, batch_loss_s: 0.0603, time:15.9131, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:38:03 \u001b[32mINFO     \u001b[0m train.py: [23/300], [440/484], step: 11572, 2.405 samples/sec, batch_loss: 0.0734, batch_loss_c: 0.0722, batch_loss_s: 0.0764, time:16.6329, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:38:25 \u001b[32mINFO     \u001b[0m train.py: [23/300], [450/484], step: 11582, 1.831 samples/sec, batch_loss: 0.5581, batch_loss_c: 0.5679, batch_loss_s: 0.5354, time:21.8453, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:38:48 \u001b[32mINFO     \u001b[0m train.py: [23/300], [460/484], step: 11592, 1.705 samples/sec, batch_loss: 0.0720, batch_loss_c: 0.0628, batch_loss_s: 0.0934, time:23.4552, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:39:03 \u001b[32mINFO     \u001b[0m train.py: [23/300], [470/484], step: 11602, 2.783 samples/sec, batch_loss: 0.0635, batch_loss_c: 0.0584, batch_loss_s: 0.0753, time:14.3726, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:39:36 \u001b[32mINFO     \u001b[0m train.py: [23/300], [480/484], step: 11612, 1.198 samples/sec, batch_loss: 0.2664, batch_loss_c: 0.2975, batch_loss_s: 0.1939, time:33.3911, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:39:41 \u001b[32mINFO     \u001b[0m train.py: [23/300], train_loss: 0.1382, time: 1061.0087, lr: 1e-05\u001b[0m\n",
            "2019-12-08 15:39:45 \u001b[32mINFO     \u001b[0m train.py: [24/300], [0/484], step: 11616, 13.673 samples/sec, batch_loss: 0.1196, batch_loss_c: 0.1143, batch_loss_s: 0.1318, time:2.9254, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:40:10 \u001b[32mINFO     \u001b[0m train.py: [24/300], [10/484], step: 11626, 1.566 samples/sec, batch_loss: 0.1024, batch_loss_c: 0.0793, batch_loss_s: 0.1564, time:25.5507, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:40:26 \u001b[32mINFO     \u001b[0m train.py: [24/300], [20/484], step: 11636, 2.530 samples/sec, batch_loss: 0.0538, batch_loss_c: 0.0457, batch_loss_s: 0.0727, time:15.8107, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:40:54 \u001b[32mINFO     \u001b[0m train.py: [24/300], [30/484], step: 11646, 1.443 samples/sec, batch_loss: 0.0813, batch_loss_c: 0.0747, batch_loss_s: 0.0967, time:27.7173, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:41:15 \u001b[32mINFO     \u001b[0m train.py: [24/300], [40/484], step: 11656, 1.886 samples/sec, batch_loss: 0.1016, batch_loss_c: 0.0888, batch_loss_s: 0.1315, time:21.2042, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:42:07 \u001b[32mINFO     \u001b[0m train.py: [24/300], [50/484], step: 11666, 0.768 samples/sec, batch_loss: 0.0629, batch_loss_c: 0.0561, batch_loss_s: 0.0789, time:52.1001, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:42:27 \u001b[32mINFO     \u001b[0m train.py: [24/300], [60/484], step: 11676, 1.993 samples/sec, batch_loss: 0.1159, batch_loss_c: 0.1114, batch_loss_s: 0.1262, time:20.0655, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:42:42 \u001b[32mINFO     \u001b[0m train.py: [24/300], [70/484], step: 11686, 2.748 samples/sec, batch_loss: 0.1076, batch_loss_c: 0.0959, batch_loss_s: 0.1348, time:14.5563, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:42:57 \u001b[32mINFO     \u001b[0m train.py: [24/300], [80/484], step: 11696, 2.615 samples/sec, batch_loss: 0.2888, batch_loss_c: 0.2865, batch_loss_s: 0.2941, time:15.2942, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:43:30 \u001b[32mINFO     \u001b[0m train.py: [24/300], [90/484], step: 11706, 1.228 samples/sec, batch_loss: 0.1032, batch_loss_c: 0.1051, batch_loss_s: 0.0989, time:32.5706, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:43:47 \u001b[32mINFO     \u001b[0m train.py: [24/300], [100/484], step: 11716, 2.261 samples/sec, batch_loss: 0.2915, batch_loss_c: 0.2886, batch_loss_s: 0.2982, time:17.6888, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:44:11 \u001b[32mINFO     \u001b[0m train.py: [24/300], [110/484], step: 11726, 1.715 samples/sec, batch_loss: 0.2933, batch_loss_c: 0.2881, batch_loss_s: 0.3056, time:23.3249, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:44:38 \u001b[32mINFO     \u001b[0m train.py: [24/300], [120/484], step: 11736, 1.474 samples/sec, batch_loss: 0.0794, batch_loss_c: 0.0706, batch_loss_s: 0.0998, time:27.1422, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:44:59 \u001b[32mINFO     \u001b[0m train.py: [24/300], [130/484], step: 11746, 1.854 samples/sec, batch_loss: 0.0622, batch_loss_c: 0.0570, batch_loss_s: 0.0744, time:21.5692, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:45:25 \u001b[32mINFO     \u001b[0m train.py: [24/300], [140/484], step: 11756, 1.560 samples/sec, batch_loss: 0.1063, batch_loss_c: 0.0853, batch_loss_s: 0.1552, time:25.6389, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:45:42 \u001b[32mINFO     \u001b[0m train.py: [24/300], [150/484], step: 11766, 2.307 samples/sec, batch_loss: 0.0552, batch_loss_c: 0.0434, batch_loss_s: 0.0830, time:17.3355, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:46:01 \u001b[32mINFO     \u001b[0m train.py: [24/300], [160/484], step: 11776, 2.123 samples/sec, batch_loss: 0.0929, batch_loss_c: 0.0788, batch_loss_s: 0.1258, time:18.8430, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:46:20 \u001b[32mINFO     \u001b[0m train.py: [24/300], [170/484], step: 11786, 2.127 samples/sec, batch_loss: 0.3006, batch_loss_c: 0.2959, batch_loss_s: 0.3114, time:18.8049, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:46:35 \u001b[32mINFO     \u001b[0m train.py: [24/300], [180/484], step: 11796, 2.662 samples/sec, batch_loss: 0.1079, batch_loss_c: 0.0991, batch_loss_s: 0.1284, time:15.0290, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:46:50 \u001b[32mINFO     \u001b[0m train.py: [24/300], [190/484], step: 11806, 2.651 samples/sec, batch_loss: 0.0719, batch_loss_c: 0.0660, batch_loss_s: 0.0857, time:15.0875, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:47:08 \u001b[32mINFO     \u001b[0m train.py: [24/300], [200/484], step: 11816, 2.269 samples/sec, batch_loss: 0.1353, batch_loss_c: 0.1357, batch_loss_s: 0.1344, time:17.6302, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:47:25 \u001b[32mINFO     \u001b[0m train.py: [24/300], [210/484], step: 11826, 2.354 samples/sec, batch_loss: 0.0675, batch_loss_c: 0.0560, batch_loss_s: 0.0945, time:16.9901, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:47:48 \u001b[32mINFO     \u001b[0m train.py: [24/300], [220/484], step: 11836, 1.713 samples/sec, batch_loss: 0.1117, batch_loss_c: 0.1111, batch_loss_s: 0.1132, time:23.3489, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:48:06 \u001b[32mINFO     \u001b[0m train.py: [24/300], [230/484], step: 11846, 2.256 samples/sec, batch_loss: 0.3644, batch_loss_c: 0.3575, batch_loss_s: 0.3804, time:17.7275, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:48:26 \u001b[32mINFO     \u001b[0m train.py: [24/300], [240/484], step: 11856, 1.948 samples/sec, batch_loss: 0.0509, batch_loss_c: 0.0439, batch_loss_s: 0.0671, time:20.5343, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:48:42 \u001b[32mINFO     \u001b[0m train.py: [24/300], [250/484], step: 11866, 2.561 samples/sec, batch_loss: 0.0654, batch_loss_c: 0.0605, batch_loss_s: 0.0767, time:15.6204, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:49:06 \u001b[32mINFO     \u001b[0m train.py: [24/300], [260/484], step: 11876, 1.639 samples/sec, batch_loss: 0.0624, batch_loss_c: 0.0608, batch_loss_s: 0.0662, time:24.4072, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:49:44 \u001b[32mINFO     \u001b[0m train.py: [24/300], [270/484], step: 11886, 1.070 samples/sec, batch_loss: 0.2992, batch_loss_c: 0.2972, batch_loss_s: 0.3040, time:37.3750, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:50:00 \u001b[32mINFO     \u001b[0m train.py: [24/300], [280/484], step: 11896, 2.464 samples/sec, batch_loss: 0.1024, batch_loss_c: 0.0903, batch_loss_s: 0.1306, time:16.2308, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:50:20 \u001b[32mINFO     \u001b[0m train.py: [24/300], [290/484], step: 11906, 1.965 samples/sec, batch_loss: 0.0358, batch_loss_c: 0.0295, batch_loss_s: 0.0507, time:20.3576, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:50:39 \u001b[32mINFO     \u001b[0m train.py: [24/300], [300/484], step: 11916, 2.161 samples/sec, batch_loss: 0.3556, batch_loss_c: 0.3716, batch_loss_s: 0.3184, time:18.5073, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:50:57 \u001b[32mINFO     \u001b[0m train.py: [24/300], [310/484], step: 11926, 2.137 samples/sec, batch_loss: 0.0548, batch_loss_c: 0.0468, batch_loss_s: 0.0735, time:18.7146, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:51:16 \u001b[32mINFO     \u001b[0m train.py: [24/300], [320/484], step: 11936, 2.173 samples/sec, batch_loss: 0.0455, batch_loss_c: 0.0418, batch_loss_s: 0.0542, time:18.4110, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:51:32 \u001b[32mINFO     \u001b[0m train.py: [24/300], [330/484], step: 11946, 2.523 samples/sec, batch_loss: 0.0570, batch_loss_c: 0.0500, batch_loss_s: 0.0734, time:15.8562, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:51:59 \u001b[32mINFO     \u001b[0m train.py: [24/300], [340/484], step: 11956, 1.477 samples/sec, batch_loss: 0.2063, batch_loss_c: 0.1579, batch_loss_s: 0.3192, time:27.0910, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:52:25 \u001b[32mINFO     \u001b[0m train.py: [24/300], [350/484], step: 11966, 1.543 samples/sec, batch_loss: 0.0407, batch_loss_c: 0.0341, batch_loss_s: 0.0559, time:25.9241, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:52:42 \u001b[32mINFO     \u001b[0m train.py: [24/300], [360/484], step: 11976, 2.358 samples/sec, batch_loss: 0.3013, batch_loss_c: 0.2983, batch_loss_s: 0.3082, time:16.9615, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:53:00 \u001b[32mINFO     \u001b[0m train.py: [24/300], [370/484], step: 11986, 2.236 samples/sec, batch_loss: 0.0627, batch_loss_c: 0.0551, batch_loss_s: 0.0805, time:17.8904, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:53:23 \u001b[32mINFO     \u001b[0m train.py: [24/300], [380/484], step: 11996, 1.735 samples/sec, batch_loss: 0.0421, batch_loss_c: 0.0360, batch_loss_s: 0.0564, time:23.0512, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:53:57 \u001b[32mINFO     \u001b[0m train.py: [24/300], [390/484], step: 12006, 1.161 samples/sec, batch_loss: 0.0560, batch_loss_c: 0.0498, batch_loss_s: 0.0703, time:34.4678, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:54:12 \u001b[32mINFO     \u001b[0m train.py: [24/300], [400/484], step: 12016, 2.745 samples/sec, batch_loss: 0.0512, batch_loss_c: 0.0444, batch_loss_s: 0.0671, time:14.5697, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:54:38 \u001b[32mINFO     \u001b[0m train.py: [24/300], [410/484], step: 12026, 1.524 samples/sec, batch_loss: 0.0419, batch_loss_c: 0.0328, batch_loss_s: 0.0633, time:26.2389, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:54:59 \u001b[32mINFO     \u001b[0m train.py: [24/300], [420/484], step: 12036, 1.908 samples/sec, batch_loss: 0.1262, batch_loss_c: 0.1189, batch_loss_s: 0.1432, time:20.9634, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:55:15 \u001b[32mINFO     \u001b[0m train.py: [24/300], [430/484], step: 12046, 2.515 samples/sec, batch_loss: 0.0638, batch_loss_c: 0.0612, batch_loss_s: 0.0700, time:15.9057, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:55:43 \u001b[32mINFO     \u001b[0m train.py: [24/300], [440/484], step: 12056, 1.397 samples/sec, batch_loss: 0.2998, batch_loss_c: 0.2882, batch_loss_s: 0.3268, time:28.6305, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:56:00 \u001b[32mINFO     \u001b[0m train.py: [24/300], [450/484], step: 12066, 2.452 samples/sec, batch_loss: 0.0543, batch_loss_c: 0.0440, batch_loss_s: 0.0781, time:16.3164, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:56:19 \u001b[32mINFO     \u001b[0m train.py: [24/300], [460/484], step: 12076, 2.118 samples/sec, batch_loss: 0.0859, batch_loss_c: 0.0766, batch_loss_s: 0.1075, time:18.8816, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:56:33 \u001b[32mINFO     \u001b[0m train.py: [24/300], [470/484], step: 12086, 2.717 samples/sec, batch_loss: 0.3103, batch_loss_c: 0.3057, batch_loss_s: 0.3210, time:14.7198, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:56:56 \u001b[32mINFO     \u001b[0m train.py: [24/300], [480/484], step: 12096, 1.770 samples/sec, batch_loss: 0.0581, batch_loss_c: 0.0529, batch_loss_s: 0.0704, time:22.5966, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:57:02 \u001b[32mINFO     \u001b[0m train.py: [24/300], train_loss: 0.1388, time: 1040.0160, lr: 1e-05\u001b[0m\n",
            "2019-12-08 15:57:05 \u001b[32mINFO     \u001b[0m train.py: [25/300], [0/484], step: 12100, 12.608 samples/sec, batch_loss: 0.3899, batch_loss_c: 0.3935, batch_loss_s: 0.3815, time:3.1726, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:57:21 \u001b[32mINFO     \u001b[0m train.py: [25/300], [10/484], step: 12110, 2.565 samples/sec, batch_loss: 0.3064, batch_loss_c: 0.2998, batch_loss_s: 0.3219, time:15.5937, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:57:38 \u001b[32mINFO     \u001b[0m train.py: [25/300], [20/484], step: 12120, 2.408 samples/sec, batch_loss: 0.1103, batch_loss_c: 0.1065, batch_loss_s: 0.1192, time:16.6119, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:58:03 \u001b[32mINFO     \u001b[0m train.py: [25/300], [30/484], step: 12130, 1.571 samples/sec, batch_loss: 0.0528, batch_loss_c: 0.0479, batch_loss_s: 0.0643, time:25.4690, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:58:20 \u001b[32mINFO     \u001b[0m train.py: [25/300], [40/484], step: 12140, 2.402 samples/sec, batch_loss: 0.2969, batch_loss_c: 0.2948, batch_loss_s: 0.3021, time:16.6529, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:58:37 \u001b[32mINFO     \u001b[0m train.py: [25/300], [50/484], step: 12150, 2.259 samples/sec, batch_loss: 0.0616, batch_loss_c: 0.0499, batch_loss_s: 0.0889, time:17.7071, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:58:57 \u001b[32mINFO     \u001b[0m train.py: [25/300], [60/484], step: 12160, 2.001 samples/sec, batch_loss: 0.0917, batch_loss_c: 0.0879, batch_loss_s: 0.1006, time:19.9894, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:59:17 \u001b[32mINFO     \u001b[0m train.py: [25/300], [70/484], step: 12170, 1.995 samples/sec, batch_loss: 0.0755, batch_loss_c: 0.0683, batch_loss_s: 0.0924, time:20.0479, lr:1e-05\u001b[0m\n",
            "2019-12-08 15:59:34 \u001b[32mINFO     \u001b[0m train.py: [25/300], [80/484], step: 12180, 2.357 samples/sec, batch_loss: 0.0692, batch_loss_c: 0.0673, batch_loss_s: 0.0737, time:16.9743, lr:1e-05\u001b[0m\n",
            "2019-12-08 16:00:01 \u001b[32mINFO     \u001b[0m train.py: [25/300], [90/484], step: 12190, 1.489 samples/sec, batch_loss: 0.2991, batch_loss_c: 0.2974, batch_loss_s: 0.3029, time:26.8615, lr:1e-05\u001b[0m\n",
            "2019-12-08 16:00:19 \u001b[32mINFO     \u001b[0m train.py: [25/300], [100/484], step: 12200, 2.273 samples/sec, batch_loss: 0.1030, batch_loss_c: 0.0944, batch_loss_s: 0.1230, time:17.5993, lr:1e-05\u001b[0m\n",
            "2019-12-08 16:00:46 \u001b[32mINFO     \u001b[0m train.py: [25/300], [110/484], step: 12210, 1.497 samples/sec, batch_loss: 0.3126, batch_loss_c: 0.3083, batch_loss_s: 0.3227, time:26.7199, lr:1e-05\u001b[0m\n",
            "2019-12-08 16:01:03 \u001b[32mINFO     \u001b[0m train.py: [25/300], [120/484], step: 12220, 2.361 samples/sec, batch_loss: 0.0743, batch_loss_c: 0.0728, batch_loss_s: 0.0780, time:16.9450, lr:1e-05\u001b[0m\n",
            "2019-12-08 16:01:19 \u001b[32mINFO     \u001b[0m train.py: [25/300], [130/484], step: 12230, 2.439 samples/sec, batch_loss: 0.0321, batch_loss_c: 0.0274, batch_loss_s: 0.0430, time:16.3978, lr:1e-05\u001b[0m\n",
            "2019-12-08 16:01:40 \u001b[32mINFO     \u001b[0m train.py: [25/300], [140/484], step: 12240, 1.919 samples/sec, batch_loss: 0.0566, batch_loss_c: 0.0514, batch_loss_s: 0.0687, time:20.8471, lr:1e-05\u001b[0m\n",
            "2019-12-08 16:02:03 \u001b[32mINFO     \u001b[0m train.py: [25/300], [150/484], step: 12250, 1.695 samples/sec, batch_loss: 0.0737, batch_loss_c: 0.0575, batch_loss_s: 0.1114, time:23.5938, lr:1e-05\u001b[0m\n",
            "2019-12-08 16:02:44 \u001b[32mINFO     \u001b[0m train.py: [25/300], [160/484], step: 12260, 0.987 samples/sec, batch_loss: 0.1646, batch_loss_c: 0.1496, batch_loss_s: 0.1997, time:40.5214, lr:1e-05\u001b[0m\n",
            "2019-12-08 16:03:01 \u001b[32mINFO     \u001b[0m train.py: [25/300], [170/484], step: 12270, 2.344 samples/sec, batch_loss: 0.0418, batch_loss_c: 0.0351, batch_loss_s: 0.0573, time:17.0646, lr:1e-05\u001b[0m\n",
            "2019-12-08 16:03:17 \u001b[32mINFO     \u001b[0m train.py: [25/300], [180/484], step: 12280, 2.420 samples/sec, batch_loss: 0.4856, batch_loss_c: 0.4628, batch_loss_s: 0.5386, time:16.5307, lr:1e-05\u001b[0m\n",
            "2019-12-08 16:03:36 \u001b[32mINFO     \u001b[0m train.py: [25/300], [190/484], step: 12290, 2.154 samples/sec, batch_loss: 0.0610, batch_loss_c: 0.0479, batch_loss_s: 0.0914, time:18.5684, lr:1e-05\u001b[0m\n",
            "2019-12-08 16:04:08 \u001b[32mINFO     \u001b[0m train.py: [25/300], [200/484], step: 12300, 1.260 samples/sec, batch_loss: 0.1215, batch_loss_c: 0.1256, batch_loss_s: 0.1119, time:31.7558, lr:1e-05\u001b[0m\n",
            "2019-12-08 16:04:32 \u001b[32mINFO     \u001b[0m train.py: [25/300], [210/484], step: 12310, 1.628 samples/sec, batch_loss: 0.1367, batch_loss_c: 0.1351, batch_loss_s: 0.1405, time:24.5656, lr:1e-05\u001b[0m\n",
            "2019-12-08 16:04:52 \u001b[32mINFO     \u001b[0m train.py: [25/300], [220/484], step: 12320, 2.080 samples/sec, batch_loss: 0.0452, batch_loss_c: 0.0397, batch_loss_s: 0.0580, time:19.2278, lr:1e-05\u001b[0m\n",
            "2019-12-08 16:05:12 \u001b[32mINFO     \u001b[0m train.py: [25/300], [230/484], step: 12330, 1.955 samples/sec, batch_loss: 0.1750, batch_loss_c: 0.2240, batch_loss_s: 0.0606, time:20.4656, lr:1e-05\u001b[0m\n",
            "2019-12-08 16:05:37 \u001b[32mINFO     \u001b[0m train.py: [25/300], [240/484], step: 12340, 1.628 samples/sec, batch_loss: 0.0985, batch_loss_c: 0.0862, batch_loss_s: 0.1270, time:24.5749, lr:1e-05\u001b[0m\n",
            "2019-12-08 16:06:09 \u001b[32mINFO     \u001b[0m train.py: [25/300], [250/484], step: 12350, 1.239 samples/sec, batch_loss: 0.1284, batch_loss_c: 0.1047, batch_loss_s: 0.1837, time:32.2902, lr:1e-05\u001b[0m\n",
            "2019-12-08 16:06:28 \u001b[32mINFO     \u001b[0m train.py: [25/300], [260/484], step: 12360, 2.043 samples/sec, batch_loss: 0.2875, batch_loss_c: 0.2795, batch_loss_s: 0.3063, time:19.5752, lr:1e-05\u001b[0m\n",
            "2019-12-08 16:06:51 \u001b[32mINFO     \u001b[0m train.py: [25/300], [270/484], step: 12370, 1.799 samples/sec, batch_loss: 0.0488, batch_loss_c: 0.0434, batch_loss_s: 0.0613, time:22.2369, lr:1e-05\u001b[0m\n",
            "2019-12-08 16:07:06 \u001b[32mINFO     \u001b[0m train.py: [25/300], [280/484], step: 12380, 2.566 samples/sec, batch_loss: 0.0781, batch_loss_c: 0.0685, batch_loss_s: 0.1004, time:15.5855, lr:1e-05\u001b[0m\n",
            "2019-12-08 16:07:38 \u001b[32mINFO     \u001b[0m train.py: [25/300], [290/484], step: 12390, 1.280 samples/sec, batch_loss: 0.3154, batch_loss_c: 0.3094, batch_loss_s: 0.3296, time:31.2535, lr:1e-05\u001b[0m\n",
            "2019-12-08 16:08:03 \u001b[32mINFO     \u001b[0m train.py: [25/300], [300/484], step: 12400, 1.570 samples/sec, batch_loss: 0.0888, batch_loss_c: 0.0888, batch_loss_s: 0.0886, time:25.4711, lr:1e-05\u001b[0m\n",
            "2019-12-08 16:08:33 \u001b[32mINFO     \u001b[0m train.py: [25/300], [310/484], step: 12410, 1.351 samples/sec, batch_loss: 0.0688, batch_loss_c: 0.0679, batch_loss_s: 0.0710, time:29.6142, lr:1e-05\u001b[0m\n",
            "2019-12-08 16:08:47 \u001b[32mINFO     \u001b[0m train.py: [25/300], [320/484], step: 12420, 2.788 samples/sec, batch_loss: 0.2123, batch_loss_c: 0.1928, batch_loss_s: 0.2577, time:14.3489, lr:1e-05\u001b[0m\n",
            "2019-12-08 16:09:08 \u001b[32mINFO     \u001b[0m train.py: [25/300], [330/484], step: 12430, 1.946 samples/sec, batch_loss: 0.2478, batch_loss_c: 0.2329, batch_loss_s: 0.2825, time:20.5498, lr:1e-05\u001b[0m\n",
            "2019-12-08 16:09:41 \u001b[32mINFO     \u001b[0m train.py: [25/300], [340/484], step: 12440, 1.196 samples/sec, batch_loss: 0.0998, batch_loss_c: 0.1022, batch_loss_s: 0.0941, time:33.4488, lr:1e-05\u001b[0m\n",
            "2019-12-08 16:09:55 \u001b[32mINFO     \u001b[0m train.py: [25/300], [350/484], step: 12450, 2.831 samples/sec, batch_loss: 0.0566, batch_loss_c: 0.0445, batch_loss_s: 0.0848, time:14.1285, lr:1e-05\u001b[0m\n",
            "2019-12-08 16:10:18 \u001b[32mINFO     \u001b[0m train.py: [25/300], [360/484], step: 12460, 1.723 samples/sec, batch_loss: 0.0659, batch_loss_c: 0.0540, batch_loss_s: 0.0936, time:23.2145, lr:1e-05\u001b[0m\n",
            "2019-12-08 16:10:38 \u001b[32mINFO     \u001b[0m train.py: [25/300], [370/484], step: 12470, 2.053 samples/sec, batch_loss: 0.2406, batch_loss_c: 0.1852, batch_loss_s: 0.3698, time:19.4824, lr:1e-05\u001b[0m\n",
            "2019-12-08 16:11:02 \u001b[32mINFO     \u001b[0m train.py: [25/300], [380/484], step: 12480, 1.672 samples/sec, batch_loss: 0.1192, batch_loss_c: 0.1187, batch_loss_s: 0.1205, time:23.9194, lr:1e-05\u001b[0m\n",
            "2019-12-08 16:11:22 \u001b[32mINFO     \u001b[0m train.py: [25/300], [390/484], step: 12490, 1.990 samples/sec, batch_loss: 0.3370, batch_loss_c: 0.3190, batch_loss_s: 0.3788, time:20.1024, lr:1e-05\u001b[0m\n",
            "2019-12-08 16:11:38 \u001b[32mINFO     \u001b[0m train.py: [25/300], [400/484], step: 12500, 2.526 samples/sec, batch_loss: 0.0458, batch_loss_c: 0.0421, batch_loss_s: 0.0544, time:15.8347, lr:1e-05\u001b[0m\n",
            "2019-12-08 16:12:07 \u001b[32mINFO     \u001b[0m train.py: [25/300], [410/484], step: 12510, 1.381 samples/sec, batch_loss: 0.0358, batch_loss_c: 0.0308, batch_loss_s: 0.0474, time:28.9588, lr:1e-05\u001b[0m\n",
            "2019-12-08 16:12:25 \u001b[32mINFO     \u001b[0m train.py: [25/300], [420/484], step: 12520, 2.225 samples/sec, batch_loss: 0.2500, batch_loss_c: 0.2724, batch_loss_s: 0.1977, time:17.9770, lr:1e-05\u001b[0m\n",
            "2019-12-08 16:12:41 \u001b[32mINFO     \u001b[0m train.py: [25/300], [430/484], step: 12530, 2.512 samples/sec, batch_loss: 0.1258, batch_loss_c: 0.1077, batch_loss_s: 0.1682, time:15.9263, lr:1e-05\u001b[0m\n",
            "2019-12-08 16:12:58 \u001b[32mINFO     \u001b[0m train.py: [25/300], [440/484], step: 12540, 2.282 samples/sec, batch_loss: 0.1186, batch_loss_c: 0.1132, batch_loss_s: 0.1314, time:17.5295, lr:1e-05\u001b[0m\n",
            "2019-12-08 16:13:13 \u001b[32mINFO     \u001b[0m train.py: [25/300], [450/484], step: 12550, 2.678 samples/sec, batch_loss: 0.3779, batch_loss_c: 0.3395, batch_loss_s: 0.4675, time:14.9385, lr:1e-05\u001b[0m\n",
            "2019-12-08 16:13:35 \u001b[32mINFO     \u001b[0m train.py: [25/300], [460/484], step: 12560, 1.817 samples/sec, batch_loss: 0.4006, batch_loss_c: 0.3426, batch_loss_s: 0.5361, time:22.0201, lr:1e-05\u001b[0m\n",
            "2019-12-08 16:13:54 \u001b[32mINFO     \u001b[0m train.py: [25/300], [470/484], step: 12570, 2.156 samples/sec, batch_loss: 0.0693, batch_loss_c: 0.0606, batch_loss_s: 0.0895, time:18.5536, lr:1e-05\u001b[0m\n",
            "2019-12-08 16:14:09 \u001b[32mINFO     \u001b[0m train.py: [25/300], [480/484], step: 12580, 2.566 samples/sec, batch_loss: 0.2829, batch_loss_c: 0.2779, batch_loss_s: 0.2946, time:15.5860, lr:1e-05\u001b[0m\n",
            "2019-12-08 16:14:14 \u001b[32mINFO     \u001b[0m train.py: [25/300], train_loss: 0.1412, time: 1032.0598, lr: 1e-05\u001b[0m\n",
            "2019-12-08 16:14:24 \u001b[32mINFO     \u001b[0m train.py: [26/300], [0/484], step: 12584, 4.067 samples/sec, batch_loss: 0.0897, batch_loss_c: 0.0712, batch_loss_s: 0.1330, time:9.8362, lr:1e-05\u001b[0m\n",
            "2019-12-08 16:14:40 \u001b[32mINFO     \u001b[0m train.py: [26/300], [10/484], step: 12594, 2.523 samples/sec, batch_loss: 0.0523, batch_loss_c: 0.0422, batch_loss_s: 0.0758, time:15.8514, lr:1e-05\u001b[0m\n",
            "2019-12-08 16:14:59 \u001b[32mINFO     \u001b[0m train.py: [26/300], [20/484], step: 12604, 2.166 samples/sec, batch_loss: 0.0702, batch_loss_c: 0.0578, batch_loss_s: 0.0992, time:18.4634, lr:1e-05\u001b[0m\n",
            "2019-12-08 16:15:16 \u001b[32mINFO     \u001b[0m train.py: [26/300], [30/484], step: 12614, 2.340 samples/sec, batch_loss: 0.1013, batch_loss_c: 0.0915, batch_loss_s: 0.1242, time:17.0942, lr:1e-05\u001b[0m\n",
            "tcmalloc: large alloc 6734348288 bytes == 0x7f1b83d62000 @  0x7f203bedd1e7 0x7f20304c1f71 0x7f203052555d 0x7f2030528e28 0x7f20305293e5 0x7f20305bffc2 0x50ac25 0x50d390 0x508245 0x50a080 0x50aa7d 0x50d390 0x509455 0x595311 0x5a067e 0x557d28 0x50c88b 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x509455 0x595311 0x5a522c 0x557f7e 0x4b5eff 0x50c810\n",
            "2019-12-08 16:16:07 \u001b[32mINFO     \u001b[0m train.py: [26/300], [40/484], step: 12624, 0.781 samples/sec, batch_loss: 0.1114, batch_loss_c: 0.0850, batch_loss_s: 0.1730, time:51.2414, lr:1e-05\u001b[0m\n",
            "2019-12-08 16:16:29 \u001b[32mINFO     \u001b[0m train.py: [26/300], [50/484], step: 12634, 1.860 samples/sec, batch_loss: 0.0539, batch_loss_c: 0.0445, batch_loss_s: 0.0758, time:21.5020, lr:1e-05\u001b[0m\n",
            "2019-12-08 16:16:43 \u001b[32mINFO     \u001b[0m train.py: [26/300], [60/484], step: 12644, 2.824 samples/sec, batch_loss: 0.0468, batch_loss_c: 0.0388, batch_loss_s: 0.0656, time:14.1629, lr:1e-05\u001b[0m\n",
            "2019-12-08 16:17:08 \u001b[32mINFO     \u001b[0m train.py: [26/300], [70/484], step: 12654, 1.601 samples/sec, batch_loss: 0.3142, batch_loss_c: 0.3037, batch_loss_s: 0.3387, time:24.9838, lr:1e-05\u001b[0m\n",
            "2019-12-08 16:17:39 \u001b[32mINFO     \u001b[0m train.py: [26/300], [80/484], step: 12664, 1.294 samples/sec, batch_loss: 0.0480, batch_loss_c: 0.0410, batch_loss_s: 0.0645, time:30.9139, lr:1e-05\u001b[0m\n",
            "2019-12-08 16:17:55 \u001b[32mINFO     \u001b[0m train.py: [26/300], [90/484], step: 12674, 2.469 samples/sec, batch_loss: 0.2036, batch_loss_c: 0.1794, batch_loss_s: 0.2602, time:16.2027, lr:1e-05\u001b[0m\n",
            "2019-12-08 16:18:18 \u001b[32mINFO     \u001b[0m train.py: [26/300], [100/484], step: 12684, 1.736 samples/sec, batch_loss: 0.5187, batch_loss_c: 0.5136, batch_loss_s: 0.5308, time:23.0447, lr:1e-05\u001b[0m\n",
            "2019-12-08 16:18:43 \u001b[32mINFO     \u001b[0m train.py: [26/300], [110/484], step: 12694, 1.616 samples/sec, batch_loss: 0.0516, batch_loss_c: 0.0415, batch_loss_s: 0.0751, time:24.7463, lr:1e-05\u001b[0m\n",
            "2019-12-08 16:19:09 \u001b[32mINFO     \u001b[0m train.py: [26/300], [120/484], step: 12704, 1.519 samples/sec, batch_loss: 0.0953, batch_loss_c: 0.0840, batch_loss_s: 0.1217, time:26.3387, lr:1e-05\u001b[0m\n",
            "2019-12-08 16:19:26 \u001b[32mINFO     \u001b[0m train.py: [26/300], [130/484], step: 12714, 2.401 samples/sec, batch_loss: 0.0576, batch_loss_c: 0.0451, batch_loss_s: 0.0866, time:16.6614, lr:1e-05\u001b[0m\n",
            "2019-12-08 16:19:51 \u001b[32mINFO     \u001b[0m train.py: [26/300], [140/484], step: 12724, 1.567 samples/sec, batch_loss: 0.2975, batch_loss_c: 0.2915, batch_loss_s: 0.3116, time:25.5255, lr:1e-05\u001b[0m\n",
            "2019-12-08 16:20:07 \u001b[32mINFO     \u001b[0m train.py: [26/300], [150/484], step: 12734, 2.482 samples/sec, batch_loss: 0.1703, batch_loss_c: 0.2158, batch_loss_s: 0.0643, time:16.1145, lr:1e-05\u001b[0m\n",
            "2019-12-08 16:20:22 \u001b[32mINFO     \u001b[0m train.py: [26/300], [160/484], step: 12744, 2.783 samples/sec, batch_loss: 0.0565, batch_loss_c: 0.0463, batch_loss_s: 0.0802, time:14.3733, lr:1e-05\u001b[0m\n",
            "2019-12-08 16:20:38 \u001b[32mINFO     \u001b[0m train.py: [26/300], [170/484], step: 12754, 2.397 samples/sec, batch_loss: 0.0646, batch_loss_c: 0.0603, batch_loss_s: 0.0746, time:16.6858, lr:1e-05\u001b[0m\n",
            "2019-12-08 16:20:53 \u001b[32mINFO     \u001b[0m train.py: [26/300], [180/484], step: 12764, 2.679 samples/sec, batch_loss: 0.0599, batch_loss_c: 0.0465, batch_loss_s: 0.0911, time:14.9283, lr:1e-05\u001b[0m\n",
            "2019-12-08 16:21:10 \u001b[32mINFO     \u001b[0m train.py: [26/300], [190/484], step: 12774, 2.444 samples/sec, batch_loss: 0.1194, batch_loss_c: 0.1218, batch_loss_s: 0.1138, time:16.3672, lr:1e-05\u001b[0m\n",
            "2019-12-08 16:21:36 \u001b[32mINFO     \u001b[0m train.py: [26/300], [200/484], step: 12784, 1.510 samples/sec, batch_loss: 0.0630, batch_loss_c: 0.0542, batch_loss_s: 0.0837, time:26.4945, lr:1e-05\u001b[0m\n",
            "2019-12-08 16:22:22 \u001b[32mINFO     \u001b[0m train.py: [26/300], [210/484], step: 12794, 0.877 samples/sec, batch_loss: 0.2401, batch_loss_c: 0.2231, batch_loss_s: 0.2797, time:45.5988, lr:1e-05\u001b[0m\n",
            "2019-12-08 16:22:38 \u001b[32mINFO     \u001b[0m train.py: [26/300], [220/484], step: 12804, 2.417 samples/sec, batch_loss: 0.2071, batch_loss_c: 0.1963, batch_loss_s: 0.2325, time:16.5522, lr:1e-05\u001b[0m\n",
            "2019-12-08 16:22:53 \u001b[32mINFO     \u001b[0m train.py: [26/300], [230/484], step: 12814, 2.655 samples/sec, batch_loss: 0.0617, batch_loss_c: 0.0560, batch_loss_s: 0.0748, time:15.0670, lr:1e-05\u001b[0m\n",
            "2019-12-08 16:23:19 \u001b[32mINFO     \u001b[0m train.py: [26/300], [240/484], step: 12824, 1.537 samples/sec, batch_loss: 0.1444, batch_loss_c: 0.1322, batch_loss_s: 0.1727, time:26.0183, lr:1e-05\u001b[0m\n",
            "2019-12-08 16:23:43 \u001b[32mINFO     \u001b[0m train.py: [26/300], [250/484], step: 12834, 1.729 samples/sec, batch_loss: 0.0727, batch_loss_c: 0.0680, batch_loss_s: 0.0835, time:23.1365, lr:1e-05\u001b[0m\n",
            "2019-12-08 16:23:59 \u001b[32mINFO     \u001b[0m train.py: [26/300], [260/484], step: 12844, 2.447 samples/sec, batch_loss: 0.0525, batch_loss_c: 0.0426, batch_loss_s: 0.0757, time:16.3473, lr:1e-05\u001b[0m\n",
            "2019-12-08 16:24:19 \u001b[32mINFO     \u001b[0m train.py: [26/300], [270/484], step: 12854, 1.986 samples/sec, batch_loss: 0.0772, batch_loss_c: 0.0631, batch_loss_s: 0.1100, time:20.1456, lr:1e-05\u001b[0m\n",
            "2019-12-08 16:24:34 \u001b[32mINFO     \u001b[0m train.py: [26/300], [280/484], step: 12864, 2.640 samples/sec, batch_loss: 0.2874, batch_loss_c: 0.2830, batch_loss_s: 0.2975, time:15.1504, lr:1e-05\u001b[0m\n",
            "2019-12-08 16:24:53 \u001b[32mINFO     \u001b[0m train.py: [26/300], [290/484], step: 12874, 2.175 samples/sec, batch_loss: 0.0691, batch_loss_c: 0.0661, batch_loss_s: 0.0761, time:18.3898, lr:1e-05\u001b[0m\n",
            "2019-12-08 16:25:17 \u001b[32mINFO     \u001b[0m train.py: [26/300], [300/484], step: 12884, 1.650 samples/sec, batch_loss: 0.3295, batch_loss_c: 0.3254, batch_loss_s: 0.3392, time:24.2377, lr:1e-05\u001b[0m\n",
            "2019-12-08 16:25:40 \u001b[32mINFO     \u001b[0m train.py: [26/300], [310/484], step: 12894, 1.712 samples/sec, batch_loss: 0.0559, batch_loss_c: 0.0487, batch_loss_s: 0.0726, time:23.3685, lr:1e-05\u001b[0m\n",
            "2019-12-08 16:26:07 \u001b[32mINFO     \u001b[0m train.py: [26/300], [320/484], step: 12904, 1.494 samples/sec, batch_loss: 0.0867, batch_loss_c: 0.0781, batch_loss_s: 0.1067, time:26.7817, lr:1e-05\u001b[0m\n",
            "2019-12-08 16:26:25 \u001b[32mINFO     \u001b[0m train.py: [26/300], [330/484], step: 12914, 2.237 samples/sec, batch_loss: 0.2927, batch_loss_c: 0.2903, batch_loss_s: 0.2984, time:17.8849, lr:1e-05\u001b[0m\n",
            "2019-12-08 16:26:42 \u001b[32mINFO     \u001b[0m train.py: [26/300], [340/484], step: 12924, 2.334 samples/sec, batch_loss: 0.0415, batch_loss_c: 0.0364, batch_loss_s: 0.0535, time:17.1377, lr:1e-05\u001b[0m\n",
            "2019-12-08 16:26:58 \u001b[32mINFO     \u001b[0m train.py: [26/300], [350/484], step: 12934, 2.547 samples/sec, batch_loss: 0.1322, batch_loss_c: 0.1590, batch_loss_s: 0.0695, time:15.7076, lr:1e-05\u001b[0m\n",
            "2019-12-08 16:27:15 \u001b[32mINFO     \u001b[0m train.py: [26/300], [360/484], step: 12944, 2.367 samples/sec, batch_loss: 0.0605, batch_loss_c: 0.0535, batch_loss_s: 0.0770, time:16.8964, lr:1e-05\u001b[0m\n",
            "2019-12-08 16:27:39 \u001b[32mINFO     \u001b[0m train.py: [26/300], [370/484], step: 12954, 1.664 samples/sec, batch_loss: 0.1587, batch_loss_c: 0.1704, batch_loss_s: 0.1314, time:24.0398, lr:1e-05\u001b[0m\n",
            "2019-12-08 16:27:52 \u001b[32mINFO     \u001b[0m train.py: [26/300], [380/484], step: 12964, 2.888 samples/sec, batch_loss: 0.0857, batch_loss_c: 0.0765, batch_loss_s: 0.1073, time:13.8519, lr:1e-05\u001b[0m\n",
            "2019-12-08 16:28:19 \u001b[32mINFO     \u001b[0m train.py: [26/300], [390/484], step: 12974, 1.518 samples/sec, batch_loss: 0.0794, batch_loss_c: 0.0789, batch_loss_s: 0.0806, time:26.3427, lr:1e-05\u001b[0m\n",
            "2019-12-08 16:28:36 \u001b[32mINFO     \u001b[0m train.py: [26/300], [400/484], step: 12984, 2.394 samples/sec, batch_loss: 0.3148, batch_loss_c: 0.3115, batch_loss_s: 0.3225, time:16.7056, lr:1e-05\u001b[0m\n",
            "2019-12-08 16:29:01 \u001b[32mINFO     \u001b[0m train.py: [26/300], [410/484], step: 12994, 1.589 samples/sec, batch_loss: 0.0442, batch_loss_c: 0.0363, batch_loss_s: 0.0625, time:25.1676, lr:1e-05\u001b[0m\n",
            "2019-12-08 16:29:21 \u001b[32mINFO     \u001b[0m train.py: [26/300], [420/484], step: 13004, 1.941 samples/sec, batch_loss: 0.0671, batch_loss_c: 0.0569, batch_loss_s: 0.0912, time:20.6066, lr:1e-05\u001b[0m\n",
            "2019-12-08 16:29:39 \u001b[32mINFO     \u001b[0m train.py: [26/300], [430/484], step: 13014, 2.287 samples/sec, batch_loss: 0.2420, batch_loss_c: 0.2080, batch_loss_s: 0.3212, time:17.4902, lr:1e-05\u001b[0m\n",
            "2019-12-08 16:29:59 \u001b[32mINFO     \u001b[0m train.py: [26/300], [440/484], step: 13024, 1.985 samples/sec, batch_loss: 0.1281, batch_loss_c: 0.1315, batch_loss_s: 0.1200, time:20.1463, lr:1e-05\u001b[0m\n",
            "2019-12-08 16:30:20 \u001b[32mINFO     \u001b[0m train.py: [26/300], [450/484], step: 13034, 1.890 samples/sec, batch_loss: 0.0884, batch_loss_c: 0.0804, batch_loss_s: 0.1070, time:21.1655, lr:1e-05\u001b[0m\n",
            "2019-12-08 16:30:39 \u001b[32mINFO     \u001b[0m train.py: [26/300], [460/484], step: 13044, 2.082 samples/sec, batch_loss: 0.2900, batch_loss_c: 0.2859, batch_loss_s: 0.2995, time:19.2118, lr:1e-05\u001b[0m\n",
            "2019-12-08 16:30:55 \u001b[32mINFO     \u001b[0m train.py: [26/300], [470/484], step: 13054, 2.633 samples/sec, batch_loss: 0.0690, batch_loss_c: 0.0610, batch_loss_s: 0.0876, time:15.1916, lr:1e-05\u001b[0m\n",
            "2019-12-08 16:31:13 \u001b[32mINFO     \u001b[0m train.py: [26/300], [480/484], step: 13064, 2.158 samples/sec, batch_loss: 0.1010, batch_loss_c: 0.1008, batch_loss_s: 0.1016, time:18.5384, lr:1e-05\u001b[0m\n",
            "2019-12-08 16:31:20 \u001b[32mINFO     \u001b[0m train.py: [26/300], train_loss: 0.1405, time: 1025.8600, lr: 1e-05\u001b[0m\n",
            "2019-12-08 16:31:22 \u001b[32mINFO     \u001b[0m train.py: [27/300], [0/484], step: 13068, 25.812 samples/sec, batch_loss: 0.0857, batch_loss_c: 0.0868, batch_loss_s: 0.0831, time:1.5497, lr:1e-05\u001b[0m\n",
            "2019-12-08 16:31:59 \u001b[32mINFO     \u001b[0m train.py: [27/300], [10/484], step: 13078, 1.107 samples/sec, batch_loss: 0.1174, batch_loss_c: 0.1157, batch_loss_s: 0.1216, time:36.1407, lr:1e-05\u001b[0m\n",
            "2019-12-08 16:32:14 \u001b[32mINFO     \u001b[0m train.py: [27/300], [20/484], step: 13088, 2.541 samples/sec, batch_loss: 0.1761, batch_loss_c: 0.1896, batch_loss_s: 0.1447, time:15.7435, lr:1e-05\u001b[0m\n",
            "2019-12-08 16:32:29 \u001b[32mINFO     \u001b[0m train.py: [27/300], [30/484], step: 13098, 2.734 samples/sec, batch_loss: 0.0448, batch_loss_c: 0.0393, batch_loss_s: 0.0575, time:14.6329, lr:1e-05\u001b[0m\n",
            "2019-12-08 16:32:49 \u001b[32mINFO     \u001b[0m train.py: [27/300], [40/484], step: 13108, 1.984 samples/sec, batch_loss: 0.1211, batch_loss_c: 0.1149, batch_loss_s: 0.1355, time:20.1658, lr:1e-05\u001b[0m\n",
            "2019-12-08 16:33:23 \u001b[32mINFO     \u001b[0m train.py: [27/300], [50/484], step: 13118, 1.167 samples/sec, batch_loss: 0.5201, batch_loss_c: 0.5159, batch_loss_s: 0.5298, time:34.2876, lr:1e-05\u001b[0m\n",
            "2019-12-08 16:33:44 \u001b[32mINFO     \u001b[0m train.py: [27/300], [60/484], step: 13128, 1.916 samples/sec, batch_loss: 0.2931, batch_loss_c: 0.2948, batch_loss_s: 0.2893, time:20.8810, lr:1e-05\u001b[0m\n",
            "2019-12-08 16:33:59 \u001b[32mINFO     \u001b[0m train.py: [27/300], [70/484], step: 13138, 2.703 samples/sec, batch_loss: 0.0555, batch_loss_c: 0.0468, batch_loss_s: 0.0758, time:14.8011, lr:1e-05\u001b[0m\n",
            "2019-12-08 16:34:23 \u001b[32mINFO     \u001b[0m train.py: [27/300], [80/484], step: 13148, 1.684 samples/sec, batch_loss: 0.3317, batch_loss_c: 0.3164, batch_loss_s: 0.3674, time:23.7552, lr:1e-05\u001b[0m\n",
            "2019-12-08 16:34:44 \u001b[32mINFO     \u001b[0m train.py: [27/300], [90/484], step: 13158, 1.934 samples/sec, batch_loss: 0.0991, batch_loss_c: 0.1049, batch_loss_s: 0.0855, time:20.6837, lr:1e-05\u001b[0m\n",
            "2019-12-08 16:35:09 \u001b[32mINFO     \u001b[0m train.py: [27/300], [100/484], step: 13168, 1.574 samples/sec, batch_loss: 0.3167, batch_loss_c: 0.3092, batch_loss_s: 0.3340, time:25.4152, lr:1e-05\u001b[0m\n",
            "2019-12-08 16:35:24 \u001b[32mINFO     \u001b[0m train.py: [27/300], [110/484], step: 13178, 2.673 samples/sec, batch_loss: 0.0483, batch_loss_c: 0.0463, batch_loss_s: 0.0528, time:14.9630, lr:1e-05\u001b[0m\n",
            "2019-12-08 16:35:39 \u001b[32mINFO     \u001b[0m train.py: [27/300], [120/484], step: 13188, 2.652 samples/sec, batch_loss: 0.1482, batch_loss_c: 0.1651, batch_loss_s: 0.1088, time:15.0840, lr:1e-05\u001b[0m\n",
            "2019-12-08 16:35:52 \u001b[32mINFO     \u001b[0m train.py: [27/300], [130/484], step: 13198, 2.980 samples/sec, batch_loss: 0.1098, batch_loss_c: 0.1016, batch_loss_s: 0.1289, time:13.4239, lr:1e-05\u001b[0m\n",
            "2019-12-08 16:36:10 \u001b[32mINFO     \u001b[0m train.py: [27/300], [140/484], step: 13208, 2.270 samples/sec, batch_loss: 0.1383, batch_loss_c: 0.1333, batch_loss_s: 0.1501, time:17.6184, lr:1e-05\u001b[0m\n",
            "2019-12-08 16:36:24 \u001b[32mINFO     \u001b[0m train.py: [27/300], [150/484], step: 13218, 2.779 samples/sec, batch_loss: 0.0812, batch_loss_c: 0.0756, batch_loss_s: 0.0944, time:14.3916, lr:1e-05\u001b[0m\n",
            "2019-12-08 16:36:47 \u001b[32mINFO     \u001b[0m train.py: [27/300], [160/484], step: 13228, 1.778 samples/sec, batch_loss: 0.0892, batch_loss_c: 0.0816, batch_loss_s: 0.1068, time:22.5017, lr:1e-05\u001b[0m\n",
            "2019-12-08 16:37:14 \u001b[32mINFO     \u001b[0m train.py: [27/300], [170/484], step: 13238, 1.470 samples/sec, batch_loss: 0.3536, batch_loss_c: 0.3163, batch_loss_s: 0.4407, time:27.2174, lr:1e-05\u001b[0m\n",
            "2019-12-08 16:37:41 \u001b[32mINFO     \u001b[0m train.py: [27/300], [180/484], step: 13248, 1.503 samples/sec, batch_loss: 0.1015, batch_loss_c: 0.1059, batch_loss_s: 0.0911, time:26.6170, lr:1e-05\u001b[0m\n",
            "2019-12-08 16:37:55 \u001b[32mINFO     \u001b[0m train.py: [27/300], [190/484], step: 13258, 2.755 samples/sec, batch_loss: 0.0517, batch_loss_c: 0.0457, batch_loss_s: 0.0659, time:14.5196, lr:1e-05\u001b[0m\n",
            "2019-12-08 16:38:10 \u001b[32mINFO     \u001b[0m train.py: [27/300], [200/484], step: 13268, 2.748 samples/sec, batch_loss: 0.0535, batch_loss_c: 0.0499, batch_loss_s: 0.0619, time:14.5574, lr:1e-05\u001b[0m\n",
            "2019-12-08 16:38:29 \u001b[32mINFO     \u001b[0m train.py: [27/300], [210/484], step: 13278, 2.077 samples/sec, batch_loss: 0.0881, batch_loss_c: 0.0707, batch_loss_s: 0.1286, time:19.2602, lr:1e-05\u001b[0m\n",
            "2019-12-08 16:38:50 \u001b[32mINFO     \u001b[0m train.py: [27/300], [220/484], step: 13288, 1.884 samples/sec, batch_loss: 0.3048, batch_loss_c: 0.3031, batch_loss_s: 0.3089, time:21.2269, lr:1e-05\u001b[0m\n",
            "2019-12-08 16:39:08 \u001b[32mINFO     \u001b[0m train.py: [27/300], [230/484], step: 13298, 2.207 samples/sec, batch_loss: 0.0510, batch_loss_c: 0.0420, batch_loss_s: 0.0721, time:18.1245, lr:1e-05\u001b[0m\n",
            "2019-12-08 16:39:33 \u001b[32mINFO     \u001b[0m train.py: [27/300], [240/484], step: 13308, 1.663 samples/sec, batch_loss: 0.0641, batch_loss_c: 0.0477, batch_loss_s: 0.1023, time:24.0511, lr:1e-05\u001b[0m\n",
            "2019-12-08 16:40:15 \u001b[32mINFO     \u001b[0m train.py: [27/300], [250/484], step: 13318, 0.937 samples/sec, batch_loss: 0.3159, batch_loss_c: 0.3111, batch_loss_s: 0.3273, time:42.6810, lr:1e-05\u001b[0m\n",
            "2019-12-08 16:40:36 \u001b[32mINFO     \u001b[0m train.py: [27/300], [260/484], step: 13328, 1.934 samples/sec, batch_loss: 0.2929, batch_loss_c: 0.2894, batch_loss_s: 0.3009, time:20.6850, lr:1e-05\u001b[0m\n",
            "2019-12-08 16:40:53 \u001b[32mINFO     \u001b[0m train.py: [27/300], [270/484], step: 13338, 2.290 samples/sec, batch_loss: 0.0812, batch_loss_c: 0.0692, batch_loss_s: 0.1091, time:17.4669, lr:1e-05\u001b[0m\n",
            "2019-12-08 16:41:10 \u001b[32mINFO     \u001b[0m train.py: [27/300], [280/484], step: 13348, 2.417 samples/sec, batch_loss: 0.2972, batch_loss_c: 0.2942, batch_loss_s: 0.3041, time:16.5463, lr:1e-05\u001b[0m\n",
            "2019-12-08 16:41:30 \u001b[32mINFO     \u001b[0m train.py: [27/300], [290/484], step: 13358, 2.014 samples/sec, batch_loss: 0.0903, batch_loss_c: 0.0657, batch_loss_s: 0.1478, time:19.8622, lr:1e-05\u001b[0m\n",
            "2019-12-08 16:41:47 \u001b[32mINFO     \u001b[0m train.py: [27/300], [300/484], step: 13368, 2.366 samples/sec, batch_loss: 0.3030, batch_loss_c: 0.2954, batch_loss_s: 0.3206, time:16.9080, lr:1e-05\u001b[0m\n",
            "2019-12-08 16:42:10 \u001b[32mINFO     \u001b[0m train.py: [27/300], [310/484], step: 13378, 1.732 samples/sec, batch_loss: 0.3014, batch_loss_c: 0.2979, batch_loss_s: 0.3094, time:23.0899, lr:1e-05\u001b[0m\n",
            "2019-12-08 16:42:26 \u001b[32mINFO     \u001b[0m train.py: [27/300], [320/484], step: 13388, 2.399 samples/sec, batch_loss: 0.1225, batch_loss_c: 0.1402, batch_loss_s: 0.0811, time:16.6750, lr:1e-05\u001b[0m\n",
            "2019-12-08 16:42:56 \u001b[32mINFO     \u001b[0m train.py: [27/300], [330/484], step: 13398, 1.359 samples/sec, batch_loss: 0.1137, batch_loss_c: 0.1109, batch_loss_s: 0.1203, time:29.4400, lr:1e-05\u001b[0m\n",
            "2019-12-08 16:43:20 \u001b[32mINFO     \u001b[0m train.py: [27/300], [340/484], step: 13408, 1.636 samples/sec, batch_loss: 0.0555, batch_loss_c: 0.0501, batch_loss_s: 0.0680, time:24.4442, lr:1e-05\u001b[0m\n",
            "2019-12-08 16:43:41 \u001b[32mINFO     \u001b[0m train.py: [27/300], [350/484], step: 13418, 1.958 samples/sec, batch_loss: 0.2811, batch_loss_c: 0.2796, batch_loss_s: 0.2844, time:20.4308, lr:1e-05\u001b[0m\n",
            "2019-12-08 16:43:55 \u001b[32mINFO     \u001b[0m train.py: [27/300], [360/484], step: 13428, 2.719 samples/sec, batch_loss: 0.0508, batch_loss_c: 0.0435, batch_loss_s: 0.0678, time:14.7089, lr:1e-05\u001b[0m\n",
            "2019-12-08 16:44:17 \u001b[32mINFO     \u001b[0m train.py: [27/300], [370/484], step: 13438, 1.863 samples/sec, batch_loss: 0.4275, batch_loss_c: 0.4825, batch_loss_s: 0.2992, time:21.4659, lr:1e-05\u001b[0m\n",
            "2019-12-08 16:44:44 \u001b[32mINFO     \u001b[0m train.py: [27/300], [380/484], step: 13448, 1.501 samples/sec, batch_loss: 0.1082, batch_loss_c: 0.1196, batch_loss_s: 0.0816, time:26.6508, lr:1e-05\u001b[0m\n",
            "2019-12-08 16:44:59 \u001b[32mINFO     \u001b[0m train.py: [27/300], [390/484], step: 13458, 2.551 samples/sec, batch_loss: 0.0696, batch_loss_c: 0.0486, batch_loss_s: 0.1184, time:15.6810, lr:1e-05\u001b[0m\n",
            "2019-12-08 16:45:15 \u001b[32mINFO     \u001b[0m train.py: [27/300], [400/484], step: 13468, 2.471 samples/sec, batch_loss: 0.0682, batch_loss_c: 0.0622, batch_loss_s: 0.0823, time:16.1908, lr:1e-05\u001b[0m\n",
            "2019-12-08 16:45:43 \u001b[32mINFO     \u001b[0m train.py: [27/300], [410/484], step: 13478, 1.450 samples/sec, batch_loss: 0.1138, batch_loss_c: 0.1087, batch_loss_s: 0.1257, time:27.5881, lr:1e-05\u001b[0m\n",
            "2019-12-08 16:46:01 \u001b[32mINFO     \u001b[0m train.py: [27/300], [420/484], step: 13488, 2.213 samples/sec, batch_loss: 0.0351, batch_loss_c: 0.0282, batch_loss_s: 0.0511, time:18.0778, lr:1e-05\u001b[0m\n",
            "2019-12-08 16:46:34 \u001b[32mINFO     \u001b[0m train.py: [27/300], [430/484], step: 13498, 1.205 samples/sec, batch_loss: 0.0526, batch_loss_c: 0.0433, batch_loss_s: 0.0743, time:33.1856, lr:1e-05\u001b[0m\n",
            "2019-12-08 16:46:50 \u001b[32mINFO     \u001b[0m train.py: [27/300], [440/484], step: 13508, 2.556 samples/sec, batch_loss: 0.2834, batch_loss_c: 0.2816, batch_loss_s: 0.2877, time:15.6497, lr:1e-05\u001b[0m\n",
            "2019-12-08 16:47:22 \u001b[32mINFO     \u001b[0m train.py: [27/300], [450/484], step: 13518, 1.238 samples/sec, batch_loss: 0.1947, batch_loss_c: 0.1742, batch_loss_s: 0.2425, time:32.3221, lr:1e-05\u001b[0m\n",
            "2019-12-08 16:47:39 \u001b[32mINFO     \u001b[0m train.py: [27/300], [460/484], step: 13528, 2.433 samples/sec, batch_loss: 0.0852, batch_loss_c: 0.0882, batch_loss_s: 0.0783, time:16.4434, lr:1e-05\u001b[0m\n",
            "2019-12-08 16:48:00 \u001b[32mINFO     \u001b[0m train.py: [27/300], [470/484], step: 13538, 1.840 samples/sec, batch_loss: 0.0606, batch_loss_c: 0.0530, batch_loss_s: 0.0783, time:21.7401, lr:1e-05\u001b[0m\n",
            "2019-12-08 16:48:16 \u001b[32mINFO     \u001b[0m train.py: [27/300], [480/484], step: 13548, 2.609 samples/sec, batch_loss: 0.1032, batch_loss_c: 0.0985, batch_loss_s: 0.1142, time:15.3329, lr:1e-05\u001b[0m\n",
            "2019-12-08 16:48:20 \u001b[32mINFO     \u001b[0m train.py: [27/300], train_loss: 0.1451, time: 1018.8716, lr: 1e-05\u001b[0m\n",
            "2019-12-08 16:48:22 \u001b[32mINFO     \u001b[0m train.py: [28/300], [0/484], step: 13552, 21.471 samples/sec, batch_loss: 0.1257, batch_loss_c: 0.1295, batch_loss_s: 0.1171, time:1.8630, lr:1e-05\u001b[0m\n",
            "2019-12-08 16:48:40 \u001b[32mINFO     \u001b[0m utils.py: models saved to /content/drive/My Drive/PSENet_2/final.pth\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}