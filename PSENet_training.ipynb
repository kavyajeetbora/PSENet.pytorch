{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PSENet_trial_run.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kavyajeetbora/PSENet.pytorch/blob/master/PSENet_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UByxwp0F3QUp",
        "colab_type": "text"
      },
      "source": [
        "## Installing softwares and other dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GSEHnmCxK_o9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, Dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y99kqKlHoYkz",
        "colab_type": "code",
        "outputId": "a457400c-56ae-4bb6-8f0f-d031981c5169",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "!pip install pyclipper"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyclipper\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e1/40/57a0d54a1c696d58253c88a95677e50ab2b305a15af0ac64b70db4320562/pyclipper-1.1.0.post3-cp36-cp36m-manylinux1_x86_64.whl (131kB)\n",
            "\r\u001b[K     |██▌                             | 10kB 25.8MB/s eta 0:00:01\r\u001b[K     |█████                           | 20kB 1.7MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 30kB 2.6MB/s eta 0:00:01\r\u001b[K     |██████████                      | 40kB 1.7MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 51kB 2.1MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 61kB 2.5MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 71kB 2.9MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 81kB 3.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 92kB 3.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 102kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 112kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 122kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 133kB 2.8MB/s \n",
            "\u001b[?25hInstalling collected packages: pyclipper\n",
            "Successfully installed pyclipper-1.1.0.post3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ncoynOlfnIW1",
        "colab_type": "code",
        "outputId": "4e437a2f-c317-413a-8405-2b166b7f93ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hhJ-6yyXcnRx",
        "colab_type": "code",
        "outputId": "468ac204-fb46-4ba0-fdf0-9f10bab04e8b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        }
      },
      "source": [
        "import shutil, os\n",
        "os.chdir('/content')\n",
        "directory = '/content/cloned-repo'\n",
        "if os.path.exists(directory):\n",
        "  shutil.rmtree(directory)\n",
        "\n",
        "!git clone https://github.com/kavyajeetbora/PSENet.pytorch.git /content/cloned-repo\n",
        "print(\"Cloned the repository\")\n",
        "os.chdir('/content/cloned-repo')\n",
        "!ls"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into '/content/cloned-repo'...\n",
            "remote: Enumerating objects: 27, done.\u001b[K\n",
            "remote: Counting objects:   3% (1/27)\u001b[K\rremote: Counting objects:   7% (2/27)\u001b[K\rremote: Counting objects:  11% (3/27)\u001b[K\rremote: Counting objects:  14% (4/27)\u001b[K\rremote: Counting objects:  18% (5/27)\u001b[K\rremote: Counting objects:  22% (6/27)\u001b[K\rremote: Counting objects:  25% (7/27)\u001b[K\rremote: Counting objects:  29% (8/27)\u001b[K\rremote: Counting objects:  33% (9/27)\u001b[K\rremote: Counting objects:  37% (10/27)\u001b[K\rremote: Counting objects:  40% (11/27)\u001b[K\rremote: Counting objects:  44% (12/27)\u001b[K\rremote: Counting objects:  48% (13/27)\u001b[K\rremote: Counting objects:  51% (14/27)\u001b[K\rremote: Counting objects:  55% (15/27)\u001b[K\rremote: Counting objects:  59% (16/27)\u001b[K\rremote: Counting objects:  62% (17/27)\u001b[K\rremote: Counting objects:  66% (18/27)\u001b[K\rremote: Counting objects:  70% (19/27)\u001b[K\rremote: Counting objects:  74% (20/27)\u001b[K\rremote: Counting objects:  77% (21/27)\u001b[K\rremote: Counting objects:  81% (22/27)\u001b[K\rremote: Counting objects:  85% (23/27)\u001b[K\rremote: Counting objects:  88% (24/27)\u001b[K\rremote: Counting objects:  92% (25/27)\u001b[K\rremote: Counting objects:  96% (26/27)\u001b[K\rremote: Counting objects: 100% (27/27)\u001b[K\rremote: Counting objects: 100% (27/27), done.\u001b[K\n",
            "remote: Compressing objects: 100% (27/27), done.\u001b[K\n",
            "remote: Total 590 (delta 11), reused 0 (delta 0), pack-reused 563\u001b[K\n",
            "Receiving objects: 100% (590/590), 28.08 MiB | 8.90 MiB/s, done.\n",
            "Resolving deltas: 100% (308/308), done.\n",
            "Cloned the repository\n",
            "cal_recall  install_dependencies.sh  PSENet.ipynb\t     train.py\n",
            "config.py   LICENSE\t\t     PSENet_predict.ipynb    utils\n",
            "dataset     models\t\t     PSENet_training.ipynb\n",
            "eval.py     predict.py\t\t     PSENet_trial_run.ipynb\n",
            "imgs\t    pse\t\t\t     README.md\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PrOmfo6_3X2I",
        "colab_type": "text"
      },
      "source": [
        "## Extracting the data and setting up the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dGLMxTujlm0w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## unzipping the files\n",
        "from zipfile import ZipFile\n",
        "\n",
        "def unzip_files(file,output_dir):\n",
        "  with ZipFile(file, 'r') as zipObj:\n",
        "    # Extract all the contents of zip file in current directory\n",
        "    zipObj.extractall(output_dir)\n",
        "  print('Extracted to',output_dir)\n",
        "\n",
        "def make_directory(directory):\n",
        "  if os.path.isdir(directory):\n",
        "    shutil.rmtree(directory)\n",
        "  \n",
        "  os.mkdir(directory)\n",
        "  print('Created a new directory')\n",
        "\n",
        "training_data_zip = '/content/drive/My Drive/Colab Notebooks/padh.ai.notebooks/15. Object Detection/Scene Text Detection Dataset/English and Hindi MLT 2019.zip'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FqJ6xb5u2dgg",
        "colab_type": "code",
        "outputId": "3618bb46-ac94-400d-b904-2ca1668883fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# make directories\n",
        "make_directory('Training Set')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Created a new directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FFSaSXr-m-K4",
        "colab_type": "code",
        "outputId": "29057fbc-6b58-4d97-e415-51d4ca827c07",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "%%time\n",
        "unzip_files(training_data_zip,'Training Set')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracted to Training Set\n",
            "CPU times: user 2.57 s, sys: 990 ms, total: 3.56 s\n",
            "Wall time: 7.54 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YRSeyluknVH4",
        "colab_type": "code",
        "outputId": "fdd626da-7247-45ba-c56f-6e02f7080d88",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(len(os.listdir('Training Set/Images')))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MOp4JsZ6nXvn",
        "colab_type": "code",
        "outputId": "e5936a93-a883-476f-fb5e-ba2168b76b3c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(len(os.listdir('Training Set/Annotations')))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mhvg89mXn8zG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from dataset.data_utils import *\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader, Dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4JeHTz3G6Lyj",
        "colab_type": "code",
        "outputId": "37611f6c-6d28-4e08-8404-5cf1b4381f82",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_data = MyDataset('Training Set',transform=transforms.ToTensor())\n",
        "len(train_data)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1938"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "odLKMn-5SBAz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device(\"cuda:0\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9a9PsyF3dqo",
        "colab_type": "text"
      },
      "source": [
        "## Training "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9x221Ya0RBGO",
        "outputId": "77c0a34e-1115-4531-b9f9-9b46756daeeb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        }
      },
      "source": [
        "!chmod +x install_dependencies.sh # make shell script executable\n",
        "!./install_dependencies.sh # run the shell script"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyclipper in /usr/local/lib/python3.6/dist-packages (1.1.0.post3)\n",
            "Collecting Polygon3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/73/a0/d07a4f3e80ed7020a33f3111db217f54ac44a485ff45da3c21ce49f65041/Polygon3-3.0.8.tar.gz (71kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 2.3MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: Polygon3\n",
            "  Building wheel for Polygon3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for Polygon3: filename=Polygon3-3.0.8-cp36-cp36m-linux_x86_64.whl size=101495 sha256=77957c89e31a8e239bcf69d6382a4ae0f8d24f763eec5e19b824050349dde591\n",
            "  Stored in directory: /root/.cache/pip/wheels/95/32/f1/5525b233996d9d99cbce2f0a8da60d137ddddc555d3e8b0e2a\n",
            "Successfully built Polygon3\n",
            "Installing collected packages: Polygon3\n",
            "Successfully installed Polygon3-3.0.8\n",
            "Collecting colorlog\n",
            "  Downloading https://files.pythonhosted.org/packages/68/4d/892728b0c14547224f0ac40884e722a3d00cb54e7a146aea0b3186806c9e/colorlog-4.0.2-py2.py3-none-any.whl\n",
            "Installing collected packages: colorlog\n",
            "Successfully installed colorlog-4.0.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xD4GuTKC3wXY",
        "colab_type": "code",
        "outputId": "8ead72b9-e40d-42e1-8266-75dfaeed2f5a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!python3 train.py"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "make: Entering directory '/content/cloned-repo/pse'\n",
            "make: 'pse.so' is up to date.\n",
            "make: Leaving directory '/content/cloned-repo/pse'\n",
            "2019-12-07 11:58:09 \u001b[32mINFO     \u001b[0m utils.py: logger init finished\u001b[0m\n",
            "2019-12-07 11:58:09 \u001b[32mINFO     \u001b[0m train.py: {'Lambda': 0.7,\n",
            " 'OHEM_ratio': 3,\n",
            " 'backbone': 'resnet50',\n",
            " 'checkpoint': '',\n",
            " 'data_shape': 640,\n",
            " 'display_input_images': False,\n",
            " 'display_interval': 10,\n",
            " 'display_output_images': False,\n",
            " 'end_lr': 1e-06,\n",
            " 'epochs': 300,\n",
            " 'gpu_id': '0',\n",
            " 'lr': 1e-05,\n",
            " 'lr_decay_step': [100, 200],\n",
            " 'lr_gamma': 0.1,\n",
            " 'm': 0.5,\n",
            " 'n': 6,\n",
            " 'output_dir': '/content/drive/My Drive/PSENet_2',\n",
            " 'pretrained': False,\n",
            " 'pretrained_path': '/content/drive/My Drive/PSENet_2/PSENet_resnet50.pth',\n",
            " 'restart_training': False,\n",
            " 'scale': 1,\n",
            " 'seed': 2,\n",
            " 'show_images_interval': 50,\n",
            " 'start_epoch': 0,\n",
            " 'testroot': 'Test Set',\n",
            " 'train_batch_size': 4,\n",
            " 'trainroot': 'Training Set',\n",
            " 'warm_up_epoch': 6,\n",
            " 'warm_up_lr': 1.0000000000000002e-06,\n",
            " 'weight_decay': 0.0005,\n",
            " 'workers': 0}\u001b[0m\n",
            "2019-12-07 11:58:09 \u001b[32mINFO     \u001b[0m train.py: train with gpu 0 and pytorch 1.3.1\u001b[0m\n",
            "2019-12-07 11:58:19 \u001b[32mINFO     \u001b[0m train.py: train dataset has 1938 samples,484 in dataloader\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:100: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule.See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "2019-12-07 11:58:34 \u001b[32mINFO     \u001b[0m train.py: [0/300], [0/484], step: 0, 2.689 samples/sec, batch_loss: 0.0644, batch_loss_c: 0.0633, batch_loss_s: 0.0670, time:14.8745, lr:1e-05\u001b[0m\n",
            "tcmalloc: large alloc 1798283264 bytes == 0xb6090000 @  0x7f214dff61e7 0x7f21425daf71 0x7f214263e55d 0x7f2142641e28 0x7f21426423e5 0x7f21426d8fc2 0x50ac25 0x50d390 0x508245 0x50a080 0x50aa7d 0x50d390 0x509455 0x595311 0x5a067e 0x557d28 0x50c88b 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x509455 0x595311 0x5a522c 0x557f7e 0x4b5eff 0x50c810\n",
            "2019-12-07 11:58:53 \u001b[32mINFO     \u001b[0m train.py: [0/300], [10/484], step: 10, 2.152 samples/sec, batch_loss: 0.0820, batch_loss_c: 0.0750, batch_loss_s: 0.0983, time:18.5850, lr:1e-05\u001b[0m\n",
            "2019-12-07 11:59:02 \u001b[32mINFO     \u001b[0m train.py: [0/300], [20/484], step: 20, 4.569 samples/sec, batch_loss: 0.0472, batch_loss_c: 0.0407, batch_loss_s: 0.0625, time:8.7544, lr:1e-05\u001b[0m\n",
            "2019-12-07 11:59:13 \u001b[32mINFO     \u001b[0m train.py: [0/300], [30/484], step: 30, 3.540 samples/sec, batch_loss: 0.1495, batch_loss_c: 0.1310, batch_loss_s: 0.1927, time:11.3009, lr:1e-05\u001b[0m\n",
            "2019-12-07 11:59:26 \u001b[32mINFO     \u001b[0m train.py: [0/300], [40/484], step: 40, 2.969 samples/sec, batch_loss: 0.0565, batch_loss_c: 0.0497, batch_loss_s: 0.0722, time:13.4736, lr:1e-05\u001b[0m\n",
            "tcmalloc: large alloc 1725898752 bytes == 0x124396000 @  0x7f214dff61e7 0x7f21425daf71 0x7f214263e55d 0x7f2142641e28 0x7f21426423e5 0x7f21426d8fc2 0x50ac25 0x50d390 0x508245 0x50a080 0x50aa7d 0x50d390 0x509455 0x595311 0x5a067e 0x557d28 0x50c88b 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x509455 0x595311 0x5a522c 0x557f7e 0x4b5eff 0x50c810\n",
            "2019-12-07 11:59:42 \u001b[32mINFO     \u001b[0m train.py: [0/300], [50/484], step: 50, 2.534 samples/sec, batch_loss: 0.0632, batch_loss_c: 0.0539, batch_loss_s: 0.0850, time:15.7837, lr:1e-05\u001b[0m\n",
            "2019-12-07 11:59:53 \u001b[32mINFO     \u001b[0m train.py: [0/300], [60/484], step: 60, 3.740 samples/sec, batch_loss: 0.3016, batch_loss_c: 0.2950, batch_loss_s: 0.3169, time:10.6958, lr:1e-05\u001b[0m\n",
            "tcmalloc: large alloc 1407582208 bytes == 0x8736e000 @  0x7f214dff61e7 0x7f21425daf71 0x7f214263e55d 0x7f2142641e28 0x7f21426423e5 0x7f21426d8fc2 0x50ac25 0x50d390 0x508245 0x50a080 0x50aa7d 0x50d390 0x509455 0x595311 0x5a067e 0x557d28 0x50c88b 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x509455 0x595311 0x5a522c 0x557f7e 0x4b5eff 0x50c810\n",
            "2019-12-07 12:00:09 \u001b[32mINFO     \u001b[0m train.py: [0/300], [70/484], step: 70, 2.554 samples/sec, batch_loss: 0.0810, batch_loss_c: 0.0824, batch_loss_s: 0.0776, time:15.6588, lr:1e-05\u001b[0m\n",
            "tcmalloc: large alloc 3216015360 bytes == 0x7f1dca4f8000 @  0x7f214dff61e7 0x7f21425daf71 0x7f214263e55d 0x7f2142641e28 0x7f21426423e5 0x7f21426d8fc2 0x50ac25 0x50d390 0x508245 0x50a080 0x50aa7d 0x50d390 0x509455 0x595311 0x5a067e 0x557d28 0x50c88b 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x509455 0x595311 0x5a522c 0x557f7e 0x4b5eff 0x50c810\n",
            "2019-12-07 12:00:26 \u001b[32mINFO     \u001b[0m train.py: [0/300], [80/484], step: 80, 2.268 samples/sec, batch_loss: 0.2555, batch_loss_c: 0.2332, batch_loss_s: 0.3075, time:17.6360, lr:1e-05\u001b[0m\n",
            "tcmalloc: large alloc 1725898752 bytes == 0x8736e000 @  0x7f214dff61e7 0x7f21425daf71 0x7f214263e55d 0x7f2142641e28 0x7f21426423e5 0x7f21426d8fc2 0x50ac25 0x50d390 0x508245 0x50a080 0x50aa7d 0x50d390 0x509455 0x595311 0x5a067e 0x557d28 0x50c88b 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x509455 0x595311 0x5a522c 0x557f7e 0x4b5eff 0x50c810\n",
            "2019-12-07 12:00:37 \u001b[32mINFO     \u001b[0m train.py: [0/300], [90/484], step: 90, 3.852 samples/sec, batch_loss: 0.2799, batch_loss_c: 0.2763, batch_loss_s: 0.2881, time:10.3853, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:00:55 \u001b[32mINFO     \u001b[0m train.py: [0/300], [100/484], step: 100, 2.204 samples/sec, batch_loss: 0.3591, batch_loss_c: 0.3645, batch_loss_s: 0.3464, time:18.1499, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:01:07 \u001b[32mINFO     \u001b[0m train.py: [0/300], [110/484], step: 110, 3.386 samples/sec, batch_loss: 0.1000, batch_loss_c: 0.1065, batch_loss_s: 0.0849, time:11.8148, lr:1e-05\u001b[0m\n",
            "tcmalloc: large alloc 2173427712 bytes == 0x8736e000 @  0x7f214dff61e7 0x7f21425daf71 0x7f214263e55d 0x7f2142641e28 0x7f21426423e5 0x7f21426d8fc2 0x50ac25 0x50d390 0x508245 0x50a080 0x50aa7d 0x50d390 0x509455 0x595311 0x5a067e 0x557d28 0x50c88b 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x509455 0x595311 0x5a522c 0x557f7e 0x4b5eff 0x50c810\n",
            "2019-12-07 12:01:24 \u001b[32mINFO     \u001b[0m train.py: [0/300], [120/484], step: 120, 2.321 samples/sec, batch_loss: 0.3147, batch_loss_c: 0.2808, batch_loss_s: 0.3938, time:17.2369, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:01:38 \u001b[32mINFO     \u001b[0m train.py: [0/300], [130/484], step: 130, 2.803 samples/sec, batch_loss: 0.0422, batch_loss_c: 0.0338, batch_loss_s: 0.0619, time:14.2697, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:01:47 \u001b[32mINFO     \u001b[0m train.py: [0/300], [140/484], step: 140, 4.478 samples/sec, batch_loss: 0.0562, batch_loss_c: 0.0501, batch_loss_s: 0.0705, time:8.9333, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:02:01 \u001b[32mINFO     \u001b[0m train.py: [0/300], [150/484], step: 150, 2.781 samples/sec, batch_loss: 0.0838, batch_loss_c: 0.0684, batch_loss_s: 0.1199, time:14.3813, lr:1e-05\u001b[0m\n",
            "tcmalloc: large alloc 2633637888 bytes == 0x7f1dca4f8000 @  0x7f214dff61e7 0x7f21425daf71 0x7f214263e55d 0x7f2142641e28 0x7f21426423e5 0x7f21426d8fc2 0x50ac25 0x50d390 0x508245 0x50a080 0x50aa7d 0x50d390 0x509455 0x595311 0x5a067e 0x557d28 0x50c88b 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x509455 0x595311 0x5a522c 0x557f7e 0x4b5eff 0x50c810\n",
            "2019-12-07 12:02:16 \u001b[32mINFO     \u001b[0m train.py: [0/300], [160/484], step: 160, 2.825 samples/sec, batch_loss: 0.2523, batch_loss_c: 0.2423, batch_loss_s: 0.2756, time:14.1571, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:02:27 \u001b[32mINFO     \u001b[0m train.py: [0/300], [170/484], step: 170, 3.497 samples/sec, batch_loss: 0.3057, batch_loss_c: 0.3036, batch_loss_s: 0.3104, time:11.4397, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:02:42 \u001b[32mINFO     \u001b[0m train.py: [0/300], [180/484], step: 180, 2.657 samples/sec, batch_loss: 0.0586, batch_loss_c: 0.0532, batch_loss_s: 0.0712, time:15.0560, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:03:00 \u001b[32mINFO     \u001b[0m train.py: [0/300], [190/484], step: 190, 2.190 samples/sec, batch_loss: 0.0830, batch_loss_c: 0.0755, batch_loss_s: 0.1004, time:18.2646, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:03:13 \u001b[32mINFO     \u001b[0m train.py: [0/300], [200/484], step: 200, 3.266 samples/sec, batch_loss: 0.2810, batch_loss_c: 0.2474, batch_loss_s: 0.3596, time:12.2471, lr:1e-05\u001b[0m\n",
            "tcmalloc: large alloc 2633637888 bytes == 0x7f1dca4f8000 @  0x7f214dff61e7 0x7f21425daf71 0x7f214263e55d 0x7f2142641e28 0x7f21426423e5 0x7f21426d8fc2 0x50ac25 0x50d390 0x508245 0x50a080 0x50aa7d 0x50d390 0x509455 0x595311 0x5a067e 0x557d28 0x50c88b 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x509455 0x595311 0x5a522c 0x557f7e 0x4b5eff 0x50c810\n",
            "2019-12-07 12:03:24 \u001b[32mINFO     \u001b[0m train.py: [0/300], [210/484], step: 210, 3.461 samples/sec, batch_loss: 0.3145, batch_loss_c: 0.3107, batch_loss_s: 0.3232, time:11.5572, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:03:37 \u001b[32mINFO     \u001b[0m train.py: [0/300], [220/484], step: 220, 3.237 samples/sec, batch_loss: 0.1390, batch_loss_c: 0.1395, batch_loss_s: 0.1379, time:12.3569, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:03:53 \u001b[32mINFO     \u001b[0m train.py: [0/300], [230/484], step: 230, 2.484 samples/sec, batch_loss: 0.0610, batch_loss_c: 0.0534, batch_loss_s: 0.0786, time:16.1002, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:04:05 \u001b[32mINFO     \u001b[0m train.py: [0/300], [240/484], step: 240, 3.252 samples/sec, batch_loss: 0.0823, batch_loss_c: 0.0751, batch_loss_s: 0.0990, time:12.2989, lr:1e-05\u001b[0m\n",
            "tcmalloc: large alloc 3172827136 bytes == 0x7f1dca4f8000 @  0x7f214dff61e7 0x7f21425daf71 0x7f214263e55d 0x7f2142641e28 0x7f21426423e5 0x7f21426d8fc2 0x50ac25 0x50d390 0x508245 0x50a080 0x50aa7d 0x50d390 0x509455 0x595311 0x5a067e 0x557d28 0x50c88b 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x509455 0x595311 0x5a522c 0x557f7e 0x4b5eff 0x50c810\n",
            "2019-12-07 12:04:29 \u001b[32mINFO     \u001b[0m train.py: [0/300], [250/484], step: 250, 1.661 samples/sec, batch_loss: 0.2915, batch_loss_c: 0.2854, batch_loss_s: 0.3057, time:24.0824, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:04:40 \u001b[32mINFO     \u001b[0m train.py: [0/300], [260/484], step: 260, 3.689 samples/sec, batch_loss: 0.0520, batch_loss_c: 0.0439, batch_loss_s: 0.0709, time:10.8432, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:04:59 \u001b[32mINFO     \u001b[0m train.py: [0/300], [270/484], step: 270, 2.102 samples/sec, batch_loss: 0.1779, batch_loss_c: 0.1918, batch_loss_s: 0.1455, time:19.0275, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:05:11 \u001b[32mINFO     \u001b[0m train.py: [0/300], [280/484], step: 280, 3.280 samples/sec, batch_loss: 0.1113, batch_loss_c: 0.1027, batch_loss_s: 0.1314, time:12.1963, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:05:22 \u001b[32mINFO     \u001b[0m train.py: [0/300], [290/484], step: 290, 3.818 samples/sec, batch_loss: 0.3167, batch_loss_c: 0.3100, batch_loss_s: 0.3323, time:10.4760, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:05:38 \u001b[32mINFO     \u001b[0m train.py: [0/300], [300/484], step: 300, 2.393 samples/sec, batch_loss: 0.0590, batch_loss_c: 0.0505, batch_loss_s: 0.0787, time:16.7133, lr:1e-05\u001b[0m\n",
            "tcmalloc: large alloc 3243024384 bytes == 0x7f1d0902e000 @  0x7f214dff61e7 0x7f21425daf71 0x7f214263e55d 0x7f2142641e28 0x7f21426423e5 0x7f21426d8fc2 0x50ac25 0x50d390 0x508245 0x50a080 0x50aa7d 0x50d390 0x509455 0x595311 0x5a067e 0x557d28 0x50c88b 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x509455 0x595311 0x5a522c 0x557f7e 0x4b5eff 0x50c810\n",
            "2019-12-07 12:06:03 \u001b[32mINFO     \u001b[0m train.py: [0/300], [310/484], step: 310, 1.593 samples/sec, batch_loss: 0.0628, batch_loss_c: 0.0553, batch_loss_s: 0.0803, time:25.1155, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:06:16 \u001b[32mINFO     \u001b[0m train.py: [0/300], [320/484], step: 320, 3.280 samples/sec, batch_loss: 0.0519, batch_loss_c: 0.0421, batch_loss_s: 0.0750, time:12.1956, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:06:25 \u001b[32mINFO     \u001b[0m train.py: [0/300], [330/484], step: 330, 4.170 samples/sec, batch_loss: 0.1682, batch_loss_c: 0.1727, batch_loss_s: 0.1577, time:9.5912, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:06:35 \u001b[32mINFO     \u001b[0m train.py: [0/300], [340/484], step: 340, 3.987 samples/sec, batch_loss: 0.0583, batch_loss_c: 0.0506, batch_loss_s: 0.0762, time:10.0322, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:06:50 \u001b[32mINFO     \u001b[0m train.py: [0/300], [350/484], step: 350, 2.749 samples/sec, batch_loss: 0.1087, batch_loss_c: 0.0943, batch_loss_s: 0.1425, time:14.5481, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:07:15 \u001b[32mINFO     \u001b[0m train.py: [0/300], [360/484], step: 360, 1.607 samples/sec, batch_loss: 0.2623, batch_loss_c: 0.3244, batch_loss_s: 0.1176, time:24.8919, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:07:26 \u001b[32mINFO     \u001b[0m train.py: [0/300], [370/484], step: 370, 3.651 samples/sec, batch_loss: 0.0963, batch_loss_c: 0.0978, batch_loss_s: 0.0930, time:10.9560, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:07:42 \u001b[32mINFO     \u001b[0m train.py: [0/300], [380/484], step: 380, 2.393 samples/sec, batch_loss: 0.0719, batch_loss_c: 0.0690, batch_loss_s: 0.0789, time:16.7124, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:07:55 \u001b[32mINFO     \u001b[0m train.py: [0/300], [390/484], step: 390, 3.266 samples/sec, batch_loss: 0.4123, batch_loss_c: 0.4209, batch_loss_s: 0.3922, time:12.2470, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:08:19 \u001b[32mINFO     \u001b[0m train.py: [0/300], [400/484], step: 400, 1.606 samples/sec, batch_loss: 0.0712, batch_loss_c: 0.0606, batch_loss_s: 0.0960, time:24.9017, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:08:43 \u001b[32mINFO     \u001b[0m train.py: [0/300], [410/484], step: 410, 1.720 samples/sec, batch_loss: 0.1209, batch_loss_c: 0.1140, batch_loss_s: 0.1370, time:23.2595, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:08:58 \u001b[32mINFO     \u001b[0m train.py: [0/300], [420/484], step: 420, 2.555 samples/sec, batch_loss: 0.1059, batch_loss_c: 0.0988, batch_loss_s: 0.1225, time:15.6574, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:09:10 \u001b[32mINFO     \u001b[0m train.py: [0/300], [430/484], step: 430, 3.336 samples/sec, batch_loss: 0.0992, batch_loss_c: 0.0547, batch_loss_s: 0.2032, time:11.9910, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:09:19 \u001b[32mINFO     \u001b[0m train.py: [0/300], [440/484], step: 440, 4.379 samples/sec, batch_loss: 0.3022, batch_loss_c: 0.2961, batch_loss_s: 0.3165, time:9.1336, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:09:31 \u001b[32mINFO     \u001b[0m train.py: [0/300], [450/484], step: 450, 3.479 samples/sec, batch_loss: 0.0686, batch_loss_c: 0.0573, batch_loss_s: 0.0951, time:11.4986, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:09:45 \u001b[32mINFO     \u001b[0m train.py: [0/300], [460/484], step: 460, 2.843 samples/sec, batch_loss: 0.0524, batch_loss_c: 0.0492, batch_loss_s: 0.0600, time:14.0716, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:10:14 \u001b[32mINFO     \u001b[0m train.py: [0/300], [470/484], step: 470, 1.360 samples/sec, batch_loss: 0.0527, batch_loss_c: 0.0440, batch_loss_s: 0.0730, time:29.4154, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:10:32 \u001b[32mINFO     \u001b[0m train.py: [0/300], [480/484], step: 480, 2.299 samples/sec, batch_loss: 0.0874, batch_loss_c: 0.0802, batch_loss_s: 0.1042, time:17.3957, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:10:36 \u001b[32mINFO     \u001b[0m train.py: [0/300], train_loss: 0.1441, time: 736.8121, lr: 1e-05\u001b[0m\n",
            "2019-12-07 12:10:43 \u001b[32mINFO     \u001b[0m train.py: [1/300], [0/484], step: 484, 6.459 samples/sec, batch_loss: 0.5336, batch_loss_c: 0.5293, batch_loss_s: 0.5438, time:6.1930, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:11:03 \u001b[32mINFO     \u001b[0m train.py: [1/300], [10/484], step: 494, 1.947 samples/sec, batch_loss: 0.3296, batch_loss_c: 0.3315, batch_loss_s: 0.3253, time:20.5478, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:11:23 \u001b[32mINFO     \u001b[0m train.py: [1/300], [20/484], step: 504, 2.091 samples/sec, batch_loss: 0.0600, batch_loss_c: 0.0528, batch_loss_s: 0.0766, time:19.1339, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:11:34 \u001b[32mINFO     \u001b[0m train.py: [1/300], [30/484], step: 514, 3.437 samples/sec, batch_loss: 0.3130, batch_loss_c: 0.3077, batch_loss_s: 0.3254, time:11.6378, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:11:46 \u001b[32mINFO     \u001b[0m train.py: [1/300], [40/484], step: 524, 3.501 samples/sec, batch_loss: 0.0437, batch_loss_c: 0.0383, batch_loss_s: 0.0562, time:11.4239, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:11:56 \u001b[32mINFO     \u001b[0m train.py: [1/300], [50/484], step: 534, 4.001 samples/sec, batch_loss: 0.0691, batch_loss_c: 0.0626, batch_loss_s: 0.0843, time:9.9986, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:12:13 \u001b[32mINFO     \u001b[0m train.py: [1/300], [60/484], step: 544, 2.356 samples/sec, batch_loss: 0.3822, batch_loss_c: 0.4078, batch_loss_s: 0.3227, time:16.9793, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:12:25 \u001b[32mINFO     \u001b[0m train.py: [1/300], [70/484], step: 554, 3.223 samples/sec, batch_loss: 0.0933, batch_loss_c: 0.0896, batch_loss_s: 0.1019, time:12.4122, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:12:34 \u001b[32mINFO     \u001b[0m train.py: [1/300], [80/484], step: 564, 4.273 samples/sec, batch_loss: 0.1079, batch_loss_c: 0.1091, batch_loss_s: 0.1052, time:9.3603, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:12:44 \u001b[32mINFO     \u001b[0m train.py: [1/300], [90/484], step: 574, 4.271 samples/sec, batch_loss: 0.1419, batch_loss_c: 0.1419, batch_loss_s: 0.1420, time:9.3651, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:12:56 \u001b[32mINFO     \u001b[0m train.py: [1/300], [100/484], step: 584, 3.333 samples/sec, batch_loss: 0.1931, batch_loss_c: 0.1527, batch_loss_s: 0.2874, time:12.0014, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:13:06 \u001b[32mINFO     \u001b[0m train.py: [1/300], [110/484], step: 594, 3.829 samples/sec, batch_loss: 0.0458, batch_loss_c: 0.0396, batch_loss_s: 0.0601, time:10.4462, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:13:17 \u001b[32mINFO     \u001b[0m train.py: [1/300], [120/484], step: 604, 3.761 samples/sec, batch_loss: 0.0432, batch_loss_c: 0.0346, batch_loss_s: 0.0632, time:10.6346, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:13:28 \u001b[32mINFO     \u001b[0m train.py: [1/300], [130/484], step: 614, 3.690 samples/sec, batch_loss: 0.0914, batch_loss_c: 0.0832, batch_loss_s: 0.1104, time:10.8389, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:13:42 \u001b[32mINFO     \u001b[0m train.py: [1/300], [140/484], step: 624, 2.889 samples/sec, batch_loss: 0.0573, batch_loss_c: 0.0529, batch_loss_s: 0.0675, time:13.8461, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:14:11 \u001b[32mINFO     \u001b[0m train.py: [1/300], [150/484], step: 634, 1.357 samples/sec, batch_loss: 0.0672, batch_loss_c: 0.0631, batch_loss_s: 0.0769, time:29.4761, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:14:29 \u001b[32mINFO     \u001b[0m train.py: [1/300], [160/484], step: 644, 2.253 samples/sec, batch_loss: 0.0941, batch_loss_c: 0.1004, batch_loss_s: 0.0794, time:17.7577, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:14:43 \u001b[32mINFO     \u001b[0m train.py: [1/300], [170/484], step: 654, 2.850 samples/sec, batch_loss: 0.0909, batch_loss_c: 0.0820, batch_loss_s: 0.1115, time:14.0345, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:14:57 \u001b[32mINFO     \u001b[0m train.py: [1/300], [180/484], step: 664, 2.913 samples/sec, batch_loss: 0.3352, batch_loss_c: 0.3272, batch_loss_s: 0.3539, time:13.7294, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:15:17 \u001b[32mINFO     \u001b[0m train.py: [1/300], [190/484], step: 674, 1.957 samples/sec, batch_loss: 0.2926, batch_loss_c: 0.2873, batch_loss_s: 0.3050, time:20.4441, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:15:40 \u001b[32mINFO     \u001b[0m train.py: [1/300], [200/484], step: 684, 1.753 samples/sec, batch_loss: 0.0489, batch_loss_c: 0.0396, batch_loss_s: 0.0706, time:22.8227, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:15:52 \u001b[32mINFO     \u001b[0m train.py: [1/300], [210/484], step: 694, 3.168 samples/sec, batch_loss: 0.1492, batch_loss_c: 0.1632, batch_loss_s: 0.1165, time:12.6270, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:16:09 \u001b[32mINFO     \u001b[0m train.py: [1/300], [220/484], step: 704, 2.484 samples/sec, batch_loss: 0.0709, batch_loss_c: 0.0699, batch_loss_s: 0.0732, time:16.1017, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:16:19 \u001b[32mINFO     \u001b[0m train.py: [1/300], [230/484], step: 714, 3.787 samples/sec, batch_loss: 0.0746, batch_loss_c: 0.0720, batch_loss_s: 0.0807, time:10.5614, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:16:42 \u001b[32mINFO     \u001b[0m train.py: [1/300], [240/484], step: 724, 1.720 samples/sec, batch_loss: 0.1336, batch_loss_c: 0.1312, batch_loss_s: 0.1393, time:23.2533, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:17:04 \u001b[32mINFO     \u001b[0m train.py: [1/300], [250/484], step: 734, 1.868 samples/sec, batch_loss: 0.0546, batch_loss_c: 0.0454, batch_loss_s: 0.0763, time:21.4148, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:17:16 \u001b[32mINFO     \u001b[0m train.py: [1/300], [260/484], step: 744, 3.376 samples/sec, batch_loss: 0.0671, batch_loss_c: 0.0529, batch_loss_s: 0.1003, time:11.8499, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:17:27 \u001b[32mINFO     \u001b[0m train.py: [1/300], [270/484], step: 754, 3.574 samples/sec, batch_loss: 0.0868, batch_loss_c: 0.0751, batch_loss_s: 0.1140, time:11.1926, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:17:44 \u001b[32mINFO     \u001b[0m train.py: [1/300], [280/484], step: 764, 2.385 samples/sec, batch_loss: 0.1101, batch_loss_c: 0.1047, batch_loss_s: 0.1225, time:16.7720, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:17:53 \u001b[32mINFO     \u001b[0m train.py: [1/300], [290/484], step: 774, 4.047 samples/sec, batch_loss: 0.0601, batch_loss_c: 0.0489, batch_loss_s: 0.0861, time:9.8840, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:18:13 \u001b[32mINFO     \u001b[0m train.py: [1/300], [300/484], step: 784, 2.018 samples/sec, batch_loss: 0.0511, batch_loss_c: 0.0470, batch_loss_s: 0.0608, time:19.8217, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:18:25 \u001b[32mINFO     \u001b[0m train.py: [1/300], [310/484], step: 794, 3.370 samples/sec, batch_loss: 0.0885, batch_loss_c: 0.0838, batch_loss_s: 0.0995, time:11.8709, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:18:37 \u001b[32mINFO     \u001b[0m train.py: [1/300], [320/484], step: 804, 3.364 samples/sec, batch_loss: 0.3192, batch_loss_c: 0.3089, batch_loss_s: 0.3433, time:11.8919, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:18:51 \u001b[32mINFO     \u001b[0m train.py: [1/300], [330/484], step: 814, 2.801 samples/sec, batch_loss: 0.2996, batch_loss_c: 0.2950, batch_loss_s: 0.3102, time:14.2790, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:19:00 \u001b[32mINFO     \u001b[0m train.py: [1/300], [340/484], step: 824, 4.644 samples/sec, batch_loss: 0.2164, batch_loss_c: 0.2476, batch_loss_s: 0.1436, time:8.6129, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:19:13 \u001b[32mINFO     \u001b[0m train.py: [1/300], [350/484], step: 834, 3.063 samples/sec, batch_loss: 0.2891, batch_loss_c: 0.2798, batch_loss_s: 0.3109, time:13.0577, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:19:30 \u001b[32mINFO     \u001b[0m train.py: [1/300], [360/484], step: 844, 2.395 samples/sec, batch_loss: 0.0782, batch_loss_c: 0.0734, batch_loss_s: 0.0894, time:16.6983, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:19:43 \u001b[32mINFO     \u001b[0m train.py: [1/300], [370/484], step: 854, 3.000 samples/sec, batch_loss: 0.1070, batch_loss_c: 0.1010, batch_loss_s: 0.1212, time:13.3327, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:19:53 \u001b[32mINFO     \u001b[0m train.py: [1/300], [380/484], step: 864, 3.851 samples/sec, batch_loss: 0.0681, batch_loss_c: 0.0609, batch_loss_s: 0.0850, time:10.3865, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:20:08 \u001b[32mINFO     \u001b[0m train.py: [1/300], [390/484], step: 874, 2.771 samples/sec, batch_loss: 0.0731, batch_loss_c: 0.0660, batch_loss_s: 0.0896, time:14.4362, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:20:24 \u001b[32mINFO     \u001b[0m train.py: [1/300], [400/484], step: 884, 2.517 samples/sec, batch_loss: 0.1190, batch_loss_c: 0.0750, batch_loss_s: 0.2218, time:15.8899, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:20:40 \u001b[32mINFO     \u001b[0m train.py: [1/300], [410/484], step: 894, 2.392 samples/sec, batch_loss: 0.0654, batch_loss_c: 0.0581, batch_loss_s: 0.0823, time:16.7232, lr:1e-05\u001b[0m\n",
            "tcmalloc: large alloc 4010754048 bytes == 0x7f1d0902e000 @  0x7f214dff61e7 0x7f21425daf71 0x7f214263e55d 0x7f2142641e28 0x7f21426423e5 0x7f21426d8fc2 0x50ac25 0x50d390 0x508245 0x50a080 0x50aa7d 0x50d390 0x509455 0x595311 0x5a067e 0x557d28 0x50c88b 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x509455 0x595311 0x5a522c 0x557f7e 0x4b5eff 0x50c810\n",
            "2019-12-07 12:21:16 \u001b[32mINFO     \u001b[0m train.py: [1/300], [420/484], step: 904, 1.140 samples/sec, batch_loss: 0.3123, batch_loss_c: 0.3076, batch_loss_s: 0.3232, time:35.0912, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:21:30 \u001b[32mINFO     \u001b[0m train.py: [1/300], [430/484], step: 914, 2.739 samples/sec, batch_loss: 0.1122, batch_loss_c: 0.1051, batch_loss_s: 0.1287, time:14.6037, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:21:39 \u001b[32mINFO     \u001b[0m train.py: [1/300], [440/484], step: 924, 4.287 samples/sec, batch_loss: 0.0809, batch_loss_c: 0.0913, batch_loss_s: 0.0568, time:9.3306, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:21:55 \u001b[32mINFO     \u001b[0m train.py: [1/300], [450/484], step: 934, 2.504 samples/sec, batch_loss: 0.2950, batch_loss_c: 0.2867, batch_loss_s: 0.3143, time:15.9740, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:22:06 \u001b[32mINFO     \u001b[0m train.py: [1/300], [460/484], step: 944, 3.844 samples/sec, batch_loss: 0.3080, batch_loss_c: 0.3082, batch_loss_s: 0.3076, time:10.4063, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:22:17 \u001b[32mINFO     \u001b[0m train.py: [1/300], [470/484], step: 954, 3.646 samples/sec, batch_loss: 0.0718, batch_loss_c: 0.0570, batch_loss_s: 0.1061, time:10.9702, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:22:32 \u001b[32mINFO     \u001b[0m train.py: [1/300], [480/484], step: 964, 2.606 samples/sec, batch_loss: 0.2825, batch_loss_c: 0.2800, batch_loss_s: 0.2885, time:15.3494, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:22:35 \u001b[32mINFO     \u001b[0m train.py: [1/300], train_loss: 0.1473, time: 718.4848, lr: 1e-05\u001b[0m\n",
            "2019-12-07 12:22:36 \u001b[32mINFO     \u001b[0m train.py: [2/300], [0/484], step: 968, 54.052 samples/sec, batch_loss: 0.0797, batch_loss_c: 0.0747, batch_loss_s: 0.0914, time:0.7400, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:22:56 \u001b[32mINFO     \u001b[0m train.py: [2/300], [10/484], step: 978, 2.021 samples/sec, batch_loss: 0.3939, batch_loss_c: 0.4001, batch_loss_s: 0.3795, time:19.7917, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:23:22 \u001b[32mINFO     \u001b[0m train.py: [2/300], [20/484], step: 988, 1.568 samples/sec, batch_loss: 0.2952, batch_loss_c: 0.2924, batch_loss_s: 0.3017, time:25.5030, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:23:50 \u001b[32mINFO     \u001b[0m train.py: [2/300], [30/484], step: 998, 1.400 samples/sec, batch_loss: 0.0801, batch_loss_c: 0.0707, batch_loss_s: 0.1022, time:28.5625, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:24:03 \u001b[32mINFO     \u001b[0m train.py: [2/300], [40/484], step: 1008, 3.094 samples/sec, batch_loss: 0.3482, batch_loss_c: 0.3383, batch_loss_s: 0.3712, time:12.9273, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:24:13 \u001b[32mINFO     \u001b[0m train.py: [2/300], [50/484], step: 1018, 4.196 samples/sec, batch_loss: 0.0990, batch_loss_c: 0.0914, batch_loss_s: 0.1167, time:9.5328, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:24:22 \u001b[32mINFO     \u001b[0m train.py: [2/300], [60/484], step: 1028, 4.384 samples/sec, batch_loss: 0.3055, batch_loss_c: 0.3010, batch_loss_s: 0.3162, time:9.1233, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:24:43 \u001b[32mINFO     \u001b[0m train.py: [2/300], [70/484], step: 1038, 1.869 samples/sec, batch_loss: 0.1320, batch_loss_c: 0.1214, batch_loss_s: 0.1568, time:21.3965, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:25:08 \u001b[32mINFO     \u001b[0m train.py: [2/300], [80/484], step: 1048, 1.602 samples/sec, batch_loss: 0.0619, batch_loss_c: 0.0568, batch_loss_s: 0.0737, time:24.9638, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:25:19 \u001b[32mINFO     \u001b[0m train.py: [2/300], [90/484], step: 1058, 3.559 samples/sec, batch_loss: 0.2411, batch_loss_c: 0.2226, batch_loss_s: 0.2841, time:11.2382, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:25:32 \u001b[32mINFO     \u001b[0m train.py: [2/300], [100/484], step: 1068, 3.284 samples/sec, batch_loss: 0.1144, batch_loss_c: 0.1200, batch_loss_s: 0.1015, time:12.1791, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:25:51 \u001b[32mINFO     \u001b[0m train.py: [2/300], [110/484], step: 1078, 2.106 samples/sec, batch_loss: 0.0662, batch_loss_c: 0.0527, batch_loss_s: 0.0977, time:18.9951, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:25:59 \u001b[32mINFO     \u001b[0m train.py: [2/300], [120/484], step: 1088, 4.571 samples/sec, batch_loss: 0.0492, batch_loss_c: 0.0463, batch_loss_s: 0.0559, time:8.7512, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:26:10 \u001b[32mINFO     \u001b[0m train.py: [2/300], [130/484], step: 1098, 3.895 samples/sec, batch_loss: 0.1047, batch_loss_c: 0.0938, batch_loss_s: 0.1302, time:10.2703, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:26:29 \u001b[32mINFO     \u001b[0m train.py: [2/300], [140/484], step: 1108, 2.067 samples/sec, batch_loss: 0.3276, batch_loss_c: 0.3286, batch_loss_s: 0.3254, time:19.3533, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:26:39 \u001b[32mINFO     \u001b[0m train.py: [2/300], [150/484], step: 1118, 4.087 samples/sec, batch_loss: 0.0732, batch_loss_c: 0.0593, batch_loss_s: 0.1056, time:9.7873, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:27:08 \u001b[32mINFO     \u001b[0m train.py: [2/300], [160/484], step: 1128, 1.353 samples/sec, batch_loss: 0.0833, batch_loss_c: 0.0771, batch_loss_s: 0.0978, time:29.5597, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:27:18 \u001b[32mINFO     \u001b[0m train.py: [2/300], [170/484], step: 1138, 4.214 samples/sec, batch_loss: 0.0515, batch_loss_c: 0.0433, batch_loss_s: 0.0707, time:9.4911, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:27:39 \u001b[32mINFO     \u001b[0m train.py: [2/300], [180/484], step: 1148, 1.872 samples/sec, batch_loss: 0.0643, batch_loss_c: 0.0556, batch_loss_s: 0.0847, time:21.3685, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:27:53 \u001b[32mINFO     \u001b[0m train.py: [2/300], [190/484], step: 1158, 2.917 samples/sec, batch_loss: 0.3132, batch_loss_c: 0.3058, batch_loss_s: 0.3305, time:13.7129, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:28:03 \u001b[32mINFO     \u001b[0m train.py: [2/300], [200/484], step: 1168, 3.936 samples/sec, batch_loss: 0.0472, batch_loss_c: 0.0398, batch_loss_s: 0.0644, time:10.1637, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:28:23 \u001b[32mINFO     \u001b[0m train.py: [2/300], [210/484], step: 1178, 2.003 samples/sec, batch_loss: 0.0596, batch_loss_c: 0.0597, batch_loss_s: 0.0596, time:19.9707, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:28:40 \u001b[32mINFO     \u001b[0m train.py: [2/300], [220/484], step: 1188, 2.368 samples/sec, batch_loss: 0.1506, batch_loss_c: 0.1519, batch_loss_s: 0.1474, time:16.8902, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:28:53 \u001b[32mINFO     \u001b[0m train.py: [2/300], [230/484], step: 1198, 3.072 samples/sec, batch_loss: 0.0876, batch_loss_c: 0.0809, batch_loss_s: 0.1034, time:13.0206, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:29:05 \u001b[32mINFO     \u001b[0m train.py: [2/300], [240/484], step: 1208, 3.395 samples/sec, batch_loss: 0.0723, batch_loss_c: 0.0691, batch_loss_s: 0.0799, time:11.7833, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:29:17 \u001b[32mINFO     \u001b[0m train.py: [2/300], [250/484], step: 1218, 3.388 samples/sec, batch_loss: 0.0468, batch_loss_c: 0.0405, batch_loss_s: 0.0617, time:11.8052, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:29:25 \u001b[32mINFO     \u001b[0m train.py: [2/300], [260/484], step: 1228, 4.768 samples/sec, batch_loss: 0.1236, batch_loss_c: 0.1144, batch_loss_s: 0.1451, time:8.3896, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:29:42 \u001b[32mINFO     \u001b[0m train.py: [2/300], [270/484], step: 1238, 2.353 samples/sec, batch_loss: 0.3467, batch_loss_c: 0.3419, batch_loss_s: 0.3578, time:16.9993, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:29:50 \u001b[32mINFO     \u001b[0m train.py: [2/300], [280/484], step: 1248, 4.723 samples/sec, batch_loss: 0.0696, batch_loss_c: 0.0590, batch_loss_s: 0.0944, time:8.4687, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:30:07 \u001b[32mINFO     \u001b[0m train.py: [2/300], [290/484], step: 1258, 2.393 samples/sec, batch_loss: 0.0750, batch_loss_c: 0.0558, batch_loss_s: 0.1198, time:16.7177, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:30:25 \u001b[32mINFO     \u001b[0m train.py: [2/300], [300/484], step: 1268, 2.204 samples/sec, batch_loss: 0.2809, batch_loss_c: 0.2698, batch_loss_s: 0.3067, time:18.1529, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:30:42 \u001b[32mINFO     \u001b[0m train.py: [2/300], [310/484], step: 1278, 2.421 samples/sec, batch_loss: 0.0740, batch_loss_c: 0.0678, batch_loss_s: 0.0882, time:16.5233, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:30:55 \u001b[32mINFO     \u001b[0m train.py: [2/300], [320/484], step: 1288, 2.991 samples/sec, batch_loss: 0.1333, batch_loss_c: 0.1389, batch_loss_s: 0.1203, time:13.3733, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:31:09 \u001b[32mINFO     \u001b[0m train.py: [2/300], [330/484], step: 1298, 2.913 samples/sec, batch_loss: 0.0504, batch_loss_c: 0.0433, batch_loss_s: 0.0669, time:13.7304, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:31:24 \u001b[32mINFO     \u001b[0m train.py: [2/300], [340/484], step: 1308, 2.594 samples/sec, batch_loss: 0.1527, batch_loss_c: 0.1395, batch_loss_s: 0.1835, time:15.4180, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:31:34 \u001b[32mINFO     \u001b[0m train.py: [2/300], [350/484], step: 1318, 4.268 samples/sec, batch_loss: 0.0643, batch_loss_c: 0.0589, batch_loss_s: 0.0768, time:9.3720, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:31:46 \u001b[32mINFO     \u001b[0m train.py: [2/300], [360/484], step: 1328, 3.139 samples/sec, batch_loss: 0.0554, batch_loss_c: 0.0484, batch_loss_s: 0.0718, time:12.7418, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:32:03 \u001b[32mINFO     \u001b[0m train.py: [2/300], [370/484], step: 1338, 2.464 samples/sec, batch_loss: 0.1650, batch_loss_c: 0.1442, batch_loss_s: 0.2136, time:16.2365, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:32:15 \u001b[32mINFO     \u001b[0m train.py: [2/300], [380/484], step: 1348, 3.337 samples/sec, batch_loss: 0.1609, batch_loss_c: 0.1454, batch_loss_s: 0.1969, time:11.9867, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:32:27 \u001b[32mINFO     \u001b[0m train.py: [2/300], [390/484], step: 1358, 3.111 samples/sec, batch_loss: 0.2857, batch_loss_c: 0.2812, batch_loss_s: 0.2964, time:12.8556, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:32:39 \u001b[32mINFO     \u001b[0m train.py: [2/300], [400/484], step: 1368, 3.468 samples/sec, batch_loss: 0.0639, batch_loss_c: 0.0524, batch_loss_s: 0.0906, time:11.5349, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:32:56 \u001b[32mINFO     \u001b[0m train.py: [2/300], [410/484], step: 1378, 2.307 samples/sec, batch_loss: 0.0453, batch_loss_c: 0.0373, batch_loss_s: 0.0639, time:17.3418, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:33:09 \u001b[32mINFO     \u001b[0m train.py: [2/300], [420/484], step: 1388, 3.142 samples/sec, batch_loss: 0.0746, batch_loss_c: 0.0696, batch_loss_s: 0.0863, time:12.7288, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:33:24 \u001b[32mINFO     \u001b[0m train.py: [2/300], [430/484], step: 1398, 2.654 samples/sec, batch_loss: 0.0813, batch_loss_c: 0.0772, batch_loss_s: 0.0906, time:15.0735, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:33:43 \u001b[32mINFO     \u001b[0m train.py: [2/300], [440/484], step: 1408, 2.182 samples/sec, batch_loss: 0.0688, batch_loss_c: 0.0644, batch_loss_s: 0.0790, time:18.3316, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:33:56 \u001b[32mINFO     \u001b[0m train.py: [2/300], [450/484], step: 1418, 3.058 samples/sec, batch_loss: 0.1608, batch_loss_c: 0.1875, batch_loss_s: 0.0986, time:13.0821, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:34:04 \u001b[32mINFO     \u001b[0m train.py: [2/300], [460/484], step: 1428, 4.551 samples/sec, batch_loss: 0.1474, batch_loss_c: 0.1155, batch_loss_s: 0.2218, time:8.7886, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:34:14 \u001b[32mINFO     \u001b[0m train.py: [2/300], [470/484], step: 1438, 4.003 samples/sec, batch_loss: 0.0795, batch_loss_c: 0.0613, batch_loss_s: 0.1219, time:9.9920, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:34:27 \u001b[32mINFO     \u001b[0m train.py: [2/300], [480/484], step: 1448, 3.198 samples/sec, batch_loss: 0.0761, batch_loss_c: 0.0656, batch_loss_s: 0.1005, time:12.5068, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:34:30 \u001b[32mINFO     \u001b[0m train.py: [2/300], train_loss: 0.1402, time: 714.5951, lr: 1e-05\u001b[0m\n",
            "2019-12-07 12:34:31 \u001b[32mINFO     \u001b[0m train.py: [3/300], [0/484], step: 1452, 51.209 samples/sec, batch_loss: 0.0488, batch_loss_c: 0.0388, batch_loss_s: 0.0720, time:0.7811, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:34:42 \u001b[32mINFO     \u001b[0m train.py: [3/300], [10/484], step: 1462, 3.903 samples/sec, batch_loss: 0.2988, batch_loss_c: 0.2909, batch_loss_s: 0.3173, time:10.2491, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:34:57 \u001b[32mINFO     \u001b[0m train.py: [3/300], [20/484], step: 1472, 2.526 samples/sec, batch_loss: 0.2833, batch_loss_c: 0.2778, batch_loss_s: 0.2961, time:15.8356, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:35:30 \u001b[32mINFO     \u001b[0m train.py: [3/300], [30/484], step: 1482, 1.218 samples/sec, batch_loss: 0.2872, batch_loss_c: 0.2795, batch_loss_s: 0.3052, time:32.8497, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:35:55 \u001b[32mINFO     \u001b[0m train.py: [3/300], [40/484], step: 1492, 1.590 samples/sec, batch_loss: 0.0769, batch_loss_c: 0.0691, batch_loss_s: 0.0954, time:25.1601, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:36:05 \u001b[32mINFO     \u001b[0m train.py: [3/300], [50/484], step: 1502, 4.198 samples/sec, batch_loss: 0.1618, batch_loss_c: 0.1636, batch_loss_s: 0.1575, time:9.5283, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:36:19 \u001b[32mINFO     \u001b[0m train.py: [3/300], [60/484], step: 1512, 2.807 samples/sec, batch_loss: 0.0665, batch_loss_c: 0.0656, batch_loss_s: 0.0685, time:14.2499, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:36:29 \u001b[32mINFO     \u001b[0m train.py: [3/300], [70/484], step: 1522, 3.990 samples/sec, batch_loss: 0.3417, batch_loss_c: 0.3553, batch_loss_s: 0.3099, time:10.0262, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:36:40 \u001b[32mINFO     \u001b[0m train.py: [3/300], [80/484], step: 1532, 3.679 samples/sec, batch_loss: 0.1269, batch_loss_c: 0.1290, batch_loss_s: 0.1219, time:10.8726, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:36:52 \u001b[32mINFO     \u001b[0m train.py: [3/300], [90/484], step: 1542, 3.469 samples/sec, batch_loss: 0.0401, batch_loss_c: 0.0340, batch_loss_s: 0.0544, time:11.5311, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:37:01 \u001b[32mINFO     \u001b[0m train.py: [3/300], [100/484], step: 1552, 4.136 samples/sec, batch_loss: 0.0536, batch_loss_c: 0.0455, batch_loss_s: 0.0725, time:9.6705, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:37:11 \u001b[32mINFO     \u001b[0m train.py: [3/300], [110/484], step: 1562, 3.978 samples/sec, batch_loss: 0.0848, batch_loss_c: 0.0753, batch_loss_s: 0.1068, time:10.0548, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:37:22 \u001b[32mINFO     \u001b[0m train.py: [3/300], [120/484], step: 1572, 3.949 samples/sec, batch_loss: 0.0470, batch_loss_c: 0.0392, batch_loss_s: 0.0651, time:10.1294, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:37:51 \u001b[32mINFO     \u001b[0m train.py: [3/300], [130/484], step: 1582, 1.336 samples/sec, batch_loss: 0.3794, batch_loss_c: 0.3838, batch_loss_s: 0.3690, time:29.9312, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:38:04 \u001b[32mINFO     \u001b[0m train.py: [3/300], [140/484], step: 1592, 3.166 samples/sec, batch_loss: 0.0705, batch_loss_c: 0.0629, batch_loss_s: 0.0881, time:12.6348, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:38:20 \u001b[32mINFO     \u001b[0m train.py: [3/300], [150/484], step: 1602, 2.571 samples/sec, batch_loss: 0.0954, batch_loss_c: 0.0984, batch_loss_s: 0.0885, time:15.5573, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:38:31 \u001b[32mINFO     \u001b[0m train.py: [3/300], [160/484], step: 1612, 3.671 samples/sec, batch_loss: 0.0804, batch_loss_c: 0.0713, batch_loss_s: 0.1017, time:10.8949, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:38:55 \u001b[32mINFO     \u001b[0m train.py: [3/300], [170/484], step: 1622, 1.658 samples/sec, batch_loss: 0.0464, batch_loss_c: 0.0404, batch_loss_s: 0.0604, time:24.1297, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:39:03 \u001b[32mINFO     \u001b[0m train.py: [3/300], [180/484], step: 1632, 5.075 samples/sec, batch_loss: 0.5965, batch_loss_c: 0.5889, batch_loss_s: 0.6143, time:7.8817, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:39:11 \u001b[32mINFO     \u001b[0m train.py: [3/300], [190/484], step: 1642, 4.586 samples/sec, batch_loss: 0.1011, batch_loss_c: 0.0826, batch_loss_s: 0.1443, time:8.7231, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:39:25 \u001b[32mINFO     \u001b[0m train.py: [3/300], [200/484], step: 1652, 3.010 samples/sec, batch_loss: 0.1930, batch_loss_c: 0.1804, batch_loss_s: 0.2224, time:13.2876, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:39:46 \u001b[32mINFO     \u001b[0m train.py: [3/300], [210/484], step: 1662, 1.844 samples/sec, batch_loss: 0.1726, batch_loss_c: 0.1927, batch_loss_s: 0.1256, time:21.6975, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:39:55 \u001b[32mINFO     \u001b[0m train.py: [3/300], [220/484], step: 1672, 4.356 samples/sec, batch_loss: 0.0496, batch_loss_c: 0.0454, batch_loss_s: 0.0595, time:9.1818, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:40:15 \u001b[32mINFO     \u001b[0m train.py: [3/300], [230/484], step: 1682, 2.034 samples/sec, batch_loss: 0.2883, batch_loss_c: 0.2782, batch_loss_s: 0.3117, time:19.6651, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:40:33 \u001b[32mINFO     \u001b[0m train.py: [3/300], [240/484], step: 1692, 2.297 samples/sec, batch_loss: 0.1572, batch_loss_c: 0.1821, batch_loss_s: 0.0991, time:17.4122, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:40:48 \u001b[32mINFO     \u001b[0m train.py: [3/300], [250/484], step: 1702, 2.649 samples/sec, batch_loss: 0.1800, batch_loss_c: 0.2140, batch_loss_s: 0.1009, time:15.0983, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:41:11 \u001b[32mINFO     \u001b[0m train.py: [3/300], [260/484], step: 1712, 1.710 samples/sec, batch_loss: 0.0726, batch_loss_c: 0.0650, batch_loss_s: 0.0905, time:23.3933, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:41:31 \u001b[32mINFO     \u001b[0m train.py: [3/300], [270/484], step: 1722, 1.992 samples/sec, batch_loss: 0.0550, batch_loss_c: 0.0481, batch_loss_s: 0.0712, time:20.0848, lr:1e-05\u001b[0m\n",
            "tcmalloc: large alloc 4695465984 bytes == 0x7f1d0902e000 @  0x7f214dff61e7 0x7f21425daf71 0x7f214263e55d 0x7f2142641e28 0x7f21426423e5 0x7f21426d8fc2 0x50ac25 0x50d390 0x508245 0x50a080 0x50aa7d 0x50d390 0x509455 0x595311 0x5a067e 0x557d28 0x50c88b 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x509455 0x595311 0x5a522c 0x557f7e 0x4b5eff 0x50c810\n",
            "2019-12-07 12:41:51 \u001b[32mINFO     \u001b[0m train.py: [3/300], [280/484], step: 1732, 2.055 samples/sec, batch_loss: 0.3437, batch_loss_c: 0.3512, batch_loss_s: 0.3263, time:19.4631, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:42:00 \u001b[32mINFO     \u001b[0m train.py: [3/300], [290/484], step: 1742, 4.258 samples/sec, batch_loss: 0.0501, batch_loss_c: 0.0441, batch_loss_s: 0.0642, time:9.3938, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:42:12 \u001b[32mINFO     \u001b[0m train.py: [3/300], [300/484], step: 1752, 3.203 samples/sec, batch_loss: 0.0485, batch_loss_c: 0.0433, batch_loss_s: 0.0605, time:12.4887, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:42:30 \u001b[32mINFO     \u001b[0m train.py: [3/300], [310/484], step: 1762, 2.233 samples/sec, batch_loss: 0.0626, batch_loss_c: 0.0573, batch_loss_s: 0.0749, time:17.9160, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:42:41 \u001b[32mINFO     \u001b[0m train.py: [3/300], [320/484], step: 1772, 3.884 samples/sec, batch_loss: 0.0814, batch_loss_c: 0.0841, batch_loss_s: 0.0750, time:10.2992, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:42:53 \u001b[32mINFO     \u001b[0m train.py: [3/300], [330/484], step: 1782, 3.138 samples/sec, batch_loss: 0.0908, batch_loss_c: 0.0858, batch_loss_s: 0.1025, time:12.7467, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:43:04 \u001b[32mINFO     \u001b[0m train.py: [3/300], [340/484], step: 1792, 3.668 samples/sec, batch_loss: 0.0599, batch_loss_c: 0.0559, batch_loss_s: 0.0690, time:10.9056, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:43:21 \u001b[32mINFO     \u001b[0m train.py: [3/300], [350/484], step: 1802, 2.418 samples/sec, batch_loss: 0.0642, batch_loss_c: 0.0544, batch_loss_s: 0.0871, time:16.5423, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:43:32 \u001b[32mINFO     \u001b[0m train.py: [3/300], [360/484], step: 1812, 3.499 samples/sec, batch_loss: 0.3556, batch_loss_c: 0.3573, batch_loss_s: 0.3517, time:11.4331, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:43:51 \u001b[32mINFO     \u001b[0m train.py: [3/300], [370/484], step: 1822, 2.179 samples/sec, batch_loss: 0.3027, batch_loss_c: 0.2997, batch_loss_s: 0.3096, time:18.3574, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:44:27 \u001b[32mINFO     \u001b[0m train.py: [3/300], [380/484], step: 1832, 1.112 samples/sec, batch_loss: 0.1043, batch_loss_c: 0.0966, batch_loss_s: 0.1223, time:35.9571, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:44:43 \u001b[32mINFO     \u001b[0m train.py: [3/300], [390/484], step: 1842, 2.478 samples/sec, batch_loss: 0.0475, batch_loss_c: 0.0445, batch_loss_s: 0.0546, time:16.1417, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:44:59 \u001b[32mINFO     \u001b[0m train.py: [3/300], [400/484], step: 1852, 2.403 samples/sec, batch_loss: 0.1582, batch_loss_c: 0.1589, batch_loss_s: 0.1565, time:16.6471, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:45:13 \u001b[32mINFO     \u001b[0m train.py: [3/300], [410/484], step: 1862, 2.999 samples/sec, batch_loss: 0.0903, batch_loss_c: 0.0794, batch_loss_s: 0.1158, time:13.3385, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:45:22 \u001b[32mINFO     \u001b[0m train.py: [3/300], [420/484], step: 1872, 4.284 samples/sec, batch_loss: 0.0627, batch_loss_c: 0.0602, batch_loss_s: 0.0686, time:9.3378, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:45:33 \u001b[32mINFO     \u001b[0m train.py: [3/300], [430/484], step: 1882, 3.549 samples/sec, batch_loss: 0.0741, batch_loss_c: 0.0668, batch_loss_s: 0.0911, time:11.2696, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:46:03 \u001b[32mINFO     \u001b[0m train.py: [3/300], [440/484], step: 1892, 1.330 samples/sec, batch_loss: 0.0432, batch_loss_c: 0.0409, batch_loss_s: 0.0486, time:30.0784, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:46:17 \u001b[32mINFO     \u001b[0m train.py: [3/300], [450/484], step: 1902, 2.924 samples/sec, batch_loss: 0.0850, batch_loss_c: 0.0814, batch_loss_s: 0.0936, time:13.6794, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:46:32 \u001b[32mINFO     \u001b[0m train.py: [3/300], [460/484], step: 1912, 2.667 samples/sec, batch_loss: 0.1926, batch_loss_c: 0.1862, batch_loss_s: 0.2077, time:14.9991, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:46:45 \u001b[32mINFO     \u001b[0m train.py: [3/300], [470/484], step: 1922, 3.007 samples/sec, batch_loss: 0.0879, batch_loss_c: 0.0854, batch_loss_s: 0.0937, time:13.3005, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:47:11 \u001b[32mINFO     \u001b[0m train.py: [3/300], [480/484], step: 1932, 1.582 samples/sec, batch_loss: 0.0403, batch_loss_c: 0.0380, batch_loss_s: 0.0459, time:25.2783, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:47:13 \u001b[32mINFO     \u001b[0m train.py: [3/300], train_loss: 0.1496, time: 762.8381, lr: 1e-05\u001b[0m\n",
            "2019-12-07 12:47:15 \u001b[32mINFO     \u001b[0m train.py: [4/300], [0/484], step: 1936, 38.799 samples/sec, batch_loss: 0.5147, batch_loss_c: 0.5050, batch_loss_s: 0.5373, time:1.0310, lr:1e-05\u001b[0m\n",
            "tcmalloc: large alloc 5197824000 bytes == 0x7f1d0902e000 @  0x7f214dff61e7 0x7f21425daf71 0x7f214263e55d 0x7f2142641e28 0x7f21426423e5 0x7f21426d8fc2 0x50ac25 0x50d390 0x508245 0x50a080 0x50aa7d 0x50d390 0x509455 0x595311 0x5a067e 0x557d28 0x50c88b 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x509455 0x595311 0x5a522c 0x557f7e 0x4b5eff 0x50c810\n",
            "2019-12-07 12:47:36 \u001b[32mINFO     \u001b[0m train.py: [4/300], [10/484], step: 1946, 1.872 samples/sec, batch_loss: 0.3048, batch_loss_c: 0.3050, batch_loss_s: 0.3043, time:21.3646, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:47:50 \u001b[32mINFO     \u001b[0m train.py: [4/300], [20/484], step: 1956, 2.848 samples/sec, batch_loss: 0.2286, batch_loss_c: 0.1724, batch_loss_s: 0.3597, time:14.0450, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:48:06 \u001b[32mINFO     \u001b[0m train.py: [4/300], [30/484], step: 1966, 2.538 samples/sec, batch_loss: 0.1276, batch_loss_c: 0.1286, batch_loss_s: 0.1255, time:15.7598, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:48:16 \u001b[32mINFO     \u001b[0m train.py: [4/300], [40/484], step: 1976, 4.201 samples/sec, batch_loss: 0.0726, batch_loss_c: 0.0670, batch_loss_s: 0.0857, time:9.5219, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:48:30 \u001b[32mINFO     \u001b[0m train.py: [4/300], [50/484], step: 1986, 2.803 samples/sec, batch_loss: 0.2927, batch_loss_c: 0.2690, batch_loss_s: 0.3481, time:14.2710, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:48:38 \u001b[32mINFO     \u001b[0m train.py: [4/300], [60/484], step: 1996, 4.878 samples/sec, batch_loss: 0.0539, batch_loss_c: 0.0497, batch_loss_s: 0.0637, time:8.2006, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:48:46 \u001b[32mINFO     \u001b[0m train.py: [4/300], [70/484], step: 2006, 4.849 samples/sec, batch_loss: 0.0991, batch_loss_c: 0.0860, batch_loss_s: 0.1296, time:8.2494, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:48:55 \u001b[32mINFO     \u001b[0m train.py: [4/300], [80/484], step: 2016, 4.426 samples/sec, batch_loss: 0.0794, batch_loss_c: 0.0857, batch_loss_s: 0.0646, time:9.0373, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:49:26 \u001b[32mINFO     \u001b[0m train.py: [4/300], [90/484], step: 2026, 1.322 samples/sec, batch_loss: 0.1073, batch_loss_c: 0.1014, batch_loss_s: 0.1211, time:30.2561, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:49:35 \u001b[32mINFO     \u001b[0m train.py: [4/300], [100/484], step: 2036, 4.181 samples/sec, batch_loss: 0.2009, batch_loss_c: 0.1819, batch_loss_s: 0.2452, time:9.5681, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:49:48 \u001b[32mINFO     \u001b[0m train.py: [4/300], [110/484], step: 2046, 3.133 samples/sec, batch_loss: 0.3237, batch_loss_c: 0.3105, batch_loss_s: 0.3545, time:12.7655, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:49:57 \u001b[32mINFO     \u001b[0m train.py: [4/300], [120/484], step: 2056, 4.336 samples/sec, batch_loss: 0.1049, batch_loss_c: 0.1079, batch_loss_s: 0.0980, time:9.2260, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:50:06 \u001b[32mINFO     \u001b[0m train.py: [4/300], [130/484], step: 2066, 4.557 samples/sec, batch_loss: 0.0496, batch_loss_c: 0.0422, batch_loss_s: 0.0670, time:8.7782, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:50:16 \u001b[32mINFO     \u001b[0m train.py: [4/300], [140/484], step: 2076, 3.981 samples/sec, batch_loss: 0.1869, batch_loss_c: 0.1880, batch_loss_s: 0.1845, time:10.0482, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:50:29 \u001b[32mINFO     \u001b[0m train.py: [4/300], [150/484], step: 2086, 3.168 samples/sec, batch_loss: 0.0555, batch_loss_c: 0.0565, batch_loss_s: 0.0530, time:12.6251, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:50:44 \u001b[32mINFO     \u001b[0m train.py: [4/300], [160/484], step: 2096, 2.560 samples/sec, batch_loss: 0.0576, batch_loss_c: 0.0472, batch_loss_s: 0.0821, time:15.6266, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:50:56 \u001b[32mINFO     \u001b[0m train.py: [4/300], [170/484], step: 2106, 3.465 samples/sec, batch_loss: 0.3645, batch_loss_c: 0.3564, batch_loss_s: 0.3835, time:11.5447, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:51:14 \u001b[32mINFO     \u001b[0m train.py: [4/300], [180/484], step: 2116, 2.197 samples/sec, batch_loss: 0.1424, batch_loss_c: 0.1273, batch_loss_s: 0.1776, time:18.2037, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:51:32 \u001b[32mINFO     \u001b[0m train.py: [4/300], [190/484], step: 2126, 2.180 samples/sec, batch_loss: 0.3038, batch_loss_c: 0.3030, batch_loss_s: 0.3055, time:18.3527, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:51:51 \u001b[32mINFO     \u001b[0m train.py: [4/300], [200/484], step: 2136, 2.189 samples/sec, batch_loss: 0.1963, batch_loss_c: 0.2068, batch_loss_s: 0.1716, time:18.2739, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:52:00 \u001b[32mINFO     \u001b[0m train.py: [4/300], [210/484], step: 2146, 4.366 samples/sec, batch_loss: 0.1451, batch_loss_c: 0.1377, batch_loss_s: 0.1624, time:9.1608, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:52:10 \u001b[32mINFO     \u001b[0m train.py: [4/300], [220/484], step: 2156, 3.803 samples/sec, batch_loss: 0.1108, batch_loss_c: 0.1053, batch_loss_s: 0.1237, time:10.5175, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:52:23 \u001b[32mINFO     \u001b[0m train.py: [4/300], [230/484], step: 2166, 3.120 samples/sec, batch_loss: 0.0510, batch_loss_c: 0.0433, batch_loss_s: 0.0689, time:12.8223, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:52:41 \u001b[32mINFO     \u001b[0m train.py: [4/300], [240/484], step: 2176, 2.203 samples/sec, batch_loss: 0.0505, batch_loss_c: 0.0472, batch_loss_s: 0.0582, time:18.1608, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:52:53 \u001b[32mINFO     \u001b[0m train.py: [4/300], [250/484], step: 2186, 3.379 samples/sec, batch_loss: 0.0660, batch_loss_c: 0.0624, batch_loss_s: 0.0745, time:11.8377, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:53:01 \u001b[32mINFO     \u001b[0m train.py: [4/300], [260/484], step: 2196, 5.095 samples/sec, batch_loss: 0.0325, batch_loss_c: 0.0273, batch_loss_s: 0.0446, time:7.8501, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:53:27 \u001b[32mINFO     \u001b[0m train.py: [4/300], [270/484], step: 2206, 1.554 samples/sec, batch_loss: 0.5780, batch_loss_c: 0.5892, batch_loss_s: 0.5519, time:25.7451, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:53:39 \u001b[32mINFO     \u001b[0m train.py: [4/300], [280/484], step: 2216, 3.353 samples/sec, batch_loss: 0.1129, batch_loss_c: 0.1112, batch_loss_s: 0.1169, time:11.9305, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:53:49 \u001b[32mINFO     \u001b[0m train.py: [4/300], [290/484], step: 2226, 3.843 samples/sec, batch_loss: 0.0459, batch_loss_c: 0.0386, batch_loss_s: 0.0629, time:10.4098, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:54:03 \u001b[32mINFO     \u001b[0m train.py: [4/300], [300/484], step: 2236, 2.823 samples/sec, batch_loss: 0.0452, batch_loss_c: 0.0405, batch_loss_s: 0.0563, time:14.1679, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:54:13 \u001b[32mINFO     \u001b[0m train.py: [4/300], [310/484], step: 2246, 3.953 samples/sec, batch_loss: 0.0676, batch_loss_c: 0.0611, batch_loss_s: 0.0827, time:10.1189, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:54:25 \u001b[32mINFO     \u001b[0m train.py: [4/300], [320/484], step: 2256, 3.522 samples/sec, batch_loss: 0.3261, batch_loss_c: 0.3218, batch_loss_s: 0.3363, time:11.3566, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:54:37 \u001b[32mINFO     \u001b[0m train.py: [4/300], [330/484], step: 2266, 3.296 samples/sec, batch_loss: 0.1174, batch_loss_c: 0.1171, batch_loss_s: 0.1181, time:12.1376, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:54:47 \u001b[32mINFO     \u001b[0m train.py: [4/300], [340/484], step: 2276, 3.859 samples/sec, batch_loss: 0.0609, batch_loss_c: 0.0550, batch_loss_s: 0.0746, time:10.3645, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:54:58 \u001b[32mINFO     \u001b[0m train.py: [4/300], [350/484], step: 2286, 3.862 samples/sec, batch_loss: 0.0688, batch_loss_c: 0.0629, batch_loss_s: 0.0824, time:10.3572, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:55:11 \u001b[32mINFO     \u001b[0m train.py: [4/300], [360/484], step: 2296, 3.005 samples/sec, batch_loss: 0.1374, batch_loss_c: 0.1258, batch_loss_s: 0.1646, time:13.3093, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:55:34 \u001b[32mINFO     \u001b[0m train.py: [4/300], [370/484], step: 2306, 1.698 samples/sec, batch_loss: 0.0571, batch_loss_c: 0.0521, batch_loss_s: 0.0688, time:23.5576, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:56:03 \u001b[32mINFO     \u001b[0m train.py: [4/300], [380/484], step: 2316, 1.413 samples/sec, batch_loss: 0.0667, batch_loss_c: 0.0646, batch_loss_s: 0.0716, time:28.3058, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:56:18 \u001b[32mINFO     \u001b[0m train.py: [4/300], [390/484], step: 2326, 2.599 samples/sec, batch_loss: 0.1342, batch_loss_c: 0.1609, batch_loss_s: 0.0719, time:15.3889, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:56:34 \u001b[32mINFO     \u001b[0m train.py: [4/300], [400/484], step: 2336, 2.546 samples/sec, batch_loss: 0.1172, batch_loss_c: 0.1081, batch_loss_s: 0.1387, time:15.7102, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:56:49 \u001b[32mINFO     \u001b[0m train.py: [4/300], [410/484], step: 2346, 2.680 samples/sec, batch_loss: 0.0706, batch_loss_c: 0.0614, batch_loss_s: 0.0920, time:14.9258, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:57:07 \u001b[32mINFO     \u001b[0m train.py: [4/300], [420/484], step: 2356, 2.168 samples/sec, batch_loss: 0.3208, batch_loss_c: 0.3187, batch_loss_s: 0.3258, time:18.4472, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:57:24 \u001b[32mINFO     \u001b[0m train.py: [4/300], [430/484], step: 2366, 2.329 samples/sec, batch_loss: 0.0499, batch_loss_c: 0.0429, batch_loss_s: 0.0664, time:17.1778, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:57:36 \u001b[32mINFO     \u001b[0m train.py: [4/300], [440/484], step: 2376, 3.370 samples/sec, batch_loss: 0.1078, batch_loss_c: 0.0906, batch_loss_s: 0.1479, time:11.8683, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:57:52 \u001b[32mINFO     \u001b[0m train.py: [4/300], [450/484], step: 2386, 2.584 samples/sec, batch_loss: 0.0631, batch_loss_c: 0.0591, batch_loss_s: 0.0724, time:15.4786, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:58:00 \u001b[32mINFO     \u001b[0m train.py: [4/300], [460/484], step: 2396, 4.690 samples/sec, batch_loss: 0.0794, batch_loss_c: 0.0759, batch_loss_s: 0.0875, time:8.5291, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:58:15 \u001b[32mINFO     \u001b[0m train.py: [4/300], [470/484], step: 2406, 2.674 samples/sec, batch_loss: 0.0921, batch_loss_c: 0.0938, batch_loss_s: 0.0883, time:14.9602, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:58:28 \u001b[32mINFO     \u001b[0m train.py: [4/300], [480/484], step: 2416, 3.109 samples/sec, batch_loss: 0.0477, batch_loss_c: 0.0399, batch_loss_s: 0.0660, time:12.8669, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:58:33 \u001b[32mINFO     \u001b[0m train.py: [4/300], train_loss: 0.1451, time: 678.9370, lr: 1e-05\u001b[0m\n",
            "2019-12-07 12:58:34 \u001b[32mINFO     \u001b[0m train.py: [5/300], [0/484], step: 2420, 54.056 samples/sec, batch_loss: 0.0474, batch_loss_c: 0.0461, batch_loss_s: 0.0506, time:0.7400, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:59:12 \u001b[32mINFO     \u001b[0m train.py: [5/300], [10/484], step: 2430, 1.061 samples/sec, batch_loss: 0.0550, batch_loss_c: 0.0450, batch_loss_s: 0.0783, time:37.6938, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:59:29 \u001b[32mINFO     \u001b[0m train.py: [5/300], [20/484], step: 2440, 2.321 samples/sec, batch_loss: 0.2703, batch_loss_c: 0.3100, batch_loss_s: 0.1777, time:17.2306, lr:1e-05\u001b[0m\n",
            "2019-12-07 12:59:44 \u001b[32mINFO     \u001b[0m train.py: [5/300], [30/484], step: 2450, 2.704 samples/sec, batch_loss: 0.1136, batch_loss_c: 0.1082, batch_loss_s: 0.1261, time:14.7954, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:00:07 \u001b[32mINFO     \u001b[0m train.py: [5/300], [40/484], step: 2460, 1.717 samples/sec, batch_loss: 0.0816, batch_loss_c: 0.0728, batch_loss_s: 0.1020, time:23.2939, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:00:17 \u001b[32mINFO     \u001b[0m train.py: [5/300], [50/484], step: 2470, 3.830 samples/sec, batch_loss: 0.0565, batch_loss_c: 0.0500, batch_loss_s: 0.0717, time:10.4436, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:00:29 \u001b[32mINFO     \u001b[0m train.py: [5/300], [60/484], step: 2480, 3.581 samples/sec, batch_loss: 0.0539, batch_loss_c: 0.0463, batch_loss_s: 0.0719, time:11.1713, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:00:43 \u001b[32mINFO     \u001b[0m train.py: [5/300], [70/484], step: 2490, 2.735 samples/sec, batch_loss: 0.1220, batch_loss_c: 0.1081, batch_loss_s: 0.1544, time:14.6240, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:01:01 \u001b[32mINFO     \u001b[0m train.py: [5/300], [80/484], step: 2500, 2.229 samples/sec, batch_loss: 0.0910, batch_loss_c: 0.0974, batch_loss_s: 0.0763, time:17.9431, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:01:11 \u001b[32mINFO     \u001b[0m train.py: [5/300], [90/484], step: 2510, 4.126 samples/sec, batch_loss: 0.0438, batch_loss_c: 0.0389, batch_loss_s: 0.0551, time:9.6949, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:01:24 \u001b[32mINFO     \u001b[0m train.py: [5/300], [100/484], step: 2520, 3.136 samples/sec, batch_loss: 0.1406, batch_loss_c: 0.1554, batch_loss_s: 0.1059, time:12.7563, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:01:35 \u001b[32mINFO     \u001b[0m train.py: [5/300], [110/484], step: 2530, 3.561 samples/sec, batch_loss: 0.3233, batch_loss_c: 0.3245, batch_loss_s: 0.3205, time:11.2339, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:01:58 \u001b[32mINFO     \u001b[0m train.py: [5/300], [120/484], step: 2540, 1.726 samples/sec, batch_loss: 0.0400, batch_loss_c: 0.0330, batch_loss_s: 0.0563, time:23.1809, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:02:13 \u001b[32mINFO     \u001b[0m train.py: [5/300], [130/484], step: 2550, 2.612 samples/sec, batch_loss: 0.1237, batch_loss_c: 0.1130, batch_loss_s: 0.1486, time:15.3123, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:02:38 \u001b[32mINFO     \u001b[0m train.py: [5/300], [140/484], step: 2560, 1.639 samples/sec, batch_loss: 0.0665, batch_loss_c: 0.0517, batch_loss_s: 0.1009, time:24.3997, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:02:55 \u001b[32mINFO     \u001b[0m train.py: [5/300], [150/484], step: 2570, 2.302 samples/sec, batch_loss: 0.0913, batch_loss_c: 0.0910, batch_loss_s: 0.0920, time:17.3740, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:03:04 \u001b[32mINFO     \u001b[0m train.py: [5/300], [160/484], step: 2580, 4.604 samples/sec, batch_loss: 0.0844, batch_loss_c: 0.0807, batch_loss_s: 0.0930, time:8.6876, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:03:14 \u001b[32mINFO     \u001b[0m train.py: [5/300], [170/484], step: 2590, 4.108 samples/sec, batch_loss: 0.0553, batch_loss_c: 0.0456, batch_loss_s: 0.0781, time:9.7373, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:03:22 \u001b[32mINFO     \u001b[0m train.py: [5/300], [180/484], step: 2600, 4.561 samples/sec, batch_loss: 0.1059, batch_loss_c: 0.1090, batch_loss_s: 0.0986, time:8.7705, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:03:33 \u001b[32mINFO     \u001b[0m train.py: [5/300], [190/484], step: 2610, 3.763 samples/sec, batch_loss: 0.2424, batch_loss_c: 0.2215, batch_loss_s: 0.2912, time:10.6292, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:03:42 \u001b[32mINFO     \u001b[0m train.py: [5/300], [200/484], step: 2620, 4.328 samples/sec, batch_loss: 0.3588, batch_loss_c: 0.3618, batch_loss_s: 0.3516, time:9.2422, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:03:51 \u001b[32mINFO     \u001b[0m train.py: [5/300], [210/484], step: 2630, 4.375 samples/sec, batch_loss: 0.1947, batch_loss_c: 0.2217, batch_loss_s: 0.1318, time:9.1423, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:04:04 \u001b[32mINFO     \u001b[0m train.py: [5/300], [220/484], step: 2640, 3.253 samples/sec, batch_loss: 0.0931, batch_loss_c: 0.0855, batch_loss_s: 0.1109, time:12.2972, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:04:18 \u001b[32mINFO     \u001b[0m train.py: [5/300], [230/484], step: 2650, 2.753 samples/sec, batch_loss: 0.1494, batch_loss_c: 0.1473, batch_loss_s: 0.1542, time:14.5293, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:04:28 \u001b[32mINFO     \u001b[0m train.py: [5/300], [240/484], step: 2660, 4.114 samples/sec, batch_loss: 0.0851, batch_loss_c: 0.0799, batch_loss_s: 0.0972, time:9.7232, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:04:41 \u001b[32mINFO     \u001b[0m train.py: [5/300], [250/484], step: 2670, 3.127 samples/sec, batch_loss: 0.0597, batch_loss_c: 0.0558, batch_loss_s: 0.0687, time:12.7905, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:04:50 \u001b[32mINFO     \u001b[0m train.py: [5/300], [260/484], step: 2680, 4.385 samples/sec, batch_loss: 0.4491, batch_loss_c: 0.4483, batch_loss_s: 0.4511, time:9.1230, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:05:08 \u001b[32mINFO     \u001b[0m train.py: [5/300], [270/484], step: 2690, 2.252 samples/sec, batch_loss: 0.3403, batch_loss_c: 0.3194, batch_loss_s: 0.3890, time:17.7594, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:05:28 \u001b[32mINFO     \u001b[0m train.py: [5/300], [280/484], step: 2700, 1.941 samples/sec, batch_loss: 0.1350, batch_loss_c: 0.1443, batch_loss_s: 0.1132, time:20.6114, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:05:41 \u001b[32mINFO     \u001b[0m train.py: [5/300], [290/484], step: 2710, 3.197 samples/sec, batch_loss: 0.1809, batch_loss_c: 0.1766, batch_loss_s: 0.1909, time:12.5109, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:06:05 \u001b[32mINFO     \u001b[0m train.py: [5/300], [300/484], step: 2720, 1.638 samples/sec, batch_loss: 0.0655, batch_loss_c: 0.0601, batch_loss_s: 0.0783, time:24.4156, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:06:18 \u001b[32mINFO     \u001b[0m train.py: [5/300], [310/484], step: 2730, 3.068 samples/sec, batch_loss: 0.0688, batch_loss_c: 0.0616, batch_loss_s: 0.0855, time:13.0388, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:06:34 \u001b[32mINFO     \u001b[0m train.py: [5/300], [320/484], step: 2740, 2.525 samples/sec, batch_loss: 0.1479, batch_loss_c: 0.1522, batch_loss_s: 0.1378, time:15.8390, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:06:45 \u001b[32mINFO     \u001b[0m train.py: [5/300], [330/484], step: 2750, 3.525 samples/sec, batch_loss: 0.1420, batch_loss_c: 0.1544, batch_loss_s: 0.1130, time:11.3473, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:07:01 \u001b[32mINFO     \u001b[0m train.py: [5/300], [340/484], step: 2760, 2.635 samples/sec, batch_loss: 0.1924, batch_loss_c: 0.1810, batch_loss_s: 0.2191, time:15.1817, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:07:14 \u001b[32mINFO     \u001b[0m train.py: [5/300], [350/484], step: 2770, 3.027 samples/sec, batch_loss: 0.1296, batch_loss_c: 0.1204, batch_loss_s: 0.1510, time:13.2138, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:07:26 \u001b[32mINFO     \u001b[0m train.py: [5/300], [360/484], step: 2780, 3.339 samples/sec, batch_loss: 0.0722, batch_loss_c: 0.0558, batch_loss_s: 0.1105, time:11.9788, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:07:47 \u001b[32mINFO     \u001b[0m train.py: [5/300], [370/484], step: 2790, 1.864 samples/sec, batch_loss: 0.0624, batch_loss_c: 0.0579, batch_loss_s: 0.0730, time:21.4604, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:07:59 \u001b[32mINFO     \u001b[0m train.py: [5/300], [380/484], step: 2800, 3.376 samples/sec, batch_loss: 0.1088, batch_loss_c: 0.1054, batch_loss_s: 0.1168, time:11.8483, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:08:18 \u001b[32mINFO     \u001b[0m train.py: [5/300], [390/484], step: 2810, 2.129 samples/sec, batch_loss: 0.1556, batch_loss_c: 0.1566, batch_loss_s: 0.1532, time:18.7838, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:08:26 \u001b[32mINFO     \u001b[0m train.py: [5/300], [400/484], step: 2820, 4.674 samples/sec, batch_loss: 0.1438, batch_loss_c: 0.1577, batch_loss_s: 0.1114, time:8.5579, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:08:44 \u001b[32mINFO     \u001b[0m train.py: [5/300], [410/484], step: 2830, 2.249 samples/sec, batch_loss: 0.3137, batch_loss_c: 0.2959, batch_loss_s: 0.3553, time:17.7892, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:09:19 \u001b[32mINFO     \u001b[0m train.py: [5/300], [420/484], step: 2840, 1.158 samples/sec, batch_loss: 0.0661, batch_loss_c: 0.0546, batch_loss_s: 0.0930, time:34.5321, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:09:28 \u001b[32mINFO     \u001b[0m train.py: [5/300], [430/484], step: 2850, 4.121 samples/sec, batch_loss: 0.3028, batch_loss_c: 0.3013, batch_loss_s: 0.3061, time:9.7071, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:09:39 \u001b[32mINFO     \u001b[0m train.py: [5/300], [440/484], step: 2860, 3.853 samples/sec, batch_loss: 0.2857, batch_loss_c: 0.2810, batch_loss_s: 0.2966, time:10.3825, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:09:49 \u001b[32mINFO     \u001b[0m train.py: [5/300], [450/484], step: 2870, 4.038 samples/sec, batch_loss: 0.3034, batch_loss_c: 0.2998, batch_loss_s: 0.3118, time:9.9064, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:10:00 \u001b[32mINFO     \u001b[0m train.py: [5/300], [460/484], step: 2880, 3.467 samples/sec, batch_loss: 0.0471, batch_loss_c: 0.0363, batch_loss_s: 0.0724, time:11.5370, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:10:12 \u001b[32mINFO     \u001b[0m train.py: [5/300], [470/484], step: 2890, 3.515 samples/sec, batch_loss: 0.3025, batch_loss_c: 0.2974, batch_loss_s: 0.3146, time:11.3794, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:10:21 \u001b[32mINFO     \u001b[0m train.py: [5/300], [480/484], step: 2900, 4.155 samples/sec, batch_loss: 0.0576, batch_loss_c: 0.0511, batch_loss_s: 0.0728, time:9.6275, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:10:29 \u001b[32mINFO     \u001b[0m train.py: [5/300], train_loss: 0.1400, time: 715.7100, lr: 1e-05\u001b[0m\n",
            "2019-12-07 13:10:30 \u001b[32mINFO     \u001b[0m train.py: [6/300], [0/484], step: 2904, 62.867 samples/sec, batch_loss: 0.0673, batch_loss_c: 0.0698, batch_loss_s: 0.0613, time:0.6363, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:10:47 \u001b[32mINFO     \u001b[0m train.py: [6/300], [10/484], step: 2914, 2.333 samples/sec, batch_loss: 0.0648, batch_loss_c: 0.0505, batch_loss_s: 0.0981, time:17.1449, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:10:57 \u001b[32mINFO     \u001b[0m train.py: [6/300], [20/484], step: 2924, 4.013 samples/sec, batch_loss: 0.3015, batch_loss_c: 0.2977, batch_loss_s: 0.3103, time:9.9687, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:11:09 \u001b[32mINFO     \u001b[0m train.py: [6/300], [30/484], step: 2934, 3.344 samples/sec, batch_loss: 0.1221, batch_loss_c: 0.1362, batch_loss_s: 0.0892, time:11.9619, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:11:17 \u001b[32mINFO     \u001b[0m train.py: [6/300], [40/484], step: 2944, 4.792 samples/sec, batch_loss: 0.0419, batch_loss_c: 0.0358, batch_loss_s: 0.0560, time:8.3476, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:11:32 \u001b[32mINFO     \u001b[0m train.py: [6/300], [50/484], step: 2954, 2.722 samples/sec, batch_loss: 0.3169, batch_loss_c: 0.3127, batch_loss_s: 0.3267, time:14.6973, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:11:46 \u001b[32mINFO     \u001b[0m train.py: [6/300], [60/484], step: 2964, 2.873 samples/sec, batch_loss: 0.3070, batch_loss_c: 0.3029, batch_loss_s: 0.3164, time:13.9250, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:12:11 \u001b[32mINFO     \u001b[0m train.py: [6/300], [70/484], step: 2974, 1.602 samples/sec, batch_loss: 0.2041, batch_loss_c: 0.2227, batch_loss_s: 0.1608, time:24.9670, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:12:21 \u001b[32mINFO     \u001b[0m train.py: [6/300], [80/484], step: 2984, 3.890 samples/sec, batch_loss: 0.0986, batch_loss_c: 0.0928, batch_loss_s: 0.1120, time:10.2837, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:12:35 \u001b[32mINFO     \u001b[0m train.py: [6/300], [90/484], step: 2994, 3.022 samples/sec, batch_loss: 0.2277, batch_loss_c: 0.2354, batch_loss_s: 0.2097, time:13.2375, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:12:47 \u001b[32mINFO     \u001b[0m train.py: [6/300], [100/484], step: 3004, 3.252 samples/sec, batch_loss: 0.0459, batch_loss_c: 0.0400, batch_loss_s: 0.0595, time:12.3000, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:12:57 \u001b[32mINFO     \u001b[0m train.py: [6/300], [110/484], step: 3014, 3.798 samples/sec, batch_loss: 0.0641, batch_loss_c: 0.0592, batch_loss_s: 0.0754, time:10.5310, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:13:10 \u001b[32mINFO     \u001b[0m train.py: [6/300], [120/484], step: 3024, 3.169 samples/sec, batch_loss: 0.0965, batch_loss_c: 0.1053, batch_loss_s: 0.0757, time:12.6218, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:13:21 \u001b[32mINFO     \u001b[0m train.py: [6/300], [130/484], step: 3034, 3.587 samples/sec, batch_loss: 0.0422, batch_loss_c: 0.0370, batch_loss_s: 0.0543, time:11.1522, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:13:43 \u001b[32mINFO     \u001b[0m train.py: [6/300], [140/484], step: 3044, 1.867 samples/sec, batch_loss: 0.0700, batch_loss_c: 0.0608, batch_loss_s: 0.0914, time:21.4291, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:14:02 \u001b[32mINFO     \u001b[0m train.py: [6/300], [150/484], step: 3054, 2.095 samples/sec, batch_loss: 0.0837, batch_loss_c: 0.0813, batch_loss_s: 0.0892, time:19.0911, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:14:12 \u001b[32mINFO     \u001b[0m train.py: [6/300], [160/484], step: 3064, 3.769 samples/sec, batch_loss: 0.3318, batch_loss_c: 0.3269, batch_loss_s: 0.3434, time:10.6122, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:14:23 \u001b[32mINFO     \u001b[0m train.py: [6/300], [170/484], step: 3074, 3.751 samples/sec, batch_loss: 0.2850, batch_loss_c: 0.2843, batch_loss_s: 0.2867, time:10.6627, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:14:32 \u001b[32mINFO     \u001b[0m train.py: [6/300], [180/484], step: 3084, 4.589 samples/sec, batch_loss: 0.0702, batch_loss_c: 0.0600, batch_loss_s: 0.0939, time:8.7172, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:14:48 \u001b[32mINFO     \u001b[0m train.py: [6/300], [190/484], step: 3094, 2.508 samples/sec, batch_loss: 0.1041, batch_loss_c: 0.0883, batch_loss_s: 0.1408, time:15.9482, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:15:07 \u001b[32mINFO     \u001b[0m train.py: [6/300], [200/484], step: 3104, 2.018 samples/sec, batch_loss: 0.0903, batch_loss_c: 0.0837, batch_loss_s: 0.1056, time:19.8232, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:15:24 \u001b[32mINFO     \u001b[0m train.py: [6/300], [210/484], step: 3114, 2.418 samples/sec, batch_loss: 0.0612, batch_loss_c: 0.0438, batch_loss_s: 0.1019, time:16.5409, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:15:47 \u001b[32mINFO     \u001b[0m train.py: [6/300], [220/484], step: 3124, 1.764 samples/sec, batch_loss: 0.0684, batch_loss_c: 0.0628, batch_loss_s: 0.0814, time:22.6735, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:16:11 \u001b[32mINFO     \u001b[0m train.py: [6/300], [230/484], step: 3134, 1.655 samples/sec, batch_loss: 0.2911, batch_loss_c: 0.2864, batch_loss_s: 0.3019, time:24.1715, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:16:24 \u001b[32mINFO     \u001b[0m train.py: [6/300], [240/484], step: 3144, 2.997 samples/sec, batch_loss: 0.1194, batch_loss_c: 0.1178, batch_loss_s: 0.1230, time:13.3485, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:16:54 \u001b[32mINFO     \u001b[0m train.py: [6/300], [250/484], step: 3154, 1.353 samples/sec, batch_loss: 0.0659, batch_loss_c: 0.0619, batch_loss_s: 0.0753, time:29.5700, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:17:02 \u001b[32mINFO     \u001b[0m train.py: [6/300], [260/484], step: 3164, 4.781 samples/sec, batch_loss: 0.0636, batch_loss_c: 0.0551, batch_loss_s: 0.0834, time:8.3666, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:17:22 \u001b[32mINFO     \u001b[0m train.py: [6/300], [270/484], step: 3174, 1.990 samples/sec, batch_loss: 0.3463, batch_loss_c: 0.3325, batch_loss_s: 0.3785, time:20.0971, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:17:34 \u001b[32mINFO     \u001b[0m train.py: [6/300], [280/484], step: 3184, 3.382 samples/sec, batch_loss: 0.0545, batch_loss_c: 0.0477, batch_loss_s: 0.0704, time:11.8269, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:17:57 \u001b[32mINFO     \u001b[0m train.py: [6/300], [290/484], step: 3194, 1.768 samples/sec, batch_loss: 0.0684, batch_loss_c: 0.0571, batch_loss_s: 0.0949, time:22.6240, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:18:08 \u001b[32mINFO     \u001b[0m train.py: [6/300], [300/484], step: 3204, 3.388 samples/sec, batch_loss: 0.0788, batch_loss_c: 0.0546, batch_loss_s: 0.1352, time:11.8056, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:18:26 \u001b[32mINFO     \u001b[0m train.py: [6/300], [310/484], step: 3214, 2.229 samples/sec, batch_loss: 0.3327, batch_loss_c: 0.3282, batch_loss_s: 0.3432, time:17.9474, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:18:46 \u001b[32mINFO     \u001b[0m train.py: [6/300], [320/484], step: 3224, 1.993 samples/sec, batch_loss: 0.0790, batch_loss_c: 0.0691, batch_loss_s: 0.1021, time:20.0668, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:18:59 \u001b[32mINFO     \u001b[0m train.py: [6/300], [330/484], step: 3234, 3.168 samples/sec, batch_loss: 0.2879, batch_loss_c: 0.2737, batch_loss_s: 0.3210, time:12.6250, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:19:16 \u001b[32mINFO     \u001b[0m train.py: [6/300], [340/484], step: 3244, 2.342 samples/sec, batch_loss: 0.3040, batch_loss_c: 0.3011, batch_loss_s: 0.3108, time:17.0781, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:19:28 \u001b[32mINFO     \u001b[0m train.py: [6/300], [350/484], step: 3254, 3.260 samples/sec, batch_loss: 0.0825, batch_loss_c: 0.0709, batch_loss_s: 0.1097, time:12.2705, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:19:37 \u001b[32mINFO     \u001b[0m train.py: [6/300], [360/484], step: 3264, 4.943 samples/sec, batch_loss: 0.0454, batch_loss_c: 0.0404, batch_loss_s: 0.0568, time:8.0920, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:19:56 \u001b[32mINFO     \u001b[0m train.py: [6/300], [370/484], step: 3274, 2.072 samples/sec, batch_loss: 0.0675, batch_loss_c: 0.0610, batch_loss_s: 0.0829, time:19.3062, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:20:12 \u001b[32mINFO     \u001b[0m train.py: [6/300], [380/484], step: 3284, 2.468 samples/sec, batch_loss: 0.1626, batch_loss_c: 0.1637, batch_loss_s: 0.1600, time:16.2078, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:20:25 \u001b[32mINFO     \u001b[0m train.py: [6/300], [390/484], step: 3294, 3.182 samples/sec, batch_loss: 0.0714, batch_loss_c: 0.0617, batch_loss_s: 0.0940, time:12.5703, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:20:34 \u001b[32mINFO     \u001b[0m train.py: [6/300], [400/484], step: 3304, 4.348 samples/sec, batch_loss: 0.0577, batch_loss_c: 0.0539, batch_loss_s: 0.0666, time:9.1990, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:20:45 \u001b[32mINFO     \u001b[0m train.py: [6/300], [410/484], step: 3314, 3.479 samples/sec, batch_loss: 0.0606, batch_loss_c: 0.0482, batch_loss_s: 0.0896, time:11.4972, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:21:01 \u001b[32mINFO     \u001b[0m train.py: [6/300], [420/484], step: 3324, 2.549 samples/sec, batch_loss: 0.0846, batch_loss_c: 0.0751, batch_loss_s: 0.1069, time:15.6899, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:21:09 \u001b[32mINFO     \u001b[0m train.py: [6/300], [430/484], step: 3334, 4.946 samples/sec, batch_loss: 0.1386, batch_loss_c: 0.0770, batch_loss_s: 0.2824, time:8.0869, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:21:43 \u001b[32mINFO     \u001b[0m train.py: [6/300], [440/484], step: 3344, 1.182 samples/sec, batch_loss: 0.1258, batch_loss_c: 0.1239, batch_loss_s: 0.1302, time:33.8470, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:21:56 \u001b[32mINFO     \u001b[0m train.py: [6/300], [450/484], step: 3354, 3.036 samples/sec, batch_loss: 0.1188, batch_loss_c: 0.1219, batch_loss_s: 0.1116, time:13.1739, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:22:07 \u001b[32mINFO     \u001b[0m train.py: [6/300], [460/484], step: 3364, 3.673 samples/sec, batch_loss: 0.0481, batch_loss_c: 0.0423, batch_loss_s: 0.0618, time:10.8896, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:22:15 \u001b[32mINFO     \u001b[0m train.py: [6/300], [470/484], step: 3374, 5.129 samples/sec, batch_loss: 0.0759, batch_loss_c: 0.0686, batch_loss_s: 0.0931, time:7.7990, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:22:32 \u001b[32mINFO     \u001b[0m train.py: [6/300], [480/484], step: 3384, 2.369 samples/sec, batch_loss: 0.0512, batch_loss_c: 0.0472, batch_loss_s: 0.0604, time:16.8843, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:22:35 \u001b[32mINFO     \u001b[0m train.py: [6/300], train_loss: 0.1344, time: 725.4449, lr: 1e-05\u001b[0m\n",
            "2019-12-07 13:22:36 \u001b[32mINFO     \u001b[0m train.py: [7/300], [0/484], step: 3388, 52.508 samples/sec, batch_loss: 0.0521, batch_loss_c: 0.0460, batch_loss_s: 0.0662, time:0.7618, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:23:04 \u001b[32mINFO     \u001b[0m train.py: [7/300], [10/484], step: 3398, 1.438 samples/sec, batch_loss: 0.0654, batch_loss_c: 0.0597, batch_loss_s: 0.0787, time:27.8092, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:23:14 \u001b[32mINFO     \u001b[0m train.py: [7/300], [20/484], step: 3408, 4.065 samples/sec, batch_loss: 0.0529, batch_loss_c: 0.0396, batch_loss_s: 0.0840, time:9.8396, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:23:27 \u001b[32mINFO     \u001b[0m train.py: [7/300], [30/484], step: 3418, 3.006 samples/sec, batch_loss: 0.0742, batch_loss_c: 0.0753, batch_loss_s: 0.0718, time:13.3069, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:23:54 \u001b[32mINFO     \u001b[0m train.py: [7/300], [40/484], step: 3428, 1.493 samples/sec, batch_loss: 0.2579, batch_loss_c: 0.2378, batch_loss_s: 0.3047, time:26.7882, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:24:04 \u001b[32mINFO     \u001b[0m train.py: [7/300], [50/484], step: 3438, 3.968 samples/sec, batch_loss: 0.1882, batch_loss_c: 0.1827, batch_loss_s: 0.2010, time:10.0808, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:24:14 \u001b[32mINFO     \u001b[0m train.py: [7/300], [60/484], step: 3448, 3.982 samples/sec, batch_loss: 0.0495, batch_loss_c: 0.0387, batch_loss_s: 0.0747, time:10.0443, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:24:25 \u001b[32mINFO     \u001b[0m train.py: [7/300], [70/484], step: 3458, 3.592 samples/sec, batch_loss: 0.0861, batch_loss_c: 0.0721, batch_loss_s: 0.1188, time:11.1352, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:24:39 \u001b[32mINFO     \u001b[0m train.py: [7/300], [80/484], step: 3468, 2.776 samples/sec, batch_loss: 0.2954, batch_loss_c: 0.2918, batch_loss_s: 0.3037, time:14.4087, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:24:50 \u001b[32mINFO     \u001b[0m train.py: [7/300], [90/484], step: 3478, 3.663 samples/sec, batch_loss: 0.1137, batch_loss_c: 0.1183, batch_loss_s: 0.1031, time:10.9186, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:25:03 \u001b[32mINFO     \u001b[0m train.py: [7/300], [100/484], step: 3488, 3.232 samples/sec, batch_loss: 0.3059, batch_loss_c: 0.2942, batch_loss_s: 0.3333, time:12.3779, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:25:11 \u001b[32mINFO     \u001b[0m train.py: [7/300], [110/484], step: 3498, 4.761 samples/sec, batch_loss: 0.0803, batch_loss_c: 0.0743, batch_loss_s: 0.0942, time:8.4022, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:25:27 \u001b[32mINFO     \u001b[0m train.py: [7/300], [120/484], step: 3508, 2.514 samples/sec, batch_loss: 0.2999, batch_loss_c: 0.2953, batch_loss_s: 0.3106, time:15.9121, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:25:37 \u001b[32mINFO     \u001b[0m train.py: [7/300], [130/484], step: 3518, 4.016 samples/sec, batch_loss: 0.0675, batch_loss_c: 0.0529, batch_loss_s: 0.1014, time:9.9595, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:25:46 \u001b[32mINFO     \u001b[0m train.py: [7/300], [140/484], step: 3528, 4.545 samples/sec, batch_loss: 0.0468, batch_loss_c: 0.0383, batch_loss_s: 0.0666, time:8.8017, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:26:06 \u001b[32mINFO     \u001b[0m train.py: [7/300], [150/484], step: 3538, 1.966 samples/sec, batch_loss: 0.0971, batch_loss_c: 0.0738, batch_loss_s: 0.1515, time:20.3441, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:26:20 \u001b[32mINFO     \u001b[0m train.py: [7/300], [160/484], step: 3548, 2.792 samples/sec, batch_loss: 0.1009, batch_loss_c: 0.0924, batch_loss_s: 0.1207, time:14.3282, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:26:33 \u001b[32mINFO     \u001b[0m train.py: [7/300], [170/484], step: 3558, 3.109 samples/sec, batch_loss: 0.0599, batch_loss_c: 0.0518, batch_loss_s: 0.0788, time:12.8663, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:26:44 \u001b[32mINFO     \u001b[0m train.py: [7/300], [180/484], step: 3568, 3.820 samples/sec, batch_loss: 0.0796, batch_loss_c: 0.0725, batch_loss_s: 0.0961, time:10.4716, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:27:11 \u001b[32mINFO     \u001b[0m train.py: [7/300], [190/484], step: 3578, 1.453 samples/sec, batch_loss: 0.1345, batch_loss_c: 0.1028, batch_loss_s: 0.2084, time:27.5269, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:27:41 \u001b[32mINFO     \u001b[0m train.py: [7/300], [200/484], step: 3588, 1.328 samples/sec, batch_loss: 0.2521, batch_loss_c: 0.1997, batch_loss_s: 0.3744, time:30.1224, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:27:54 \u001b[32mINFO     \u001b[0m train.py: [7/300], [210/484], step: 3598, 3.164 samples/sec, batch_loss: 0.3979, batch_loss_c: 0.3987, batch_loss_s: 0.3958, time:12.6418, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:28:08 \u001b[32mINFO     \u001b[0m train.py: [7/300], [220/484], step: 3608, 2.926 samples/sec, batch_loss: 0.0563, batch_loss_c: 0.0502, batch_loss_s: 0.0704, time:13.6724, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:28:37 \u001b[32mINFO     \u001b[0m train.py: [7/300], [230/484], step: 3618, 1.359 samples/sec, batch_loss: 0.2992, batch_loss_c: 0.2824, batch_loss_s: 0.3385, time:29.4289, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:28:47 \u001b[32mINFO     \u001b[0m train.py: [7/300], [240/484], step: 3628, 3.995 samples/sec, batch_loss: 0.1016, batch_loss_c: 0.0858, batch_loss_s: 0.1386, time:10.0130, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:28:56 \u001b[32mINFO     \u001b[0m train.py: [7/300], [250/484], step: 3638, 4.362 samples/sec, batch_loss: 0.0955, batch_loss_c: 0.0953, batch_loss_s: 0.0960, time:9.1697, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:29:07 \u001b[32mINFO     \u001b[0m train.py: [7/300], [260/484], step: 3648, 3.704 samples/sec, batch_loss: 0.0783, batch_loss_c: 0.0643, batch_loss_s: 0.1111, time:10.7984, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:29:21 \u001b[32mINFO     \u001b[0m train.py: [7/300], [270/484], step: 3658, 2.851 samples/sec, batch_loss: 0.0812, batch_loss_c: 0.0835, batch_loss_s: 0.0758, time:14.0307, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:29:33 \u001b[32mINFO     \u001b[0m train.py: [7/300], [280/484], step: 3668, 3.480 samples/sec, batch_loss: 0.1061, batch_loss_c: 0.1060, batch_loss_s: 0.1061, time:11.4946, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:29:47 \u001b[32mINFO     \u001b[0m train.py: [7/300], [290/484], step: 3678, 2.760 samples/sec, batch_loss: 0.1070, batch_loss_c: 0.0985, batch_loss_s: 0.1270, time:14.4947, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:30:02 \u001b[32mINFO     \u001b[0m train.py: [7/300], [300/484], step: 3688, 2.677 samples/sec, batch_loss: 0.3254, batch_loss_c: 0.3180, batch_loss_s: 0.3428, time:14.9429, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:30:23 \u001b[32mINFO     \u001b[0m train.py: [7/300], [310/484], step: 3698, 1.888 samples/sec, batch_loss: 0.0630, batch_loss_c: 0.0549, batch_loss_s: 0.0818, time:21.1897, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:30:34 \u001b[32mINFO     \u001b[0m train.py: [7/300], [320/484], step: 3708, 3.748 samples/sec, batch_loss: 0.3017, batch_loss_c: 0.2938, batch_loss_s: 0.3203, time:10.6734, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:30:42 \u001b[32mINFO     \u001b[0m train.py: [7/300], [330/484], step: 3718, 4.813 samples/sec, batch_loss: 0.1667, batch_loss_c: 0.1865, batch_loss_s: 0.1205, time:8.3109, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:30:52 \u001b[32mINFO     \u001b[0m train.py: [7/300], [340/484], step: 3728, 3.970 samples/sec, batch_loss: 0.1517, batch_loss_c: 0.1429, batch_loss_s: 0.1720, time:10.0754, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:31:04 \u001b[32mINFO     \u001b[0m train.py: [7/300], [350/484], step: 3738, 3.509 samples/sec, batch_loss: 0.0416, batch_loss_c: 0.0356, batch_loss_s: 0.0556, time:11.3997, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:31:26 \u001b[32mINFO     \u001b[0m train.py: [7/300], [360/484], step: 3748, 1.802 samples/sec, batch_loss: 0.1147, batch_loss_c: 0.1183, batch_loss_s: 0.1063, time:22.2000, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:31:36 \u001b[32mINFO     \u001b[0m train.py: [7/300], [370/484], step: 3758, 4.017 samples/sec, batch_loss: 0.0577, batch_loss_c: 0.0468, batch_loss_s: 0.0833, time:9.9568, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:31:54 \u001b[32mINFO     \u001b[0m train.py: [7/300], [380/484], step: 3768, 2.191 samples/sec, batch_loss: 0.2038, batch_loss_c: 0.1891, batch_loss_s: 0.2379, time:18.2552, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:32:07 \u001b[32mINFO     \u001b[0m train.py: [7/300], [390/484], step: 3778, 3.165 samples/sec, batch_loss: 0.0676, batch_loss_c: 0.0603, batch_loss_s: 0.0846, time:12.6377, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:32:28 \u001b[32mINFO     \u001b[0m train.py: [7/300], [400/484], step: 3788, 1.927 samples/sec, batch_loss: 0.1738, batch_loss_c: 0.2011, batch_loss_s: 0.1101, time:20.7557, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:32:47 \u001b[32mINFO     \u001b[0m train.py: [7/300], [410/484], step: 3798, 2.082 samples/sec, batch_loss: 0.5416, batch_loss_c: 0.5337, batch_loss_s: 0.5599, time:19.2118, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:33:07 \u001b[32mINFO     \u001b[0m train.py: [7/300], [420/484], step: 3808, 1.981 samples/sec, batch_loss: 0.2006, batch_loss_c: 0.2114, batch_loss_s: 0.1752, time:20.1893, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:33:20 \u001b[32mINFO     \u001b[0m train.py: [7/300], [430/484], step: 3818, 3.031 samples/sec, batch_loss: 0.0560, batch_loss_c: 0.0494, batch_loss_s: 0.0713, time:13.1966, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:33:31 \u001b[32mINFO     \u001b[0m train.py: [7/300], [440/484], step: 3828, 3.575 samples/sec, batch_loss: 0.1066, batch_loss_c: 0.1084, batch_loss_s: 0.1024, time:11.1878, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:33:47 \u001b[32mINFO     \u001b[0m train.py: [7/300], [450/484], step: 3838, 2.565 samples/sec, batch_loss: 0.0566, batch_loss_c: 0.0511, batch_loss_s: 0.0695, time:15.5969, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:33:57 \u001b[32mINFO     \u001b[0m train.py: [7/300], [460/484], step: 3848, 3.932 samples/sec, batch_loss: 0.0998, batch_loss_c: 0.0963, batch_loss_s: 0.1080, time:10.1738, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:34:15 \u001b[32mINFO     \u001b[0m train.py: [7/300], [470/484], step: 3858, 2.254 samples/sec, batch_loss: 0.0506, batch_loss_c: 0.0458, batch_loss_s: 0.0618, time:17.7441, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:34:35 \u001b[32mINFO     \u001b[0m train.py: [7/300], [480/484], step: 3868, 2.007 samples/sec, batch_loss: 0.3279, batch_loss_c: 0.3218, batch_loss_s: 0.3423, time:19.9290, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:34:42 \u001b[32mINFO     \u001b[0m train.py: [7/300], train_loss: 0.1526, time: 727.0988, lr: 1e-05\u001b[0m\n",
            "2019-12-07 13:34:43 \u001b[32mINFO     \u001b[0m train.py: [8/300], [0/484], step: 3872, 57.600 samples/sec, batch_loss: 0.0712, batch_loss_c: 0.0592, batch_loss_s: 0.0991, time:0.6944, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:34:55 \u001b[32mINFO     \u001b[0m train.py: [8/300], [10/484], step: 3882, 3.421 samples/sec, batch_loss: 0.0775, batch_loss_c: 0.0762, batch_loss_s: 0.0805, time:11.6927, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:35:06 \u001b[32mINFO     \u001b[0m train.py: [8/300], [20/484], step: 3892, 3.851 samples/sec, batch_loss: 0.0665, batch_loss_c: 0.0531, batch_loss_s: 0.0978, time:10.3858, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:35:20 \u001b[32mINFO     \u001b[0m train.py: [8/300], [30/484], step: 3902, 2.735 samples/sec, batch_loss: 0.0536, batch_loss_c: 0.0404, batch_loss_s: 0.0842, time:14.6268, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:35:33 \u001b[32mINFO     \u001b[0m train.py: [8/300], [40/484], step: 3912, 3.026 samples/sec, batch_loss: 0.0647, batch_loss_c: 0.0559, batch_loss_s: 0.0852, time:13.2186, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:35:49 \u001b[32mINFO     \u001b[0m train.py: [8/300], [50/484], step: 3922, 2.545 samples/sec, batch_loss: 0.0826, batch_loss_c: 0.0740, batch_loss_s: 0.1026, time:15.7192, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:36:00 \u001b[32mINFO     \u001b[0m train.py: [8/300], [60/484], step: 3932, 3.698 samples/sec, batch_loss: 0.0650, batch_loss_c: 0.0483, batch_loss_s: 0.1041, time:10.8156, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:36:13 \u001b[32mINFO     \u001b[0m train.py: [8/300], [70/484], step: 3942, 3.040 samples/sec, batch_loss: 0.0401, batch_loss_c: 0.0295, batch_loss_s: 0.0646, time:13.1571, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:36:28 \u001b[32mINFO     \u001b[0m train.py: [8/300], [80/484], step: 3952, 2.773 samples/sec, batch_loss: 0.1648, batch_loss_c: 0.1868, batch_loss_s: 0.1133, time:14.4273, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:36:44 \u001b[32mINFO     \u001b[0m train.py: [8/300], [90/484], step: 3962, 2.493 samples/sec, batch_loss: 0.1009, batch_loss_c: 0.0905, batch_loss_s: 0.1250, time:16.0419, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:36:56 \u001b[32mINFO     \u001b[0m train.py: [8/300], [100/484], step: 3972, 3.151 samples/sec, batch_loss: 0.1001, batch_loss_c: 0.0878, batch_loss_s: 0.1287, time:12.6935, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:37:07 \u001b[32mINFO     \u001b[0m train.py: [8/300], [110/484], step: 3982, 3.901 samples/sec, batch_loss: 0.3316, batch_loss_c: 0.2894, batch_loss_s: 0.4302, time:10.2540, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:37:25 \u001b[32mINFO     \u001b[0m train.py: [8/300], [120/484], step: 3992, 2.133 samples/sec, batch_loss: 0.0666, batch_loss_c: 0.0630, batch_loss_s: 0.0751, time:18.7523, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:37:37 \u001b[32mINFO     \u001b[0m train.py: [8/300], [130/484], step: 4002, 3.376 samples/sec, batch_loss: 0.0615, batch_loss_c: 0.0553, batch_loss_s: 0.0758, time:11.8472, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:37:49 \u001b[32mINFO     \u001b[0m train.py: [8/300], [140/484], step: 4012, 3.283 samples/sec, batch_loss: 0.0645, batch_loss_c: 0.0538, batch_loss_s: 0.0896, time:12.1847, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:38:13 \u001b[32mINFO     \u001b[0m train.py: [8/300], [150/484], step: 4022, 1.724 samples/sec, batch_loss: 0.0511, batch_loss_c: 0.0431, batch_loss_s: 0.0698, time:23.2021, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:38:27 \u001b[32mINFO     \u001b[0m train.py: [8/300], [160/484], step: 4032, 2.764 samples/sec, batch_loss: 0.3102, batch_loss_c: 0.3074, batch_loss_s: 0.3168, time:14.4724, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:38:39 \u001b[32mINFO     \u001b[0m train.py: [8/300], [170/484], step: 4042, 3.206 samples/sec, batch_loss: 0.1168, batch_loss_c: 0.1016, batch_loss_s: 0.1522, time:12.4784, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:39:04 \u001b[32mINFO     \u001b[0m train.py: [8/300], [180/484], step: 4052, 1.607 samples/sec, batch_loss: 0.0776, batch_loss_c: 0.0667, batch_loss_s: 0.1029, time:24.8955, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:39:20 \u001b[32mINFO     \u001b[0m train.py: [8/300], [190/484], step: 4062, 2.489 samples/sec, batch_loss: 0.0683, batch_loss_c: 0.0645, batch_loss_s: 0.0773, time:16.0690, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:39:29 \u001b[32mINFO     \u001b[0m train.py: [8/300], [200/484], step: 4072, 4.935 samples/sec, batch_loss: 0.0738, batch_loss_c: 0.0718, batch_loss_s: 0.0783, time:8.1051, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:39:39 \u001b[32mINFO     \u001b[0m train.py: [8/300], [210/484], step: 4082, 3.858 samples/sec, batch_loss: 0.3316, batch_loss_c: 0.3227, batch_loss_s: 0.3524, time:10.3674, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:39:51 \u001b[32mINFO     \u001b[0m train.py: [8/300], [220/484], step: 4092, 3.427 samples/sec, batch_loss: 0.3435, batch_loss_c: 0.3453, batch_loss_s: 0.3394, time:11.6731, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:40:02 \u001b[32mINFO     \u001b[0m train.py: [8/300], [230/484], step: 4102, 3.618 samples/sec, batch_loss: 0.0442, batch_loss_c: 0.0381, batch_loss_s: 0.0586, time:11.0563, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:40:26 \u001b[32mINFO     \u001b[0m train.py: [8/300], [240/484], step: 4112, 1.623 samples/sec, batch_loss: 0.2049, batch_loss_c: 0.1614, batch_loss_s: 0.3065, time:24.6413, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:40:36 \u001b[32mINFO     \u001b[0m train.py: [8/300], [250/484], step: 4122, 4.301 samples/sec, batch_loss: 0.1121, batch_loss_c: 0.1000, batch_loss_s: 0.1406, time:9.2997, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:40:59 \u001b[32mINFO     \u001b[0m train.py: [8/300], [260/484], step: 4132, 1.736 samples/sec, batch_loss: 0.3290, batch_loss_c: 0.3188, batch_loss_s: 0.3528, time:23.0446, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:41:09 \u001b[32mINFO     \u001b[0m train.py: [8/300], [270/484], step: 4142, 4.010 samples/sec, batch_loss: 0.0775, batch_loss_c: 0.0667, batch_loss_s: 0.1027, time:9.9741, lr:1e-05\u001b[0m\n",
            "tcmalloc: large alloc 5313191936 bytes == 0x7f1d0902e000 @  0x7f214dff61e7 0x7f21425daf71 0x7f214263e55d 0x7f2142641e28 0x7f21426423e5 0x7f21426d8fc2 0x50ac25 0x50d390 0x508245 0x50a080 0x50aa7d 0x50d390 0x509455 0x595311 0x5a067e 0x557d28 0x50c88b 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x509455 0x595311 0x5a522c 0x557f7e 0x4b5eff 0x50c810\n",
            "2019-12-07 13:41:39 \u001b[32mINFO     \u001b[0m train.py: [8/300], [280/484], step: 4152, 1.302 samples/sec, batch_loss: 0.2791, batch_loss_c: 0.2738, batch_loss_s: 0.2916, time:30.7119, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:41:49 \u001b[32mINFO     \u001b[0m train.py: [8/300], [290/484], step: 4162, 3.965 samples/sec, batch_loss: 0.0926, batch_loss_c: 0.0939, batch_loss_s: 0.0896, time:10.0890, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:42:12 \u001b[32mINFO     \u001b[0m train.py: [8/300], [300/484], step: 4172, 1.751 samples/sec, batch_loss: 0.0529, batch_loss_c: 0.0399, batch_loss_s: 0.0832, time:22.8455, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:42:27 \u001b[32mINFO     \u001b[0m train.py: [8/300], [310/484], step: 4182, 2.798 samples/sec, batch_loss: 0.0470, batch_loss_c: 0.0371, batch_loss_s: 0.0699, time:14.2972, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:42:36 \u001b[32mINFO     \u001b[0m train.py: [8/300], [320/484], step: 4192, 4.235 samples/sec, batch_loss: 0.0777, batch_loss_c: 0.0715, batch_loss_s: 0.0920, time:9.4441, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:42:50 \u001b[32mINFO     \u001b[0m train.py: [8/300], [330/484], step: 4202, 2.922 samples/sec, batch_loss: 0.0963, batch_loss_c: 0.0948, batch_loss_s: 0.0999, time:13.6900, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:43:01 \u001b[32mINFO     \u001b[0m train.py: [8/300], [340/484], step: 4212, 3.427 samples/sec, batch_loss: 0.0778, batch_loss_c: 0.0670, batch_loss_s: 0.1031, time:11.6718, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:43:34 \u001b[32mINFO     \u001b[0m train.py: [8/300], [350/484], step: 4222, 1.214 samples/sec, batch_loss: 0.3886, batch_loss_c: 0.3728, batch_loss_s: 0.4254, time:32.9528, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:43:44 \u001b[32mINFO     \u001b[0m train.py: [8/300], [360/484], step: 4232, 4.178 samples/sec, batch_loss: 0.1156, batch_loss_c: 0.0915, batch_loss_s: 0.1719, time:9.5732, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:43:55 \u001b[32mINFO     \u001b[0m train.py: [8/300], [370/484], step: 4242, 3.457 samples/sec, batch_loss: 0.1053, batch_loss_c: 0.1025, batch_loss_s: 0.1118, time:11.5694, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:44:05 \u001b[32mINFO     \u001b[0m train.py: [8/300], [380/484], step: 4252, 4.008 samples/sec, batch_loss: 0.0790, batch_loss_c: 0.0726, batch_loss_s: 0.0939, time:9.9795, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:44:46 \u001b[32mINFO     \u001b[0m train.py: [8/300], [390/484], step: 4262, 0.996 samples/sec, batch_loss: 0.0457, batch_loss_c: 0.0345, batch_loss_s: 0.0719, time:40.1629, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:44:54 \u001b[32mINFO     \u001b[0m train.py: [8/300], [400/484], step: 4272, 4.608 samples/sec, batch_loss: 0.0744, batch_loss_c: 0.0666, batch_loss_s: 0.0927, time:8.6811, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:45:10 \u001b[32mINFO     \u001b[0m train.py: [8/300], [410/484], step: 4282, 2.478 samples/sec, batch_loss: 0.3019, batch_loss_c: 0.2955, batch_loss_s: 0.3167, time:16.1402, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:45:36 \u001b[32mINFO     \u001b[0m train.py: [8/300], [420/484], step: 4292, 1.541 samples/sec, batch_loss: 0.0871, batch_loss_c: 0.0891, batch_loss_s: 0.0824, time:25.9536, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:45:47 \u001b[32mINFO     \u001b[0m train.py: [8/300], [430/484], step: 4302, 3.925 samples/sec, batch_loss: 0.1968, batch_loss_c: 0.1858, batch_loss_s: 0.2227, time:10.1922, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:45:56 \u001b[32mINFO     \u001b[0m train.py: [8/300], [440/484], step: 4312, 4.138 samples/sec, batch_loss: 0.1043, batch_loss_c: 0.1142, batch_loss_s: 0.0811, time:9.6655, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:46:20 \u001b[32mINFO     \u001b[0m train.py: [8/300], [450/484], step: 4322, 1.666 samples/sec, batch_loss: 0.2777, batch_loss_c: 0.2733, batch_loss_s: 0.2879, time:24.0108, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:46:39 \u001b[32mINFO     \u001b[0m train.py: [8/300], [460/484], step: 4332, 2.097 samples/sec, batch_loss: 0.0788, batch_loss_c: 0.0696, batch_loss_s: 0.1001, time:19.0727, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:46:49 \u001b[32mINFO     \u001b[0m train.py: [8/300], [470/484], step: 4342, 4.028 samples/sec, batch_loss: 0.0778, batch_loss_c: 0.0728, batch_loss_s: 0.0895, time:9.9304, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:46:57 \u001b[32mINFO     \u001b[0m train.py: [8/300], [480/484], step: 4352, 4.995 samples/sec, batch_loss: 0.0691, batch_loss_c: 0.0595, batch_loss_s: 0.0916, time:8.0075, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:47:02 \u001b[32mINFO     \u001b[0m train.py: [8/300], train_loss: 0.1373, time: 738.9891, lr: 1e-05\u001b[0m\n",
            "2019-12-07 13:47:03 \u001b[32mINFO     \u001b[0m train.py: [9/300], [0/484], step: 4356, 55.789 samples/sec, batch_loss: 0.0771, batch_loss_c: 0.0659, batch_loss_s: 0.1033, time:0.7170, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:47:14 \u001b[32mINFO     \u001b[0m train.py: [9/300], [10/484], step: 4366, 3.477 samples/sec, batch_loss: 0.0541, batch_loss_c: 0.0474, batch_loss_s: 0.0695, time:11.5046, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:47:43 \u001b[32mINFO     \u001b[0m train.py: [9/300], [20/484], step: 4376, 1.419 samples/sec, batch_loss: 0.3351, batch_loss_c: 0.3386, batch_loss_s: 0.3269, time:28.1854, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:47:58 \u001b[32mINFO     \u001b[0m train.py: [9/300], [30/484], step: 4386, 2.520 samples/sec, batch_loss: 0.0467, batch_loss_c: 0.0382, batch_loss_s: 0.0665, time:15.8759, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:48:07 \u001b[32mINFO     \u001b[0m train.py: [9/300], [40/484], step: 4396, 4.939 samples/sec, batch_loss: 0.0609, batch_loss_c: 0.0526, batch_loss_s: 0.0804, time:8.0988, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:48:37 \u001b[32mINFO     \u001b[0m train.py: [9/300], [50/484], step: 4406, 1.321 samples/sec, batch_loss: 0.3437, batch_loss_c: 0.3439, batch_loss_s: 0.3435, time:30.2836, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:48:51 \u001b[32mINFO     \u001b[0m train.py: [9/300], [60/484], step: 4416, 2.884 samples/sec, batch_loss: 0.1623, batch_loss_c: 0.1069, batch_loss_s: 0.2916, time:13.8687, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:49:12 \u001b[32mINFO     \u001b[0m train.py: [9/300], [70/484], step: 4426, 1.893 samples/sec, batch_loss: 0.1174, batch_loss_c: 0.1208, batch_loss_s: 0.1092, time:21.1334, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:49:22 \u001b[32mINFO     \u001b[0m train.py: [9/300], [80/484], step: 4436, 3.926 samples/sec, batch_loss: 0.2990, batch_loss_c: 0.2897, batch_loss_s: 0.3206, time:10.1895, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:49:37 \u001b[32mINFO     \u001b[0m train.py: [9/300], [90/484], step: 4446, 2.745 samples/sec, batch_loss: 0.2084, batch_loss_c: 0.1875, batch_loss_s: 0.2572, time:14.5730, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:49:48 \u001b[32mINFO     \u001b[0m train.py: [9/300], [100/484], step: 4456, 3.519 samples/sec, batch_loss: 0.1250, batch_loss_c: 0.1334, batch_loss_s: 0.1052, time:11.3654, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:50:07 \u001b[32mINFO     \u001b[0m train.py: [9/300], [110/484], step: 4466, 2.055 samples/sec, batch_loss: 0.1095, batch_loss_c: 0.0914, batch_loss_s: 0.1518, time:19.4692, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:50:21 \u001b[32mINFO     \u001b[0m train.py: [9/300], [120/484], step: 4476, 2.863 samples/sec, batch_loss: 0.0624, batch_loss_c: 0.0584, batch_loss_s: 0.0717, time:13.9713, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:50:38 \u001b[32mINFO     \u001b[0m train.py: [9/300], [130/484], step: 4486, 2.429 samples/sec, batch_loss: 0.0567, batch_loss_c: 0.0537, batch_loss_s: 0.0635, time:16.4695, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:51:11 \u001b[32mINFO     \u001b[0m train.py: [9/300], [140/484], step: 4496, 1.214 samples/sec, batch_loss: 0.0610, batch_loss_c: 0.0494, batch_loss_s: 0.0881, time:32.9506, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:51:38 \u001b[32mINFO     \u001b[0m train.py: [9/300], [150/484], step: 4506, 1.452 samples/sec, batch_loss: 0.0582, batch_loss_c: 0.0517, batch_loss_s: 0.0734, time:27.5490, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:51:53 \u001b[32mINFO     \u001b[0m train.py: [9/300], [160/484], step: 4516, 2.682 samples/sec, batch_loss: 0.3745, batch_loss_c: 0.3683, batch_loss_s: 0.3888, time:14.9141, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:52:04 \u001b[32mINFO     \u001b[0m train.py: [9/300], [170/484], step: 4526, 3.887 samples/sec, batch_loss: 0.0900, batch_loss_c: 0.0926, batch_loss_s: 0.0841, time:10.2919, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:52:18 \u001b[32mINFO     \u001b[0m train.py: [9/300], [180/484], step: 4536, 2.769 samples/sec, batch_loss: 0.0413, batch_loss_c: 0.0327, batch_loss_s: 0.0612, time:14.4433, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:52:34 \u001b[32mINFO     \u001b[0m train.py: [9/300], [190/484], step: 4546, 2.450 samples/sec, batch_loss: 0.0677, batch_loss_c: 0.0607, batch_loss_s: 0.0842, time:16.3297, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:52:46 \u001b[32mINFO     \u001b[0m train.py: [9/300], [200/484], step: 4556, 3.303 samples/sec, batch_loss: 0.1158, batch_loss_c: 0.1097, batch_loss_s: 0.1299, time:12.1102, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:53:03 \u001b[32mINFO     \u001b[0m train.py: [9/300], [210/484], step: 4566, 2.469 samples/sec, batch_loss: 0.0790, batch_loss_c: 0.0602, batch_loss_s: 0.1230, time:16.2013, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:53:26 \u001b[32mINFO     \u001b[0m train.py: [9/300], [220/484], step: 4576, 1.715 samples/sec, batch_loss: 0.2843, batch_loss_c: 0.2771, batch_loss_s: 0.3012, time:23.3294, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:53:37 \u001b[32mINFO     \u001b[0m train.py: [9/300], [230/484], step: 4586, 3.656 samples/sec, batch_loss: 0.2949, batch_loss_c: 0.2881, batch_loss_s: 0.3107, time:10.9413, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:53:45 \u001b[32mINFO     \u001b[0m train.py: [9/300], [240/484], step: 4596, 4.779 samples/sec, batch_loss: 0.0789, batch_loss_c: 0.0697, batch_loss_s: 0.1002, time:8.3706, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:53:53 \u001b[32mINFO     \u001b[0m train.py: [9/300], [250/484], step: 4606, 4.990 samples/sec, batch_loss: 0.1628, batch_loss_c: 0.1722, batch_loss_s: 0.1409, time:8.0162, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:54:12 \u001b[32mINFO     \u001b[0m train.py: [9/300], [260/484], step: 4616, 2.088 samples/sec, batch_loss: 0.2364, batch_loss_c: 0.2195, batch_loss_s: 0.2759, time:19.1614, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:54:22 \u001b[32mINFO     \u001b[0m train.py: [9/300], [270/484], step: 4626, 4.062 samples/sec, batch_loss: 0.4700, batch_loss_c: 0.5002, batch_loss_s: 0.3994, time:9.8471, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:54:41 \u001b[32mINFO     \u001b[0m train.py: [9/300], [280/484], step: 4636, 2.131 samples/sec, batch_loss: 0.0495, batch_loss_c: 0.0437, batch_loss_s: 0.0630, time:18.7678, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:54:56 \u001b[32mINFO     \u001b[0m train.py: [9/300], [290/484], step: 4646, 2.629 samples/sec, batch_loss: 0.2309, batch_loss_c: 0.2190, batch_loss_s: 0.2587, time:15.2148, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:55:19 \u001b[32mINFO     \u001b[0m train.py: [9/300], [300/484], step: 4656, 1.793 samples/sec, batch_loss: 0.0946, batch_loss_c: 0.0902, batch_loss_s: 0.1049, time:22.3086, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:55:28 \u001b[32mINFO     \u001b[0m train.py: [9/300], [310/484], step: 4666, 4.427 samples/sec, batch_loss: 0.1308, batch_loss_c: 0.1258, batch_loss_s: 0.1423, time:9.0365, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:55:42 \u001b[32mINFO     \u001b[0m train.py: [9/300], [320/484], step: 4676, 2.884 samples/sec, batch_loss: 0.0694, batch_loss_c: 0.0680, batch_loss_s: 0.0726, time:13.8713, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:55:54 \u001b[32mINFO     \u001b[0m train.py: [9/300], [330/484], step: 4686, 3.272 samples/sec, batch_loss: 0.0419, batch_loss_c: 0.0360, batch_loss_s: 0.0555, time:12.2235, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:56:11 \u001b[32mINFO     \u001b[0m train.py: [9/300], [340/484], step: 4696, 2.289 samples/sec, batch_loss: 0.0635, batch_loss_c: 0.0566, batch_loss_s: 0.0794, time:17.4775, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:56:25 \u001b[32mINFO     \u001b[0m train.py: [9/300], [350/484], step: 4706, 2.883 samples/sec, batch_loss: 0.0574, batch_loss_c: 0.0532, batch_loss_s: 0.0675, time:13.8756, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:56:43 \u001b[32mINFO     \u001b[0m train.py: [9/300], [360/484], step: 4716, 2.261 samples/sec, batch_loss: 0.0821, batch_loss_c: 0.0776, batch_loss_s: 0.0926, time:17.6904, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:56:55 \u001b[32mINFO     \u001b[0m train.py: [9/300], [370/484], step: 4726, 3.176 samples/sec, batch_loss: 0.0693, batch_loss_c: 0.0597, batch_loss_s: 0.0916, time:12.5945, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:57:14 \u001b[32mINFO     \u001b[0m train.py: [9/300], [380/484], step: 4736, 2.104 samples/sec, batch_loss: 0.3941, batch_loss_c: 0.4103, batch_loss_s: 0.3563, time:19.0130, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:57:26 \u001b[32mINFO     \u001b[0m train.py: [9/300], [390/484], step: 4746, 3.583 samples/sec, batch_loss: 0.1375, batch_loss_c: 0.1576, batch_loss_s: 0.0907, time:11.1644, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:57:34 \u001b[32mINFO     \u001b[0m train.py: [9/300], [400/484], step: 4756, 4.887 samples/sec, batch_loss: 0.0642, batch_loss_c: 0.0570, batch_loss_s: 0.0810, time:8.1857, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:57:54 \u001b[32mINFO     \u001b[0m train.py: [9/300], [410/484], step: 4766, 1.952 samples/sec, batch_loss: 0.0898, batch_loss_c: 0.0956, batch_loss_s: 0.0764, time:20.4914, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:58:08 \u001b[32mINFO     \u001b[0m train.py: [9/300], [420/484], step: 4776, 2.999 samples/sec, batch_loss: 0.2897, batch_loss_c: 0.3206, batch_loss_s: 0.2174, time:13.3398, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:58:19 \u001b[32mINFO     \u001b[0m train.py: [9/300], [430/484], step: 4786, 3.555 samples/sec, batch_loss: 0.0530, batch_loss_c: 0.0454, batch_loss_s: 0.0705, time:11.2509, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:58:29 \u001b[32mINFO     \u001b[0m train.py: [9/300], [440/484], step: 4796, 3.963 samples/sec, batch_loss: 0.0664, batch_loss_c: 0.0580, batch_loss_s: 0.0859, time:10.0925, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:58:50 \u001b[32mINFO     \u001b[0m train.py: [9/300], [450/484], step: 4806, 1.933 samples/sec, batch_loss: 0.0748, batch_loss_c: 0.0696, batch_loss_s: 0.0868, time:20.6947, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:59:01 \u001b[32mINFO     \u001b[0m train.py: [9/300], [460/484], step: 4816, 3.542 samples/sec, batch_loss: 0.0634, batch_loss_c: 0.0555, batch_loss_s: 0.0816, time:11.2915, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:59:38 \u001b[32mINFO     \u001b[0m train.py: [9/300], [470/484], step: 4826, 1.091 samples/sec, batch_loss: 0.0678, batch_loss_c: 0.0499, batch_loss_s: 0.1095, time:36.6623, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:59:48 \u001b[32mINFO     \u001b[0m train.py: [9/300], [480/484], step: 4836, 3.945 samples/sec, batch_loss: 0.0574, batch_loss_c: 0.0497, batch_loss_s: 0.0755, time:10.1390, lr:1e-05\u001b[0m\n",
            "2019-12-07 13:59:51 \u001b[32mINFO     \u001b[0m train.py: [9/300], train_loss: 0.1408, time: 768.3886, lr: 1e-05\u001b[0m\n",
            "2019-12-07 13:59:52 \u001b[32mINFO     \u001b[0m train.py: [10/300], [0/484], step: 4840, 35.773 samples/sec, batch_loss: 0.0562, batch_loss_c: 0.0439, batch_loss_s: 0.0848, time:1.1182, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:00:03 \u001b[32mINFO     \u001b[0m train.py: [10/300], [10/484], step: 4850, 3.559 samples/sec, batch_loss: 0.5122, batch_loss_c: 0.4904, batch_loss_s: 0.5630, time:11.2405, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:00:26 \u001b[32mINFO     \u001b[0m train.py: [10/300], [20/484], step: 4860, 1.729 samples/sec, batch_loss: 0.0357, batch_loss_c: 0.0288, batch_loss_s: 0.0517, time:23.1312, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:00:50 \u001b[32mINFO     \u001b[0m train.py: [10/300], [30/484], step: 4870, 1.665 samples/sec, batch_loss: 0.2519, batch_loss_c: 0.2326, batch_loss_s: 0.2969, time:24.0274, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:01:06 \u001b[32mINFO     \u001b[0m train.py: [10/300], [40/484], step: 4880, 2.623 samples/sec, batch_loss: 0.0934, batch_loss_c: 0.0918, batch_loss_s: 0.0971, time:15.2485, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:01:16 \u001b[32mINFO     \u001b[0m train.py: [10/300], [50/484], step: 4890, 3.827 samples/sec, batch_loss: 0.1790, batch_loss_c: 0.1236, batch_loss_s: 0.3083, time:10.4523, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:01:30 \u001b[32mINFO     \u001b[0m train.py: [10/300], [60/484], step: 4900, 2.962 samples/sec, batch_loss: 0.3807, batch_loss_c: 0.4021, batch_loss_s: 0.3308, time:13.5062, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:01:56 \u001b[32mINFO     \u001b[0m train.py: [10/300], [70/484], step: 4910, 1.516 samples/sec, batch_loss: 0.2897, batch_loss_c: 0.2841, batch_loss_s: 0.3028, time:26.3919, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:02:04 \u001b[32mINFO     \u001b[0m train.py: [10/300], [80/484], step: 4920, 4.953 samples/sec, batch_loss: 0.1326, batch_loss_c: 0.1446, batch_loss_s: 0.1047, time:8.0764, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:02:24 \u001b[32mINFO     \u001b[0m train.py: [10/300], [90/484], step: 4930, 1.970 samples/sec, batch_loss: 0.0821, batch_loss_c: 0.0735, batch_loss_s: 0.1022, time:20.3062, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:02:37 \u001b[32mINFO     \u001b[0m train.py: [10/300], [100/484], step: 4940, 3.291 samples/sec, batch_loss: 0.0630, batch_loss_c: 0.0599, batch_loss_s: 0.0703, time:12.1543, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:02:47 \u001b[32mINFO     \u001b[0m train.py: [10/300], [110/484], step: 4950, 3.724 samples/sec, batch_loss: 0.1680, batch_loss_c: 0.1685, batch_loss_s: 0.1668, time:10.7420, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:03:00 \u001b[32mINFO     \u001b[0m train.py: [10/300], [120/484], step: 4960, 3.048 samples/sec, batch_loss: 0.0945, batch_loss_c: 0.0907, batch_loss_s: 0.1032, time:13.1215, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:03:21 \u001b[32mINFO     \u001b[0m train.py: [10/300], [130/484], step: 4970, 1.932 samples/sec, batch_loss: 0.4287, batch_loss_c: 0.4814, batch_loss_s: 0.3057, time:20.7018, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:03:37 \u001b[32mINFO     \u001b[0m train.py: [10/300], [140/484], step: 4980, 2.564 samples/sec, batch_loss: 0.0865, batch_loss_c: 0.0770, batch_loss_s: 0.1086, time:15.6024, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:03:50 \u001b[32mINFO     \u001b[0m train.py: [10/300], [150/484], step: 4990, 3.096 samples/sec, batch_loss: 0.0861, batch_loss_c: 0.0820, batch_loss_s: 0.0958, time:12.9184, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:04:01 \u001b[32mINFO     \u001b[0m train.py: [10/300], [160/484], step: 5000, 3.514 samples/sec, batch_loss: 0.1543, batch_loss_c: 0.1768, batch_loss_s: 0.1019, time:11.3835, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:04:09 \u001b[32mINFO     \u001b[0m train.py: [10/300], [170/484], step: 5010, 4.963 samples/sec, batch_loss: 0.1145, batch_loss_c: 0.1054, batch_loss_s: 0.1357, time:8.0589, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:04:20 \u001b[32mINFO     \u001b[0m train.py: [10/300], [180/484], step: 5020, 3.788 samples/sec, batch_loss: 0.0567, batch_loss_c: 0.0435, batch_loss_s: 0.0875, time:10.5592, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:04:30 \u001b[32mINFO     \u001b[0m train.py: [10/300], [190/484], step: 5030, 3.745 samples/sec, batch_loss: 0.2986, batch_loss_c: 0.2875, batch_loss_s: 0.3244, time:10.6795, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:04:40 \u001b[32mINFO     \u001b[0m train.py: [10/300], [200/484], step: 5040, 3.959 samples/sec, batch_loss: 0.0588, batch_loss_c: 0.0601, batch_loss_s: 0.0558, time:10.1030, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:04:56 \u001b[32mINFO     \u001b[0m train.py: [10/300], [210/484], step: 5050, 2.593 samples/sec, batch_loss: 0.2540, batch_loss_c: 0.2229, batch_loss_s: 0.3264, time:15.4274, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:05:07 \u001b[32mINFO     \u001b[0m train.py: [10/300], [220/484], step: 5060, 3.694 samples/sec, batch_loss: 0.1415, batch_loss_c: 0.1326, batch_loss_s: 0.1623, time:10.8271, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:05:19 \u001b[32mINFO     \u001b[0m train.py: [10/300], [230/484], step: 5070, 3.161 samples/sec, batch_loss: 0.1016, batch_loss_c: 0.0647, batch_loss_s: 0.1875, time:12.6553, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:05:45 \u001b[32mINFO     \u001b[0m train.py: [10/300], [240/484], step: 5080, 1.541 samples/sec, batch_loss: 0.3061, batch_loss_c: 0.3014, batch_loss_s: 0.3170, time:25.9631, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:06:02 \u001b[32mINFO     \u001b[0m train.py: [10/300], [250/484], step: 5090, 2.451 samples/sec, batch_loss: 0.2108, batch_loss_c: 0.2166, batch_loss_s: 0.1974, time:16.3190, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:06:26 \u001b[32mINFO     \u001b[0m train.py: [10/300], [260/484], step: 5100, 1.619 samples/sec, batch_loss: 0.0516, batch_loss_c: 0.0471, batch_loss_s: 0.0619, time:24.7116, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:06:37 \u001b[32mINFO     \u001b[0m train.py: [10/300], [270/484], step: 5110, 3.725 samples/sec, batch_loss: 0.0823, batch_loss_c: 0.0695, batch_loss_s: 0.1123, time:10.7390, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:06:47 \u001b[32mINFO     \u001b[0m train.py: [10/300], [280/484], step: 5120, 4.228 samples/sec, batch_loss: 0.1202, batch_loss_c: 0.1203, batch_loss_s: 0.1200, time:9.4606, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:06:58 \u001b[32mINFO     \u001b[0m train.py: [10/300], [290/484], step: 5130, 3.406 samples/sec, batch_loss: 0.1092, batch_loss_c: 0.1023, batch_loss_s: 0.1254, time:11.7453, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:07:11 \u001b[32mINFO     \u001b[0m train.py: [10/300], [300/484], step: 5140, 3.099 samples/sec, batch_loss: 0.0874, batch_loss_c: 0.0811, batch_loss_s: 0.1023, time:12.9076, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:07:33 \u001b[32mINFO     \u001b[0m train.py: [10/300], [310/484], step: 5150, 1.797 samples/sec, batch_loss: 0.2062, batch_loss_c: 0.1934, batch_loss_s: 0.2361, time:22.2550, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:07:48 \u001b[32mINFO     \u001b[0m train.py: [10/300], [320/484], step: 5160, 2.847 samples/sec, batch_loss: 0.1618, batch_loss_c: 0.1733, batch_loss_s: 0.1350, time:14.0522, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:07:57 \u001b[32mINFO     \u001b[0m train.py: [10/300], [330/484], step: 5170, 4.179 samples/sec, batch_loss: 0.0737, batch_loss_c: 0.0627, batch_loss_s: 0.0992, time:9.5714, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:08:09 \u001b[32mINFO     \u001b[0m train.py: [10/300], [340/484], step: 5180, 3.495 samples/sec, batch_loss: 0.1274, batch_loss_c: 0.1426, batch_loss_s: 0.0918, time:11.4450, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:08:39 \u001b[32mINFO     \u001b[0m train.py: [10/300], [350/484], step: 5190, 1.307 samples/sec, batch_loss: 0.5209, batch_loss_c: 0.5202, batch_loss_s: 0.5224, time:30.6024, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:08:48 \u001b[32mINFO     \u001b[0m train.py: [10/300], [360/484], step: 5200, 4.560 samples/sec, batch_loss: 0.0751, batch_loss_c: 0.0685, batch_loss_s: 0.0907, time:8.7715, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:09:16 \u001b[32mINFO     \u001b[0m train.py: [10/300], [370/484], step: 5210, 1.445 samples/sec, batch_loss: 0.0582, batch_loss_c: 0.0534, batch_loss_s: 0.0695, time:27.6910, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:09:28 \u001b[32mINFO     \u001b[0m train.py: [10/300], [380/484], step: 5220, 3.329 samples/sec, batch_loss: 0.0828, batch_loss_c: 0.0840, batch_loss_s: 0.0798, time:12.0154, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:09:59 \u001b[32mINFO     \u001b[0m train.py: [10/300], [390/484], step: 5230, 1.295 samples/sec, batch_loss: 0.3217, batch_loss_c: 0.3140, batch_loss_s: 0.3399, time:30.8984, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:10:08 \u001b[32mINFO     \u001b[0m train.py: [10/300], [400/484], step: 5240, 4.247 samples/sec, batch_loss: 0.4642, batch_loss_c: 0.4365, batch_loss_s: 0.5288, time:9.4189, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:10:19 \u001b[32mINFO     \u001b[0m train.py: [10/300], [410/484], step: 5250, 3.506 samples/sec, batch_loss: 0.3072, batch_loss_c: 0.3008, batch_loss_s: 0.3221, time:11.4090, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:10:37 \u001b[32mINFO     \u001b[0m train.py: [10/300], [420/484], step: 5260, 2.241 samples/sec, batch_loss: 0.2812, batch_loss_c: 0.2734, batch_loss_s: 0.2996, time:17.8454, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:10:48 \u001b[32mINFO     \u001b[0m train.py: [10/300], [430/484], step: 5270, 3.657 samples/sec, batch_loss: 0.0657, batch_loss_c: 0.0590, batch_loss_s: 0.0815, time:10.9372, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:11:00 \u001b[32mINFO     \u001b[0m train.py: [10/300], [440/484], step: 5280, 3.429 samples/sec, batch_loss: 0.1084, batch_loss_c: 0.0994, batch_loss_s: 0.1293, time:11.6668, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:11:20 \u001b[32mINFO     \u001b[0m train.py: [10/300], [450/484], step: 5290, 2.022 samples/sec, batch_loss: 0.0697, batch_loss_c: 0.0583, batch_loss_s: 0.0963, time:19.7793, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:11:30 \u001b[32mINFO     \u001b[0m train.py: [10/300], [460/484], step: 5300, 3.983 samples/sec, batch_loss: 0.0509, batch_loss_c: 0.0400, batch_loss_s: 0.0763, time:10.0416, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:11:49 \u001b[32mINFO     \u001b[0m train.py: [10/300], [470/484], step: 5310, 2.019 samples/sec, batch_loss: 0.0602, batch_loss_c: 0.0577, batch_loss_s: 0.0659, time:19.8073, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:12:01 \u001b[32mINFO     \u001b[0m train.py: [10/300], [480/484], step: 5320, 3.388 samples/sec, batch_loss: 0.3232, batch_loss_c: 0.2922, batch_loss_s: 0.3954, time:11.8051, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:12:16 \u001b[32mINFO     \u001b[0m train.py: [10/300], train_loss: 0.1434, time: 745.4547, lr: 1e-05\u001b[0m\n",
            "2019-12-07 14:12:21 \u001b[32mINFO     \u001b[0m train.py: [11/300], [0/484], step: 5324, 10.843 samples/sec, batch_loss: 0.2943, batch_loss_c: 0.2879, batch_loss_s: 0.3092, time:3.6892, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:12:35 \u001b[32mINFO     \u001b[0m train.py: [11/300], [10/484], step: 5334, 2.750 samples/sec, batch_loss: 0.2008, batch_loss_c: 0.1800, batch_loss_s: 0.2494, time:14.5429, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:12:43 \u001b[32mINFO     \u001b[0m train.py: [11/300], [20/484], step: 5344, 5.158 samples/sec, batch_loss: 0.2921, batch_loss_c: 0.2903, batch_loss_s: 0.2965, time:7.7545, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:12:55 \u001b[32mINFO     \u001b[0m train.py: [11/300], [30/484], step: 5354, 3.318 samples/sec, batch_loss: 0.0667, batch_loss_c: 0.0625, batch_loss_s: 0.0764, time:12.0539, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:13:10 \u001b[32mINFO     \u001b[0m train.py: [11/300], [40/484], step: 5364, 2.781 samples/sec, batch_loss: 0.1081, batch_loss_c: 0.1095, batch_loss_s: 0.1049, time:14.3845, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:13:18 \u001b[32mINFO     \u001b[0m train.py: [11/300], [50/484], step: 5374, 4.585 samples/sec, batch_loss: 0.1477, batch_loss_c: 0.1498, batch_loss_s: 0.1427, time:8.7246, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:13:30 \u001b[32mINFO     \u001b[0m train.py: [11/300], [60/484], step: 5384, 3.549 samples/sec, batch_loss: 0.0669, batch_loss_c: 0.0575, batch_loss_s: 0.0889, time:11.2715, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:13:45 \u001b[32mINFO     \u001b[0m train.py: [11/300], [70/484], step: 5394, 2.575 samples/sec, batch_loss: 0.1080, batch_loss_c: 0.0923, batch_loss_s: 0.1446, time:15.5327, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:14:00 \u001b[32mINFO     \u001b[0m train.py: [11/300], [80/484], step: 5404, 2.763 samples/sec, batch_loss: 0.0585, batch_loss_c: 0.0459, batch_loss_s: 0.0878, time:14.4763, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:14:21 \u001b[32mINFO     \u001b[0m train.py: [11/300], [90/484], step: 5414, 1.825 samples/sec, batch_loss: 0.2829, batch_loss_c: 0.2796, batch_loss_s: 0.2904, time:21.9199, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:14:36 \u001b[32mINFO     \u001b[0m train.py: [11/300], [100/484], step: 5424, 2.725 samples/sec, batch_loss: 0.0753, batch_loss_c: 0.0599, batch_loss_s: 0.1113, time:14.6788, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:14:47 \u001b[32mINFO     \u001b[0m train.py: [11/300], [110/484], step: 5434, 3.549 samples/sec, batch_loss: 0.3165, batch_loss_c: 0.3121, batch_loss_s: 0.3268, time:11.2692, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:15:00 \u001b[32mINFO     \u001b[0m train.py: [11/300], [120/484], step: 5444, 3.110 samples/sec, batch_loss: 0.0612, batch_loss_c: 0.0464, batch_loss_s: 0.0958, time:12.8621, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:15:15 \u001b[32mINFO     \u001b[0m train.py: [11/300], [130/484], step: 5454, 2.641 samples/sec, batch_loss: 0.1196, batch_loss_c: 0.1269, batch_loss_s: 0.1026, time:15.1465, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:15:38 \u001b[32mINFO     \u001b[0m train.py: [11/300], [140/484], step: 5464, 1.747 samples/sec, batch_loss: 0.2112, batch_loss_c: 0.2139, batch_loss_s: 0.2050, time:22.8936, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:15:58 \u001b[32mINFO     \u001b[0m train.py: [11/300], [150/484], step: 5474, 1.984 samples/sec, batch_loss: 0.1152, batch_loss_c: 0.1079, batch_loss_s: 0.1322, time:20.1568, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:16:10 \u001b[32mINFO     \u001b[0m train.py: [11/300], [160/484], step: 5484, 3.422 samples/sec, batch_loss: 0.3002, batch_loss_c: 0.2950, batch_loss_s: 0.3122, time:11.6876, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:16:25 \u001b[32mINFO     \u001b[0m train.py: [11/300], [170/484], step: 5494, 2.756 samples/sec, batch_loss: 0.3326, batch_loss_c: 0.3139, batch_loss_s: 0.3762, time:14.5130, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:16:41 \u001b[32mINFO     \u001b[0m train.py: [11/300], [180/484], step: 5504, 2.388 samples/sec, batch_loss: 0.2300, batch_loss_c: 0.2089, batch_loss_s: 0.2792, time:16.7533, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:17:09 \u001b[32mINFO     \u001b[0m train.py: [11/300], [190/484], step: 5514, 1.454 samples/sec, batch_loss: 0.2211, batch_loss_c: 0.1870, batch_loss_s: 0.3005, time:27.5133, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:17:20 \u001b[32mINFO     \u001b[0m train.py: [11/300], [200/484], step: 5524, 3.534 samples/sec, batch_loss: 0.0809, batch_loss_c: 0.0739, batch_loss_s: 0.0972, time:11.3187, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:17:35 \u001b[32mINFO     \u001b[0m train.py: [11/300], [210/484], step: 5534, 2.681 samples/sec, batch_loss: 0.0725, batch_loss_c: 0.0619, batch_loss_s: 0.0973, time:14.9202, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:17:50 \u001b[32mINFO     \u001b[0m train.py: [11/300], [220/484], step: 5544, 2.712 samples/sec, batch_loss: 0.2795, batch_loss_c: 0.2753, batch_loss_s: 0.2894, time:14.7502, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:18:00 \u001b[32mINFO     \u001b[0m train.py: [11/300], [230/484], step: 5554, 3.796 samples/sec, batch_loss: 0.0744, batch_loss_c: 0.0685, batch_loss_s: 0.0881, time:10.5368, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:18:24 \u001b[32mINFO     \u001b[0m train.py: [11/300], [240/484], step: 5564, 1.681 samples/sec, batch_loss: 0.0566, batch_loss_c: 0.0431, batch_loss_s: 0.0881, time:23.7969, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:18:34 \u001b[32mINFO     \u001b[0m train.py: [11/300], [250/484], step: 5574, 4.189 samples/sec, batch_loss: 0.0466, batch_loss_c: 0.0379, batch_loss_s: 0.0670, time:9.5489, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:18:48 \u001b[32mINFO     \u001b[0m train.py: [11/300], [260/484], step: 5584, 2.873 samples/sec, batch_loss: 0.0702, batch_loss_c: 0.0653, batch_loss_s: 0.0816, time:13.9242, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:19:01 \u001b[32mINFO     \u001b[0m train.py: [11/300], [270/484], step: 5594, 3.087 samples/sec, batch_loss: 0.1337, batch_loss_c: 0.1389, batch_loss_s: 0.1215, time:12.9582, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:19:19 \u001b[32mINFO     \u001b[0m train.py: [11/300], [280/484], step: 5604, 2.225 samples/sec, batch_loss: 0.0775, batch_loss_c: 0.0698, batch_loss_s: 0.0955, time:17.9790, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:19:35 \u001b[32mINFO     \u001b[0m train.py: [11/300], [290/484], step: 5614, 2.443 samples/sec, batch_loss: 0.1105, batch_loss_c: 0.1029, batch_loss_s: 0.1281, time:16.3712, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:19:48 \u001b[32mINFO     \u001b[0m train.py: [11/300], [300/484], step: 5624, 3.119 samples/sec, batch_loss: 0.0540, batch_loss_c: 0.0479, batch_loss_s: 0.0681, time:12.8264, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:20:02 \u001b[32mINFO     \u001b[0m train.py: [11/300], [310/484], step: 5634, 2.831 samples/sec, batch_loss: 0.1582, batch_loss_c: 0.1549, batch_loss_s: 0.1661, time:14.1290, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:20:16 \u001b[32mINFO     \u001b[0m train.py: [11/300], [320/484], step: 5644, 2.936 samples/sec, batch_loss: 0.0673, batch_loss_c: 0.0544, batch_loss_s: 0.0973, time:13.6262, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:20:28 \u001b[32mINFO     \u001b[0m train.py: [11/300], [330/484], step: 5654, 3.210 samples/sec, batch_loss: 0.0832, batch_loss_c: 0.0818, batch_loss_s: 0.0866, time:12.4600, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:20:44 \u001b[32mINFO     \u001b[0m train.py: [11/300], [340/484], step: 5664, 2.466 samples/sec, batch_loss: 0.0538, batch_loss_c: 0.0433, batch_loss_s: 0.0783, time:16.2208, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:20:55 \u001b[32mINFO     \u001b[0m train.py: [11/300], [350/484], step: 5674, 3.796 samples/sec, batch_loss: 0.0676, batch_loss_c: 0.0616, batch_loss_s: 0.0816, time:10.5383, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:21:28 \u001b[32mINFO     \u001b[0m train.py: [11/300], [360/484], step: 5684, 1.222 samples/sec, batch_loss: 0.3513, batch_loss_c: 0.3506, batch_loss_s: 0.3529, time:32.7343, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:21:43 \u001b[32mINFO     \u001b[0m train.py: [11/300], [370/484], step: 5694, 2.631 samples/sec, batch_loss: 0.3936, batch_loss_c: 0.3888, batch_loss_s: 0.4050, time:15.2030, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:22:02 \u001b[32mINFO     \u001b[0m train.py: [11/300], [380/484], step: 5704, 2.087 samples/sec, batch_loss: 0.6540, batch_loss_c: 0.6483, batch_loss_s: 0.6672, time:19.1640, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:22:19 \u001b[32mINFO     \u001b[0m train.py: [11/300], [390/484], step: 5714, 2.416 samples/sec, batch_loss: 0.1005, batch_loss_c: 0.1075, batch_loss_s: 0.0839, time:16.5584, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:22:29 \u001b[32mINFO     \u001b[0m train.py: [11/300], [400/484], step: 5724, 3.773 samples/sec, batch_loss: 0.0534, batch_loss_c: 0.0396, batch_loss_s: 0.0856, time:10.6010, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:22:43 \u001b[32mINFO     \u001b[0m train.py: [11/300], [410/484], step: 5734, 2.922 samples/sec, batch_loss: 0.1689, batch_loss_c: 0.1585, batch_loss_s: 0.1931, time:13.6898, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:23:17 \u001b[32mINFO     \u001b[0m train.py: [11/300], [420/484], step: 5744, 1.158 samples/sec, batch_loss: 0.0466, batch_loss_c: 0.0380, batch_loss_s: 0.0666, time:34.5413, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:23:28 \u001b[32mINFO     \u001b[0m train.py: [11/300], [430/484], step: 5754, 3.895 samples/sec, batch_loss: 0.0710, batch_loss_c: 0.0644, batch_loss_s: 0.0866, time:10.2694, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:23:38 \u001b[32mINFO     \u001b[0m train.py: [11/300], [440/484], step: 5764, 3.724 samples/sec, batch_loss: 0.4488, batch_loss_c: 0.4259, batch_loss_s: 0.5022, time:10.7398, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:23:51 \u001b[32mINFO     \u001b[0m train.py: [11/300], [450/484], step: 5774, 3.115 samples/sec, batch_loss: 0.0923, batch_loss_c: 0.0844, batch_loss_s: 0.1107, time:12.8407, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:24:31 \u001b[32mINFO     \u001b[0m train.py: [11/300], [460/484], step: 5784, 0.996 samples/sec, batch_loss: 0.1371, batch_loss_c: 0.1275, batch_loss_s: 0.1594, time:40.1619, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:24:42 \u001b[32mINFO     \u001b[0m train.py: [11/300], [470/484], step: 5794, 3.601 samples/sec, batch_loss: 0.0584, batch_loss_c: 0.0513, batch_loss_s: 0.0751, time:11.1077, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:24:54 \u001b[32mINFO     \u001b[0m train.py: [11/300], [480/484], step: 5804, 3.428 samples/sec, batch_loss: 0.3147, batch_loss_c: 0.3119, batch_loss_s: 0.3212, time:11.6672, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:24:57 \u001b[32mINFO     \u001b[0m train.py: [11/300], train_loss: 0.1541, time: 759.8635, lr: 1e-05\u001b[0m\n",
            "2019-12-07 14:25:01 \u001b[32mINFO     \u001b[0m train.py: [12/300], [0/484], step: 5808, 10.392 samples/sec, batch_loss: 0.0624, batch_loss_c: 0.0549, batch_loss_s: 0.0799, time:3.8492, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:25:15 \u001b[32mINFO     \u001b[0m train.py: [12/300], [10/484], step: 5818, 2.847 samples/sec, batch_loss: 0.1866, batch_loss_c: 0.1470, batch_loss_s: 0.2791, time:14.0475, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:25:25 \u001b[32mINFO     \u001b[0m train.py: [12/300], [20/484], step: 5828, 4.027 samples/sec, batch_loss: 0.5362, batch_loss_c: 0.5309, batch_loss_s: 0.5486, time:9.9323, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:25:34 \u001b[32mINFO     \u001b[0m train.py: [12/300], [30/484], step: 5838, 4.590 samples/sec, batch_loss: 0.0830, batch_loss_c: 0.0779, batch_loss_s: 0.0951, time:8.7154, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:25:55 \u001b[32mINFO     \u001b[0m train.py: [12/300], [40/484], step: 5848, 1.887 samples/sec, batch_loss: 0.0873, batch_loss_c: 0.0824, batch_loss_s: 0.0987, time:21.1948, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:26:08 \u001b[32mINFO     \u001b[0m train.py: [12/300], [50/484], step: 5858, 3.021 samples/sec, batch_loss: 0.4978, batch_loss_c: 0.4876, batch_loss_s: 0.5214, time:13.2391, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:26:33 \u001b[32mINFO     \u001b[0m train.py: [12/300], [60/484], step: 5868, 1.643 samples/sec, batch_loss: 0.0526, batch_loss_c: 0.0443, batch_loss_s: 0.0720, time:24.3401, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:26:46 \u001b[32mINFO     \u001b[0m train.py: [12/300], [70/484], step: 5878, 2.944 samples/sec, batch_loss: 0.0496, batch_loss_c: 0.0409, batch_loss_s: 0.0698, time:13.5861, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:26:57 \u001b[32mINFO     \u001b[0m train.py: [12/300], [80/484], step: 5888, 3.844 samples/sec, batch_loss: 0.2625, batch_loss_c: 0.2528, batch_loss_s: 0.2851, time:10.4058, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:27:07 \u001b[32mINFO     \u001b[0m train.py: [12/300], [90/484], step: 5898, 3.857 samples/sec, batch_loss: 0.0687, batch_loss_c: 0.0548, batch_loss_s: 0.1012, time:10.3716, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:27:21 \u001b[32mINFO     \u001b[0m train.py: [12/300], [100/484], step: 5908, 2.918 samples/sec, batch_loss: 0.0647, batch_loss_c: 0.0580, batch_loss_s: 0.0803, time:13.7068, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:27:28 \u001b[32mINFO     \u001b[0m train.py: [12/300], [110/484], step: 5918, 5.276 samples/sec, batch_loss: 0.0753, batch_loss_c: 0.0657, batch_loss_s: 0.0976, time:7.5811, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:27:40 \u001b[32mINFO     \u001b[0m train.py: [12/300], [120/484], step: 5928, 3.411 samples/sec, batch_loss: 0.0639, batch_loss_c: 0.0645, batch_loss_s: 0.0624, time:11.7259, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:27:53 \u001b[32mINFO     \u001b[0m train.py: [12/300], [130/484], step: 5938, 3.084 samples/sec, batch_loss: 0.3057, batch_loss_c: 0.2786, batch_loss_s: 0.3689, time:12.9704, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:28:01 \u001b[32mINFO     \u001b[0m train.py: [12/300], [140/484], step: 5948, 4.961 samples/sec, batch_loss: 0.1063, batch_loss_c: 0.1155, batch_loss_s: 0.0850, time:8.0628, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:28:11 \u001b[32mINFO     \u001b[0m train.py: [12/300], [150/484], step: 5958, 4.194 samples/sec, batch_loss: 0.0587, batch_loss_c: 0.0523, batch_loss_s: 0.0737, time:9.5366, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:28:32 \u001b[32mINFO     \u001b[0m train.py: [12/300], [160/484], step: 5968, 1.838 samples/sec, batch_loss: 0.0689, batch_loss_c: 0.0580, batch_loss_s: 0.0944, time:21.7671, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:28:50 \u001b[32mINFO     \u001b[0m train.py: [12/300], [170/484], step: 5978, 2.256 samples/sec, batch_loss: 0.2823, batch_loss_c: 0.2799, batch_loss_s: 0.2879, time:17.7287, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:29:09 \u001b[32mINFO     \u001b[0m train.py: [12/300], [180/484], step: 5988, 2.081 samples/sec, batch_loss: 0.0583, batch_loss_c: 0.0565, batch_loss_s: 0.0624, time:19.2177, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:29:26 \u001b[32mINFO     \u001b[0m train.py: [12/300], [190/484], step: 5998, 2.446 samples/sec, batch_loss: 0.0408, batch_loss_c: 0.0338, batch_loss_s: 0.0571, time:16.3564, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:29:36 \u001b[32mINFO     \u001b[0m train.py: [12/300], [200/484], step: 6008, 3.832 samples/sec, batch_loss: 0.1269, batch_loss_c: 0.1398, batch_loss_s: 0.0969, time:10.4388, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:29:46 \u001b[32mINFO     \u001b[0m train.py: [12/300], [210/484], step: 6018, 4.061 samples/sec, batch_loss: 0.1066, batch_loss_c: 0.1169, batch_loss_s: 0.0826, time:9.8500, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:30:03 \u001b[32mINFO     \u001b[0m train.py: [12/300], [220/484], step: 6028, 2.406 samples/sec, batch_loss: 0.0521, batch_loss_c: 0.0463, batch_loss_s: 0.0657, time:16.6258, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:30:22 \u001b[32mINFO     \u001b[0m train.py: [12/300], [230/484], step: 6038, 2.089 samples/sec, batch_loss: 0.2104, batch_loss_c: 0.1887, batch_loss_s: 0.2612, time:19.1485, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:30:46 \u001b[32mINFO     \u001b[0m train.py: [12/300], [240/484], step: 6048, 1.659 samples/sec, batch_loss: 0.0853, batch_loss_c: 0.0644, batch_loss_s: 0.1339, time:24.1069, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:30:59 \u001b[32mINFO     \u001b[0m train.py: [12/300], [250/484], step: 6058, 3.113 samples/sec, batch_loss: 0.0835, batch_loss_c: 0.0735, batch_loss_s: 0.1068, time:12.8498, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:31:09 \u001b[32mINFO     \u001b[0m train.py: [12/300], [260/484], step: 6068, 3.797 samples/sec, batch_loss: 0.0484, batch_loss_c: 0.0473, batch_loss_s: 0.0511, time:10.5343, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:31:18 \u001b[32mINFO     \u001b[0m train.py: [12/300], [270/484], step: 6078, 4.517 samples/sec, batch_loss: 0.0607, batch_loss_c: 0.0521, batch_loss_s: 0.0806, time:8.8547, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:31:35 \u001b[32mINFO     \u001b[0m train.py: [12/300], [280/484], step: 6088, 2.348 samples/sec, batch_loss: 0.2988, batch_loss_c: 0.2923, batch_loss_s: 0.3140, time:17.0334, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:32:08 \u001b[32mINFO     \u001b[0m train.py: [12/300], [290/484], step: 6098, 1.232 samples/sec, batch_loss: 0.3212, batch_loss_c: 0.2887, batch_loss_s: 0.3971, time:32.4589, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:32:35 \u001b[32mINFO     \u001b[0m train.py: [12/300], [300/484], step: 6108, 1.442 samples/sec, batch_loss: 0.1103, batch_loss_c: 0.1079, batch_loss_s: 0.1159, time:27.7413, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:32:50 \u001b[32mINFO     \u001b[0m train.py: [12/300], [310/484], step: 6118, 2.707 samples/sec, batch_loss: 0.3098, batch_loss_c: 0.3127, batch_loss_s: 0.3031, time:14.7775, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:33:00 \u001b[32mINFO     \u001b[0m train.py: [12/300], [320/484], step: 6128, 3.970 samples/sec, batch_loss: 0.3162, batch_loss_c: 0.3106, batch_loss_s: 0.3293, time:10.0754, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:33:10 \u001b[32mINFO     \u001b[0m train.py: [12/300], [330/484], step: 6138, 4.056 samples/sec, batch_loss: 0.1012, batch_loss_c: 0.1000, batch_loss_s: 0.1041, time:9.8629, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:33:22 \u001b[32mINFO     \u001b[0m train.py: [12/300], [340/484], step: 6148, 3.311 samples/sec, batch_loss: 0.1819, batch_loss_c: 0.1586, batch_loss_s: 0.2361, time:12.0810, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:33:37 \u001b[32mINFO     \u001b[0m train.py: [12/300], [350/484], step: 6158, 2.760 samples/sec, batch_loss: 0.1337, batch_loss_c: 0.1316, batch_loss_s: 0.1386, time:14.4904, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:33:47 \u001b[32mINFO     \u001b[0m train.py: [12/300], [360/484], step: 6168, 3.928 samples/sec, batch_loss: 0.0720, batch_loss_c: 0.0675, batch_loss_s: 0.0827, time:10.1836, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:34:09 \u001b[32mINFO     \u001b[0m train.py: [12/300], [370/484], step: 6178, 1.798 samples/sec, batch_loss: 0.2457, batch_loss_c: 0.2418, batch_loss_s: 0.2550, time:22.2463, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:34:25 \u001b[32mINFO     \u001b[0m train.py: [12/300], [380/484], step: 6188, 2.518 samples/sec, batch_loss: 0.0955, batch_loss_c: 0.1010, batch_loss_s: 0.0826, time:15.8846, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:34:48 \u001b[32mINFO     \u001b[0m train.py: [12/300], [390/484], step: 6198, 1.751 samples/sec, batch_loss: 0.3011, batch_loss_c: 0.2971, batch_loss_s: 0.3104, time:22.8415, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:35:00 \u001b[32mINFO     \u001b[0m train.py: [12/300], [400/484], step: 6208, 3.281 samples/sec, batch_loss: 0.2934, batch_loss_c: 0.2867, batch_loss_s: 0.3091, time:12.1908, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:35:15 \u001b[32mINFO     \u001b[0m train.py: [12/300], [410/484], step: 6218, 2.676 samples/sec, batch_loss: 0.2812, batch_loss_c: 0.2664, batch_loss_s: 0.3159, time:14.9457, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:35:35 \u001b[32mINFO     \u001b[0m train.py: [12/300], [420/484], step: 6228, 2.015 samples/sec, batch_loss: 0.4478, batch_loss_c: 0.4284, batch_loss_s: 0.4930, time:19.8497, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:36:02 \u001b[32mINFO     \u001b[0m train.py: [12/300], [430/484], step: 6238, 1.472 samples/sec, batch_loss: 0.0649, batch_loss_c: 0.0593, batch_loss_s: 0.0779, time:27.1697, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:36:13 \u001b[32mINFO     \u001b[0m train.py: [12/300], [440/484], step: 6248, 3.683 samples/sec, batch_loss: 0.0680, batch_loss_c: 0.0594, batch_loss_s: 0.0881, time:10.8599, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:36:25 \u001b[32mINFO     \u001b[0m train.py: [12/300], [450/484], step: 6258, 3.320 samples/sec, batch_loss: 0.0852, batch_loss_c: 0.0500, batch_loss_s: 0.1674, time:12.0499, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:36:47 \u001b[32mINFO     \u001b[0m train.py: [12/300], [460/484], step: 6268, 1.841 samples/sec, batch_loss: 0.3081, batch_loss_c: 0.2926, batch_loss_s: 0.3442, time:21.7254, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:36:56 \u001b[32mINFO     \u001b[0m train.py: [12/300], [470/484], step: 6278, 4.399 samples/sec, batch_loss: 0.2904, batch_loss_c: 0.2786, batch_loss_s: 0.3179, time:9.0934, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:37:07 \u001b[32mINFO     \u001b[0m train.py: [12/300], [480/484], step: 6288, 3.683 samples/sec, batch_loss: 0.0559, batch_loss_c: 0.0404, batch_loss_s: 0.0920, time:10.8605, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:37:10 \u001b[32mINFO     \u001b[0m train.py: [12/300], train_loss: 0.1561, time: 732.7238, lr: 1e-05\u001b[0m\n",
            "2019-12-07 14:37:12 \u001b[32mINFO     \u001b[0m train.py: [13/300], [0/484], step: 6292, 28.792 samples/sec, batch_loss: 0.0981, batch_loss_c: 0.0929, batch_loss_s: 0.1102, time:1.3893, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:37:26 \u001b[32mINFO     \u001b[0m train.py: [13/300], [10/484], step: 6302, 2.838 samples/sec, batch_loss: 0.0513, batch_loss_c: 0.0447, batch_loss_s: 0.0666, time:14.0944, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:37:36 \u001b[32mINFO     \u001b[0m train.py: [13/300], [20/484], step: 6312, 3.850 samples/sec, batch_loss: 0.0888, batch_loss_c: 0.0824, batch_loss_s: 0.1035, time:10.3889, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:38:00 \u001b[32mINFO     \u001b[0m train.py: [13/300], [30/484], step: 6322, 1.682 samples/sec, batch_loss: 0.0441, batch_loss_c: 0.0401, batch_loss_s: 0.0534, time:23.7794, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:38:17 \u001b[32mINFO     \u001b[0m train.py: [13/300], [40/484], step: 6332, 2.336 samples/sec, batch_loss: 0.3370, batch_loss_c: 0.3339, batch_loss_s: 0.3442, time:17.1232, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:38:31 \u001b[32mINFO     \u001b[0m train.py: [13/300], [50/484], step: 6342, 3.027 samples/sec, batch_loss: 0.3048, batch_loss_c: 0.2936, batch_loss_s: 0.3311, time:13.2122, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:38:48 \u001b[32mINFO     \u001b[0m train.py: [13/300], [60/484], step: 6352, 2.333 samples/sec, batch_loss: 0.0671, batch_loss_c: 0.0521, batch_loss_s: 0.1021, time:17.1436, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:38:59 \u001b[32mINFO     \u001b[0m train.py: [13/300], [70/484], step: 6362, 3.668 samples/sec, batch_loss: 0.1191, batch_loss_c: 0.1187, batch_loss_s: 0.1200, time:10.9044, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:39:10 \u001b[32mINFO     \u001b[0m train.py: [13/300], [80/484], step: 6372, 3.562 samples/sec, batch_loss: 0.0811, batch_loss_c: 0.0748, batch_loss_s: 0.0958, time:11.2311, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:39:20 \u001b[32mINFO     \u001b[0m train.py: [13/300], [90/484], step: 6382, 3.886 samples/sec, batch_loss: 0.0723, batch_loss_c: 0.0708, batch_loss_s: 0.0760, time:10.2935, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:39:31 \u001b[32mINFO     \u001b[0m train.py: [13/300], [100/484], step: 6392, 3.741 samples/sec, batch_loss: 0.0677, batch_loss_c: 0.0620, batch_loss_s: 0.0811, time:10.6913, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:39:52 \u001b[32mINFO     \u001b[0m train.py: [13/300], [110/484], step: 6402, 1.841 samples/sec, batch_loss: 0.0377, batch_loss_c: 0.0329, batch_loss_s: 0.0491, time:21.7221, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:40:04 \u001b[32mINFO     \u001b[0m train.py: [13/300], [120/484], step: 6412, 3.333 samples/sec, batch_loss: 0.3583, batch_loss_c: 0.3603, batch_loss_s: 0.3535, time:12.0018, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:40:36 \u001b[32mINFO     \u001b[0m train.py: [13/300], [130/484], step: 6422, 1.285 samples/sec, batch_loss: 0.0748, batch_loss_c: 0.0589, batch_loss_s: 0.1121, time:31.1285, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:40:49 \u001b[32mINFO     \u001b[0m train.py: [13/300], [140/484], step: 6432, 2.914 samples/sec, batch_loss: 0.1446, batch_loss_c: 0.1396, batch_loss_s: 0.1565, time:13.7254, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:41:02 \u001b[32mINFO     \u001b[0m train.py: [13/300], [150/484], step: 6442, 3.228 samples/sec, batch_loss: 0.5591, batch_loss_c: 0.5582, batch_loss_s: 0.5612, time:12.3911, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:41:11 \u001b[32mINFO     \u001b[0m train.py: [13/300], [160/484], step: 6452, 4.123 samples/sec, batch_loss: 0.2140, batch_loss_c: 0.1868, batch_loss_s: 0.2774, time:9.7015, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:41:29 \u001b[32mINFO     \u001b[0m train.py: [13/300], [170/484], step: 6462, 2.306 samples/sec, batch_loss: 0.0793, batch_loss_c: 0.0740, batch_loss_s: 0.0918, time:17.3455, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:41:38 \u001b[32mINFO     \u001b[0m train.py: [13/300], [180/484], step: 6472, 4.511 samples/sec, batch_loss: 0.0796, batch_loss_c: 0.0712, batch_loss_s: 0.0992, time:8.8668, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:42:02 \u001b[32mINFO     \u001b[0m train.py: [13/300], [190/484], step: 6482, 1.651 samples/sec, batch_loss: 0.0491, batch_loss_c: 0.0443, batch_loss_s: 0.0602, time:24.2309, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:42:15 \u001b[32mINFO     \u001b[0m train.py: [13/300], [200/484], step: 6492, 2.988 samples/sec, batch_loss: 0.3808, batch_loss_c: 0.3606, batch_loss_s: 0.4281, time:13.3889, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:42:28 \u001b[32mINFO     \u001b[0m train.py: [13/300], [210/484], step: 6502, 3.079 samples/sec, batch_loss: 0.1604, batch_loss_c: 0.1444, batch_loss_s: 0.1976, time:12.9912, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:42:47 \u001b[32mINFO     \u001b[0m train.py: [13/300], [220/484], step: 6512, 2.081 samples/sec, batch_loss: 0.0761, batch_loss_c: 0.0717, batch_loss_s: 0.0864, time:19.2198, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:43:01 \u001b[32mINFO     \u001b[0m train.py: [13/300], [230/484], step: 6522, 2.890 samples/sec, batch_loss: 0.0480, batch_loss_c: 0.0427, batch_loss_s: 0.0604, time:13.8432, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:43:16 \u001b[32mINFO     \u001b[0m train.py: [13/300], [240/484], step: 6532, 2.644 samples/sec, batch_loss: 0.0847, batch_loss_c: 0.0701, batch_loss_s: 0.1189, time:15.1304, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:43:31 \u001b[32mINFO     \u001b[0m train.py: [13/300], [250/484], step: 6542, 2.666 samples/sec, batch_loss: 0.0981, batch_loss_c: 0.0940, batch_loss_s: 0.1076, time:15.0042, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:43:45 \u001b[32mINFO     \u001b[0m train.py: [13/300], [260/484], step: 6552, 2.852 samples/sec, batch_loss: 0.0848, batch_loss_c: 0.0790, batch_loss_s: 0.0981, time:14.0238, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:44:05 \u001b[32mINFO     \u001b[0m train.py: [13/300], [270/484], step: 6562, 2.083 samples/sec, batch_loss: 0.2356, batch_loss_c: 0.1746, batch_loss_s: 0.3780, time:19.1997, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:44:24 \u001b[32mINFO     \u001b[0m train.py: [13/300], [280/484], step: 6572, 2.104 samples/sec, batch_loss: 0.3197, batch_loss_c: 0.3228, batch_loss_s: 0.3125, time:19.0124, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:44:36 \u001b[32mINFO     \u001b[0m train.py: [13/300], [290/484], step: 6582, 3.218 samples/sec, batch_loss: 0.0955, batch_loss_c: 0.0925, batch_loss_s: 0.1026, time:12.4305, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:44:48 \u001b[32mINFO     \u001b[0m train.py: [13/300], [300/484], step: 6592, 3.516 samples/sec, batch_loss: 0.1047, batch_loss_c: 0.0811, batch_loss_s: 0.1598, time:11.3771, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:45:06 \u001b[32mINFO     \u001b[0m train.py: [13/300], [310/484], step: 6602, 2.201 samples/sec, batch_loss: 0.0659, batch_loss_c: 0.0546, batch_loss_s: 0.0923, time:18.1770, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:45:16 \u001b[32mINFO     \u001b[0m train.py: [13/300], [320/484], step: 6612, 3.955 samples/sec, batch_loss: 0.0692, batch_loss_c: 0.0615, batch_loss_s: 0.0872, time:10.1135, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:45:34 \u001b[32mINFO     \u001b[0m train.py: [13/300], [330/484], step: 6622, 2.168 samples/sec, batch_loss: 0.4270, batch_loss_c: 0.4196, batch_loss_s: 0.4440, time:18.4498, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:45:43 \u001b[32mINFO     \u001b[0m train.py: [13/300], [340/484], step: 6632, 4.471 samples/sec, batch_loss: 0.1694, batch_loss_c: 0.1682, batch_loss_s: 0.1722, time:8.9466, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:46:00 \u001b[32mINFO     \u001b[0m train.py: [13/300], [350/484], step: 6642, 2.419 samples/sec, batch_loss: 0.1261, batch_loss_c: 0.1275, batch_loss_s: 0.1229, time:16.5384, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:46:21 \u001b[32mINFO     \u001b[0m train.py: [13/300], [360/484], step: 6652, 1.895 samples/sec, batch_loss: 0.2906, batch_loss_c: 0.2863, batch_loss_s: 0.3008, time:21.1089, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:46:34 \u001b[32mINFO     \u001b[0m train.py: [13/300], [370/484], step: 6662, 3.047 samples/sec, batch_loss: 0.0809, batch_loss_c: 0.0722, batch_loss_s: 0.1012, time:13.1275, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:46:43 \u001b[32mINFO     \u001b[0m train.py: [13/300], [380/484], step: 6672, 4.286 samples/sec, batch_loss: 0.0581, batch_loss_c: 0.0488, batch_loss_s: 0.0799, time:9.3324, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:47:00 \u001b[32mINFO     \u001b[0m train.py: [13/300], [390/484], step: 6682, 2.370 samples/sec, batch_loss: 0.0696, batch_loss_c: 0.0667, batch_loss_s: 0.0765, time:16.8750, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:47:31 \u001b[32mINFO     \u001b[0m train.py: [13/300], [400/484], step: 6692, 1.286 samples/sec, batch_loss: 0.2093, batch_loss_c: 0.2220, batch_loss_s: 0.1798, time:31.1077, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:47:47 \u001b[32mINFO     \u001b[0m train.py: [13/300], [410/484], step: 6702, 2.548 samples/sec, batch_loss: 0.3120, batch_loss_c: 0.2946, batch_loss_s: 0.3527, time:15.6956, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:47:59 \u001b[32mINFO     \u001b[0m train.py: [13/300], [420/484], step: 6712, 3.298 samples/sec, batch_loss: 0.0358, batch_loss_c: 0.0338, batch_loss_s: 0.0406, time:12.1272, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:48:24 \u001b[32mINFO     \u001b[0m train.py: [13/300], [430/484], step: 6722, 1.590 samples/sec, batch_loss: 0.1233, batch_loss_c: 0.1208, batch_loss_s: 0.1290, time:25.1580, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:48:34 \u001b[32mINFO     \u001b[0m train.py: [13/300], [440/484], step: 6732, 4.144 samples/sec, batch_loss: 0.3206, batch_loss_c: 0.3124, batch_loss_s: 0.3399, time:9.6516, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:48:54 \u001b[32mINFO     \u001b[0m train.py: [13/300], [450/484], step: 6742, 2.010 samples/sec, batch_loss: 0.0527, batch_loss_c: 0.0427, batch_loss_s: 0.0759, time:19.9032, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:49:08 \u001b[32mINFO     \u001b[0m train.py: [13/300], [460/484], step: 6752, 2.904 samples/sec, batch_loss: 0.0374, batch_loss_c: 0.0301, batch_loss_s: 0.0544, time:13.7759, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:49:29 \u001b[32mINFO     \u001b[0m train.py: [13/300], [470/484], step: 6762, 1.828 samples/sec, batch_loss: 0.0739, batch_loss_c: 0.0693, batch_loss_s: 0.0845, time:21.8783, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:49:42 \u001b[32mINFO     \u001b[0m train.py: [13/300], [480/484], step: 6772, 3.227 samples/sec, batch_loss: 0.3073, batch_loss_c: 0.3016, batch_loss_s: 0.3208, time:12.3960, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:49:44 \u001b[32mINFO     \u001b[0m train.py: [13/300], train_loss: 0.1455, time: 753.4526, lr: 1e-05\u001b[0m\n",
            "2019-12-07 14:49:45 \u001b[32mINFO     \u001b[0m train.py: [14/300], [0/484], step: 6776, 50.586 samples/sec, batch_loss: 0.0636, batch_loss_c: 0.0482, batch_loss_s: 0.0994, time:0.7907, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:50:20 \u001b[32mINFO     \u001b[0m train.py: [14/300], [10/484], step: 6786, 1.144 samples/sec, batch_loss: 0.1163, batch_loss_c: 0.1071, batch_loss_s: 0.1376, time:34.9766, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:50:30 \u001b[32mINFO     \u001b[0m train.py: [14/300], [20/484], step: 6796, 3.967 samples/sec, batch_loss: 0.0846, batch_loss_c: 0.0771, batch_loss_s: 0.1019, time:10.0841, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:50:47 \u001b[32mINFO     \u001b[0m train.py: [14/300], [30/484], step: 6806, 2.367 samples/sec, batch_loss: 0.0618, batch_loss_c: 0.0528, batch_loss_s: 0.0829, time:16.9016, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:51:02 \u001b[32mINFO     \u001b[0m train.py: [14/300], [40/484], step: 6816, 2.737 samples/sec, batch_loss: 0.3754, batch_loss_c: 0.3694, batch_loss_s: 0.3893, time:14.6145, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:51:18 \u001b[32mINFO     \u001b[0m train.py: [14/300], [50/484], step: 6826, 2.412 samples/sec, batch_loss: 0.0593, batch_loss_c: 0.0524, batch_loss_s: 0.0754, time:16.5804, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:51:30 \u001b[32mINFO     \u001b[0m train.py: [14/300], [60/484], step: 6836, 3.456 samples/sec, batch_loss: 0.0471, batch_loss_c: 0.0385, batch_loss_s: 0.0672, time:11.5729, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:51:54 \u001b[32mINFO     \u001b[0m train.py: [14/300], [70/484], step: 6846, 1.634 samples/sec, batch_loss: 0.3161, batch_loss_c: 0.3092, batch_loss_s: 0.3322, time:24.4747, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:52:20 \u001b[32mINFO     \u001b[0m train.py: [14/300], [80/484], step: 6856, 1.550 samples/sec, batch_loss: 0.0660, batch_loss_c: 0.0586, batch_loss_s: 0.0834, time:25.7993, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:52:34 \u001b[32mINFO     \u001b[0m train.py: [14/300], [90/484], step: 6866, 2.996 samples/sec, batch_loss: 0.0904, batch_loss_c: 0.0791, batch_loss_s: 0.1167, time:13.3520, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:52:45 \u001b[32mINFO     \u001b[0m train.py: [14/300], [100/484], step: 6876, 3.344 samples/sec, batch_loss: 0.0622, batch_loss_c: 0.0557, batch_loss_s: 0.0775, time:11.9618, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:53:05 \u001b[32mINFO     \u001b[0m train.py: [14/300], [110/484], step: 6886, 2.081 samples/sec, batch_loss: 0.0782, batch_loss_c: 0.0716, batch_loss_s: 0.0936, time:19.2189, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:53:17 \u001b[32mINFO     \u001b[0m train.py: [14/300], [120/484], step: 6896, 3.201 samples/sec, batch_loss: 0.0790, batch_loss_c: 0.0668, batch_loss_s: 0.1075, time:12.4969, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:53:30 \u001b[32mINFO     \u001b[0m train.py: [14/300], [130/484], step: 6906, 3.081 samples/sec, batch_loss: 0.1008, batch_loss_c: 0.0945, batch_loss_s: 0.1156, time:12.9814, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:53:40 \u001b[32mINFO     \u001b[0m train.py: [14/300], [140/484], step: 6916, 3.989 samples/sec, batch_loss: 0.2909, batch_loss_c: 0.2874, batch_loss_s: 0.2990, time:10.0264, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:53:50 \u001b[32mINFO     \u001b[0m train.py: [14/300], [150/484], step: 6926, 4.247 samples/sec, batch_loss: 0.0531, batch_loss_c: 0.0482, batch_loss_s: 0.0645, time:9.4188, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:54:00 \u001b[32mINFO     \u001b[0m train.py: [14/300], [160/484], step: 6936, 4.034 samples/sec, batch_loss: 0.1156, batch_loss_c: 0.1138, batch_loss_s: 0.1199, time:9.9147, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:54:10 \u001b[32mINFO     \u001b[0m train.py: [14/300], [170/484], step: 6946, 3.815 samples/sec, batch_loss: 0.0837, batch_loss_c: 0.0798, batch_loss_s: 0.0926, time:10.4836, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:54:29 \u001b[32mINFO     \u001b[0m train.py: [14/300], [180/484], step: 6956, 2.116 samples/sec, batch_loss: 0.0661, batch_loss_c: 0.0540, batch_loss_s: 0.0944, time:18.9072, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:54:40 \u001b[32mINFO     \u001b[0m train.py: [14/300], [190/484], step: 6966, 3.772 samples/sec, batch_loss: 0.3045, batch_loss_c: 0.2987, batch_loss_s: 0.3181, time:10.6056, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:55:12 \u001b[32mINFO     \u001b[0m train.py: [14/300], [200/484], step: 6976, 1.235 samples/sec, batch_loss: 0.0877, batch_loss_c: 0.0661, batch_loss_s: 0.1380, time:32.3872, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:55:21 \u001b[32mINFO     \u001b[0m train.py: [14/300], [210/484], step: 6986, 4.211 samples/sec, batch_loss: 0.0664, batch_loss_c: 0.0584, batch_loss_s: 0.0850, time:9.4982, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:55:45 \u001b[32mINFO     \u001b[0m train.py: [14/300], [220/484], step: 6996, 1.689 samples/sec, batch_loss: 0.0713, batch_loss_c: 0.0709, batch_loss_s: 0.0722, time:23.6857, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:55:59 \u001b[32mINFO     \u001b[0m train.py: [14/300], [230/484], step: 7006, 2.957 samples/sec, batch_loss: 0.0728, batch_loss_c: 0.0653, batch_loss_s: 0.0904, time:13.5256, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:56:26 \u001b[32mINFO     \u001b[0m train.py: [14/300], [240/484], step: 7016, 1.476 samples/sec, batch_loss: 0.4405, batch_loss_c: 0.4652, batch_loss_s: 0.3829, time:27.1073, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:56:48 \u001b[32mINFO     \u001b[0m train.py: [14/300], [250/484], step: 7026, 1.814 samples/sec, batch_loss: 0.0747, batch_loss_c: 0.0654, batch_loss_s: 0.0963, time:22.0480, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:57:01 \u001b[32mINFO     \u001b[0m train.py: [14/300], [260/484], step: 7036, 3.087 samples/sec, batch_loss: 0.0650, batch_loss_c: 0.0595, batch_loss_s: 0.0778, time:12.9578, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:57:12 \u001b[32mINFO     \u001b[0m train.py: [14/300], [270/484], step: 7046, 3.557 samples/sec, batch_loss: 0.3017, batch_loss_c: 0.2926, batch_loss_s: 0.3229, time:11.2467, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:57:33 \u001b[32mINFO     \u001b[0m train.py: [14/300], [280/484], step: 7056, 1.934 samples/sec, batch_loss: 0.0539, batch_loss_c: 0.0522, batch_loss_s: 0.0577, time:20.6807, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:57:42 \u001b[32mINFO     \u001b[0m train.py: [14/300], [290/484], step: 7066, 4.366 samples/sec, batch_loss: 0.0855, batch_loss_c: 0.0747, batch_loss_s: 0.1106, time:9.1619, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:57:57 \u001b[32mINFO     \u001b[0m train.py: [14/300], [300/484], step: 7076, 2.621 samples/sec, batch_loss: 0.0699, batch_loss_c: 0.0606, batch_loss_s: 0.0917, time:15.2604, lr:1e-05\u001b[0m\n",
            "tcmalloc: large alloc 5936644096 bytes == 0x7f1d0902e000 @  0x7f214dff61e7 0x7f21425daf71 0x7f214263e55d 0x7f2142641e28 0x7f21426423e5 0x7f21426d8fc2 0x50ac25 0x50d390 0x508245 0x50a080 0x50aa7d 0x50d390 0x509455 0x595311 0x5a067e 0x557d28 0x50c88b 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x509455 0x595311 0x5a522c 0x557f7e 0x4b5eff 0x50c810\n",
            "2019-12-07 14:58:33 \u001b[32mINFO     \u001b[0m train.py: [14/300], [310/484], step: 7086, 1.101 samples/sec, batch_loss: 0.1802, batch_loss_c: 0.1688, batch_loss_s: 0.2066, time:36.3349, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:58:45 \u001b[32mINFO     \u001b[0m train.py: [14/300], [320/484], step: 7096, 3.348 samples/sec, batch_loss: 0.5719, batch_loss_c: 0.5753, batch_loss_s: 0.5640, time:11.9458, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:59:11 \u001b[32mINFO     \u001b[0m train.py: [14/300], [330/484], step: 7106, 1.591 samples/sec, batch_loss: 0.1430, batch_loss_c: 0.1405, batch_loss_s: 0.1488, time:25.1447, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:59:18 \u001b[32mINFO     \u001b[0m train.py: [14/300], [340/484], step: 7116, 5.111 samples/sec, batch_loss: 0.2710, batch_loss_c: 0.2757, batch_loss_s: 0.2600, time:7.8264, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:59:37 \u001b[32mINFO     \u001b[0m train.py: [14/300], [350/484], step: 7126, 2.183 samples/sec, batch_loss: 0.1141, batch_loss_c: 0.0943, batch_loss_s: 0.1602, time:18.3217, lr:1e-05\u001b[0m\n",
            "2019-12-07 14:59:57 \u001b[32mINFO     \u001b[0m train.py: [14/300], [360/484], step: 7136, 2.013 samples/sec, batch_loss: 0.0458, batch_loss_c: 0.0355, batch_loss_s: 0.0700, time:19.8721, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:00:14 \u001b[32mINFO     \u001b[0m train.py: [14/300], [370/484], step: 7146, 2.278 samples/sec, batch_loss: 0.0869, batch_loss_c: 0.0827, batch_loss_s: 0.0967, time:17.5564, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:00:24 \u001b[32mINFO     \u001b[0m train.py: [14/300], [380/484], step: 7156, 3.897 samples/sec, batch_loss: 0.0554, batch_loss_c: 0.0485, batch_loss_s: 0.0716, time:10.2643, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:00:37 \u001b[32mINFO     \u001b[0m train.py: [14/300], [390/484], step: 7166, 3.139 samples/sec, batch_loss: 0.0778, batch_loss_c: 0.0610, batch_loss_s: 0.1168, time:12.7420, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:00:50 \u001b[32mINFO     \u001b[0m train.py: [14/300], [400/484], step: 7176, 3.200 samples/sec, batch_loss: 0.0769, batch_loss_c: 0.0697, batch_loss_s: 0.0939, time:12.4989, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:01:20 \u001b[32mINFO     \u001b[0m train.py: [14/300], [410/484], step: 7186, 1.338 samples/sec, batch_loss: 0.0501, batch_loss_c: 0.0443, batch_loss_s: 0.0637, time:29.8976, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:01:30 \u001b[32mINFO     \u001b[0m train.py: [14/300], [420/484], step: 7196, 3.791 samples/sec, batch_loss: 0.2778, batch_loss_c: 0.2746, batch_loss_s: 0.2851, time:10.5523, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:02:11 \u001b[32mINFO     \u001b[0m train.py: [14/300], [430/484], step: 7206, 0.982 samples/sec, batch_loss: 0.2529, batch_loss_c: 0.2272, batch_loss_s: 0.3128, time:40.7313, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:02:43 \u001b[32mINFO     \u001b[0m train.py: [14/300], [440/484], step: 7216, 1.255 samples/sec, batch_loss: 0.0666, batch_loss_c: 0.0645, batch_loss_s: 0.0715, time:31.8780, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:02:57 \u001b[32mINFO     \u001b[0m train.py: [14/300], [450/484], step: 7226, 2.789 samples/sec, batch_loss: 0.2886, batch_loss_c: 0.2845, batch_loss_s: 0.2980, time:14.3430, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:03:09 \u001b[32mINFO     \u001b[0m train.py: [14/300], [460/484], step: 7236, 3.253 samples/sec, batch_loss: 0.0766, batch_loss_c: 0.0727, batch_loss_s: 0.0857, time:12.2956, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:03:19 \u001b[32mINFO     \u001b[0m train.py: [14/300], [470/484], step: 7246, 4.001 samples/sec, batch_loss: 0.0724, batch_loss_c: 0.0697, batch_loss_s: 0.0786, time:9.9976, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:03:35 \u001b[32mINFO     \u001b[0m train.py: [14/300], [480/484], step: 7256, 2.563 samples/sec, batch_loss: 0.2457, batch_loss_c: 0.2279, batch_loss_s: 0.2872, time:15.6071, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:03:38 \u001b[32mINFO     \u001b[0m train.py: [14/300], train_loss: 0.1392, time: 833.3565, lr: 1e-05\u001b[0m\n",
            "2019-12-07 15:03:39 \u001b[32mINFO     \u001b[0m train.py: [15/300], [0/484], step: 7260, 41.232 samples/sec, batch_loss: 0.0563, batch_loss_c: 0.0439, batch_loss_s: 0.0852, time:0.9701, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:03:49 \u001b[32mINFO     \u001b[0m train.py: [15/300], [10/484], step: 7270, 3.858 samples/sec, batch_loss: 0.1004, batch_loss_c: 0.0822, batch_loss_s: 0.1429, time:10.3682, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:04:00 \u001b[32mINFO     \u001b[0m train.py: [15/300], [20/484], step: 7280, 3.669 samples/sec, batch_loss: 0.0445, batch_loss_c: 0.0404, batch_loss_s: 0.0541, time:10.9022, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:04:13 \u001b[32mINFO     \u001b[0m train.py: [15/300], [30/484], step: 7290, 3.074 samples/sec, batch_loss: 0.1236, batch_loss_c: 0.1285, batch_loss_s: 0.1120, time:13.0142, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:04:29 \u001b[32mINFO     \u001b[0m train.py: [15/300], [40/484], step: 7300, 2.532 samples/sec, batch_loss: 0.2871, batch_loss_c: 0.2793, batch_loss_s: 0.3053, time:15.7990, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:04:40 \u001b[32mINFO     \u001b[0m train.py: [15/300], [50/484], step: 7310, 3.860 samples/sec, batch_loss: 0.0762, batch_loss_c: 0.0630, batch_loss_s: 0.1069, time:10.3638, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:05:08 \u001b[32mINFO     \u001b[0m train.py: [15/300], [60/484], step: 7320, 1.433 samples/sec, batch_loss: 0.1140, batch_loss_c: 0.1129, batch_loss_s: 0.1164, time:27.9213, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:05:21 \u001b[32mINFO     \u001b[0m train.py: [15/300], [70/484], step: 7330, 3.006 samples/sec, batch_loss: 0.3046, batch_loss_c: 0.3032, batch_loss_s: 0.3080, time:13.3077, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:05:34 \u001b[32mINFO     \u001b[0m train.py: [15/300], [80/484], step: 7340, 3.021 samples/sec, batch_loss: 0.1180, batch_loss_c: 0.1199, batch_loss_s: 0.1136, time:13.2394, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:05:43 \u001b[32mINFO     \u001b[0m train.py: [15/300], [90/484], step: 7350, 4.249 samples/sec, batch_loss: 0.3202, batch_loss_c: 0.3272, batch_loss_s: 0.3038, time:9.4147, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:05:55 \u001b[32mINFO     \u001b[0m train.py: [15/300], [100/484], step: 7360, 3.531 samples/sec, batch_loss: 0.1499, batch_loss_c: 0.1450, batch_loss_s: 0.1615, time:11.3280, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:06:07 \u001b[32mINFO     \u001b[0m train.py: [15/300], [110/484], step: 7370, 3.302 samples/sec, batch_loss: 0.0618, batch_loss_c: 0.0564, batch_loss_s: 0.0744, time:12.1122, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:06:26 \u001b[32mINFO     \u001b[0m train.py: [15/300], [120/484], step: 7380, 2.133 samples/sec, batch_loss: 0.0775, batch_loss_c: 0.0654, batch_loss_s: 0.1057, time:18.7487, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:06:39 \u001b[32mINFO     \u001b[0m train.py: [15/300], [130/484], step: 7390, 2.985 samples/sec, batch_loss: 0.4361, batch_loss_c: 0.4078, batch_loss_s: 0.5020, time:13.3985, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:06:55 \u001b[32mINFO     \u001b[0m train.py: [15/300], [140/484], step: 7400, 2.475 samples/sec, batch_loss: 0.0767, batch_loss_c: 0.0677, batch_loss_s: 0.0975, time:16.1626, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:07:08 \u001b[32mINFO     \u001b[0m train.py: [15/300], [150/484], step: 7410, 3.077 samples/sec, batch_loss: 0.0510, batch_loss_c: 0.0407, batch_loss_s: 0.0750, time:13.0015, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:07:27 \u001b[32mINFO     \u001b[0m train.py: [15/300], [160/484], step: 7420, 2.094 samples/sec, batch_loss: 0.2974, batch_loss_c: 0.2890, batch_loss_s: 0.3169, time:19.1037, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:07:56 \u001b[32mINFO     \u001b[0m train.py: [15/300], [170/484], step: 7430, 1.376 samples/sec, batch_loss: 0.2864, batch_loss_c: 0.2595, batch_loss_s: 0.3489, time:29.0615, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:08:06 \u001b[32mINFO     \u001b[0m train.py: [15/300], [180/484], step: 7440, 3.971 samples/sec, batch_loss: 0.0436, batch_loss_c: 0.0370, batch_loss_s: 0.0589, time:10.0737, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:08:17 \u001b[32mINFO     \u001b[0m train.py: [15/300], [190/484], step: 7450, 3.643 samples/sec, batch_loss: 0.3830, batch_loss_c: 0.3815, batch_loss_s: 0.3866, time:10.9806, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:08:36 \u001b[32mINFO     \u001b[0m train.py: [15/300], [200/484], step: 7460, 2.138 samples/sec, batch_loss: 0.2998, batch_loss_c: 0.2891, batch_loss_s: 0.3247, time:18.7127, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:08:50 \u001b[32mINFO     \u001b[0m train.py: [15/300], [210/484], step: 7470, 2.878 samples/sec, batch_loss: 0.0469, batch_loss_c: 0.0438, batch_loss_s: 0.0541, time:13.8996, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:09:00 \u001b[32mINFO     \u001b[0m train.py: [15/300], [220/484], step: 7480, 3.915 samples/sec, batch_loss: 0.0609, batch_loss_c: 0.0545, batch_loss_s: 0.0759, time:10.2164, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:09:12 \u001b[32mINFO     \u001b[0m train.py: [15/300], [230/484], step: 7490, 3.537 samples/sec, batch_loss: 0.4623, batch_loss_c: 0.4564, batch_loss_s: 0.4759, time:11.3101, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:09:33 \u001b[32mINFO     \u001b[0m train.py: [15/300], [240/484], step: 7500, 1.907 samples/sec, batch_loss: 0.3776, batch_loss_c: 0.3919, batch_loss_s: 0.3442, time:20.9762, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:09:42 \u001b[32mINFO     \u001b[0m train.py: [15/300], [250/484], step: 7510, 4.236 samples/sec, batch_loss: 0.0856, batch_loss_c: 0.0576, batch_loss_s: 0.1511, time:9.4437, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:09:57 \u001b[32mINFO     \u001b[0m train.py: [15/300], [260/484], step: 7520, 2.705 samples/sec, batch_loss: 0.0653, batch_loss_c: 0.0655, batch_loss_s: 0.0648, time:14.7858, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:10:09 \u001b[32mINFO     \u001b[0m train.py: [15/300], [270/484], step: 7530, 3.397 samples/sec, batch_loss: 0.2156, batch_loss_c: 0.1760, batch_loss_s: 0.3080, time:11.7752, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:10:21 \u001b[32mINFO     \u001b[0m train.py: [15/300], [280/484], step: 7540, 3.199 samples/sec, batch_loss: 0.0598, batch_loss_c: 0.0533, batch_loss_s: 0.0751, time:12.5029, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:10:37 \u001b[32mINFO     \u001b[0m train.py: [15/300], [290/484], step: 7550, 2.484 samples/sec, batch_loss: 0.0862, batch_loss_c: 0.0800, batch_loss_s: 0.1006, time:16.1024, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:10:47 \u001b[32mINFO     \u001b[0m train.py: [15/300], [300/484], step: 7560, 4.155 samples/sec, batch_loss: 0.0432, batch_loss_c: 0.0355, batch_loss_s: 0.0611, time:9.6261, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:11:00 \u001b[32mINFO     \u001b[0m train.py: [15/300], [310/484], step: 7570, 3.010 samples/sec, batch_loss: 0.0427, batch_loss_c: 0.0359, batch_loss_s: 0.0585, time:13.2909, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:11:24 \u001b[32mINFO     \u001b[0m train.py: [15/300], [320/484], step: 7580, 1.644 samples/sec, batch_loss: 0.0443, batch_loss_c: 0.0398, batch_loss_s: 0.0548, time:24.3331, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:11:51 \u001b[32mINFO     \u001b[0m train.py: [15/300], [330/484], step: 7590, 1.507 samples/sec, batch_loss: 0.2346, batch_loss_c: 0.2437, batch_loss_s: 0.2133, time:26.5466, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:12:00 \u001b[32mINFO     \u001b[0m train.py: [15/300], [340/484], step: 7600, 4.427 samples/sec, batch_loss: 0.0677, batch_loss_c: 0.0576, batch_loss_s: 0.0912, time:9.0364, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:12:09 \u001b[32mINFO     \u001b[0m train.py: [15/300], [350/484], step: 7610, 4.620 samples/sec, batch_loss: 0.0613, batch_loss_c: 0.0615, batch_loss_s: 0.0608, time:8.6584, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:12:33 \u001b[32mINFO     \u001b[0m train.py: [15/300], [360/484], step: 7620, 1.678 samples/sec, batch_loss: 0.1508, batch_loss_c: 0.1504, batch_loss_s: 0.1517, time:23.8341, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:12:51 \u001b[32mINFO     \u001b[0m train.py: [15/300], [370/484], step: 7630, 2.212 samples/sec, batch_loss: 0.1040, batch_loss_c: 0.1069, batch_loss_s: 0.0972, time:18.0826, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:13:13 \u001b[32mINFO     \u001b[0m train.py: [15/300], [380/484], step: 7640, 1.753 samples/sec, batch_loss: 0.0676, batch_loss_c: 0.0549, batch_loss_s: 0.0975, time:22.8117, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:13:24 \u001b[32mINFO     \u001b[0m train.py: [15/300], [390/484], step: 7650, 3.798 samples/sec, batch_loss: 0.0682, batch_loss_c: 0.0664, batch_loss_s: 0.0726, time:10.5328, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:13:47 \u001b[32mINFO     \u001b[0m train.py: [15/300], [400/484], step: 7660, 1.714 samples/sec, batch_loss: 0.1371, batch_loss_c: 0.1422, batch_loss_s: 0.1252, time:23.3384, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:14:05 \u001b[32mINFO     \u001b[0m train.py: [15/300], [410/484], step: 7670, 2.200 samples/sec, batch_loss: 0.3125, batch_loss_c: 0.3078, batch_loss_s: 0.3237, time:18.1838, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:14:29 \u001b[32mINFO     \u001b[0m train.py: [15/300], [420/484], step: 7680, 1.713 samples/sec, batch_loss: 0.0989, batch_loss_c: 0.0722, batch_loss_s: 0.1612, time:23.3450, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:14:51 \u001b[32mINFO     \u001b[0m train.py: [15/300], [430/484], step: 7690, 1.763 samples/sec, batch_loss: 0.2929, batch_loss_c: 0.2869, batch_loss_s: 0.3070, time:22.6931, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:15:23 \u001b[32mINFO     \u001b[0m train.py: [15/300], [440/484], step: 7700, 1.274 samples/sec, batch_loss: 0.2139, batch_loss_c: 0.2565, batch_loss_s: 0.1146, time:31.4063, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:15:52 \u001b[32mINFO     \u001b[0m train.py: [15/300], [450/484], step: 7710, 1.385 samples/sec, batch_loss: 0.2760, batch_loss_c: 0.2697, batch_loss_s: 0.2907, time:28.8735, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:16:03 \u001b[32mINFO     \u001b[0m train.py: [15/300], [460/484], step: 7720, 3.567 samples/sec, batch_loss: 0.3463, batch_loss_c: 0.3447, batch_loss_s: 0.3501, time:11.2127, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:16:23 \u001b[32mINFO     \u001b[0m train.py: [15/300], [470/484], step: 7730, 2.033 samples/sec, batch_loss: 0.0881, batch_loss_c: 0.0748, batch_loss_s: 0.1190, time:19.6734, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:16:42 \u001b[32mINFO     \u001b[0m train.py: [15/300], [480/484], step: 7740, 2.018 samples/sec, batch_loss: 0.0700, batch_loss_c: 0.0641, batch_loss_s: 0.0839, time:19.8225, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:16:45 \u001b[32mINFO     \u001b[0m train.py: [15/300], train_loss: 0.1447, time: 786.6250, lr: 1e-05\u001b[0m\n",
            "2019-12-07 15:16:46 \u001b[32mINFO     \u001b[0m train.py: [16/300], [0/484], step: 7744, 52.353 samples/sec, batch_loss: 0.0629, batch_loss_c: 0.0514, batch_loss_s: 0.0898, time:0.7640, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:17:05 \u001b[32mINFO     \u001b[0m train.py: [16/300], [10/484], step: 7754, 2.044 samples/sec, batch_loss: 0.0717, batch_loss_c: 0.0623, batch_loss_s: 0.0938, time:19.5670, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:17:23 \u001b[32mINFO     \u001b[0m train.py: [16/300], [20/484], step: 7764, 2.303 samples/sec, batch_loss: 0.1025, batch_loss_c: 0.0948, batch_loss_s: 0.1205, time:17.3653, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:17:33 \u001b[32mINFO     \u001b[0m train.py: [16/300], [30/484], step: 7774, 4.147 samples/sec, batch_loss: 0.0720, batch_loss_c: 0.0744, batch_loss_s: 0.0666, time:9.6451, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:17:48 \u001b[32mINFO     \u001b[0m train.py: [16/300], [40/484], step: 7784, 2.577 samples/sec, batch_loss: 0.0700, batch_loss_c: 0.0664, batch_loss_s: 0.0784, time:15.5235, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:18:08 \u001b[32mINFO     \u001b[0m train.py: [16/300], [50/484], step: 7794, 2.040 samples/sec, batch_loss: 0.0644, batch_loss_c: 0.0636, batch_loss_s: 0.0662, time:19.6088, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:18:20 \u001b[32mINFO     \u001b[0m train.py: [16/300], [60/484], step: 7804, 3.290 samples/sec, batch_loss: 0.3219, batch_loss_c: 0.3034, batch_loss_s: 0.3651, time:12.1568, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:18:33 \u001b[32mINFO     \u001b[0m train.py: [16/300], [70/484], step: 7814, 3.005 samples/sec, batch_loss: 0.0732, batch_loss_c: 0.0704, batch_loss_s: 0.0797, time:13.3119, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:18:44 \u001b[32mINFO     \u001b[0m train.py: [16/300], [80/484], step: 7824, 3.814 samples/sec, batch_loss: 0.3117, batch_loss_c: 0.3110, batch_loss_s: 0.3132, time:10.4868, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:18:56 \u001b[32mINFO     \u001b[0m train.py: [16/300], [90/484], step: 7834, 3.256 samples/sec, batch_loss: 0.0749, batch_loss_c: 0.0659, batch_loss_s: 0.0956, time:12.2866, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:19:07 \u001b[32mINFO     \u001b[0m train.py: [16/300], [100/484], step: 7844, 3.628 samples/sec, batch_loss: 0.0596, batch_loss_c: 0.0552, batch_loss_s: 0.0700, time:11.0248, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:19:33 \u001b[32mINFO     \u001b[0m train.py: [16/300], [110/484], step: 7854, 1.563 samples/sec, batch_loss: 0.3802, batch_loss_c: 0.3975, batch_loss_s: 0.3398, time:25.5882, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:19:47 \u001b[32mINFO     \u001b[0m train.py: [16/300], [120/484], step: 7864, 2.839 samples/sec, batch_loss: 0.2940, batch_loss_c: 0.2890, batch_loss_s: 0.3055, time:14.0901, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:19:57 \u001b[32mINFO     \u001b[0m train.py: [16/300], [130/484], step: 7874, 4.033 samples/sec, batch_loss: 0.0514, batch_loss_c: 0.0424, batch_loss_s: 0.0722, time:9.9178, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:20:09 \u001b[32mINFO     \u001b[0m train.py: [16/300], [140/484], step: 7884, 3.234 samples/sec, batch_loss: 0.2888, batch_loss_c: 0.2853, batch_loss_s: 0.2968, time:12.3698, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:20:40 \u001b[32mINFO     \u001b[0m train.py: [16/300], [150/484], step: 7894, 1.284 samples/sec, batch_loss: 0.0734, batch_loss_c: 0.0781, batch_loss_s: 0.0623, time:31.1439, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:20:48 \u001b[32mINFO     \u001b[0m train.py: [16/300], [160/484], step: 7904, 4.783 samples/sec, batch_loss: 0.0693, batch_loss_c: 0.0667, batch_loss_s: 0.0751, time:8.3625, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:21:05 \u001b[32mINFO     \u001b[0m train.py: [16/300], [170/484], step: 7914, 2.436 samples/sec, batch_loss: 0.1834, batch_loss_c: 0.1725, batch_loss_s: 0.2088, time:16.4219, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:21:29 \u001b[32mINFO     \u001b[0m train.py: [16/300], [180/484], step: 7924, 1.686 samples/sec, batch_loss: 0.2879, batch_loss_c: 0.2840, batch_loss_s: 0.2969, time:23.7283, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:21:41 \u001b[32mINFO     \u001b[0m train.py: [16/300], [190/484], step: 7934, 3.326 samples/sec, batch_loss: 0.0328, batch_loss_c: 0.0276, batch_loss_s: 0.0450, time:12.0274, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:21:51 \u001b[32mINFO     \u001b[0m train.py: [16/300], [200/484], step: 7944, 3.949 samples/sec, batch_loss: 0.0591, batch_loss_c: 0.0525, batch_loss_s: 0.0744, time:10.1283, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:22:07 \u001b[32mINFO     \u001b[0m train.py: [16/300], [210/484], step: 7954, 2.400 samples/sec, batch_loss: 0.0584, batch_loss_c: 0.0492, batch_loss_s: 0.0799, time:16.6649, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:22:23 \u001b[32mINFO     \u001b[0m train.py: [16/300], [220/484], step: 7964, 2.480 samples/sec, batch_loss: 0.0545, batch_loss_c: 0.0479, batch_loss_s: 0.0697, time:16.1266, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:22:44 \u001b[32mINFO     \u001b[0m train.py: [16/300], [230/484], step: 7974, 1.960 samples/sec, batch_loss: 0.5305, batch_loss_c: 0.5135, batch_loss_s: 0.5702, time:20.4116, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:22:52 \u001b[32mINFO     \u001b[0m train.py: [16/300], [240/484], step: 7984, 4.854 samples/sec, batch_loss: 0.2785, batch_loss_c: 0.2747, batch_loss_s: 0.2874, time:8.2411, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:23:09 \u001b[32mINFO     \u001b[0m train.py: [16/300], [250/484], step: 7994, 2.349 samples/sec, batch_loss: 0.0665, batch_loss_c: 0.0596, batch_loss_s: 0.0826, time:17.0276, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:23:21 \u001b[32mINFO     \u001b[0m train.py: [16/300], [260/484], step: 8004, 3.350 samples/sec, batch_loss: 0.1071, batch_loss_c: 0.0968, batch_loss_s: 0.1312, time:11.9415, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:23:34 \u001b[32mINFO     \u001b[0m train.py: [16/300], [270/484], step: 8014, 3.126 samples/sec, batch_loss: 0.1693, batch_loss_c: 0.1522, batch_loss_s: 0.2092, time:12.7944, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:23:46 \u001b[32mINFO     \u001b[0m train.py: [16/300], [280/484], step: 8024, 3.351 samples/sec, batch_loss: 0.0797, batch_loss_c: 0.0758, batch_loss_s: 0.0888, time:11.9370, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:23:56 \u001b[32mINFO     \u001b[0m train.py: [16/300], [290/484], step: 8034, 4.061 samples/sec, batch_loss: 0.0546, batch_loss_c: 0.0489, batch_loss_s: 0.0678, time:9.8509, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:24:06 \u001b[32mINFO     \u001b[0m train.py: [16/300], [300/484], step: 8044, 3.796 samples/sec, batch_loss: 0.0489, batch_loss_c: 0.0368, batch_loss_s: 0.0771, time:10.5375, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:24:24 \u001b[32mINFO     \u001b[0m train.py: [16/300], [310/484], step: 8054, 2.226 samples/sec, batch_loss: 0.5661, batch_loss_c: 0.5608, batch_loss_s: 0.5785, time:17.9713, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:24:39 \u001b[32mINFO     \u001b[0m train.py: [16/300], [320/484], step: 8064, 2.618 samples/sec, batch_loss: 0.0852, batch_loss_c: 0.0661, batch_loss_s: 0.1297, time:15.2799, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:24:50 \u001b[32mINFO     \u001b[0m train.py: [16/300], [330/484], step: 8074, 3.960 samples/sec, batch_loss: 0.2033, batch_loss_c: 0.1769, batch_loss_s: 0.2648, time:10.1014, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:25:12 \u001b[32mINFO     \u001b[0m train.py: [16/300], [340/484], step: 8084, 1.772 samples/sec, batch_loss: 0.1762, batch_loss_c: 0.1566, batch_loss_s: 0.2219, time:22.5725, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:25:22 \u001b[32mINFO     \u001b[0m train.py: [16/300], [350/484], step: 8094, 3.936 samples/sec, batch_loss: 0.1018, batch_loss_c: 0.0515, batch_loss_s: 0.2191, time:10.1629, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:25:37 \u001b[32mINFO     \u001b[0m train.py: [16/300], [360/484], step: 8104, 2.686 samples/sec, batch_loss: 0.0759, batch_loss_c: 0.0697, batch_loss_s: 0.0904, time:14.8913, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:25:48 \u001b[32mINFO     \u001b[0m train.py: [16/300], [370/484], step: 8114, 3.847 samples/sec, batch_loss: 0.0773, batch_loss_c: 0.0749, batch_loss_s: 0.0828, time:10.3990, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:25:59 \u001b[32mINFO     \u001b[0m train.py: [16/300], [380/484], step: 8124, 3.456 samples/sec, batch_loss: 0.0615, batch_loss_c: 0.0453, batch_loss_s: 0.0992, time:11.5735, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:26:12 \u001b[32mINFO     \u001b[0m train.py: [16/300], [390/484], step: 8134, 3.244 samples/sec, batch_loss: 0.0664, batch_loss_c: 0.0505, batch_loss_s: 0.1035, time:12.3307, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:26:21 \u001b[32mINFO     \u001b[0m train.py: [16/300], [400/484], step: 8144, 4.272 samples/sec, batch_loss: 0.0646, batch_loss_c: 0.0550, batch_loss_s: 0.0872, time:9.3633, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:26:38 \u001b[32mINFO     \u001b[0m train.py: [16/300], [410/484], step: 8154, 2.334 samples/sec, batch_loss: 0.2088, batch_loss_c: 0.2033, batch_loss_s: 0.2219, time:17.1399, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:27:02 \u001b[32mINFO     \u001b[0m train.py: [16/300], [420/484], step: 8164, 1.652 samples/sec, batch_loss: 0.0708, batch_loss_c: 0.0651, batch_loss_s: 0.0840, time:24.2108, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:27:14 \u001b[32mINFO     \u001b[0m train.py: [16/300], [430/484], step: 8174, 3.340 samples/sec, batch_loss: 0.0719, batch_loss_c: 0.0589, batch_loss_s: 0.1021, time:11.9767, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:27:30 \u001b[32mINFO     \u001b[0m train.py: [16/300], [440/484], step: 8184, 2.461 samples/sec, batch_loss: 0.0462, batch_loss_c: 0.0390, batch_loss_s: 0.0629, time:16.2516, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:27:42 \u001b[32mINFO     \u001b[0m train.py: [16/300], [450/484], step: 8194, 3.449 samples/sec, batch_loss: 0.0901, batch_loss_c: 0.0942, batch_loss_s: 0.0804, time:11.5968, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:27:52 \u001b[32mINFO     \u001b[0m train.py: [16/300], [460/484], step: 8204, 3.878 samples/sec, batch_loss: 0.0639, batch_loss_c: 0.0642, batch_loss_s: 0.0632, time:10.3153, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:28:15 \u001b[32mINFO     \u001b[0m train.py: [16/300], [470/484], step: 8214, 1.789 samples/sec, batch_loss: 0.0589, batch_loss_c: 0.0525, batch_loss_s: 0.0740, time:22.3628, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:28:34 \u001b[32mINFO     \u001b[0m train.py: [16/300], [480/484], step: 8224, 2.129 samples/sec, batch_loss: 0.2876, batch_loss_c: 0.2771, batch_loss_s: 0.3123, time:18.7850, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:28:36 \u001b[32mINFO     \u001b[0m train.py: [16/300], train_loss: 0.1495, time: 710.9951, lr: 1e-05\u001b[0m\n",
            "2019-12-07 15:28:38 \u001b[32mINFO     \u001b[0m train.py: [17/300], [0/484], step: 8228, 25.650 samples/sec, batch_loss: 0.3037, batch_loss_c: 0.2981, batch_loss_s: 0.3168, time:1.5595, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:28:51 \u001b[32mINFO     \u001b[0m train.py: [17/300], [10/484], step: 8238, 2.999 samples/sec, batch_loss: 0.2878, batch_loss_c: 0.2827, batch_loss_s: 0.2996, time:13.3361, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:29:01 \u001b[32mINFO     \u001b[0m train.py: [17/300], [20/484], step: 8248, 4.061 samples/sec, batch_loss: 0.0698, batch_loss_c: 0.0584, batch_loss_s: 0.0964, time:9.8497, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:29:13 \u001b[32mINFO     \u001b[0m train.py: [17/300], [30/484], step: 8258, 3.306 samples/sec, batch_loss: 0.1244, batch_loss_c: 0.1128, batch_loss_s: 0.1516, time:12.0993, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:29:35 \u001b[32mINFO     \u001b[0m train.py: [17/300], [40/484], step: 8268, 1.855 samples/sec, batch_loss: 0.0617, batch_loss_c: 0.0549, batch_loss_s: 0.0776, time:21.5636, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:29:47 \u001b[32mINFO     \u001b[0m train.py: [17/300], [50/484], step: 8278, 3.288 samples/sec, batch_loss: 0.1095, batch_loss_c: 0.1029, batch_loss_s: 0.1249, time:12.1671, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:29:58 \u001b[32mINFO     \u001b[0m train.py: [17/300], [60/484], step: 8288, 3.827 samples/sec, batch_loss: 0.1971, batch_loss_c: 0.1886, batch_loss_s: 0.2170, time:10.4514, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:30:19 \u001b[32mINFO     \u001b[0m train.py: [17/300], [70/484], step: 8298, 1.903 samples/sec, batch_loss: 0.3759, batch_loss_c: 0.3111, batch_loss_s: 0.5270, time:21.0193, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:30:28 \u001b[32mINFO     \u001b[0m train.py: [17/300], [80/484], step: 8308, 4.156 samples/sec, batch_loss: 0.0472, batch_loss_c: 0.0434, batch_loss_s: 0.0561, time:9.6241, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:30:41 \u001b[32mINFO     \u001b[0m train.py: [17/300], [90/484], step: 8318, 3.208 samples/sec, batch_loss: 0.2083, batch_loss_c: 0.1981, batch_loss_s: 0.2320, time:12.4683, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:30:55 \u001b[32mINFO     \u001b[0m train.py: [17/300], [100/484], step: 8328, 2.789 samples/sec, batch_loss: 0.0833, batch_loss_c: 0.0867, batch_loss_s: 0.0755, time:14.3408, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:31:05 \u001b[32mINFO     \u001b[0m train.py: [17/300], [110/484], step: 8338, 3.961 samples/sec, batch_loss: 0.0627, batch_loss_c: 0.0532, batch_loss_s: 0.0849, time:10.0993, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:31:29 \u001b[32mINFO     \u001b[0m train.py: [17/300], [120/484], step: 8348, 1.688 samples/sec, batch_loss: 0.0823, batch_loss_c: 0.0813, batch_loss_s: 0.0847, time:23.7023, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:31:44 \u001b[32mINFO     \u001b[0m train.py: [17/300], [130/484], step: 8358, 2.585 samples/sec, batch_loss: 0.0391, batch_loss_c: 0.0321, batch_loss_s: 0.0556, time:15.4748, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:31:58 \u001b[32mINFO     \u001b[0m train.py: [17/300], [140/484], step: 8368, 2.892 samples/sec, batch_loss: 0.2148, batch_loss_c: 0.2394, batch_loss_s: 0.1574, time:13.8326, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:32:09 \u001b[32mINFO     \u001b[0m train.py: [17/300], [150/484], step: 8378, 3.855 samples/sec, batch_loss: 0.0760, batch_loss_c: 0.0709, batch_loss_s: 0.0877, time:10.3769, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:32:20 \u001b[32mINFO     \u001b[0m train.py: [17/300], [160/484], step: 8388, 3.373 samples/sec, batch_loss: 0.0788, batch_loss_c: 0.0821, batch_loss_s: 0.0711, time:11.8593, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:32:32 \u001b[32mINFO     \u001b[0m train.py: [17/300], [170/484], step: 8398, 3.323 samples/sec, batch_loss: 0.2965, batch_loss_c: 0.2913, batch_loss_s: 0.3086, time:12.0378, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:32:48 \u001b[32mINFO     \u001b[0m train.py: [17/300], [180/484], step: 8408, 2.549 samples/sec, batch_loss: 0.0910, batch_loss_c: 0.0856, batch_loss_s: 0.1036, time:15.6897, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:33:01 \u001b[32mINFO     \u001b[0m train.py: [17/300], [190/484], step: 8418, 3.175 samples/sec, batch_loss: 0.3456, batch_loss_c: 0.3591, batch_loss_s: 0.3141, time:12.5997, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:33:11 \u001b[32mINFO     \u001b[0m train.py: [17/300], [200/484], step: 8428, 3.776 samples/sec, batch_loss: 0.1373, batch_loss_c: 0.1456, batch_loss_s: 0.1180, time:10.5941, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:33:24 \u001b[32mINFO     \u001b[0m train.py: [17/300], [210/484], step: 8438, 3.167 samples/sec, batch_loss: 0.1906, batch_loss_c: 0.1868, batch_loss_s: 0.1995, time:12.6301, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:33:33 \u001b[32mINFO     \u001b[0m train.py: [17/300], [220/484], step: 8448, 4.256 samples/sec, batch_loss: 0.0956, batch_loss_c: 0.0829, batch_loss_s: 0.1253, time:9.3993, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:33:47 \u001b[32mINFO     \u001b[0m train.py: [17/300], [230/484], step: 8458, 2.871 samples/sec, batch_loss: 0.0697, batch_loss_c: 0.0655, batch_loss_s: 0.0796, time:13.9313, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:34:06 \u001b[32mINFO     \u001b[0m train.py: [17/300], [240/484], step: 8468, 2.164 samples/sec, batch_loss: 0.2960, batch_loss_c: 0.2928, batch_loss_s: 0.3037, time:18.4852, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:34:31 \u001b[32mINFO     \u001b[0m train.py: [17/300], [250/484], step: 8478, 1.595 samples/sec, batch_loss: 0.0533, batch_loss_c: 0.0474, batch_loss_s: 0.0670, time:25.0793, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:34:49 \u001b[32mINFO     \u001b[0m train.py: [17/300], [260/484], step: 8488, 2.214 samples/sec, batch_loss: 0.0826, batch_loss_c: 0.0710, batch_loss_s: 0.1096, time:18.0709, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:35:16 \u001b[32mINFO     \u001b[0m train.py: [17/300], [270/484], step: 8498, 1.472 samples/sec, batch_loss: 0.4507, batch_loss_c: 0.4584, batch_loss_s: 0.4327, time:27.1684, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:35:26 \u001b[32mINFO     \u001b[0m train.py: [17/300], [280/484], step: 8508, 3.839 samples/sec, batch_loss: 0.1198, batch_loss_c: 0.1226, batch_loss_s: 0.1132, time:10.4187, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:35:36 \u001b[32mINFO     \u001b[0m train.py: [17/300], [290/484], step: 8518, 4.173 samples/sec, batch_loss: 0.0765, batch_loss_c: 0.0749, batch_loss_s: 0.0804, time:9.5856, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:35:51 \u001b[32mINFO     \u001b[0m train.py: [17/300], [300/484], step: 8528, 2.750 samples/sec, batch_loss: 0.0856, batch_loss_c: 0.0797, batch_loss_s: 0.0994, time:14.5454, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:36:00 \u001b[32mINFO     \u001b[0m train.py: [17/300], [310/484], step: 8538, 4.467 samples/sec, batch_loss: 0.0909, batch_loss_c: 0.0935, batch_loss_s: 0.0849, time:8.9553, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:36:26 \u001b[32mINFO     \u001b[0m train.py: [17/300], [320/484], step: 8548, 1.491 samples/sec, batch_loss: 0.4124, batch_loss_c: 0.4298, batch_loss_s: 0.3717, time:26.8332, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:36:41 \u001b[32mINFO     \u001b[0m train.py: [17/300], [330/484], step: 8558, 2.711 samples/sec, batch_loss: 0.1049, batch_loss_c: 0.1002, batch_loss_s: 0.1160, time:14.7537, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:37:00 \u001b[32mINFO     \u001b[0m train.py: [17/300], [340/484], step: 8568, 2.087 samples/sec, batch_loss: 0.0625, batch_loss_c: 0.0517, batch_loss_s: 0.0877, time:19.1677, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:37:12 \u001b[32mINFO     \u001b[0m train.py: [17/300], [350/484], step: 8578, 3.474 samples/sec, batch_loss: 0.0502, batch_loss_c: 0.0462, batch_loss_s: 0.0596, time:11.5127, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:37:32 \u001b[32mINFO     \u001b[0m train.py: [17/300], [360/484], step: 8588, 1.984 samples/sec, batch_loss: 0.1716, batch_loss_c: 0.1788, batch_loss_s: 0.1547, time:20.1625, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:37:57 \u001b[32mINFO     \u001b[0m train.py: [17/300], [370/484], step: 8598, 1.588 samples/sec, batch_loss: 0.0561, batch_loss_c: 0.0521, batch_loss_s: 0.0656, time:25.1858, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:38:10 \u001b[32mINFO     \u001b[0m train.py: [17/300], [380/484], step: 8608, 3.133 samples/sec, batch_loss: 0.0838, batch_loss_c: 0.0728, batch_loss_s: 0.1095, time:12.7657, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:38:27 \u001b[32mINFO     \u001b[0m train.py: [17/300], [390/484], step: 8618, 2.379 samples/sec, batch_loss: 0.2017, batch_loss_c: 0.2080, batch_loss_s: 0.1871, time:16.8103, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:38:38 \u001b[32mINFO     \u001b[0m train.py: [17/300], [400/484], step: 8628, 3.479 samples/sec, batch_loss: 0.1405, batch_loss_c: 0.1464, batch_loss_s: 0.1266, time:11.4976, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:38:49 \u001b[32mINFO     \u001b[0m train.py: [17/300], [410/484], step: 8638, 3.809 samples/sec, batch_loss: 0.2842, batch_loss_c: 0.2814, batch_loss_s: 0.2906, time:10.5005, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:39:04 \u001b[32mINFO     \u001b[0m train.py: [17/300], [420/484], step: 8648, 2.686 samples/sec, batch_loss: 0.0670, batch_loss_c: 0.0546, batch_loss_s: 0.0960, time:14.8930, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:39:20 \u001b[32mINFO     \u001b[0m train.py: [17/300], [430/484], step: 8658, 2.411 samples/sec, batch_loss: 0.2040, batch_loss_c: 0.1860, batch_loss_s: 0.2458, time:16.5897, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:39:41 \u001b[32mINFO     \u001b[0m train.py: [17/300], [440/484], step: 8668, 1.963 samples/sec, batch_loss: 0.2280, batch_loss_c: 0.2501, batch_loss_s: 0.1765, time:20.3772, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:39:52 \u001b[32mINFO     \u001b[0m train.py: [17/300], [450/484], step: 8678, 3.577 samples/sec, batch_loss: 0.1031, batch_loss_c: 0.0934, batch_loss_s: 0.1259, time:11.1827, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:40:16 \u001b[32mINFO     \u001b[0m train.py: [17/300], [460/484], step: 8688, 1.629 samples/sec, batch_loss: 0.0588, batch_loss_c: 0.0502, batch_loss_s: 0.0786, time:24.5546, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:40:32 \u001b[32mINFO     \u001b[0m train.py: [17/300], [470/484], step: 8698, 2.629 samples/sec, batch_loss: 0.0470, batch_loss_c: 0.0424, batch_loss_s: 0.0575, time:15.2154, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:40:45 \u001b[32mINFO     \u001b[0m train.py: [17/300], [480/484], step: 8708, 2.977 samples/sec, batch_loss: 0.0469, batch_loss_c: 0.0400, batch_loss_s: 0.0631, time:13.4344, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:40:48 \u001b[32mINFO     \u001b[0m train.py: [17/300], train_loss: 0.1454, time: 731.0668, lr: 1e-05\u001b[0m\n",
            "2019-12-07 15:40:49 \u001b[32mINFO     \u001b[0m train.py: [18/300], [0/484], step: 8712, 41.993 samples/sec, batch_loss: 0.0639, batch_loss_c: 0.0590, batch_loss_s: 0.0752, time:0.9525, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:41:05 \u001b[32mINFO     \u001b[0m train.py: [18/300], [10/484], step: 8722, 2.504 samples/sec, batch_loss: 0.0743, batch_loss_c: 0.0724, batch_loss_s: 0.0787, time:15.9760, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:41:22 \u001b[32mINFO     \u001b[0m train.py: [18/300], [20/484], step: 8732, 2.386 samples/sec, batch_loss: 0.0628, batch_loss_c: 0.0541, batch_loss_s: 0.0832, time:16.7651, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:41:30 \u001b[32mINFO     \u001b[0m train.py: [18/300], [30/484], step: 8742, 4.801 samples/sec, batch_loss: 0.0525, batch_loss_c: 0.0454, batch_loss_s: 0.0693, time:8.3316, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:41:50 \u001b[32mINFO     \u001b[0m train.py: [18/300], [40/484], step: 8752, 1.959 samples/sec, batch_loss: 0.0544, batch_loss_c: 0.0466, batch_loss_s: 0.0725, time:20.4210, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:42:06 \u001b[32mINFO     \u001b[0m train.py: [18/300], [50/484], step: 8762, 2.618 samples/sec, batch_loss: 0.0515, batch_loss_c: 0.0377, batch_loss_s: 0.0836, time:15.2786, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:42:16 \u001b[32mINFO     \u001b[0m train.py: [18/300], [60/484], step: 8772, 3.831 samples/sec, batch_loss: 0.0758, batch_loss_c: 0.0637, batch_loss_s: 0.1043, time:10.4407, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:42:40 \u001b[32mINFO     \u001b[0m train.py: [18/300], [70/484], step: 8782, 1.675 samples/sec, batch_loss: 0.2323, batch_loss_c: 0.2175, batch_loss_s: 0.2666, time:23.8784, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:42:59 \u001b[32mINFO     \u001b[0m train.py: [18/300], [80/484], step: 8792, 2.077 samples/sec, batch_loss: 0.2948, batch_loss_c: 0.2911, batch_loss_s: 0.3036, time:19.2593, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:43:14 \u001b[32mINFO     \u001b[0m train.py: [18/300], [90/484], step: 8802, 2.676 samples/sec, batch_loss: 0.0485, batch_loss_c: 0.0431, batch_loss_s: 0.0611, time:14.9463, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:43:26 \u001b[32mINFO     \u001b[0m train.py: [18/300], [100/484], step: 8812, 3.391 samples/sec, batch_loss: 0.0536, batch_loss_c: 0.0455, batch_loss_s: 0.0727, time:11.7957, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:43:40 \u001b[32mINFO     \u001b[0m train.py: [18/300], [110/484], step: 8822, 2.918 samples/sec, batch_loss: 0.1037, batch_loss_c: 0.0908, batch_loss_s: 0.1336, time:13.7072, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:43:56 \u001b[32mINFO     \u001b[0m train.py: [18/300], [120/484], step: 8832, 2.442 samples/sec, batch_loss: 0.1077, batch_loss_c: 0.0972, batch_loss_s: 0.1321, time:16.3781, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:44:16 \u001b[32mINFO     \u001b[0m train.py: [18/300], [130/484], step: 8842, 2.003 samples/sec, batch_loss: 0.0638, batch_loss_c: 0.0516, batch_loss_s: 0.0922, time:19.9664, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:44:28 \u001b[32mINFO     \u001b[0m train.py: [18/300], [140/484], step: 8852, 3.321 samples/sec, batch_loss: 0.3131, batch_loss_c: 0.3053, batch_loss_s: 0.3312, time:12.0459, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:44:44 \u001b[32mINFO     \u001b[0m train.py: [18/300], [150/484], step: 8862, 2.573 samples/sec, batch_loss: 0.0619, batch_loss_c: 0.0519, batch_loss_s: 0.0853, time:15.5462, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:45:07 \u001b[32mINFO     \u001b[0m train.py: [18/300], [160/484], step: 8872, 1.684 samples/sec, batch_loss: 0.0720, batch_loss_c: 0.0396, batch_loss_s: 0.1478, time:23.7559, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:45:20 \u001b[32mINFO     \u001b[0m train.py: [18/300], [170/484], step: 8882, 3.234 samples/sec, batch_loss: 0.3193, batch_loss_c: 0.3171, batch_loss_s: 0.3246, time:12.3686, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:45:29 \u001b[32mINFO     \u001b[0m train.py: [18/300], [180/484], step: 8892, 4.450 samples/sec, batch_loss: 0.0771, batch_loss_c: 0.0701, batch_loss_s: 0.0934, time:8.9897, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:45:37 \u001b[32mINFO     \u001b[0m train.py: [18/300], [190/484], step: 8902, 4.933 samples/sec, batch_loss: 0.0768, batch_loss_c: 0.0740, batch_loss_s: 0.0833, time:8.1094, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:45:47 \u001b[32mINFO     \u001b[0m train.py: [18/300], [200/484], step: 8912, 4.149 samples/sec, batch_loss: 0.0704, batch_loss_c: 0.0499, batch_loss_s: 0.1182, time:9.6419, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:46:26 \u001b[32mINFO     \u001b[0m train.py: [18/300], [210/484], step: 8922, 1.020 samples/sec, batch_loss: 0.1370, batch_loss_c: 0.1393, batch_loss_s: 0.1316, time:39.2175, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:46:41 \u001b[32mINFO     \u001b[0m train.py: [18/300], [220/484], step: 8932, 2.602 samples/sec, batch_loss: 0.2943, batch_loss_c: 0.2919, batch_loss_s: 0.2998, time:15.3706, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:47:05 \u001b[32mINFO     \u001b[0m train.py: [18/300], [230/484], step: 8942, 1.668 samples/sec, batch_loss: 0.3477, batch_loss_c: 0.3487, batch_loss_s: 0.3451, time:23.9749, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:47:21 \u001b[32mINFO     \u001b[0m train.py: [18/300], [240/484], step: 8952, 2.536 samples/sec, batch_loss: 0.2141, batch_loss_c: 0.2030, batch_loss_s: 0.2399, time:15.7732, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:47:34 \u001b[32mINFO     \u001b[0m train.py: [18/300], [250/484], step: 8962, 2.960 samples/sec, batch_loss: 0.7169, batch_loss_c: 0.6975, batch_loss_s: 0.7622, time:13.5132, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:47:56 \u001b[32mINFO     \u001b[0m train.py: [18/300], [260/484], step: 8972, 1.896 samples/sec, batch_loss: 0.0691, batch_loss_c: 0.0526, batch_loss_s: 0.1076, time:21.0967, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:48:12 \u001b[32mINFO     \u001b[0m train.py: [18/300], [270/484], step: 8982, 2.478 samples/sec, batch_loss: 0.2594, batch_loss_c: 0.2434, batch_loss_s: 0.2970, time:16.1394, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:48:43 \u001b[32mINFO     \u001b[0m train.py: [18/300], [280/484], step: 8992, 1.263 samples/sec, batch_loss: 0.0496, batch_loss_c: 0.0454, batch_loss_s: 0.0593, time:31.6598, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:48:54 \u001b[32mINFO     \u001b[0m train.py: [18/300], [290/484], step: 9002, 3.680 samples/sec, batch_loss: 0.0633, batch_loss_c: 0.0537, batch_loss_s: 0.0858, time:10.8691, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:49:06 \u001b[32mINFO     \u001b[0m train.py: [18/300], [300/484], step: 9012, 3.339 samples/sec, batch_loss: 0.1803, batch_loss_c: 0.1427, batch_loss_s: 0.2679, time:11.9811, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:49:33 \u001b[32mINFO     \u001b[0m train.py: [18/300], [310/484], step: 9022, 1.478 samples/sec, batch_loss: 0.0796, batch_loss_c: 0.0780, batch_loss_s: 0.0834, time:27.0620, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:49:45 \u001b[32mINFO     \u001b[0m train.py: [18/300], [320/484], step: 9032, 3.310 samples/sec, batch_loss: 0.5541, batch_loss_c: 0.5560, batch_loss_s: 0.5497, time:12.0836, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:50:01 \u001b[32mINFO     \u001b[0m train.py: [18/300], [330/484], step: 9042, 2.537 samples/sec, batch_loss: 0.0880, batch_loss_c: 0.0712, batch_loss_s: 0.1270, time:15.7681, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:50:17 \u001b[32mINFO     \u001b[0m train.py: [18/300], [340/484], step: 9052, 2.454 samples/sec, batch_loss: 0.0683, batch_loss_c: 0.0674, batch_loss_s: 0.0705, time:16.2989, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:50:32 \u001b[32mINFO     \u001b[0m train.py: [18/300], [350/484], step: 9062, 2.670 samples/sec, batch_loss: 0.2768, batch_loss_c: 0.2701, batch_loss_s: 0.2923, time:14.9794, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:50:45 \u001b[32mINFO     \u001b[0m train.py: [18/300], [360/484], step: 9072, 3.075 samples/sec, batch_loss: 0.2780, batch_loss_c: 0.2678, batch_loss_s: 0.3019, time:13.0080, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:51:06 \u001b[32mINFO     \u001b[0m train.py: [18/300], [370/484], step: 9082, 1.978 samples/sec, batch_loss: 0.3304, batch_loss_c: 0.3288, batch_loss_s: 0.3342, time:20.2271, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:51:18 \u001b[32mINFO     \u001b[0m train.py: [18/300], [380/484], step: 9092, 3.321 samples/sec, batch_loss: 0.0360, batch_loss_c: 0.0298, batch_loss_s: 0.0506, time:12.0452, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:51:30 \u001b[32mINFO     \u001b[0m train.py: [18/300], [390/484], step: 9102, 3.245 samples/sec, batch_loss: 0.0644, batch_loss_c: 0.0594, batch_loss_s: 0.0760, time:12.3255, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:51:47 \u001b[32mINFO     \u001b[0m train.py: [18/300], [400/484], step: 9112, 2.339 samples/sec, batch_loss: 0.2834, batch_loss_c: 0.2806, batch_loss_s: 0.2898, time:17.1009, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:52:03 \u001b[32mINFO     \u001b[0m train.py: [18/300], [410/484], step: 9122, 2.512 samples/sec, batch_loss: 0.1246, batch_loss_c: 0.1425, batch_loss_s: 0.0828, time:15.9258, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:52:14 \u001b[32mINFO     \u001b[0m train.py: [18/300], [420/484], step: 9132, 3.641 samples/sec, batch_loss: 0.0989, batch_loss_c: 0.1011, batch_loss_s: 0.0936, time:10.9874, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:52:35 \u001b[32mINFO     \u001b[0m train.py: [18/300], [430/484], step: 9142, 1.885 samples/sec, batch_loss: 0.0637, batch_loss_c: 0.0557, batch_loss_s: 0.0825, time:21.2248, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:52:48 \u001b[32mINFO     \u001b[0m train.py: [18/300], [440/484], step: 9152, 3.219 samples/sec, batch_loss: 0.0611, batch_loss_c: 0.0463, batch_loss_s: 0.0956, time:12.4249, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:52:59 \u001b[32mINFO     \u001b[0m train.py: [18/300], [450/484], step: 9162, 3.457 samples/sec, batch_loss: 0.0704, batch_loss_c: 0.0596, batch_loss_s: 0.0954, time:11.5710, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:53:10 \u001b[32mINFO     \u001b[0m train.py: [18/300], [460/484], step: 9172, 3.812 samples/sec, batch_loss: 0.3943, batch_loss_c: 0.3781, batch_loss_s: 0.4321, time:10.4919, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:53:26 \u001b[32mINFO     \u001b[0m train.py: [18/300], [470/484], step: 9182, 2.516 samples/sec, batch_loss: 0.1314, batch_loss_c: 0.1316, batch_loss_s: 0.1310, time:15.8996, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:53:36 \u001b[32mINFO     \u001b[0m train.py: [18/300], [480/484], step: 9192, 3.815 samples/sec, batch_loss: 0.0599, batch_loss_c: 0.0513, batch_loss_s: 0.0802, time:10.4838, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:53:41 \u001b[32mINFO     \u001b[0m train.py: [18/300], train_loss: 0.1474, time: 772.5002, lr: 1e-05\u001b[0m\n",
            "2019-12-07 15:53:42 \u001b[32mINFO     \u001b[0m train.py: [19/300], [0/484], step: 9196, 43.378 samples/sec, batch_loss: 0.1481, batch_loss_c: 0.1093, batch_loss_s: 0.2385, time:0.9221, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:53:50 \u001b[32mINFO     \u001b[0m train.py: [19/300], [10/484], step: 9206, 4.650 samples/sec, batch_loss: 0.1960, batch_loss_c: 0.1837, batch_loss_s: 0.2250, time:8.6027, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:54:06 \u001b[32mINFO     \u001b[0m train.py: [19/300], [20/484], step: 9216, 2.602 samples/sec, batch_loss: 0.2892, batch_loss_c: 0.2823, batch_loss_s: 0.3050, time:15.3710, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:54:17 \u001b[32mINFO     \u001b[0m train.py: [19/300], [30/484], step: 9226, 3.596 samples/sec, batch_loss: 0.0603, batch_loss_c: 0.0592, batch_loss_s: 0.0629, time:11.1240, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:54:30 \u001b[32mINFO     \u001b[0m train.py: [19/300], [40/484], step: 9236, 3.168 samples/sec, batch_loss: 0.0632, batch_loss_c: 0.0532, batch_loss_s: 0.0865, time:12.6247, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:54:45 \u001b[32mINFO     \u001b[0m train.py: [19/300], [50/484], step: 9246, 2.585 samples/sec, batch_loss: 0.0932, batch_loss_c: 0.0915, batch_loss_s: 0.0973, time:15.4743, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:54:54 \u001b[32mINFO     \u001b[0m train.py: [19/300], [60/484], step: 9256, 4.412 samples/sec, batch_loss: 0.0705, batch_loss_c: 0.0542, batch_loss_s: 0.1086, time:9.0659, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:55:05 \u001b[32mINFO     \u001b[0m train.py: [19/300], [70/484], step: 9266, 3.549 samples/sec, batch_loss: 0.0879, batch_loss_c: 0.0786, batch_loss_s: 0.1095, time:11.2709, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:55:17 \u001b[32mINFO     \u001b[0m train.py: [19/300], [80/484], step: 9276, 3.449 samples/sec, batch_loss: 0.0907, batch_loss_c: 0.0766, batch_loss_s: 0.1234, time:11.5976, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:55:33 \u001b[32mINFO     \u001b[0m train.py: [19/300], [90/484], step: 9286, 2.540 samples/sec, batch_loss: 0.1640, batch_loss_c: 0.1794, batch_loss_s: 0.1279, time:15.7510, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:55:46 \u001b[32mINFO     \u001b[0m train.py: [19/300], [100/484], step: 9296, 2.932 samples/sec, batch_loss: 0.2939, batch_loss_c: 0.2875, batch_loss_s: 0.3089, time:13.6425, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:55:59 \u001b[32mINFO     \u001b[0m train.py: [19/300], [110/484], step: 9306, 3.053 samples/sec, batch_loss: 0.1048, batch_loss_c: 0.0981, batch_loss_s: 0.1204, time:13.0999, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:56:13 \u001b[32mINFO     \u001b[0m train.py: [19/300], [120/484], step: 9316, 2.895 samples/sec, batch_loss: 0.3540, batch_loss_c: 0.3583, batch_loss_s: 0.3440, time:13.8185, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:56:31 \u001b[32mINFO     \u001b[0m train.py: [19/300], [130/484], step: 9326, 2.234 samples/sec, batch_loss: 0.0638, batch_loss_c: 0.0504, batch_loss_s: 0.0951, time:17.9086, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:56:54 \u001b[32mINFO     \u001b[0m train.py: [19/300], [140/484], step: 9336, 1.784 samples/sec, batch_loss: 0.3236, batch_loss_c: 0.3264, batch_loss_s: 0.3170, time:22.4234, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:57:06 \u001b[32mINFO     \u001b[0m train.py: [19/300], [150/484], step: 9346, 3.161 samples/sec, batch_loss: 0.0627, batch_loss_c: 0.0615, batch_loss_s: 0.0653, time:12.6556, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:58:02 \u001b[32mINFO     \u001b[0m train.py: [19/300], [160/484], step: 9356, 0.714 samples/sec, batch_loss: 0.0749, batch_loss_c: 0.0706, batch_loss_s: 0.0847, time:56.0282, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:58:16 \u001b[32mINFO     \u001b[0m train.py: [19/300], [170/484], step: 9366, 3.023 samples/sec, batch_loss: 0.1125, batch_loss_c: 0.0857, batch_loss_s: 0.1750, time:13.2327, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:58:34 \u001b[32mINFO     \u001b[0m train.py: [19/300], [180/484], step: 9376, 2.210 samples/sec, batch_loss: 0.0767, batch_loss_c: 0.0729, batch_loss_s: 0.0857, time:18.1023, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:58:45 \u001b[32mINFO     \u001b[0m train.py: [19/300], [190/484], step: 9386, 3.439 samples/sec, batch_loss: 0.1134, batch_loss_c: 0.1124, batch_loss_s: 0.1158, time:11.6316, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:58:55 \u001b[32mINFO     \u001b[0m train.py: [19/300], [200/484], step: 9396, 3.971 samples/sec, batch_loss: 0.3195, batch_loss_c: 0.3077, batch_loss_s: 0.3472, time:10.0739, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:59:12 \u001b[32mINFO     \u001b[0m train.py: [19/300], [210/484], step: 9406, 2.466 samples/sec, batch_loss: 0.2885, batch_loss_c: 0.2826, batch_loss_s: 0.3022, time:16.2233, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:59:21 \u001b[32mINFO     \u001b[0m train.py: [19/300], [220/484], step: 9416, 4.231 samples/sec, batch_loss: 0.0655, batch_loss_c: 0.0606, batch_loss_s: 0.0769, time:9.4546, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:59:37 \u001b[32mINFO     \u001b[0m train.py: [19/300], [230/484], step: 9426, 2.552 samples/sec, batch_loss: 0.1267, batch_loss_c: 0.1185, batch_loss_s: 0.1456, time:15.6748, lr:1e-05\u001b[0m\n",
            "2019-12-07 15:59:51 \u001b[32mINFO     \u001b[0m train.py: [19/300], [240/484], step: 9436, 2.884 samples/sec, batch_loss: 0.0380, batch_loss_c: 0.0313, batch_loss_s: 0.0536, time:13.8715, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:00:12 \u001b[32mINFO     \u001b[0m train.py: [19/300], [250/484], step: 9446, 1.837 samples/sec, batch_loss: 0.0455, batch_loss_c: 0.0390, batch_loss_s: 0.0606, time:21.7705, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:00:27 \u001b[32mINFO     \u001b[0m train.py: [19/300], [260/484], step: 9456, 2.704 samples/sec, batch_loss: 0.0599, batch_loss_c: 0.0540, batch_loss_s: 0.0737, time:14.7947, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:00:45 \u001b[32mINFO     \u001b[0m train.py: [19/300], [270/484], step: 9466, 2.238 samples/sec, batch_loss: 0.3271, batch_loss_c: 0.3279, batch_loss_s: 0.3253, time:17.8699, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:00:53 \u001b[32mINFO     \u001b[0m train.py: [19/300], [280/484], step: 9476, 4.921 samples/sec, batch_loss: 0.0618, batch_loss_c: 0.0514, batch_loss_s: 0.0860, time:8.1281, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:01:15 \u001b[32mINFO     \u001b[0m train.py: [19/300], [290/484], step: 9486, 1.865 samples/sec, batch_loss: 0.3070, batch_loss_c: 0.3061, batch_loss_s: 0.3092, time:21.4431, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:01:27 \u001b[32mINFO     \u001b[0m train.py: [19/300], [300/484], step: 9496, 3.145 samples/sec, batch_loss: 0.1022, batch_loss_c: 0.0908, batch_loss_s: 0.1288, time:12.7206, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:01:38 \u001b[32mINFO     \u001b[0m train.py: [19/300], [310/484], step: 9506, 3.743 samples/sec, batch_loss: 0.1378, batch_loss_c: 0.1201, batch_loss_s: 0.1791, time:10.6853, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:01:48 \u001b[32mINFO     \u001b[0m train.py: [19/300], [320/484], step: 9516, 3.975 samples/sec, batch_loss: 0.0602, batch_loss_c: 0.0515, batch_loss_s: 0.0804, time:10.0641, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:02:00 \u001b[32mINFO     \u001b[0m train.py: [19/300], [330/484], step: 9526, 3.343 samples/sec, batch_loss: 0.0480, batch_loss_c: 0.0460, batch_loss_s: 0.0527, time:11.9662, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:02:11 \u001b[32mINFO     \u001b[0m train.py: [19/300], [340/484], step: 9536, 3.697 samples/sec, batch_loss: 0.0762, batch_loss_c: 0.0558, batch_loss_s: 0.1239, time:10.8182, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:02:40 \u001b[32mINFO     \u001b[0m train.py: [19/300], [350/484], step: 9546, 1.393 samples/sec, batch_loss: 0.0473, batch_loss_c: 0.0435, batch_loss_s: 0.0561, time:28.7186, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:02:50 \u001b[32mINFO     \u001b[0m train.py: [19/300], [360/484], step: 9556, 3.648 samples/sec, batch_loss: 0.0710, batch_loss_c: 0.0591, batch_loss_s: 0.0990, time:10.9637, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:03:08 \u001b[32mINFO     \u001b[0m train.py: [19/300], [370/484], step: 9566, 2.269 samples/sec, batch_loss: 0.0722, batch_loss_c: 0.0611, batch_loss_s: 0.0979, time:17.6288, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:03:30 \u001b[32mINFO     \u001b[0m train.py: [19/300], [380/484], step: 9576, 1.832 samples/sec, batch_loss: 0.1205, batch_loss_c: 0.1083, batch_loss_s: 0.1490, time:21.8370, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:03:44 \u001b[32mINFO     \u001b[0m train.py: [19/300], [390/484], step: 9586, 2.796 samples/sec, batch_loss: 0.2670, batch_loss_c: 0.2571, batch_loss_s: 0.2901, time:14.3050, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:03:59 \u001b[32mINFO     \u001b[0m train.py: [19/300], [400/484], step: 9596, 2.706 samples/sec, batch_loss: 0.0722, batch_loss_c: 0.0694, batch_loss_s: 0.0789, time:14.7798, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:04:14 \u001b[32mINFO     \u001b[0m train.py: [19/300], [410/484], step: 9606, 2.713 samples/sec, batch_loss: 0.0497, batch_loss_c: 0.0436, batch_loss_s: 0.0639, time:14.7449, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:04:27 \u001b[32mINFO     \u001b[0m train.py: [19/300], [420/484], step: 9616, 2.979 samples/sec, batch_loss: 0.2328, batch_loss_c: 0.2130, batch_loss_s: 0.2790, time:13.4287, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:04:45 \u001b[32mINFO     \u001b[0m train.py: [19/300], [430/484], step: 9626, 2.203 samples/sec, batch_loss: 0.0829, batch_loss_c: 0.0745, batch_loss_s: 0.1024, time:18.1552, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:04:58 \u001b[32mINFO     \u001b[0m train.py: [19/300], [440/484], step: 9636, 3.298 samples/sec, batch_loss: 0.0528, batch_loss_c: 0.0425, batch_loss_s: 0.0771, time:12.1273, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:05:27 \u001b[32mINFO     \u001b[0m train.py: [19/300], [450/484], step: 9646, 1.374 samples/sec, batch_loss: 0.0740, batch_loss_c: 0.0626, batch_loss_s: 0.1006, time:29.1022, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:05:38 \u001b[32mINFO     \u001b[0m train.py: [19/300], [460/484], step: 9656, 3.389 samples/sec, batch_loss: 0.0807, batch_loss_c: 0.0822, batch_loss_s: 0.0772, time:11.8044, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:05:50 \u001b[32mINFO     \u001b[0m train.py: [19/300], [470/484], step: 9666, 3.359 samples/sec, batch_loss: 0.2959, batch_loss_c: 0.2906, batch_loss_s: 0.3081, time:11.9087, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:06:12 \u001b[32mINFO     \u001b[0m train.py: [19/300], [480/484], step: 9676, 1.825 samples/sec, batch_loss: 0.0699, batch_loss_c: 0.0601, batch_loss_s: 0.0928, time:21.9143, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:06:15 \u001b[32mINFO     \u001b[0m train.py: [19/300], train_loss: 0.1479, time: 754.6129, lr: 1e-05\u001b[0m\n",
            "2019-12-07 16:06:17 \u001b[32mINFO     \u001b[0m train.py: [20/300], [0/484], step: 9680, 55.179 samples/sec, batch_loss: 0.0700, batch_loss_c: 0.0597, batch_loss_s: 0.0939, time:0.7249, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:06:33 \u001b[32mINFO     \u001b[0m train.py: [20/300], [10/484], step: 9690, 2.441 samples/sec, batch_loss: 0.0465, batch_loss_c: 0.0392, batch_loss_s: 0.0635, time:16.3879, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:06:44 \u001b[32mINFO     \u001b[0m train.py: [20/300], [20/484], step: 9700, 3.720 samples/sec, batch_loss: 0.1565, batch_loss_c: 0.1523, batch_loss_s: 0.1664, time:10.7522, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:07:10 \u001b[32mINFO     \u001b[0m train.py: [20/300], [30/484], step: 9710, 1.504 samples/sec, batch_loss: 0.2769, batch_loss_c: 0.2732, batch_loss_s: 0.2854, time:26.5979, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:07:23 \u001b[32mINFO     \u001b[0m train.py: [20/300], [40/484], step: 9720, 3.225 samples/sec, batch_loss: 0.0562, batch_loss_c: 0.0499, batch_loss_s: 0.0708, time:12.4023, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:07:31 \u001b[32mINFO     \u001b[0m train.py: [20/300], [50/484], step: 9730, 4.638 samples/sec, batch_loss: 0.0497, batch_loss_c: 0.0379, batch_loss_s: 0.0772, time:8.6239, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:08:00 \u001b[32mINFO     \u001b[0m train.py: [20/300], [60/484], step: 9740, 1.406 samples/sec, batch_loss: 0.1359, batch_loss_c: 0.1446, batch_loss_s: 0.1156, time:28.4564, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:08:12 \u001b[32mINFO     \u001b[0m train.py: [20/300], [70/484], step: 9750, 3.326 samples/sec, batch_loss: 0.4342, batch_loss_c: 0.4768, batch_loss_s: 0.3348, time:12.0272, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:08:34 \u001b[32mINFO     \u001b[0m train.py: [20/300], [80/484], step: 9760, 1.794 samples/sec, batch_loss: 0.0529, batch_loss_c: 0.0459, batch_loss_s: 0.0691, time:22.3027, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:08:52 \u001b[32mINFO     \u001b[0m train.py: [20/300], [90/484], step: 9770, 2.228 samples/sec, batch_loss: 0.0779, batch_loss_c: 0.0656, batch_loss_s: 0.1067, time:17.9516, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:09:06 \u001b[32mINFO     \u001b[0m train.py: [20/300], [100/484], step: 9780, 2.956 samples/sec, batch_loss: 0.3188, batch_loss_c: 0.3027, batch_loss_s: 0.3564, time:13.5316, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:09:20 \u001b[32mINFO     \u001b[0m train.py: [20/300], [110/484], step: 9790, 2.786 samples/sec, batch_loss: 0.0810, batch_loss_c: 0.0778, batch_loss_s: 0.0886, time:14.3566, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:09:33 \u001b[32mINFO     \u001b[0m train.py: [20/300], [120/484], step: 9800, 3.011 samples/sec, batch_loss: 0.0624, batch_loss_c: 0.0550, batch_loss_s: 0.0797, time:13.2835, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:09:43 \u001b[32mINFO     \u001b[0m train.py: [20/300], [130/484], step: 9810, 3.992 samples/sec, batch_loss: 0.1199, batch_loss_c: 0.1166, batch_loss_s: 0.1276, time:10.0190, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:09:53 \u001b[32mINFO     \u001b[0m train.py: [20/300], [140/484], step: 9820, 4.101 samples/sec, batch_loss: 0.3129, batch_loss_c: 0.3149, batch_loss_s: 0.3083, time:9.7531, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:10:03 \u001b[32mINFO     \u001b[0m train.py: [20/300], [150/484], step: 9830, 4.115 samples/sec, batch_loss: 0.3317, batch_loss_c: 0.3344, batch_loss_s: 0.3254, time:9.7209, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:10:13 \u001b[32mINFO     \u001b[0m train.py: [20/300], [160/484], step: 9840, 4.049 samples/sec, batch_loss: 0.1391, batch_loss_c: 0.1451, batch_loss_s: 0.1251, time:9.8798, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:10:22 \u001b[32mINFO     \u001b[0m train.py: [20/300], [170/484], step: 9850, 4.098 samples/sec, batch_loss: 0.2974, batch_loss_c: 0.2966, batch_loss_s: 0.2991, time:9.7600, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:10:30 \u001b[32mINFO     \u001b[0m train.py: [20/300], [180/484], step: 9860, 5.025 samples/sec, batch_loss: 0.0525, batch_loss_c: 0.0450, batch_loss_s: 0.0700, time:7.9596, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:10:45 \u001b[32mINFO     \u001b[0m train.py: [20/300], [190/484], step: 9870, 2.664 samples/sec, batch_loss: 0.1477, batch_loss_c: 0.1659, batch_loss_s: 0.1052, time:15.0171, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:10:58 \u001b[32mINFO     \u001b[0m train.py: [20/300], [200/484], step: 9880, 3.124 samples/sec, batch_loss: 0.1230, batch_loss_c: 0.1194, batch_loss_s: 0.1316, time:12.8059, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:11:14 \u001b[32mINFO     \u001b[0m train.py: [20/300], [210/484], step: 9890, 2.619 samples/sec, batch_loss: 0.0799, batch_loss_c: 0.0688, batch_loss_s: 0.1057, time:15.2708, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:11:28 \u001b[32mINFO     \u001b[0m train.py: [20/300], [220/484], step: 9900, 2.730 samples/sec, batch_loss: 0.1236, batch_loss_c: 0.1255, batch_loss_s: 0.1192, time:14.6502, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:11:42 \u001b[32mINFO     \u001b[0m train.py: [20/300], [230/484], step: 9910, 2.925 samples/sec, batch_loss: 0.0478, batch_loss_c: 0.0366, batch_loss_s: 0.0740, time:13.6744, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:11:52 \u001b[32mINFO     \u001b[0m train.py: [20/300], [240/484], step: 9920, 4.051 samples/sec, batch_loss: 0.0848, batch_loss_c: 0.0809, batch_loss_s: 0.0939, time:9.8738, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:12:02 \u001b[32mINFO     \u001b[0m train.py: [20/300], [250/484], step: 9930, 3.899 samples/sec, batch_loss: 0.0618, batch_loss_c: 0.0544, batch_loss_s: 0.0788, time:10.2601, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:12:18 \u001b[32mINFO     \u001b[0m train.py: [20/300], [260/484], step: 9940, 2.443 samples/sec, batch_loss: 0.1263, batch_loss_c: 0.1115, batch_loss_s: 0.1609, time:16.3733, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:12:34 \u001b[32mINFO     \u001b[0m train.py: [20/300], [270/484], step: 9950, 2.556 samples/sec, batch_loss: 0.1807, batch_loss_c: 0.1366, batch_loss_s: 0.2836, time:15.6478, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:12:45 \u001b[32mINFO     \u001b[0m train.py: [20/300], [280/484], step: 9960, 3.812 samples/sec, batch_loss: 0.2984, batch_loss_c: 0.2925, batch_loss_s: 0.3122, time:10.4930, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:12:56 \u001b[32mINFO     \u001b[0m train.py: [20/300], [290/484], step: 9970, 3.538 samples/sec, batch_loss: 0.1789, batch_loss_c: 0.2087, batch_loss_s: 0.1093, time:11.3057, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:13:08 \u001b[32mINFO     \u001b[0m train.py: [20/300], [300/484], step: 9980, 3.240 samples/sec, batch_loss: 0.0784, batch_loss_c: 0.0669, batch_loss_s: 0.1053, time:12.3454, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:13:28 \u001b[32mINFO     \u001b[0m train.py: [20/300], [310/484], step: 9990, 2.004 samples/sec, batch_loss: 0.1804, batch_loss_c: 0.1975, batch_loss_s: 0.1406, time:19.9616, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:13:40 \u001b[32mINFO     \u001b[0m train.py: [20/300], [320/484], step: 10000, 3.336 samples/sec, batch_loss: 0.0972, batch_loss_c: 0.0923, batch_loss_s: 0.1086, time:11.9905, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:13:53 \u001b[32mINFO     \u001b[0m train.py: [20/300], [330/484], step: 10010, 3.139 samples/sec, batch_loss: 0.0593, batch_loss_c: 0.0522, batch_loss_s: 0.0759, time:12.7441, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:14:14 \u001b[32mINFO     \u001b[0m train.py: [20/300], [340/484], step: 10020, 1.870 samples/sec, batch_loss: 0.3023, batch_loss_c: 0.3034, batch_loss_s: 0.2996, time:21.3949, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:14:23 \u001b[32mINFO     \u001b[0m train.py: [20/300], [350/484], step: 10030, 4.516 samples/sec, batch_loss: 0.3237, batch_loss_c: 0.3068, batch_loss_s: 0.3632, time:8.8571, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:14:35 \u001b[32mINFO     \u001b[0m train.py: [20/300], [360/484], step: 10040, 3.376 samples/sec, batch_loss: 0.0438, batch_loss_c: 0.0369, batch_loss_s: 0.0597, time:11.8468, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:14:47 \u001b[32mINFO     \u001b[0m train.py: [20/300], [370/484], step: 10050, 3.290 samples/sec, batch_loss: 0.0758, batch_loss_c: 0.0727, batch_loss_s: 0.0830, time:12.1575, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:15:00 \u001b[32mINFO     \u001b[0m train.py: [20/300], [380/484], step: 10060, 3.194 samples/sec, batch_loss: 0.1346, batch_loss_c: 0.1180, batch_loss_s: 0.1733, time:12.5216, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:15:24 \u001b[32mINFO     \u001b[0m train.py: [20/300], [390/484], step: 10070, 1.626 samples/sec, batch_loss: 0.3328, batch_loss_c: 0.3269, batch_loss_s: 0.3465, time:24.5987, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:15:34 \u001b[32mINFO     \u001b[0m train.py: [20/300], [400/484], step: 10080, 4.322 samples/sec, batch_loss: 0.1019, batch_loss_c: 0.0941, batch_loss_s: 0.1204, time:9.2551, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:15:46 \u001b[32mINFO     \u001b[0m train.py: [20/300], [410/484], step: 10090, 3.282 samples/sec, batch_loss: 0.3236, batch_loss_c: 0.3113, batch_loss_s: 0.3523, time:12.1876, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:16:01 \u001b[32mINFO     \u001b[0m train.py: [20/300], [420/484], step: 10100, 2.665 samples/sec, batch_loss: 0.0492, batch_loss_c: 0.0401, batch_loss_s: 0.0703, time:15.0104, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:16:22 \u001b[32mINFO     \u001b[0m train.py: [20/300], [430/484], step: 10110, 1.923 samples/sec, batch_loss: 0.0632, batch_loss_c: 0.0580, batch_loss_s: 0.0751, time:20.8043, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:16:37 \u001b[32mINFO     \u001b[0m train.py: [20/300], [440/484], step: 10120, 2.540 samples/sec, batch_loss: 0.0656, batch_loss_c: 0.0574, batch_loss_s: 0.0846, time:15.7489, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:16:59 \u001b[32mINFO     \u001b[0m train.py: [20/300], [450/484], step: 10130, 1.852 samples/sec, batch_loss: 0.0827, batch_loss_c: 0.0659, batch_loss_s: 0.1217, time:21.6039, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:17:17 \u001b[32mINFO     \u001b[0m train.py: [20/300], [460/484], step: 10140, 2.242 samples/sec, batch_loss: 0.1266, batch_loss_c: 0.1314, batch_loss_s: 0.1154, time:17.8397, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:17:28 \u001b[32mINFO     \u001b[0m train.py: [20/300], [470/484], step: 10150, 3.475 samples/sec, batch_loss: 0.0696, batch_loss_c: 0.0639, batch_loss_s: 0.0830, time:11.5116, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:17:39 \u001b[32mINFO     \u001b[0m train.py: [20/300], [480/484], step: 10160, 3.732 samples/sec, batch_loss: 0.1289, batch_loss_c: 0.1303, batch_loss_s: 0.1255, time:10.7193, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:17:42 \u001b[32mINFO     \u001b[0m train.py: [20/300], train_loss: 0.1389, time: 685.6287, lr: 1e-05\u001b[0m\n",
            "2019-12-07 16:17:43 \u001b[32mINFO     \u001b[0m train.py: [21/300], [0/484], step: 10164, 57.823 samples/sec, batch_loss: 0.0634, batch_loss_c: 0.0568, batch_loss_s: 0.0787, time:0.6918, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:17:57 \u001b[32mINFO     \u001b[0m train.py: [21/300], [10/484], step: 10174, 2.889 samples/sec, batch_loss: 0.0490, batch_loss_c: 0.0422, batch_loss_s: 0.0646, time:13.8433, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:18:12 \u001b[32mINFO     \u001b[0m train.py: [21/300], [20/484], step: 10184, 2.616 samples/sec, batch_loss: 0.0586, batch_loss_c: 0.0481, batch_loss_s: 0.0832, time:15.2902, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:18:37 \u001b[32mINFO     \u001b[0m train.py: [21/300], [30/484], step: 10194, 1.607 samples/sec, batch_loss: 0.0518, batch_loss_c: 0.0405, batch_loss_s: 0.0782, time:24.8914, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:18:47 \u001b[32mINFO     \u001b[0m train.py: [21/300], [40/484], step: 10204, 3.730 samples/sec, batch_loss: 0.1081, batch_loss_c: 0.1118, batch_loss_s: 0.0993, time:10.7228, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:18:58 \u001b[32mINFO     \u001b[0m train.py: [21/300], [50/484], step: 10214, 3.615 samples/sec, batch_loss: 0.0792, batch_loss_c: 0.0720, batch_loss_s: 0.0959, time:11.0658, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:19:08 \u001b[32mINFO     \u001b[0m train.py: [21/300], [60/484], step: 10224, 4.200 samples/sec, batch_loss: 0.0601, batch_loss_c: 0.0514, batch_loss_s: 0.0802, time:9.5230, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:19:29 \u001b[32mINFO     \u001b[0m train.py: [21/300], [70/484], step: 10234, 1.870 samples/sec, batch_loss: 0.1181, batch_loss_c: 0.1243, batch_loss_s: 0.1039, time:21.3915, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:19:39 \u001b[32mINFO     \u001b[0m train.py: [21/300], [80/484], step: 10244, 4.377 samples/sec, batch_loss: 0.0785, batch_loss_c: 0.0752, batch_loss_s: 0.0863, time:9.1387, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:19:52 \u001b[32mINFO     \u001b[0m train.py: [21/300], [90/484], step: 10254, 3.021 samples/sec, batch_loss: 0.0916, batch_loss_c: 0.0834, batch_loss_s: 0.1107, time:13.2399, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:20:02 \u001b[32mINFO     \u001b[0m train.py: [21/300], [100/484], step: 10264, 3.774 samples/sec, batch_loss: 0.0553, batch_loss_c: 0.0460, batch_loss_s: 0.0770, time:10.6001, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:20:11 \u001b[32mINFO     \u001b[0m train.py: [21/300], [110/484], step: 10274, 4.517 samples/sec, batch_loss: 0.0521, batch_loss_c: 0.0470, batch_loss_s: 0.0641, time:8.8556, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:20:24 \u001b[32mINFO     \u001b[0m train.py: [21/300], [120/484], step: 10284, 3.120 samples/sec, batch_loss: 0.1145, batch_loss_c: 0.1068, batch_loss_s: 0.1327, time:12.8199, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:20:37 \u001b[32mINFO     \u001b[0m train.py: [21/300], [130/484], step: 10294, 2.984 samples/sec, batch_loss: 0.0620, batch_loss_c: 0.0562, batch_loss_s: 0.0754, time:13.4052, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:20:51 \u001b[32mINFO     \u001b[0m train.py: [21/300], [140/484], step: 10304, 2.926 samples/sec, batch_loss: 0.3174, batch_loss_c: 0.3149, batch_loss_s: 0.3233, time:13.6710, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:21:16 \u001b[32mINFO     \u001b[0m train.py: [21/300], [150/484], step: 10314, 1.584 samples/sec, batch_loss: 0.0668, batch_loss_c: 0.0556, batch_loss_s: 0.0929, time:25.2551, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:21:29 \u001b[32mINFO     \u001b[0m train.py: [21/300], [160/484], step: 10324, 3.066 samples/sec, batch_loss: 0.2421, batch_loss_c: 0.2409, batch_loss_s: 0.2451, time:13.0458, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:21:40 \u001b[32mINFO     \u001b[0m train.py: [21/300], [170/484], step: 10334, 3.831 samples/sec, batch_loss: 0.0553, batch_loss_c: 0.0477, batch_loss_s: 0.0730, time:10.4404, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:21:53 \u001b[32mINFO     \u001b[0m train.py: [21/300], [180/484], step: 10344, 3.026 samples/sec, batch_loss: 0.3478, batch_loss_c: 0.3362, batch_loss_s: 0.3749, time:13.2177, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:22:17 \u001b[32mINFO     \u001b[0m train.py: [21/300], [190/484], step: 10354, 1.645 samples/sec, batch_loss: 0.0721, batch_loss_c: 0.0762, batch_loss_s: 0.0625, time:24.3142, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:22:34 \u001b[32mINFO     \u001b[0m train.py: [21/300], [200/484], step: 10364, 2.374 samples/sec, batch_loss: 0.0981, batch_loss_c: 0.0654, batch_loss_s: 0.1744, time:16.8471, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:22:51 \u001b[32mINFO     \u001b[0m train.py: [21/300], [210/484], step: 10374, 2.412 samples/sec, batch_loss: 0.0671, batch_loss_c: 0.0551, batch_loss_s: 0.0950, time:16.5867, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:23:12 \u001b[32mINFO     \u001b[0m train.py: [21/300], [220/484], step: 10384, 1.915 samples/sec, batch_loss: 0.0843, batch_loss_c: 0.0930, batch_loss_s: 0.0641, time:20.8921, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:23:25 \u001b[32mINFO     \u001b[0m train.py: [21/300], [230/484], step: 10394, 2.930 samples/sec, batch_loss: 0.3158, batch_loss_c: 0.3034, batch_loss_s: 0.3445, time:13.6530, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:23:42 \u001b[32mINFO     \u001b[0m train.py: [21/300], [240/484], step: 10404, 2.357 samples/sec, batch_loss: 0.1628, batch_loss_c: 0.1581, batch_loss_s: 0.1738, time:16.9723, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:23:51 \u001b[32mINFO     \u001b[0m train.py: [21/300], [250/484], step: 10414, 4.728 samples/sec, batch_loss: 0.1258, batch_loss_c: 0.1203, batch_loss_s: 0.1386, time:8.4605, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:23:59 \u001b[32mINFO     \u001b[0m train.py: [21/300], [260/484], step: 10424, 4.662 samples/sec, batch_loss: 0.0461, batch_loss_c: 0.0416, batch_loss_s: 0.0565, time:8.5802, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:24:13 \u001b[32mINFO     \u001b[0m train.py: [21/300], [270/484], step: 10434, 2.846 samples/sec, batch_loss: 0.1233, batch_loss_c: 0.1047, batch_loss_s: 0.1668, time:14.0561, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:24:22 \u001b[32mINFO     \u001b[0m train.py: [21/300], [280/484], step: 10444, 4.582 samples/sec, batch_loss: 0.0855, batch_loss_c: 0.0790, batch_loss_s: 0.1006, time:8.7291, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:24:32 \u001b[32mINFO     \u001b[0m train.py: [21/300], [290/484], step: 10454, 4.246 samples/sec, batch_loss: 0.3165, batch_loss_c: 0.3130, batch_loss_s: 0.3247, time:9.4208, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:24:52 \u001b[32mINFO     \u001b[0m train.py: [21/300], [300/484], step: 10464, 1.939 samples/sec, batch_loss: 0.0542, batch_loss_c: 0.0481, batch_loss_s: 0.0683, time:20.6345, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:25:11 \u001b[32mINFO     \u001b[0m train.py: [21/300], [310/484], step: 10474, 2.101 samples/sec, batch_loss: 0.0514, batch_loss_c: 0.0468, batch_loss_s: 0.0620, time:19.0416, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:25:23 \u001b[32mINFO     \u001b[0m train.py: [21/300], [320/484], step: 10484, 3.286 samples/sec, batch_loss: 0.0417, batch_loss_c: 0.0334, batch_loss_s: 0.0609, time:12.1741, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:25:39 \u001b[32mINFO     \u001b[0m train.py: [21/300], [330/484], step: 10494, 2.606 samples/sec, batch_loss: 0.2963, batch_loss_c: 0.2917, batch_loss_s: 0.3072, time:15.3464, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:25:47 \u001b[32mINFO     \u001b[0m train.py: [21/300], [340/484], step: 10504, 4.742 samples/sec, batch_loss: 0.1127, batch_loss_c: 0.1058, batch_loss_s: 0.1289, time:8.4348, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:25:58 \u001b[32mINFO     \u001b[0m train.py: [21/300], [350/484], step: 10514, 3.851 samples/sec, batch_loss: 0.0700, batch_loss_c: 0.0540, batch_loss_s: 0.1075, time:10.3864, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:26:15 \u001b[32mINFO     \u001b[0m train.py: [21/300], [360/484], step: 10524, 2.363 samples/sec, batch_loss: 0.3188, batch_loss_c: 0.3169, batch_loss_s: 0.3234, time:16.9254, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:26:27 \u001b[32mINFO     \u001b[0m train.py: [21/300], [370/484], step: 10534, 3.203 samples/sec, batch_loss: 0.1218, batch_loss_c: 0.1197, batch_loss_s: 0.1268, time:12.4874, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:26:51 \u001b[32mINFO     \u001b[0m train.py: [21/300], [380/484], step: 10544, 1.685 samples/sec, batch_loss: 0.0719, batch_loss_c: 0.0616, batch_loss_s: 0.0961, time:23.7412, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:27:05 \u001b[32mINFO     \u001b[0m train.py: [21/300], [390/484], step: 10554, 2.905 samples/sec, batch_loss: 0.3108, batch_loss_c: 0.3032, batch_loss_s: 0.3284, time:13.7715, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:27:18 \u001b[32mINFO     \u001b[0m train.py: [21/300], [400/484], step: 10564, 3.032 samples/sec, batch_loss: 0.0406, batch_loss_c: 0.0318, batch_loss_s: 0.0612, time:13.1917, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:27:29 \u001b[32mINFO     \u001b[0m train.py: [21/300], [410/484], step: 10574, 3.664 samples/sec, batch_loss: 0.0998, batch_loss_c: 0.0937, batch_loss_s: 0.1142, time:10.9179, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:27:40 \u001b[32mINFO     \u001b[0m train.py: [21/300], [420/484], step: 10584, 3.529 samples/sec, batch_loss: 0.3282, batch_loss_c: 0.3115, batch_loss_s: 0.3671, time:11.3341, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:27:53 \u001b[32mINFO     \u001b[0m train.py: [21/300], [430/484], step: 10594, 3.046 samples/sec, batch_loss: 0.0537, batch_loss_c: 0.0417, batch_loss_s: 0.0817, time:13.1333, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:28:03 \u001b[32mINFO     \u001b[0m train.py: [21/300], [440/484], step: 10604, 4.147 samples/sec, batch_loss: 0.0567, batch_loss_c: 0.0533, batch_loss_s: 0.0648, time:9.6463, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:28:20 \u001b[32mINFO     \u001b[0m train.py: [21/300], [450/484], step: 10614, 2.378 samples/sec, batch_loss: 0.1424, batch_loss_c: 0.1340, batch_loss_s: 0.1619, time:16.8209, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:28:51 \u001b[32mINFO     \u001b[0m train.py: [21/300], [460/484], step: 10624, 1.270 samples/sec, batch_loss: 0.0526, batch_loss_c: 0.0444, batch_loss_s: 0.0718, time:31.5025, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:29:04 \u001b[32mINFO     \u001b[0m train.py: [21/300], [470/484], step: 10634, 3.027 samples/sec, batch_loss: 0.2900, batch_loss_c: 0.2845, batch_loss_s: 0.3030, time:13.2131, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:29:25 \u001b[32mINFO     \u001b[0m train.py: [21/300], [480/484], step: 10644, 1.918 samples/sec, batch_loss: 0.2840, batch_loss_c: 0.2834, batch_loss_s: 0.2855, time:20.8577, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:29:36 \u001b[32mINFO     \u001b[0m train.py: [21/300], train_loss: 0.1415, time: 714.0959, lr: 1e-05\u001b[0m\n",
            "2019-12-07 16:29:37 \u001b[32mINFO     \u001b[0m train.py: [22/300], [0/484], step: 10648, 46.282 samples/sec, batch_loss: 0.0799, batch_loss_c: 0.0702, batch_loss_s: 0.1025, time:0.8643, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:29:55 \u001b[32mINFO     \u001b[0m train.py: [22/300], [10/484], step: 10658, 2.284 samples/sec, batch_loss: 0.0483, batch_loss_c: 0.0441, batch_loss_s: 0.0581, time:17.5111, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:30:15 \u001b[32mINFO     \u001b[0m train.py: [22/300], [20/484], step: 10668, 1.994 samples/sec, batch_loss: 0.0403, batch_loss_c: 0.0365, batch_loss_s: 0.0493, time:20.0597, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:30:25 \u001b[32mINFO     \u001b[0m train.py: [22/300], [30/484], step: 10678, 3.920 samples/sec, batch_loss: 0.0629, batch_loss_c: 0.0528, batch_loss_s: 0.0865, time:10.2044, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:30:45 \u001b[32mINFO     \u001b[0m train.py: [22/300], [40/484], step: 10688, 2.054 samples/sec, batch_loss: 0.1117, batch_loss_c: 0.0849, batch_loss_s: 0.1743, time:19.4762, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:30:55 \u001b[32mINFO     \u001b[0m train.py: [22/300], [50/484], step: 10698, 3.720 samples/sec, batch_loss: 0.3523, batch_loss_c: 0.3524, batch_loss_s: 0.3521, time:10.7541, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:31:10 \u001b[32mINFO     \u001b[0m train.py: [22/300], [60/484], step: 10708, 2.650 samples/sec, batch_loss: 0.1376, batch_loss_c: 0.1446, batch_loss_s: 0.1213, time:15.0942, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:31:30 \u001b[32mINFO     \u001b[0m train.py: [22/300], [70/484], step: 10718, 2.026 samples/sec, batch_loss: 0.3016, batch_loss_c: 0.2916, batch_loss_s: 0.3248, time:19.7480, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:31:40 \u001b[32mINFO     \u001b[0m train.py: [22/300], [80/484], step: 10728, 3.954 samples/sec, batch_loss: 0.0357, batch_loss_c: 0.0279, batch_loss_s: 0.0537, time:10.1156, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:32:05 \u001b[32mINFO     \u001b[0m train.py: [22/300], [90/484], step: 10738, 1.650 samples/sec, batch_loss: 0.7934, batch_loss_c: 0.7932, batch_loss_s: 0.7939, time:24.2456, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:32:17 \u001b[32mINFO     \u001b[0m train.py: [22/300], [100/484], step: 10748, 3.138 samples/sec, batch_loss: 0.0526, batch_loss_c: 0.0502, batch_loss_s: 0.0582, time:12.7485, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:32:55 \u001b[32mINFO     \u001b[0m train.py: [22/300], [110/484], step: 10758, 1.057 samples/sec, batch_loss: 0.0541, batch_loss_c: 0.0411, batch_loss_s: 0.0844, time:37.8588, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:33:07 \u001b[32mINFO     \u001b[0m train.py: [22/300], [120/484], step: 10768, 3.402 samples/sec, batch_loss: 0.2096, batch_loss_c: 0.2232, batch_loss_s: 0.1776, time:11.7569, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:33:22 \u001b[32mINFO     \u001b[0m train.py: [22/300], [130/484], step: 10778, 2.597 samples/sec, batch_loss: 0.0747, batch_loss_c: 0.0694, batch_loss_s: 0.0871, time:15.4015, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:33:35 \u001b[32mINFO     \u001b[0m train.py: [22/300], [140/484], step: 10788, 3.191 samples/sec, batch_loss: 0.0451, batch_loss_c: 0.0399, batch_loss_s: 0.0574, time:12.5362, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:33:59 \u001b[32mINFO     \u001b[0m train.py: [22/300], [150/484], step: 10798, 1.624 samples/sec, batch_loss: 0.0622, batch_loss_c: 0.0542, batch_loss_s: 0.0810, time:24.6292, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:34:32 \u001b[32mINFO     \u001b[0m train.py: [22/300], [160/484], step: 10808, 1.226 samples/sec, batch_loss: 0.6099, batch_loss_c: 0.6262, batch_loss_s: 0.5717, time:32.6339, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:34:41 \u001b[32mINFO     \u001b[0m train.py: [22/300], [170/484], step: 10818, 4.419 samples/sec, batch_loss: 0.0348, batch_loss_c: 0.0284, batch_loss_s: 0.0498, time:9.0513, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:34:59 \u001b[32mINFO     \u001b[0m train.py: [22/300], [180/484], step: 10828, 2.262 samples/sec, batch_loss: 0.1817, batch_loss_c: 0.1953, batch_loss_s: 0.1499, time:17.6839, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:35:15 \u001b[32mINFO     \u001b[0m train.py: [22/300], [190/484], step: 10838, 2.439 samples/sec, batch_loss: 0.2857, batch_loss_c: 0.2800, batch_loss_s: 0.2992, time:16.3992, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:35:27 \u001b[32mINFO     \u001b[0m train.py: [22/300], [200/484], step: 10848, 3.497 samples/sec, batch_loss: 0.1402, batch_loss_c: 0.1014, batch_loss_s: 0.2308, time:11.4389, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:35:37 \u001b[32mINFO     \u001b[0m train.py: [22/300], [210/484], step: 10858, 3.764 samples/sec, batch_loss: 0.0756, batch_loss_c: 0.0684, batch_loss_s: 0.0925, time:10.6283, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:35:56 \u001b[32mINFO     \u001b[0m train.py: [22/300], [220/484], step: 10868, 2.151 samples/sec, batch_loss: 0.3546, batch_loss_c: 0.3537, batch_loss_s: 0.3568, time:18.5960, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:36:06 \u001b[32mINFO     \u001b[0m train.py: [22/300], [230/484], step: 10878, 4.087 samples/sec, batch_loss: 0.1705, batch_loss_c: 0.1308, batch_loss_s: 0.2629, time:9.7861, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:36:23 \u001b[32mINFO     \u001b[0m train.py: [22/300], [240/484], step: 10888, 2.268 samples/sec, batch_loss: 0.1586, batch_loss_c: 0.1563, batch_loss_s: 0.1639, time:17.6368, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:36:38 \u001b[32mINFO     \u001b[0m train.py: [22/300], [250/484], step: 10898, 2.644 samples/sec, batch_loss: 0.3241, batch_loss_c: 0.3362, batch_loss_s: 0.2957, time:15.1270, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:36:56 \u001b[32mINFO     \u001b[0m train.py: [22/300], [260/484], step: 10908, 2.225 samples/sec, batch_loss: 0.3063, batch_loss_c: 0.3068, batch_loss_s: 0.3050, time:17.9762, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:37:09 \u001b[32mINFO     \u001b[0m train.py: [22/300], [270/484], step: 10918, 3.296 samples/sec, batch_loss: 0.0614, batch_loss_c: 0.0500, batch_loss_s: 0.0880, time:12.1372, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:37:20 \u001b[32mINFO     \u001b[0m train.py: [22/300], [280/484], step: 10928, 3.378 samples/sec, batch_loss: 0.1527, batch_loss_c: 0.1330, batch_loss_s: 0.1985, time:11.8418, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:37:35 \u001b[32mINFO     \u001b[0m train.py: [22/300], [290/484], step: 10938, 2.671 samples/sec, batch_loss: 0.1427, batch_loss_c: 0.1359, batch_loss_s: 0.1585, time:14.9760, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:37:56 \u001b[32mINFO     \u001b[0m train.py: [22/300], [300/484], step: 10948, 1.906 samples/sec, batch_loss: 0.2719, batch_loss_c: 0.3146, batch_loss_s: 0.1723, time:20.9858, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:38:15 \u001b[32mINFO     \u001b[0m train.py: [22/300], [310/484], step: 10958, 2.194 samples/sec, batch_loss: 0.0697, batch_loss_c: 0.0655, batch_loss_s: 0.0794, time:18.2351, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:38:25 \u001b[32mINFO     \u001b[0m train.py: [22/300], [320/484], step: 10968, 3.979 samples/sec, batch_loss: 0.1439, batch_loss_c: 0.0830, batch_loss_s: 0.2861, time:10.0525, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:38:41 \u001b[32mINFO     \u001b[0m train.py: [22/300], [330/484], step: 10978, 2.391 samples/sec, batch_loss: 0.1052, batch_loss_c: 0.0918, batch_loss_s: 0.1365, time:16.7306, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:38:50 \u001b[32mINFO     \u001b[0m train.py: [22/300], [340/484], step: 10988, 4.453 samples/sec, batch_loss: 0.3079, batch_loss_c: 0.3019, batch_loss_s: 0.3219, time:8.9821, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:39:01 \u001b[32mINFO     \u001b[0m train.py: [22/300], [350/484], step: 10998, 3.641 samples/sec, batch_loss: 0.0581, batch_loss_c: 0.0471, batch_loss_s: 0.0836, time:10.9852, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:39:13 \u001b[32mINFO     \u001b[0m train.py: [22/300], [360/484], step: 11008, 3.384 samples/sec, batch_loss: 0.3140, batch_loss_c: 0.3050, batch_loss_s: 0.3351, time:11.8220, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:39:35 \u001b[32mINFO     \u001b[0m train.py: [22/300], [370/484], step: 11018, 1.807 samples/sec, batch_loss: 0.5741, batch_loss_c: 0.5316, batch_loss_s: 0.6734, time:22.1317, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:39:55 \u001b[32mINFO     \u001b[0m train.py: [22/300], [380/484], step: 11028, 1.999 samples/sec, batch_loss: 0.0734, batch_loss_c: 0.0687, batch_loss_s: 0.0842, time:20.0056, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:40:17 \u001b[32mINFO     \u001b[0m train.py: [22/300], [390/484], step: 11038, 1.806 samples/sec, batch_loss: 0.3050, batch_loss_c: 0.3004, batch_loss_s: 0.3156, time:22.1448, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:40:36 \u001b[32mINFO     \u001b[0m train.py: [22/300], [400/484], step: 11048, 2.152 samples/sec, batch_loss: 0.2881, batch_loss_c: 0.2817, batch_loss_s: 0.3031, time:18.5856, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:40:48 \u001b[32mINFO     \u001b[0m train.py: [22/300], [410/484], step: 11058, 3.234 samples/sec, batch_loss: 0.0580, batch_loss_c: 0.0539, batch_loss_s: 0.0675, time:12.3697, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:40:56 \u001b[32mINFO     \u001b[0m train.py: [22/300], [420/484], step: 11068, 5.051 samples/sec, batch_loss: 0.0625, batch_loss_c: 0.0589, batch_loss_s: 0.0711, time:7.9192, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:41:07 \u001b[32mINFO     \u001b[0m train.py: [22/300], [430/484], step: 11078, 3.889 samples/sec, batch_loss: 0.0806, batch_loss_c: 0.0654, batch_loss_s: 0.1160, time:10.2867, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:41:27 \u001b[32mINFO     \u001b[0m train.py: [22/300], [440/484], step: 11088, 1.983 samples/sec, batch_loss: 0.0521, batch_loss_c: 0.0462, batch_loss_s: 0.0658, time:20.1688, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:41:43 \u001b[32mINFO     \u001b[0m train.py: [22/300], [450/484], step: 11098, 2.493 samples/sec, batch_loss: 0.1233, batch_loss_c: 0.1295, batch_loss_s: 0.1087, time:16.0473, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:42:03 \u001b[32mINFO     \u001b[0m train.py: [22/300], [460/484], step: 11108, 2.024 samples/sec, batch_loss: 0.2118, batch_loss_c: 0.2224, batch_loss_s: 0.1868, time:19.7644, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:42:27 \u001b[32mINFO     \u001b[0m train.py: [22/300], [470/484], step: 11118, 1.666 samples/sec, batch_loss: 0.1182, batch_loss_c: 0.1023, batch_loss_s: 0.1553, time:24.0099, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:42:37 \u001b[32mINFO     \u001b[0m train.py: [22/300], [480/484], step: 11128, 3.870 samples/sec, batch_loss: 0.0615, batch_loss_c: 0.0565, batch_loss_s: 0.0732, time:10.3353, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:42:43 \u001b[32mINFO     \u001b[0m train.py: [22/300], train_loss: 0.1491, time: 786.1917, lr: 1e-05\u001b[0m\n",
            "2019-12-07 16:42:44 \u001b[32mINFO     \u001b[0m train.py: [23/300], [0/484], step: 11132, 63.212 samples/sec, batch_loss: 0.0468, batch_loss_c: 0.0425, batch_loss_s: 0.0568, time:0.6328, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:43:06 \u001b[32mINFO     \u001b[0m train.py: [23/300], [10/484], step: 11142, 1.834 samples/sec, batch_loss: 0.1695, batch_loss_c: 0.1520, batch_loss_s: 0.2103, time:21.8152, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:43:14 \u001b[32mINFO     \u001b[0m train.py: [23/300], [20/484], step: 11152, 4.871 samples/sec, batch_loss: 0.1534, batch_loss_c: 0.1448, batch_loss_s: 0.1734, time:8.2123, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:43:27 \u001b[32mINFO     \u001b[0m train.py: [23/300], [30/484], step: 11162, 3.120 samples/sec, batch_loss: 0.0716, batch_loss_c: 0.0683, batch_loss_s: 0.0795, time:12.8204, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:43:37 \u001b[32mINFO     \u001b[0m train.py: [23/300], [40/484], step: 11172, 3.761 samples/sec, batch_loss: 0.0842, batch_loss_c: 0.0696, batch_loss_s: 0.1183, time:10.6366, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:43:56 \u001b[32mINFO     \u001b[0m train.py: [23/300], [50/484], step: 11182, 2.098 samples/sec, batch_loss: 0.0455, batch_loss_c: 0.0353, batch_loss_s: 0.0694, time:19.0684, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:44:17 \u001b[32mINFO     \u001b[0m train.py: [23/300], [60/484], step: 11192, 1.889 samples/sec, batch_loss: 0.2911, batch_loss_c: 0.2841, batch_loss_s: 0.3075, time:21.1754, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:44:27 \u001b[32mINFO     \u001b[0m train.py: [23/300], [70/484], step: 11202, 4.002 samples/sec, batch_loss: 0.0662, batch_loss_c: 0.0514, batch_loss_s: 0.1008, time:9.9951, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:44:49 \u001b[32mINFO     \u001b[0m train.py: [23/300], [80/484], step: 11212, 1.831 samples/sec, batch_loss: 0.3102, batch_loss_c: 0.3018, batch_loss_s: 0.3299, time:21.8511, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:45:00 \u001b[32mINFO     \u001b[0m train.py: [23/300], [90/484], step: 11222, 3.912 samples/sec, batch_loss: 0.0611, batch_loss_c: 0.0491, batch_loss_s: 0.0891, time:10.2247, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:45:10 \u001b[32mINFO     \u001b[0m train.py: [23/300], [100/484], step: 11232, 3.712 samples/sec, batch_loss: 0.2215, batch_loss_c: 0.2107, batch_loss_s: 0.2465, time:10.7761, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:45:21 \u001b[32mINFO     \u001b[0m train.py: [23/300], [110/484], step: 11242, 3.805 samples/sec, batch_loss: 0.2918, batch_loss_c: 0.2903, batch_loss_s: 0.2955, time:10.5118, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:45:37 \u001b[32mINFO     \u001b[0m train.py: [23/300], [120/484], step: 11252, 2.417 samples/sec, batch_loss: 0.1285, batch_loss_c: 0.0994, batch_loss_s: 0.1963, time:16.5465, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:45:59 \u001b[32mINFO     \u001b[0m train.py: [23/300], [130/484], step: 11262, 1.822 samples/sec, batch_loss: 0.3202, batch_loss_c: 0.3189, batch_loss_s: 0.3231, time:21.9535, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:46:11 \u001b[32mINFO     \u001b[0m train.py: [23/300], [140/484], step: 11272, 3.529 samples/sec, batch_loss: 0.0650, batch_loss_c: 0.0494, batch_loss_s: 0.1013, time:11.3362, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:46:24 \u001b[32mINFO     \u001b[0m train.py: [23/300], [150/484], step: 11282, 3.002 samples/sec, batch_loss: 0.0774, batch_loss_c: 0.0685, batch_loss_s: 0.0981, time:13.3248, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:46:38 \u001b[32mINFO     \u001b[0m train.py: [23/300], [160/484], step: 11292, 2.858 samples/sec, batch_loss: 0.1817, batch_loss_c: 0.1626, batch_loss_s: 0.2262, time:13.9974, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:47:03 \u001b[32mINFO     \u001b[0m train.py: [23/300], [170/484], step: 11302, 1.582 samples/sec, batch_loss: 0.0409, batch_loss_c: 0.0347, batch_loss_s: 0.0554, time:25.2867, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:47:25 \u001b[32mINFO     \u001b[0m train.py: [23/300], [180/484], step: 11312, 1.881 samples/sec, batch_loss: 0.0479, batch_loss_c: 0.0375, batch_loss_s: 0.0723, time:21.2674, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:47:38 \u001b[32mINFO     \u001b[0m train.py: [23/300], [190/484], step: 11322, 2.916 samples/sec, batch_loss: 0.0439, batch_loss_c: 0.0389, batch_loss_s: 0.0555, time:13.7187, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:47:48 \u001b[32mINFO     \u001b[0m train.py: [23/300], [200/484], step: 11332, 4.037 samples/sec, batch_loss: 0.1196, batch_loss_c: 0.1274, batch_loss_s: 0.1015, time:9.9084, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:48:16 \u001b[32mINFO     \u001b[0m train.py: [23/300], [210/484], step: 11342, 1.433 samples/sec, batch_loss: 0.0345, batch_loss_c: 0.0291, batch_loss_s: 0.0468, time:27.9052, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:48:28 \u001b[32mINFO     \u001b[0m train.py: [23/300], [220/484], step: 11352, 3.317 samples/sec, batch_loss: 0.2046, batch_loss_c: 0.2458, batch_loss_s: 0.1084, time:12.0598, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:48:40 \u001b[32mINFO     \u001b[0m train.py: [23/300], [230/484], step: 11362, 3.403 samples/sec, batch_loss: 0.0426, batch_loss_c: 0.0343, batch_loss_s: 0.0619, time:11.7549, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:48:50 \u001b[32mINFO     \u001b[0m train.py: [23/300], [240/484], step: 11372, 3.856 samples/sec, batch_loss: 0.2111, batch_loss_c: 0.1994, batch_loss_s: 0.2384, time:10.3725, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:49:02 \u001b[32mINFO     \u001b[0m train.py: [23/300], [250/484], step: 11382, 3.506 samples/sec, batch_loss: 0.3341, batch_loss_c: 0.3245, batch_loss_s: 0.3565, time:11.4100, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:49:28 \u001b[32mINFO     \u001b[0m train.py: [23/300], [260/484], step: 11392, 1.538 samples/sec, batch_loss: 0.3026, batch_loss_c: 0.2959, batch_loss_s: 0.3183, time:26.0144, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:49:38 \u001b[32mINFO     \u001b[0m train.py: [23/300], [270/484], step: 11402, 3.832 samples/sec, batch_loss: 0.0508, batch_loss_c: 0.0449, batch_loss_s: 0.0647, time:10.4395, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:49:56 \u001b[32mINFO     \u001b[0m train.py: [23/300], [280/484], step: 11412, 2.184 samples/sec, batch_loss: 0.0469, batch_loss_c: 0.0420, batch_loss_s: 0.0582, time:18.3185, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:50:08 \u001b[32mINFO     \u001b[0m train.py: [23/300], [290/484], step: 11422, 3.404 samples/sec, batch_loss: 0.1064, batch_loss_c: 0.0962, batch_loss_s: 0.1303, time:11.7504, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:50:35 \u001b[32mINFO     \u001b[0m train.py: [23/300], [300/484], step: 11432, 1.470 samples/sec, batch_loss: 0.0539, batch_loss_c: 0.0457, batch_loss_s: 0.0732, time:27.2042, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:51:00 \u001b[32mINFO     \u001b[0m train.py: [23/300], [310/484], step: 11442, 1.653 samples/sec, batch_loss: 0.0638, batch_loss_c: 0.0549, batch_loss_s: 0.0846, time:24.2018, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:51:14 \u001b[32mINFO     \u001b[0m train.py: [23/300], [320/484], step: 11452, 2.860 samples/sec, batch_loss: 0.0604, batch_loss_c: 0.0515, batch_loss_s: 0.0812, time:13.9843, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:51:38 \u001b[32mINFO     \u001b[0m train.py: [23/300], [330/484], step: 11462, 1.609 samples/sec, batch_loss: 0.0404, batch_loss_c: 0.0287, batch_loss_s: 0.0678, time:24.8621, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:51:48 \u001b[32mINFO     \u001b[0m train.py: [23/300], [340/484], step: 11472, 4.162 samples/sec, batch_loss: 0.5244, batch_loss_c: 0.5235, batch_loss_s: 0.5264, time:9.6111, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:52:03 \u001b[32mINFO     \u001b[0m train.py: [23/300], [350/484], step: 11482, 2.771 samples/sec, batch_loss: 0.0707, batch_loss_c: 0.0638, batch_loss_s: 0.0869, time:14.4361, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:52:18 \u001b[32mINFO     \u001b[0m train.py: [23/300], [360/484], step: 11492, 2.613 samples/sec, batch_loss: 0.3120, batch_loss_c: 0.3114, batch_loss_s: 0.3133, time:15.3095, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:52:39 \u001b[32mINFO     \u001b[0m train.py: [23/300], [370/484], step: 11502, 1.903 samples/sec, batch_loss: 0.1422, batch_loss_c: 0.1304, batch_loss_s: 0.1699, time:21.0192, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:52:51 \u001b[32mINFO     \u001b[0m train.py: [23/300], [380/484], step: 11512, 3.302 samples/sec, batch_loss: 0.3158, batch_loss_c: 0.3099, batch_loss_s: 0.3297, time:12.1135, lr:1e-05\u001b[0m\n",
            "2019-12-07 16:53:13 \u001b[32mINFO     \u001b[0m train.py: [23/300], [390/484], step: 11522, 1.801 samples/sec, batch_loss: 0.0737, batch_loss_c: 0.0631, batch_loss_s: 0.0985, time:22.2118, lr:1e-05\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}