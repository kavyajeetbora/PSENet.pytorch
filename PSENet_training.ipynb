{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PSENet_trial_run.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kavyajeetbora/PSENet.pytorch/blob/master/PSENet_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UByxwp0F3QUp",
        "colab_type": "text"
      },
      "source": [
        "## Installing softwares and other dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GSEHnmCxK_o9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, Dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y99kqKlHoYkz",
        "colab_type": "code",
        "outputId": "0b4b31e7-5e6b-4ad6-bd4f-9aa875a787ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "!pip install pyclipper"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyclipper\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e1/40/57a0d54a1c696d58253c88a95677e50ab2b305a15af0ac64b70db4320562/pyclipper-1.1.0.post3-cp36-cp36m-manylinux1_x86_64.whl (131kB)\n",
            "\r\u001b[K     |██▌                             | 10kB 22.5MB/s eta 0:00:01\r\u001b[K     |█████                           | 20kB 1.8MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 30kB 2.6MB/s eta 0:00:01\r\u001b[K     |██████████                      | 40kB 1.7MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 51kB 2.1MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 61kB 2.5MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 71kB 2.9MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 81kB 3.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 92kB 3.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 102kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 112kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 122kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 133kB 2.8MB/s \n",
            "\u001b[?25hInstalling collected packages: pyclipper\n",
            "Successfully installed pyclipper-1.1.0.post3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ncoynOlfnIW1",
        "colab_type": "code",
        "outputId": "9d574b96-9ccf-4011-9816-48c29e86e490",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hhJ-6yyXcnRx",
        "colab_type": "code",
        "outputId": "b90f6f59-fd99-4bf0-d02f-b8f0e848391b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        }
      },
      "source": [
        "import shutil, os\n",
        "os.chdir('/content')\n",
        "directory = '/content/cloned-repo'\n",
        "if os.path.exists(directory):\n",
        "  shutil.rmtree(directory)\n",
        "\n",
        "!git clone https://github.com/kavyajeetbora/PSENet.pytorch.git /content/cloned-repo\n",
        "print(\"Cloned the repository\")\n",
        "os.chdir('/content/cloned-repo')\n",
        "!ls"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into '/content/cloned-repo'...\n",
            "remote: Enumerating objects: 563, done.\u001b[K\n",
            "remote: Total 563 (delta 0), reused 0 (delta 0), pack-reused 563\u001b[K\n",
            "Receiving objects: 100% (563/563), 15.47 MiB | 6.66 MiB/s, done.\n",
            "Resolving deltas: 100% (297/297), done.\n",
            "Cloned the repository\n",
            "cal_recall  install_dependencies.sh  PSENet.ipynb\t     train.py\n",
            "config.py   LICENSE\t\t     PSENet_predict.ipynb    utils\n",
            "dataset     models\t\t     PSENet_training.ipynb\n",
            "eval.py     predict.py\t\t     PSENet_trial_run.ipynb\n",
            "imgs\t    pse\t\t\t     README.md\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PrOmfo6_3X2I",
        "colab_type": "text"
      },
      "source": [
        "## Extracting the data and setting up the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dGLMxTujlm0w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## unzipping the files\n",
        "from zipfile import ZipFile\n",
        "\n",
        "def unzip_files(file,output_dir):\n",
        "  with ZipFile(file, 'r') as zipObj:\n",
        "    # Extract all the contents of zip file in current directory\n",
        "    zipObj.extractall(output_dir)\n",
        "  print('Extracted to',output_dir)\n",
        "\n",
        "def make_directory(directory):\n",
        "  if os.path.isdir(directory):\n",
        "    shutil.rmtree(directory)\n",
        "  \n",
        "  os.mkdir(directory)\n",
        "  print('Created a new directory')\n",
        "\n",
        "training_data_zip = '/content/drive/My Drive/Colab Notebooks/padh.ai.notebooks/15. Object Detection/Scene Text Detection Dataset/English and Hindi MLT 2019.zip'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FqJ6xb5u2dgg",
        "colab_type": "code",
        "outputId": "b96b41fa-8813-4309-ca65-82f09d1da445",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# make directories\n",
        "make_directory('Training Set')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Created a new directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FFSaSXr-m-K4",
        "colab_type": "code",
        "outputId": "f3f26fa5-be4c-4dee-bb74-f77b2dae1af2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "%%time\n",
        "unzip_files(training_data_zip,'Training Set')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracted to Training Set\n",
            "CPU times: user 3 s, sys: 1.19 s, total: 4.2 s\n",
            "Wall time: 9.88 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YRSeyluknVH4",
        "colab_type": "code",
        "outputId": "4a657f1a-92db-4130-fe9a-889d287cf395",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(len(os.listdir('Training Set/Images')))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MOp4JsZ6nXvn",
        "colab_type": "code",
        "outputId": "ad71e9e3-1352-489c-99d2-80edeb295cc6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(len(os.listdir('Training Set/Annotations')))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mhvg89mXn8zG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from dataset.data_utils import *\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader, Dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4JeHTz3G6Lyj",
        "colab_type": "code",
        "outputId": "0e61e1c6-3d36-439f-fb9b-bb47d92958fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_data = MyDataset('Training Set',transform=transforms.ToTensor())\n",
        "len(train_data)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1938"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "odLKMn-5SBAz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device(\"cuda:0\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9a9PsyF3dqo",
        "colab_type": "text"
      },
      "source": [
        "## Training "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9x221Ya0RBGO",
        "outputId": "92b74be9-d8ea-4a39-a2b9-26c391251335",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        }
      },
      "source": [
        "!chmod +x install_dependencies.sh # make shell script executable\n",
        "!./install_dependencies.sh # run the shell script"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyclipper in /usr/local/lib/python3.6/dist-packages (1.1.0.post3)\n",
            "Collecting Polygon3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/73/a0/d07a4f3e80ed7020a33f3111db217f54ac44a485ff45da3c21ce49f65041/Polygon3-3.0.8.tar.gz (71kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 2.2MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: Polygon3\n",
            "  Building wheel for Polygon3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for Polygon3: filename=Polygon3-3.0.8-cp36-cp36m-linux_x86_64.whl size=101490 sha256=fb45919b8d9122d4b953f0aef4debe086bb9fc12c2660b49be0e045a158d6dd6\n",
            "  Stored in directory: /root/.cache/pip/wheels/95/32/f1/5525b233996d9d99cbce2f0a8da60d137ddddc555d3e8b0e2a\n",
            "Successfully built Polygon3\n",
            "Installing collected packages: Polygon3\n",
            "Successfully installed Polygon3-3.0.8\n",
            "Collecting colorlog\n",
            "  Downloading https://files.pythonhosted.org/packages/68/4d/892728b0c14547224f0ac40884e722a3d00cb54e7a146aea0b3186806c9e/colorlog-4.0.2-py2.py3-none-any.whl\n",
            "Installing collected packages: colorlog\n",
            "Successfully installed colorlog-4.0.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xD4GuTKC3wXY",
        "colab_type": "code",
        "outputId": "58f9e9cb-12c4-403f-ecd2-e6beb3250445",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!python3 train.py"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "make: Entering directory '/content/cloned-repo/pse'\n",
            "make: 'pse.so' is up to date.\n",
            "make: Leaving directory '/content/cloned-repo/pse'\n",
            "2019-12-05 15:11:00 \u001b[32mINFO     \u001b[0m utils.py: logger init finished\u001b[0m\n",
            "2019-12-05 15:11:00 \u001b[32mINFO     \u001b[0m train.py: {'Lambda': 0.7,\n",
            " 'OHEM_ratio': 3,\n",
            " 'backbone': 'resnet50',\n",
            " 'checkpoint': '',\n",
            " 'data_shape': 640,\n",
            " 'display_input_images': False,\n",
            " 'display_interval': 10,\n",
            " 'display_output_images': False,\n",
            " 'end_lr': 1e-06,\n",
            " 'epochs': 300,\n",
            " 'gpu_id': '0',\n",
            " 'lr': 0.0001,\n",
            " 'lr_decay_step': [100, 200],\n",
            " 'lr_gamma': 0.1,\n",
            " 'm': 0.5,\n",
            " 'n': 6,\n",
            " 'output_dir': '/content/drive/My Drive/PSENet_2',\n",
            " 'pretrained': False,\n",
            " 'pretrained_path': '/content/drive/My Drive/PSENet_2/PSENet_resnet50.pth',\n",
            " 'restart_training': False,\n",
            " 'scale': 1,\n",
            " 'seed': 2,\n",
            " 'show_images_interval': 50,\n",
            " 'start_epoch': 0,\n",
            " 'testroot': 'Test Set',\n",
            " 'train_batch_size': 4,\n",
            " 'trainroot': 'Training Set',\n",
            " 'warm_up_epoch': 6,\n",
            " 'warm_up_lr': 1e-05,\n",
            " 'weight_decay': 0.0005,\n",
            " 'workers': 0}\u001b[0m\n",
            "2019-12-05 15:11:00 \u001b[32mINFO     \u001b[0m train.py: train with gpu 0 and pytorch 1.3.1\u001b[0m\n",
            "2019-12-05 15:11:12 \u001b[32mINFO     \u001b[0m train.py: train dataset has 1938 samples,484 in dataloader\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:100: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule.See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "2019-12-05 15:11:27 \u001b[32mINFO     \u001b[0m train.py: [0/300], [0/484], step: 0, 2.680 samples/sec, batch_loss: 0.1405, batch_loss_c: 0.1458, batch_loss_s: 0.1280, time:14.9241, lr:0.0001\u001b[0m\n",
            "tcmalloc: large alloc 1725898752 bytes == 0xaa75a000 @  0x7fe737aa21e7 0x7fe72c086f71 0x7fe72c0ea55d 0x7fe72c0ede28 0x7fe72c0ee3e5 0x7fe72c184fc2 0x50ac25 0x50d390 0x508245 0x50a080 0x50aa7d 0x50d390 0x509455 0x595311 0x5a067e 0x557d28 0x50c88b 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x509455 0x595311 0x5a522c 0x557f7e 0x4b5eff 0x50c810\n",
            "tcmalloc: large alloc 3385409536 bytes == 0x11458a000 @  0x7fe737aa21e7 0x7fe72c086f71 0x7fe72c0ea55d 0x7fe72c0ede28 0x7fe72c0ee3e5 0x7fe72c184fc2 0x50ac25 0x50d390 0x508245 0x50a080 0x50aa7d 0x50d390 0x509455 0x595311 0x5a067e 0x557d28 0x50c88b 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x509455 0x595311 0x5a522c 0x557f7e 0x4b5eff 0x50c810\n",
            "2019-12-05 15:11:57 \u001b[32mINFO     \u001b[0m train.py: [0/300], [10/484], step: 10, 1.370 samples/sec, batch_loss: 0.0515, batch_loss_c: 0.0412, batch_loss_s: 0.0754, time:29.2064, lr:0.0001\u001b[0m\n",
            "2019-12-05 15:12:10 \u001b[32mINFO     \u001b[0m train.py: [0/300], [20/484], step: 20, 3.030 samples/sec, batch_loss: 0.1063, batch_loss_c: 0.1197, batch_loss_s: 0.0748, time:13.2018, lr:0.0001\u001b[0m\n",
            "tcmalloc: large alloc 2904662016 bytes == 0x11458a000 @  0x7fe737aa21e7 0x7fe72c086f71 0x7fe72c0ea55d 0x7fe72c0ede28 0x7fe72c0ee3e5 0x7fe72c184fc2 0x50ac25 0x50d390 0x508245 0x50a080 0x50aa7d 0x50d390 0x509455 0x595311 0x5a067e 0x557d28 0x50c88b 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x509455 0x595311 0x5a522c 0x557f7e 0x4b5eff 0x50c810\n",
            "2019-12-05 15:12:35 \u001b[32mINFO     \u001b[0m train.py: [0/300], [30/484], step: 30, 1.557 samples/sec, batch_loss: 0.0743, batch_loss_c: 0.0617, batch_loss_s: 0.1038, time:25.6936, lr:0.0001\u001b[0m\n",
            "2019-12-05 15:12:48 \u001b[32mINFO     \u001b[0m train.py: [0/300], [40/484], step: 40, 3.269 samples/sec, batch_loss: 0.4396, batch_loss_c: 0.4819, batch_loss_s: 0.3410, time:12.2355, lr:0.0001\u001b[0m\n",
            "2019-12-05 15:13:06 \u001b[32mINFO     \u001b[0m train.py: [0/300], [50/484], step: 50, 2.169 samples/sec, batch_loss: 0.3454, batch_loss_c: 0.3418, batch_loss_s: 0.3537, time:18.4438, lr:0.0001\u001b[0m\n",
            "tcmalloc: large alloc 1874919424 bytes == 0x11458a000 @  0x7fe737aa21e7 0x7fe72c086f71 0x7fe72c0ea55d 0x7fe72c0ede28 0x7fe72c0ee3e5 0x7fe72c184fc2 0x50ac25 0x50d390 0x508245 0x50a080 0x50aa7d 0x50d390 0x509455 0x595311 0x5a067e 0x557d28 0x50c88b 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x509455 0x595311 0x5a522c 0x557f7e 0x4b5eff 0x50c810\n",
            "2019-12-05 15:13:31 \u001b[32mINFO     \u001b[0m train.py: [0/300], [60/484], step: 60, 1.599 samples/sec, batch_loss: 0.1173, batch_loss_c: 0.1150, batch_loss_s: 0.1226, time:25.0155, lr:0.0001\u001b[0m\n",
            "tcmalloc: large alloc 2142478336 bytes == 0x11458a000 @  0x7fe737aa21e7 0x7fe72c086f71 0x7fe72c0ea55d 0x7fe72c0ede28 0x7fe72c0ee3e5 0x7fe72c184fc2 0x50ac25 0x50d390 0x508245 0x50a080 0x50aa7d 0x50d390 0x509455 0x595311 0x5a067e 0x557d28 0x50c88b 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x509455 0x595311 0x5a522c 0x557f7e 0x4b5eff 0x50c810\n",
            "2019-12-05 15:14:01 \u001b[32mINFO     \u001b[0m train.py: [0/300], [70/484], step: 70, 1.356 samples/sec, batch_loss: 0.2387, batch_loss_c: 0.2345, batch_loss_s: 0.2485, time:29.5007, lr:0.0001\u001b[0m\n",
            "2019-12-05 15:14:15 \u001b[32mINFO     \u001b[0m train.py: [0/300], [80/484], step: 80, 2.810 samples/sec, batch_loss: 0.0734, batch_loss_c: 0.0637, batch_loss_s: 0.0958, time:14.2349, lr:0.0001\u001b[0m\n",
            "2019-12-05 15:14:27 \u001b[32mINFO     \u001b[0m train.py: [0/300], [90/484], step: 90, 3.176 samples/sec, batch_loss: 0.0893, batch_loss_c: 0.0836, batch_loss_s: 0.1025, time:12.5957, lr:0.0001\u001b[0m\n",
            "tcmalloc: large alloc 2684239872 bytes == 0x11458a000 @  0x7fe737aa21e7 0x7fe72c086f71 0x7fe72c0ea55d 0x7fe72c0ede28 0x7fe72c0ee3e5 0x7fe72c184fc2 0x50ac25 0x50d390 0x508245 0x50a080 0x50aa7d 0x50d390 0x509455 0x595311 0x5a067e 0x557d28 0x50c88b 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x509455 0x595311 0x5a522c 0x557f7e 0x4b5eff 0x50c810\n",
            "2019-12-05 15:14:52 \u001b[32mINFO     \u001b[0m train.py: [0/300], [100/484], step: 100, 1.650 samples/sec, batch_loss: 0.0446, batch_loss_c: 0.0367, batch_loss_s: 0.0630, time:24.2412, lr:0.0001\u001b[0m\n",
            "2019-12-05 15:15:03 \u001b[32mINFO     \u001b[0m train.py: [0/300], [110/484], step: 110, 3.689 samples/sec, batch_loss: 0.5050, batch_loss_c: 0.4839, batch_loss_s: 0.5541, time:10.8433, lr:0.0001\u001b[0m\n",
            "tcmalloc: large alloc 2633637888 bytes == 0x11458a000 @  0x7fe737aa21e7 0x7fe72c086f71 0x7fe72c0ea55d 0x7fe72c0ede28 0x7fe72c0ee3e5 0x7fe72c184fc2 0x50ac25 0x50d390 0x508245 0x50a080 0x50aa7d 0x50d390 0x509455 0x595311 0x5a067e 0x557d28 0x50c88b 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x509455 0x595311 0x5a522c 0x557f7e 0x4b5eff 0x50c810\n",
            "2019-12-05 15:15:15 \u001b[32mINFO     \u001b[0m train.py: [0/300], [120/484], step: 120, 3.245 samples/sec, batch_loss: 0.1694, batch_loss_c: 0.1717, batch_loss_s: 0.1643, time:12.3284, lr:0.0001\u001b[0m\n",
            "2019-12-05 15:15:26 \u001b[32mINFO     \u001b[0m train.py: [0/300], [130/484], step: 130, 3.454 samples/sec, batch_loss: 0.0619, batch_loss_c: 0.0486, batch_loss_s: 0.0932, time:11.5816, lr:0.0001\u001b[0m\n",
            "2019-12-05 15:15:43 \u001b[32mINFO     \u001b[0m train.py: [0/300], [140/484], step: 140, 2.416 samples/sec, batch_loss: 0.1613, batch_loss_c: 0.1663, batch_loss_s: 0.1496, time:16.5578, lr:0.0001\u001b[0m\n",
            "2019-12-05 15:16:01 \u001b[32mINFO     \u001b[0m train.py: [0/300], [150/484], step: 150, 2.188 samples/sec, batch_loss: 0.3271, batch_loss_c: 0.3186, batch_loss_s: 0.3467, time:18.2781, lr:0.0001\u001b[0m\n",
            "2019-12-05 15:16:13 \u001b[32mINFO     \u001b[0m train.py: [0/300], [160/484], step: 160, 3.454 samples/sec, batch_loss: 0.0660, batch_loss_c: 0.0567, batch_loss_s: 0.0877, time:11.5817, lr:0.0001\u001b[0m\n",
            "tcmalloc: large alloc 2633637888 bytes == 0x11458a000 @  0x7fe737aa21e7 0x7fe72c086f71 0x7fe72c0ea55d 0x7fe72c0ede28 0x7fe72c0ee3e5 0x7fe72c184fc2 0x50ac25 0x50d390 0x508245 0x50a080 0x50aa7d 0x50d390 0x509455 0x595311 0x5a067e 0x557d28 0x50c88b 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x509455 0x595311 0x5a522c 0x557f7e 0x4b5eff 0x50c810\n",
            "2019-12-05 15:16:36 \u001b[32mINFO     \u001b[0m train.py: [0/300], [170/484], step: 170, 1.746 samples/sec, batch_loss: 0.3083, batch_loss_c: 0.3010, batch_loss_s: 0.3254, time:22.9146, lr:0.0001\u001b[0m\n",
            "2019-12-05 15:16:47 \u001b[32mINFO     \u001b[0m train.py: [0/300], [180/484], step: 180, 3.589 samples/sec, batch_loss: 0.1131, batch_loss_c: 0.1170, batch_loss_s: 0.1039, time:11.1458, lr:0.0001\u001b[0m\n",
            "2019-12-05 15:17:07 \u001b[32mINFO     \u001b[0m train.py: [0/300], [190/484], step: 190, 2.007 samples/sec, batch_loss: 0.1773, batch_loss_c: 0.1739, batch_loss_s: 0.1851, time:19.9288, lr:0.0001\u001b[0m\n",
            "2019-12-05 15:17:25 \u001b[32mINFO     \u001b[0m train.py: [0/300], [200/484], step: 200, 2.177 samples/sec, batch_loss: 0.0936, batch_loss_c: 0.0914, batch_loss_s: 0.0989, time:18.3736, lr:0.0001\u001b[0m\n",
            "2019-12-05 15:17:37 \u001b[32mINFO     \u001b[0m train.py: [0/300], [210/484], step: 210, 3.513 samples/sec, batch_loss: 0.0568, batch_loss_c: 0.0483, batch_loss_s: 0.0766, time:11.3875, lr:0.0001\u001b[0m\n",
            "tcmalloc: large alloc 3344629760 bytes == 0x11458a000 @  0x7fe737aa21e7 0x7fe72c086f71 0x7fe72c0ea55d 0x7fe72c0ede28 0x7fe72c0ee3e5 0x7fe72c184fc2 0x50ac25 0x50d390 0x508245 0x50a080 0x50aa7d 0x50d390 0x509455 0x595311 0x5a067e 0x557d28 0x50c88b 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x509455 0x595311 0x5a522c 0x557f7e 0x4b5eff 0x50c810\n",
            "2019-12-05 15:18:14 \u001b[32mINFO     \u001b[0m train.py: [0/300], [220/484], step: 220, 1.066 samples/sec, batch_loss: 0.2993, batch_loss_c: 0.3347, batch_loss_s: 0.2166, time:37.5403, lr:0.0001\u001b[0m\n",
            "2019-12-05 15:18:44 \u001b[32mINFO     \u001b[0m train.py: [0/300], [230/484], step: 230, 1.348 samples/sec, batch_loss: 0.0558, batch_loss_c: 0.0395, batch_loss_s: 0.0937, time:29.6760, lr:0.0001\u001b[0m\n",
            "2019-12-05 15:19:03 \u001b[32mINFO     \u001b[0m train.py: [0/300], [240/484], step: 240, 2.121 samples/sec, batch_loss: 0.0728, batch_loss_c: 0.0697, batch_loss_s: 0.0801, time:18.8613, lr:0.0001\u001b[0m\n",
            "2019-12-05 15:19:18 \u001b[32mINFO     \u001b[0m train.py: [0/300], [250/484], step: 250, 2.632 samples/sec, batch_loss: 0.0933, batch_loss_c: 0.0834, batch_loss_s: 0.1163, time:15.1978, lr:0.0001\u001b[0m\n",
            "2019-12-05 15:19:31 \u001b[32mINFO     \u001b[0m train.py: [0/300], [260/484], step: 260, 3.153 samples/sec, batch_loss: 0.3024, batch_loss_c: 0.2979, batch_loss_s: 0.3128, time:12.6861, lr:0.0001\u001b[0m\n",
            "2019-12-05 15:19:49 \u001b[32mINFO     \u001b[0m train.py: [0/300], [270/484], step: 270, 2.177 samples/sec, batch_loss: 0.0874, batch_loss_c: 0.0718, batch_loss_s: 0.1239, time:18.3720, lr:0.0001\u001b[0m\n",
            "2019-12-05 15:20:00 \u001b[32mINFO     \u001b[0m train.py: [0/300], [280/484], step: 280, 3.726 samples/sec, batch_loss: 0.1191, batch_loss_c: 0.1150, batch_loss_s: 0.1287, time:10.7354, lr:0.0001\u001b[0m\n",
            "2019-12-05 15:20:13 \u001b[32mINFO     \u001b[0m train.py: [0/300], [290/484], step: 290, 2.971 samples/sec, batch_loss: 0.0865, batch_loss_c: 0.0797, batch_loss_s: 0.1023, time:13.4650, lr:0.0001\u001b[0m\n",
            "2019-12-05 15:20:33 \u001b[32mINFO     \u001b[0m train.py: [0/300], [300/484], step: 300, 2.047 samples/sec, batch_loss: 0.0564, batch_loss_c: 0.0509, batch_loss_s: 0.0693, time:19.5419, lr:0.0001\u001b[0m\n",
            "2019-12-05 15:21:08 \u001b[32mINFO     \u001b[0m train.py: [0/300], [310/484], step: 310, 1.143 samples/sec, batch_loss: 0.3805, batch_loss_c: 0.3959, batch_loss_s: 0.3445, time:34.9867, lr:0.0001\u001b[0m\n",
            "2019-12-05 15:21:20 \u001b[32mINFO     \u001b[0m train.py: [0/300], [320/484], step: 320, 3.184 samples/sec, batch_loss: 0.3285, batch_loss_c: 0.3169, batch_loss_s: 0.3554, time:12.5634, lr:0.0001\u001b[0m\n",
            "2019-12-05 15:21:32 \u001b[32mINFO     \u001b[0m train.py: [0/300], [330/484], step: 330, 3.530 samples/sec, batch_loss: 0.0886, batch_loss_c: 0.0785, batch_loss_s: 0.1122, time:11.3326, lr:0.0001\u001b[0m\n",
            "2019-12-05 15:21:43 \u001b[32mINFO     \u001b[0m train.py: [0/300], [340/484], step: 340, 3.495 samples/sec, batch_loss: 0.2999, batch_loss_c: 0.2984, batch_loss_s: 0.3036, time:11.4454, lr:0.0001\u001b[0m\n",
            "2019-12-05 15:22:00 \u001b[32mINFO     \u001b[0m train.py: [0/300], [350/484], step: 350, 2.420 samples/sec, batch_loss: 0.3609, batch_loss_c: 0.3663, batch_loss_s: 0.3483, time:16.5282, lr:0.0001\u001b[0m\n",
            "2019-12-05 15:22:11 \u001b[32mINFO     \u001b[0m train.py: [0/300], [360/484], step: 360, 3.434 samples/sec, batch_loss: 0.0436, batch_loss_c: 0.0389, batch_loss_s: 0.0546, time:11.6486, lr:0.0001\u001b[0m\n",
            "2019-12-05 15:22:30 \u001b[32mINFO     \u001b[0m train.py: [0/300], [370/484], step: 370, 2.179 samples/sec, batch_loss: 0.1501, batch_loss_c: 0.1351, batch_loss_s: 0.1850, time:18.3570, lr:0.0001\u001b[0m\n",
            "2019-12-05 15:22:43 \u001b[32mINFO     \u001b[0m train.py: [0/300], [380/484], step: 380, 2.947 samples/sec, batch_loss: 0.0700, batch_loss_c: 0.0669, batch_loss_s: 0.0772, time:13.5711, lr:0.0001\u001b[0m\n",
            "2019-12-05 15:22:55 \u001b[32mINFO     \u001b[0m train.py: [0/300], [390/484], step: 390, 3.403 samples/sec, batch_loss: 0.3155, batch_loss_c: 0.3186, batch_loss_s: 0.3081, time:11.7551, lr:0.0001\u001b[0m\n",
            "2019-12-05 15:23:07 \u001b[32mINFO     \u001b[0m train.py: [0/300], [400/484], step: 400, 3.191 samples/sec, batch_loss: 0.0651, batch_loss_c: 0.0593, batch_loss_s: 0.0788, time:12.5362, lr:0.0001\u001b[0m\n",
            "2019-12-05 15:23:26 \u001b[32mINFO     \u001b[0m train.py: [0/300], [410/484], step: 410, 2.203 samples/sec, batch_loss: 0.1193, batch_loss_c: 0.0980, batch_loss_s: 0.1690, time:18.1546, lr:0.0001\u001b[0m\n",
            "2019-12-05 15:23:34 \u001b[32mINFO     \u001b[0m train.py: [0/300], [420/484], step: 420, 4.800 samples/sec, batch_loss: 0.0786, batch_loss_c: 0.0720, batch_loss_s: 0.0939, time:8.3329, lr:0.0001\u001b[0m\n",
            "tcmalloc: large alloc 3188178944 bytes == 0x11458a000 @  0x7fe737aa21e7 0x7fe72c086f71 0x7fe72c0ea55d 0x7fe72c0ede28 0x7fe72c0ee3e5 0x7fe72c184fc2 0x50ac25 0x50d390 0x508245 0x50a080 0x50aa7d 0x50d390 0x509455 0x595311 0x5a067e 0x557d28 0x50c88b 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x509455 0x595311 0x5a522c 0x557f7e 0x4b5eff 0x50c810\n",
            "2019-12-05 15:23:56 \u001b[32mINFO     \u001b[0m train.py: [0/300], [430/484], step: 430, 1.798 samples/sec, batch_loss: 0.3211, batch_loss_c: 0.3006, batch_loss_s: 0.3691, time:22.2474, lr:0.0001\u001b[0m\n",
            "2019-12-05 15:24:13 \u001b[32mINFO     \u001b[0m train.py: [0/300], [440/484], step: 440, 2.325 samples/sec, batch_loss: 0.2510, batch_loss_c: 0.2564, batch_loss_s: 0.2382, time:17.2077, lr:0.0001\u001b[0m\n",
            "2019-12-05 15:24:32 \u001b[32mINFO     \u001b[0m train.py: [0/300], [450/484], step: 450, 2.157 samples/sec, batch_loss: 0.3022, batch_loss_c: 0.3030, batch_loss_s: 0.3004, time:18.5444, lr:0.0001\u001b[0m\n",
            "2019-12-05 15:24:45 \u001b[32mINFO     \u001b[0m train.py: [0/300], [460/484], step: 460, 3.023 samples/sec, batch_loss: 0.0546, batch_loss_c: 0.0484, batch_loss_s: 0.0689, time:13.2340, lr:0.0001\u001b[0m\n",
            "2019-12-05 15:25:15 \u001b[32mINFO     \u001b[0m train.py: [0/300], [470/484], step: 470, 1.338 samples/sec, batch_loss: 0.1186, batch_loss_c: 0.1183, batch_loss_s: 0.1195, time:29.8857, lr:0.0001\u001b[0m\n",
            "2019-12-05 15:25:32 \u001b[32mINFO     \u001b[0m train.py: [0/300], [480/484], step: 480, 2.327 samples/sec, batch_loss: 0.3428, batch_loss_c: 0.3390, batch_loss_s: 0.3517, time:17.1904, lr:0.0001\u001b[0m\n",
            "2019-12-05 15:25:35 \u001b[32mINFO     \u001b[0m train.py: [0/300], train_loss: 0.1665, time: 862.3007, lr: 0.0001\u001b[0m\n",
            "2019-12-05 15:25:36 \u001b[32mINFO     \u001b[0m train.py: [1/300], [0/484], step: 484, 41.045 samples/sec, batch_loss: 0.0765, batch_loss_c: 0.0743, batch_loss_s: 0.0818, time:0.9745, lr:0.0001\u001b[0m\n",
            "2019-12-05 15:25:46 \u001b[32mINFO     \u001b[0m train.py: [1/300], [10/484], step: 494, 3.861 samples/sec, batch_loss: 0.0638, batch_loss_c: 0.0580, batch_loss_s: 0.0774, time:10.3593, lr:0.0001\u001b[0m\n",
            "2019-12-05 15:26:05 \u001b[32mINFO     \u001b[0m train.py: [1/300], [20/484], step: 504, 2.206 samples/sec, batch_loss: 0.4550, batch_loss_c: 0.4676, batch_loss_s: 0.4257, time:18.1335, lr:0.0001\u001b[0m\n",
            "2019-12-05 15:26:15 \u001b[32mINFO     \u001b[0m train.py: [1/300], [30/484], step: 514, 3.948 samples/sec, batch_loss: 0.1394, batch_loss_c: 0.1236, batch_loss_s: 0.1762, time:10.1323, lr:0.0001\u001b[0m\n",
            "2019-12-05 15:26:47 \u001b[32mINFO     \u001b[0m train.py: [1/300], [40/484], step: 524, 1.244 samples/sec, batch_loss: 0.1252, batch_loss_c: 0.1173, batch_loss_s: 0.1437, time:32.1549, lr:0.0001\u001b[0m\n",
            "2019-12-05 15:26:58 \u001b[32mINFO     \u001b[0m train.py: [1/300], [50/484], step: 534, 3.480 samples/sec, batch_loss: 0.0832, batch_loss_c: 0.0821, batch_loss_s: 0.0858, time:11.4927, lr:0.0001\u001b[0m\n",
            "2019-12-05 15:27:12 \u001b[32mINFO     \u001b[0m train.py: [1/300], [60/484], step: 544, 2.868 samples/sec, batch_loss: 0.3068, batch_loss_c: 0.2998, batch_loss_s: 0.3233, time:13.9461, lr:0.0001\u001b[0m\n",
            "2019-12-05 15:27:24 \u001b[32mINFO     \u001b[0m train.py: [1/300], [70/484], step: 554, 3.370 samples/sec, batch_loss: 0.0815, batch_loss_c: 0.0718, batch_loss_s: 0.1042, time:11.8680, lr:0.0001\u001b[0m\n",
            "2019-12-05 15:27:46 \u001b[32mINFO     \u001b[0m train.py: [1/300], [80/484], step: 564, 1.841 samples/sec, batch_loss: 0.0552, batch_loss_c: 0.0484, batch_loss_s: 0.0710, time:21.7249, lr:0.0001\u001b[0m\n",
            "2019-12-05 15:28:09 \u001b[32mINFO     \u001b[0m train.py: [1/300], [90/484], step: 574, 1.726 samples/sec, batch_loss: 0.0830, batch_loss_c: 0.0795, batch_loss_s: 0.0911, time:23.1723, lr:0.0001\u001b[0m\n",
            "2019-12-05 15:28:28 \u001b[32mINFO     \u001b[0m train.py: [1/300], [100/484], step: 584, 2.124 samples/sec, batch_loss: 0.1068, batch_loss_c: 0.1080, batch_loss_s: 0.1040, time:18.8354, lr:0.0001\u001b[0m\n",
            "2019-12-05 15:28:43 \u001b[32mINFO     \u001b[0m train.py: [1/300], [110/484], step: 594, 2.695 samples/sec, batch_loss: 0.1290, batch_loss_c: 0.1446, batch_loss_s: 0.0926, time:14.8409, lr:0.0001\u001b[0m\n",
            "2019-12-05 15:29:25 \u001b[32mINFO     \u001b[0m train.py: [1/300], [120/484], step: 604, 0.950 samples/sec, batch_loss: 0.3201, batch_loss_c: 0.3187, batch_loss_s: 0.3234, time:42.0877, lr:0.0001\u001b[0m\n",
            "2019-12-05 15:29:36 \u001b[32mINFO     \u001b[0m train.py: [1/300], [130/484], step: 614, 3.608 samples/sec, batch_loss: 0.3691, batch_loss_c: 0.3582, batch_loss_s: 0.3945, time:11.0874, lr:0.0001\u001b[0m\n",
            "2019-12-05 15:29:46 \u001b[32mINFO     \u001b[0m train.py: [1/300], [140/484], step: 624, 3.988 samples/sec, batch_loss: 0.0770, batch_loss_c: 0.0724, batch_loss_s: 0.0877, time:10.0301, lr:0.0001\u001b[0m\n",
            "2019-12-05 15:30:08 \u001b[32mINFO     \u001b[0m train.py: [1/300], [150/484], step: 634, 1.829 samples/sec, batch_loss: 0.0649, batch_loss_c: 0.0467, batch_loss_s: 0.1074, time:21.8727, lr:0.0001\u001b[0m\n",
            "2019-12-05 15:30:18 \u001b[32mINFO     \u001b[0m train.py: [1/300], [160/484], step: 644, 3.825 samples/sec, batch_loss: 0.0667, batch_loss_c: 0.0574, batch_loss_s: 0.0885, time:10.4574, lr:0.0001\u001b[0m\n",
            "2019-12-05 15:30:30 \u001b[32mINFO     \u001b[0m train.py: [1/300], [170/484], step: 654, 3.304 samples/sec, batch_loss: 0.1249, batch_loss_c: 0.1170, batch_loss_s: 0.1433, time:12.1053, lr:0.0001\u001b[0m\n",
            "2019-12-05 15:30:41 \u001b[32mINFO     \u001b[0m train.py: [1/300], [180/484], step: 664, 3.761 samples/sec, batch_loss: 0.0744, batch_loss_c: 0.0695, batch_loss_s: 0.0857, time:10.6343, lr:0.0001\u001b[0m\n",
            "2019-12-05 15:30:58 \u001b[32mINFO     \u001b[0m train.py: [1/300], [190/484], step: 674, 2.303 samples/sec, batch_loss: 0.0718, batch_loss_c: 0.0658, batch_loss_s: 0.0857, time:17.3658, lr:0.0001\u001b[0m\n",
            "2019-12-05 15:31:24 \u001b[32mINFO     \u001b[0m train.py: [1/300], [200/484], step: 684, 1.564 samples/sec, batch_loss: 0.3121, batch_loss_c: 0.3068, batch_loss_s: 0.3243, time:25.5833, lr:0.0001\u001b[0m\n",
            "2019-12-05 15:31:45 \u001b[32mINFO     \u001b[0m train.py: [1/300], [210/484], step: 694, 1.944 samples/sec, batch_loss: 0.1349, batch_loss_c: 0.1416, batch_loss_s: 0.1194, time:20.5739, lr:0.0001\u001b[0m\n",
            "2019-12-05 15:32:04 \u001b[32mINFO     \u001b[0m train.py: [1/300], [220/484], step: 704, 2.012 samples/sec, batch_loss: 0.0705, batch_loss_c: 0.0614, batch_loss_s: 0.0920, time:19.8787, lr:0.0001\u001b[0m\n",
            "2019-12-05 15:32:20 \u001b[32mINFO     \u001b[0m train.py: [1/300], [230/484], step: 714, 2.512 samples/sec, batch_loss: 0.2889, batch_loss_c: 0.2818, batch_loss_s: 0.3056, time:15.9208, lr:0.0001\u001b[0m\n",
            "2019-12-05 15:32:34 \u001b[32mINFO     \u001b[0m train.py: [1/300], [240/484], step: 724, 2.872 samples/sec, batch_loss: 0.1406, batch_loss_c: 0.1473, batch_loss_s: 0.1249, time:13.9256, lr:0.0001\u001b[0m\n",
            "2019-12-05 15:32:53 \u001b[32mINFO     \u001b[0m train.py: [1/300], [250/484], step: 734, 2.132 samples/sec, batch_loss: 0.0563, batch_loss_c: 0.0521, batch_loss_s: 0.0659, time:18.7648, lr:0.0001\u001b[0m\n",
            "tcmalloc: large alloc 4695465984 bytes == 0x7fe292ee4000 @  0x7fe737aa21e7 0x7fe72c086f71 0x7fe72c0ea55d 0x7fe72c0ede28 0x7fe72c0ee3e5 0x7fe72c184fc2 0x50ac25 0x50d390 0x508245 0x50a080 0x50aa7d 0x50d390 0x509455 0x595311 0x5a067e 0x557d28 0x50c88b 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x509455 0x595311 0x5a522c 0x557f7e 0x4b5eff 0x50c810\n",
            "2019-12-05 15:33:18 \u001b[32mINFO     \u001b[0m train.py: [1/300], [260/484], step: 744, 1.603 samples/sec, batch_loss: 0.2968, batch_loss_c: 0.2925, batch_loss_s: 0.3069, time:24.9493, lr:0.0001\u001b[0m\n",
            "2019-12-05 15:33:30 \u001b[32mINFO     \u001b[0m train.py: [1/300], [270/484], step: 754, 3.448 samples/sec, batch_loss: 0.0885, batch_loss_c: 0.0881, batch_loss_s: 0.0893, time:11.6013, lr:0.0001\u001b[0m\n",
            "2019-12-05 15:33:45 \u001b[32mINFO     \u001b[0m train.py: [1/300], [280/484], step: 764, 2.608 samples/sec, batch_loss: 0.3263, batch_loss_c: 0.3241, batch_loss_s: 0.3316, time:15.3363, lr:0.0001\u001b[0m\n",
            "2019-12-05 15:34:08 \u001b[32mINFO     \u001b[0m train.py: [1/300], [290/484], step: 774, 1.707 samples/sec, batch_loss: 0.0514, batch_loss_c: 0.0442, batch_loss_s: 0.0684, time:23.4382, lr:0.0001\u001b[0m\n",
            "2019-12-05 15:34:23 \u001b[32mINFO     \u001b[0m train.py: [1/300], [300/484], step: 784, 2.813 samples/sec, batch_loss: 0.0652, batch_loss_c: 0.0643, batch_loss_s: 0.0671, time:14.2172, lr:0.0001\u001b[0m\n",
            "2019-12-05 15:34:37 \u001b[32mINFO     \u001b[0m train.py: [1/300], [310/484], step: 794, 2.887 samples/sec, batch_loss: 0.5809, batch_loss_c: 0.5819, batch_loss_s: 0.5785, time:13.8553, lr:0.0001\u001b[0m\n",
            "2019-12-05 15:34:46 \u001b[32mINFO     \u001b[0m train.py: [1/300], [320/484], step: 804, 4.195 samples/sec, batch_loss: 0.1073, batch_loss_c: 0.1055, batch_loss_s: 0.1117, time:9.5361, lr:0.0001\u001b[0m\n",
            "2019-12-05 15:35:02 \u001b[32mINFO     \u001b[0m train.py: [1/300], [330/484], step: 814, 2.528 samples/sec, batch_loss: 0.4612, batch_loss_c: 0.4972, batch_loss_s: 0.3772, time:15.8240, lr:0.0001\u001b[0m\n",
            "2019-12-05 15:35:12 \u001b[32mINFO     \u001b[0m train.py: [1/300], [340/484], step: 824, 3.992 samples/sec, batch_loss: 0.0472, batch_loss_c: 0.0434, batch_loss_s: 0.0560, time:10.0199, lr:0.0001\u001b[0m\n",
            "2019-12-05 15:35:28 \u001b[32mINFO     \u001b[0m train.py: [1/300], [350/484], step: 834, 2.550 samples/sec, batch_loss: 0.0708, batch_loss_c: 0.0603, batch_loss_s: 0.0953, time:15.6847, lr:0.0001\u001b[0m\n",
            "2019-12-05 15:35:39 \u001b[32mINFO     \u001b[0m train.py: [1/300], [360/484], step: 844, 3.440 samples/sec, batch_loss: 0.0571, batch_loss_c: 0.0502, batch_loss_s: 0.0732, time:11.6282, lr:0.0001\u001b[0m\n",
            "2019-12-05 15:35:59 \u001b[32mINFO     \u001b[0m train.py: [1/300], [370/484], step: 854, 2.057 samples/sec, batch_loss: 0.2300, batch_loss_c: 0.2450, batch_loss_s: 0.1949, time:19.4422, lr:0.0001\u001b[0m\n",
            "2019-12-05 15:36:17 \u001b[32mINFO     \u001b[0m train.py: [1/300], [380/484], step: 864, 2.218 samples/sec, batch_loss: 0.0980, batch_loss_c: 0.0868, batch_loss_s: 0.1241, time:18.0351, lr:0.0001\u001b[0m\n",
            "2019-12-05 15:36:34 \u001b[32mINFO     \u001b[0m train.py: [1/300], [390/484], step: 874, 2.335 samples/sec, batch_loss: 0.2109, batch_loss_c: 0.2492, batch_loss_s: 0.1213, time:17.1303, lr:0.0001\u001b[0m\n",
            "2019-12-05 15:36:48 \u001b[32mINFO     \u001b[0m train.py: [1/300], [400/484], step: 884, 2.783 samples/sec, batch_loss: 0.3982, batch_loss_c: 0.4253, batch_loss_s: 0.3351, time:14.3719, lr:0.0001\u001b[0m\n",
            "2019-12-05 15:37:02 \u001b[32mINFO     \u001b[0m train.py: [1/300], [410/484], step: 894, 2.925 samples/sec, batch_loss: 0.1528, batch_loss_c: 0.1618, batch_loss_s: 0.1318, time:13.6733, lr:0.0001\u001b[0m\n",
            "2019-12-05 15:37:17 \u001b[32mINFO     \u001b[0m train.py: [1/300], [420/484], step: 904, 2.654 samples/sec, batch_loss: 0.0932, batch_loss_c: 0.0954, batch_loss_s: 0.0880, time:15.0744, lr:0.0001\u001b[0m\n",
            "2019-12-05 15:37:26 \u001b[32mINFO     \u001b[0m train.py: [1/300], [430/484], step: 914, 4.561 samples/sec, batch_loss: 0.1067, batch_loss_c: 0.1087, batch_loss_s: 0.1022, time:8.7708, lr:0.0001\u001b[0m\n",
            "2019-12-05 15:37:36 \u001b[32mINFO     \u001b[0m train.py: [1/300], [440/484], step: 924, 3.994 samples/sec, batch_loss: 0.2950, batch_loss_c: 0.2819, batch_loss_s: 0.3254, time:10.0158, lr:0.0001\u001b[0m\n",
            "2019-12-05 15:37:48 \u001b[32mINFO     \u001b[0m train.py: [1/300], [450/484], step: 934, 3.237 samples/sec, batch_loss: 0.3454, batch_loss_c: 0.3335, batch_loss_s: 0.3732, time:12.3563, lr:0.0001\u001b[0m\n",
            "2019-12-05 15:38:03 \u001b[32mINFO     \u001b[0m train.py: [1/300], [460/484], step: 944, 2.690 samples/sec, batch_loss: 0.0540, batch_loss_c: 0.0471, batch_loss_s: 0.0700, time:14.8673, lr:0.0001\u001b[0m\n",
            "2019-12-05 15:38:23 \u001b[32mINFO     \u001b[0m train.py: [1/300], [470/484], step: 954, 2.034 samples/sec, batch_loss: 0.3859, batch_loss_c: 0.3747, batch_loss_s: 0.4122, time:19.6696, lr:0.0001\u001b[0m\n",
            "2019-12-05 15:38:39 \u001b[32mINFO     \u001b[0m train.py: [1/300], [480/484], step: 964, 2.439 samples/sec, batch_loss: 0.0697, batch_loss_c: 0.0632, batch_loss_s: 0.0848, time:16.3994, lr:0.0001\u001b[0m\n",
            "2019-12-05 15:38:41 \u001b[32mINFO     \u001b[0m train.py: [1/300], train_loss: 0.1528, time: 786.0103, lr: 0.0001\u001b[0m\n",
            "2019-12-05 15:38:43 \u001b[32mINFO     \u001b[0m train.py: [2/300], [0/484], step: 968, 24.111 samples/sec, batch_loss: 0.3124, batch_loss_c: 0.3033, batch_loss_s: 0.3336, time:1.6590, lr:0.0001\u001b[0m\n",
            "2019-12-05 15:38:54 \u001b[32mINFO     \u001b[0m train.py: [2/300], [10/484], step: 978, 3.828 samples/sec, batch_loss: 0.0633, batch_loss_c: 0.0633, batch_loss_s: 0.0632, time:10.4482, lr:0.0001\u001b[0m\n",
            "2019-12-05 15:39:09 \u001b[32mINFO     \u001b[0m train.py: [2/300], [20/484], step: 988, 2.692 samples/sec, batch_loss: 0.0622, batch_loss_c: 0.0526, batch_loss_s: 0.0848, time:14.8571, lr:0.0001\u001b[0m\n",
            "2019-12-05 15:39:24 \u001b[32mINFO     \u001b[0m train.py: [2/300], [30/484], step: 998, 2.668 samples/sec, batch_loss: 0.1758, batch_loss_c: 0.1816, batch_loss_s: 0.1622, time:14.9903, lr:0.0001\u001b[0m\n",
            "2019-12-05 15:39:36 \u001b[32mINFO     \u001b[0m train.py: [2/300], [40/484], step: 1008, 3.347 samples/sec, batch_loss: 0.1669, batch_loss_c: 0.2028, batch_loss_s: 0.0833, time:11.9495, lr:0.0001\u001b[0m\n",
            "2019-12-05 15:39:58 \u001b[32mINFO     \u001b[0m train.py: [2/300], [50/484], step: 1018, 1.753 samples/sec, batch_loss: 0.1360, batch_loss_c: 0.1296, batch_loss_s: 0.1510, time:22.8157, lr:0.0001\u001b[0m\n",
            "2019-12-05 15:40:11 \u001b[32mINFO     \u001b[0m train.py: [2/300], [60/484], step: 1028, 3.158 samples/sec, batch_loss: 0.0674, batch_loss_c: 0.0622, batch_loss_s: 0.0795, time:12.6678, lr:0.0001\u001b[0m\n",
            "2019-12-05 15:40:41 \u001b[32mINFO     \u001b[0m train.py: [2/300], [70/484], step: 1038, 1.346 samples/sec, batch_loss: 0.0741, batch_loss_c: 0.0706, batch_loss_s: 0.0822, time:29.7265, lr:0.0001\u001b[0m\n",
            "2019-12-05 15:40:53 \u001b[32mINFO     \u001b[0m train.py: [2/300], [80/484], step: 1048, 3.166 samples/sec, batch_loss: 0.1023, batch_loss_c: 0.0901, batch_loss_s: 0.1310, time:12.6360, lr:0.0001\u001b[0m\n",
            "2019-12-05 15:41:10 \u001b[32mINFO     \u001b[0m train.py: [2/300], [90/484], step: 1058, 2.357 samples/sec, batch_loss: 0.0581, batch_loss_c: 0.0498, batch_loss_s: 0.0774, time:16.9678, lr:0.0001\u001b[0m\n",
            "2019-12-05 15:41:22 \u001b[32mINFO     \u001b[0m train.py: [2/300], [100/484], step: 1068, 3.572 samples/sec, batch_loss: 0.0919, batch_loss_c: 0.0740, batch_loss_s: 0.1336, time:11.1983, lr:0.0001\u001b[0m\n",
            "2019-12-05 15:41:35 \u001b[32mINFO     \u001b[0m train.py: [2/300], [110/484], step: 1078, 2.947 samples/sec, batch_loss: 0.0692, batch_loss_c: 0.0651, batch_loss_s: 0.0788, time:13.5713, lr:0.0001\u001b[0m\n",
            "2019-12-05 15:41:47 \u001b[32mINFO     \u001b[0m train.py: [2/300], [120/484], step: 1088, 3.484 samples/sec, batch_loss: 0.0616, batch_loss_c: 0.0538, batch_loss_s: 0.0798, time:11.4823, lr:0.0001\u001b[0m\n",
            "2019-12-05 15:42:02 \u001b[32mINFO     \u001b[0m train.py: [2/300], [130/484], step: 1098, 2.590 samples/sec, batch_loss: 0.2578, batch_loss_c: 0.2789, batch_loss_s: 0.2085, time:15.4456, lr:0.0001\u001b[0m\n",
            "2019-12-05 15:42:16 \u001b[32mINFO     \u001b[0m train.py: [2/300], [140/484], step: 1108, 2.820 samples/sec, batch_loss: 0.3316, batch_loss_c: 0.3126, batch_loss_s: 0.3759, time:14.1823, lr:0.0001\u001b[0m\n",
            "2019-12-05 15:42:26 \u001b[32mINFO     \u001b[0m train.py: [2/300], [150/484], step: 1118, 4.233 samples/sec, batch_loss: 0.0903, batch_loss_c: 0.0772, batch_loss_s: 0.1208, time:9.4505, lr:0.0001\u001b[0m\n",
            "tcmalloc: large alloc 5197824000 bytes == 0x7fe292ee4000 @  0x7fe737aa21e7 0x7fe72c086f71 0x7fe72c0ea55d 0x7fe72c0ede28 0x7fe72c0ee3e5 0x7fe72c184fc2 0x50ac25 0x50d390 0x508245 0x50a080 0x50aa7d 0x50d390 0x509455 0x595311 0x5a067e 0x557d28 0x50c88b 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x509455 0x595311 0x5a522c 0x557f7e 0x4b5eff 0x50c810\n",
            "2019-12-05 15:42:41 \u001b[32mINFO     \u001b[0m train.py: [2/300], [160/484], step: 1128, 2.547 samples/sec, batch_loss: 0.2920, batch_loss_c: 0.2851, batch_loss_s: 0.3081, time:15.7030, lr:0.0001\u001b[0m\n",
            "2019-12-05 15:43:21 \u001b[32mINFO     \u001b[0m train.py: [2/300], [170/484], step: 1138, 1.013 samples/sec, batch_loss: 0.0884, batch_loss_c: 0.0841, batch_loss_s: 0.0985, time:39.5042, lr:0.0001\u001b[0m\n",
            "2019-12-05 15:43:34 \u001b[32mINFO     \u001b[0m train.py: [2/300], [180/484], step: 1148, 3.055 samples/sec, batch_loss: 0.1524, batch_loss_c: 0.1314, batch_loss_s: 0.2014, time:13.0930, lr:0.0001\u001b[0m\n",
            "2019-12-05 15:43:47 \u001b[32mINFO     \u001b[0m train.py: [2/300], [190/484], step: 1158, 2.993 samples/sec, batch_loss: 0.1125, batch_loss_c: 0.1042, batch_loss_s: 0.1319, time:13.3653, lr:0.0001\u001b[0m\n",
            "2019-12-05 15:44:07 \u001b[32mINFO     \u001b[0m train.py: [2/300], [200/484], step: 1168, 2.010 samples/sec, batch_loss: 0.1113, batch_loss_c: 0.1108, batch_loss_s: 0.1124, time:19.8980, lr:0.0001\u001b[0m\n",
            "2019-12-05 15:44:24 \u001b[32mINFO     \u001b[0m train.py: [2/300], [210/484], step: 1178, 2.347 samples/sec, batch_loss: 0.0954, batch_loss_c: 0.0982, batch_loss_s: 0.0888, time:17.0431, lr:0.0001\u001b[0m\n",
            "2019-12-05 15:44:36 \u001b[32mINFO     \u001b[0m train.py: [2/300], [220/484], step: 1188, 3.391 samples/sec, batch_loss: 0.1528, batch_loss_c: 0.1681, batch_loss_s: 0.1170, time:11.7959, lr:0.0001\u001b[0m\n",
            "2019-12-05 15:44:49 \u001b[32mINFO     \u001b[0m train.py: [2/300], [230/484], step: 1198, 3.145 samples/sec, batch_loss: 0.1079, batch_loss_c: 0.1036, batch_loss_s: 0.1180, time:12.7201, lr:0.0001\u001b[0m\n",
            "2019-12-05 15:45:12 \u001b[32mINFO     \u001b[0m train.py: [2/300], [240/484], step: 1208, 1.700 samples/sec, batch_loss: 0.0541, batch_loss_c: 0.0458, batch_loss_s: 0.0734, time:23.5265, lr:0.0001\u001b[0m\n",
            "2019-12-05 15:45:22 \u001b[32mINFO     \u001b[0m train.py: [2/300], [250/484], step: 1218, 4.231 samples/sec, batch_loss: 0.2811, batch_loss_c: 0.2746, batch_loss_s: 0.2964, time:9.4539, lr:0.0001\u001b[0m\n",
            "2019-12-05 15:45:43 \u001b[32mINFO     \u001b[0m train.py: [2/300], [260/484], step: 1228, 1.893 samples/sec, batch_loss: 0.1895, batch_loss_c: 0.1949, batch_loss_s: 0.1768, time:21.1273, lr:0.0001\u001b[0m\n",
            "2019-12-05 15:45:54 \u001b[32mINFO     \u001b[0m train.py: [2/300], [270/484], step: 1238, 3.626 samples/sec, batch_loss: 0.2808, batch_loss_c: 0.2594, batch_loss_s: 0.3305, time:11.0317, lr:0.0001\u001b[0m\n",
            "2019-12-05 15:46:04 \u001b[32mINFO     \u001b[0m train.py: [2/300], [280/484], step: 1248, 3.899 samples/sec, batch_loss: 0.0818, batch_loss_c: 0.0720, batch_loss_s: 0.1049, time:10.2597, lr:0.0001\u001b[0m\n",
            "2019-12-05 15:46:24 \u001b[32mINFO     \u001b[0m train.py: [2/300], [290/484], step: 1258, 1.995 samples/sec, batch_loss: 0.2151, batch_loss_c: 0.1874, batch_loss_s: 0.2797, time:20.0513, lr:0.0001\u001b[0m\n",
            "2019-12-05 15:46:49 \u001b[32mINFO     \u001b[0m train.py: [2/300], [300/484], step: 1268, 1.618 samples/sec, batch_loss: 0.0925, batch_loss_c: 0.0847, batch_loss_s: 0.1107, time:24.7208, lr:0.0001\u001b[0m\n",
            "2019-12-05 15:47:08 \u001b[32mINFO     \u001b[0m train.py: [2/300], [310/484], step: 1278, 2.120 samples/sec, batch_loss: 0.5205, batch_loss_c: 0.5165, batch_loss_s: 0.5300, time:18.8641, lr:0.0001\u001b[0m\n",
            "2019-12-05 15:47:29 \u001b[32mINFO     \u001b[0m train.py: [2/300], [320/484], step: 1288, 1.872 samples/sec, batch_loss: 0.0654, batch_loss_c: 0.0560, batch_loss_s: 0.0874, time:21.3669, lr:0.0001\u001b[0m\n",
            "2019-12-05 15:47:39 \u001b[32mINFO     \u001b[0m train.py: [2/300], [330/484], step: 1298, 3.925 samples/sec, batch_loss: 0.1403, batch_loss_c: 0.1341, batch_loss_s: 0.1548, time:10.1899, lr:0.0001\u001b[0m\n",
            "2019-12-05 15:47:54 \u001b[32mINFO     \u001b[0m train.py: [2/300], [340/484], step: 1308, 2.783 samples/sec, batch_loss: 0.0966, batch_loss_c: 0.0954, batch_loss_s: 0.0997, time:14.3730, lr:0.0001\u001b[0m\n",
            "2019-12-05 15:48:17 \u001b[32mINFO     \u001b[0m train.py: [2/300], [350/484], step: 1318, 1.717 samples/sec, batch_loss: 0.1328, batch_loss_c: 0.1559, batch_loss_s: 0.0790, time:23.2914, lr:0.0001\u001b[0m\n",
            "2019-12-05 15:48:30 \u001b[32mINFO     \u001b[0m train.py: [2/300], [360/484], step: 1328, 3.017 samples/sec, batch_loss: 0.0800, batch_loss_c: 0.0842, batch_loss_s: 0.0702, time:13.2569, lr:0.0001\u001b[0m\n",
            "2019-12-05 15:48:46 \u001b[32mINFO     \u001b[0m train.py: [2/300], [370/484], step: 1338, 2.532 samples/sec, batch_loss: 0.2183, batch_loss_c: 0.2038, batch_loss_s: 0.2522, time:15.7965, lr:0.0001\u001b[0m\n",
            "2019-12-05 15:48:57 \u001b[32mINFO     \u001b[0m train.py: [2/300], [380/484], step: 1348, 3.771 samples/sec, batch_loss: 0.1151, batch_loss_c: 0.1203, batch_loss_s: 0.1029, time:10.6078, lr:0.0001\u001b[0m\n",
            "2019-12-05 15:49:12 \u001b[32mINFO     \u001b[0m train.py: [2/300], [390/484], step: 1358, 2.552 samples/sec, batch_loss: 0.0476, batch_loss_c: 0.0380, batch_loss_s: 0.0700, time:15.6720, lr:0.0001\u001b[0m\n",
            "2019-12-05 15:49:21 \u001b[32mINFO     \u001b[0m train.py: [2/300], [400/484], step: 1368, 4.748 samples/sec, batch_loss: 0.0800, batch_loss_c: 0.0635, batch_loss_s: 0.1185, time:8.4240, lr:0.0001\u001b[0m\n",
            "2019-12-05 15:49:38 \u001b[32mINFO     \u001b[0m train.py: [2/300], [410/484], step: 1378, 2.399 samples/sec, batch_loss: 0.0938, batch_loss_c: 0.0942, batch_loss_s: 0.0930, time:16.6753, lr:0.0001\u001b[0m\n",
            "2019-12-05 15:49:48 \u001b[32mINFO     \u001b[0m train.py: [2/300], [420/484], step: 1388, 3.910 samples/sec, batch_loss: 0.0730, batch_loss_c: 0.0640, batch_loss_s: 0.0940, time:10.2306, lr:0.0001\u001b[0m\n",
            "2019-12-05 15:50:05 \u001b[32mINFO     \u001b[0m train.py: [2/300], [430/484], step: 1398, 2.338 samples/sec, batch_loss: 0.2927, batch_loss_c: 0.2879, batch_loss_s: 0.3037, time:17.1090, lr:0.0001\u001b[0m\n",
            "2019-12-05 15:50:20 \u001b[32mINFO     \u001b[0m train.py: [2/300], [440/484], step: 1408, 2.642 samples/sec, batch_loss: 0.5178, batch_loss_c: 0.5115, batch_loss_s: 0.5325, time:15.1425, lr:0.0001\u001b[0m\n",
            "2019-12-05 15:50:38 \u001b[32mINFO     \u001b[0m train.py: [2/300], [450/484], step: 1418, 2.221 samples/sec, batch_loss: 0.0596, batch_loss_c: 0.0537, batch_loss_s: 0.0734, time:18.0118, lr:0.0001\u001b[0m\n",
            "2019-12-05 15:50:50 \u001b[32mINFO     \u001b[0m train.py: [2/300], [460/484], step: 1428, 3.397 samples/sec, batch_loss: 0.0642, batch_loss_c: 0.0564, batch_loss_s: 0.0823, time:11.7746, lr:0.0001\u001b[0m\n",
            "2019-12-05 15:51:13 \u001b[32mINFO     \u001b[0m train.py: [2/300], [470/484], step: 1438, 1.734 samples/sec, batch_loss: 0.0623, batch_loss_c: 0.0566, batch_loss_s: 0.0757, time:23.0738, lr:0.0001\u001b[0m\n",
            "2019-12-05 15:51:26 \u001b[32mINFO     \u001b[0m train.py: [2/300], [480/484], step: 1448, 3.016 samples/sec, batch_loss: 0.0472, batch_loss_c: 0.0414, batch_loss_s: 0.0607, time:13.2638, lr:0.0001\u001b[0m\n",
            "2019-12-05 15:51:36 \u001b[32mINFO     \u001b[0m train.py: [2/300], train_loss: 0.1588, time: 773.8475, lr: 0.0001\u001b[0m\n",
            "2019-12-05 15:51:37 \u001b[32mINFO     \u001b[0m train.py: [3/300], [0/484], step: 1452, 50.072 samples/sec, batch_loss: 0.0941, batch_loss_c: 0.0880, batch_loss_s: 0.1083, time:0.7988, lr:0.0001\u001b[0m\n",
            "2019-12-05 15:51:48 \u001b[32mINFO     \u001b[0m train.py: [3/300], [10/484], step: 1462, 3.602 samples/sec, batch_loss: 0.1083, batch_loss_c: 0.1097, batch_loss_s: 0.1051, time:11.1049, lr:0.0001\u001b[0m\n",
            "2019-12-05 15:52:02 \u001b[32mINFO     \u001b[0m train.py: [3/300], [20/484], step: 1472, 2.768 samples/sec, batch_loss: 0.5775, batch_loss_c: 0.5751, batch_loss_s: 0.5830, time:14.4488, lr:0.0001\u001b[0m\n",
            "2019-12-05 15:52:13 \u001b[32mINFO     \u001b[0m train.py: [3/300], [30/484], step: 1482, 3.882 samples/sec, batch_loss: 0.1020, batch_loss_c: 0.1062, batch_loss_s: 0.0922, time:10.3034, lr:0.0001\u001b[0m\n",
            "2019-12-05 15:52:28 \u001b[32mINFO     \u001b[0m train.py: [3/300], [40/484], step: 1492, 2.679 samples/sec, batch_loss: 0.1222, batch_loss_c: 0.1201, batch_loss_s: 0.1271, time:14.9282, lr:0.0001\u001b[0m\n",
            "2019-12-05 15:52:48 \u001b[32mINFO     \u001b[0m train.py: [3/300], [50/484], step: 1502, 1.984 samples/sec, batch_loss: 0.0964, batch_loss_c: 0.0909, batch_loss_s: 0.1094, time:20.1597, lr:0.0001\u001b[0m\n",
            "2019-12-05 15:53:02 \u001b[32mINFO     \u001b[0m train.py: [3/300], [60/484], step: 1512, 2.855 samples/sec, batch_loss: 0.0755, batch_loss_c: 0.0732, batch_loss_s: 0.0809, time:14.0101, lr:0.0001\u001b[0m\n",
            "2019-12-05 15:53:24 \u001b[32mINFO     \u001b[0m train.py: [3/300], [70/484], step: 1522, 1.785 samples/sec, batch_loss: 0.0799, batch_loss_c: 0.0619, batch_loss_s: 0.1219, time:22.4029, lr:0.0001\u001b[0m\n",
            "2019-12-05 15:53:34 \u001b[32mINFO     \u001b[0m train.py: [3/300], [80/484], step: 1532, 4.155 samples/sec, batch_loss: 0.1546, batch_loss_c: 0.1784, batch_loss_s: 0.0990, time:9.6262, lr:0.0001\u001b[0m\n",
            "2019-12-05 15:54:02 \u001b[32mINFO     \u001b[0m train.py: [3/300], [90/484], step: 1542, 1.431 samples/sec, batch_loss: 0.1081, batch_loss_c: 0.1102, batch_loss_s: 0.1032, time:27.9446, lr:0.0001\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P47Jx7_DPeO7",
        "colab_type": "text"
      },
      "source": [
        "[The IIIT Scene Text Retrieval (STR) Dataset](https://cvit.iiit.ac.in/research/projects/cvit-projects/the-iiit-scene-text-retrieval-str-dataset)"
      ]
    }
  ]
}