{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PSENet_trial_run.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kavyajeetbora/PSENet.pytorch/blob/master/PSENet_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UByxwp0F3QUp",
        "colab_type": "text"
      },
      "source": [
        "## Installing softwares and other dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GSEHnmCxK_o9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, Dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y99kqKlHoYkz",
        "colab_type": "code",
        "outputId": "aff467f5-fa64-4ddd-a5a6-9b83409a822f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "!pip install pyclipper"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyclipper\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/67/2691f7cbb28fb9dbf423f2302fe489f9cee34d9a50a743c95032a24ac597/pyclipper-1.1.0.post1-cp36-cp36m-manylinux1_x86_64.whl (129kB)\n",
            "\r\u001b[K     |██▌                             | 10kB 26.2MB/s eta 0:00:01\r\u001b[K     |█████                           | 20kB 6.6MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 30kB 9.4MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 40kB 5.9MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 51kB 7.2MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 61kB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 71kB 9.6MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 81kB 10.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 92kB 11.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 102kB 9.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 112kB 9.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 122kB 9.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 133kB 9.5MB/s \n",
            "\u001b[?25hInstalling collected packages: pyclipper\n",
            "Successfully installed pyclipper-1.1.0.post1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ncoynOlfnIW1",
        "colab_type": "code",
        "outputId": "3e1d904c-2593-4490-cfd6-3f0e437ab1a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hhJ-6yyXcnRx",
        "colab_type": "code",
        "outputId": "2567d1ed-c0b3-4202-9de7-1c46ab58bafe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        }
      },
      "source": [
        "import shutil, os\n",
        "os.chdir('/content')\n",
        "directory = '/content/cloned-repo'\n",
        "if os.path.exists(directory):\n",
        "  shutil.rmtree(directory)\n",
        "\n",
        "!git clone https://github.com/kavyajeetbora/PSENet.pytorch.git /content/cloned-repo\n",
        "print(\"Cloned the repository\")\n",
        "os.chdir('/content/cloned-repo')\n",
        "!ls"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into '/content/cloned-repo'...\n",
            "remote: Enumerating objects: 101, done.\u001b[K\n",
            "remote: Counting objects:   0% (1/101)\u001b[K\rremote: Counting objects:   1% (2/101)\u001b[K\rremote: Counting objects:   2% (3/101)\u001b[K\rremote: Counting objects:   3% (4/101)\u001b[K\rremote: Counting objects:   4% (5/101)\u001b[K\rremote: Counting objects:   5% (6/101)\u001b[K\rremote: Counting objects:   6% (7/101)\u001b[K\rremote: Counting objects:   7% (8/101)\u001b[K\rremote: Counting objects:   8% (9/101)\u001b[K\rremote: Counting objects:   9% (10/101)\u001b[K\rremote: Counting objects:  10% (11/101)\u001b[K\rremote: Counting objects:  11% (12/101)\u001b[K\rremote: Counting objects:  12% (13/101)\u001b[K\rremote: Counting objects:  13% (14/101)\u001b[K\rremote: Counting objects:  14% (15/101)\u001b[K\rremote: Counting objects:  15% (16/101)\u001b[K\rremote: Counting objects:  16% (17/101)\u001b[K\rremote: Counting objects:  17% (18/101)\u001b[K\rremote: Counting objects:  18% (19/101)\u001b[K\rremote: Counting objects:  19% (20/101)\u001b[K\rremote: Counting objects:  20% (21/101)\u001b[K\rremote: Counting objects:  21% (22/101)\u001b[K\rremote: Counting objects:  22% (23/101)\u001b[K\rremote: Counting objects:  23% (24/101)\u001b[K\rremote: Counting objects:  24% (25/101)\u001b[K\rremote: Counting objects:  25% (26/101)\u001b[K\rremote: Counting objects:  26% (27/101)\u001b[K\rremote: Counting objects:  27% (28/101)\u001b[K\rremote: Counting objects:  28% (29/101)\u001b[K\rremote: Counting objects:  29% (30/101)\u001b[K\rremote: Counting objects:  30% (31/101)\u001b[K\rremote: Counting objects:  31% (32/101)\u001b[K\rremote: Counting objects:  32% (33/101)\u001b[K\rremote: Counting objects:  33% (34/101)\u001b[K\rremote: Counting objects:  34% (35/101)\u001b[K\rremote: Counting objects:  35% (36/101)\u001b[K\rremote: Counting objects:  36% (37/101)\u001b[K\rremote: Counting objects:  37% (38/101)\u001b[K\rremote: Counting objects:  38% (39/101)\u001b[K\rremote: Counting objects:  39% (40/101)\u001b[K\rremote: Counting objects:  40% (41/101)\u001b[K\rremote: Counting objects:  41% (42/101)\u001b[K\rremote: Counting objects:  42% (43/101)\u001b[K\rremote: Counting objects:  43% (44/101)\u001b[K\rremote: Counting objects:  44% (45/101)\u001b[K\rremote: Counting objects:  45% (46/101)\u001b[K\rremote: Counting objects:  46% (47/101)\u001b[K\rremote: Counting objects:  47% (48/101)\u001b[K\rremote: Counting objects:  48% (49/101)\u001b[K\rremote: Counting objects:  49% (50/101)\u001b[K\rremote: Counting objects:  50% (51/101)\u001b[K\rremote: Counting objects:  51% (52/101)\u001b[K\rremote: Counting objects:  52% (53/101)\u001b[K\rremote: Counting objects:  53% (54/101)\u001b[K\rremote: Counting objects:  54% (55/101)\u001b[K\rremote: Counting objects:  55% (56/101)\u001b[K\rremote: Counting objects:  56% (57/101)\u001b[K\rremote: Counting objects:  57% (58/101)\u001b[K\rremote: Counting objects:  58% (59/101)\u001b[K\rremote: Counting objects:  59% (60/101)\u001b[K\rremote: Counting objects:  60% (61/101)\u001b[K\rremote: Counting objects:  61% (62/101)\u001b[K\rremote: Counting objects:  62% (63/101)\u001b[K\rremote: Counting objects:  63% (64/101)\u001b[K\rremote: Counting objects:  64% (65/101)\u001b[K\rremote: Counting objects:  65% (66/101)\u001b[K\rremote: Counting objects:  66% (67/101)\u001b[K\rremote: Counting objects:  67% (68/101)\u001b[K\rremote: Counting objects:  68% (69/101)\u001b[K\rremote: Counting objects:  69% (70/101)\u001b[K\rremote: Counting objects:  70% (71/101)\u001b[K\rremote: Counting objects:  71% (72/101)\u001b[K\rremote: Counting objects:  72% (73/101)\u001b[K\rremote: Counting objects:  73% (74/101)\u001b[K\rremote: Counting objects:  74% (75/101)\u001b[K\rremote: Counting objects:  75% (76/101)\u001b[K\rremote: Counting objects:  76% (77/101)\u001b[K\rremote: Counting objects:  77% (78/101)\u001b[K\rremote: Counting objects:  78% (79/101)\u001b[K\rremote: Counting objects:  79% (80/101)\u001b[K\rremote: Counting objects:  80% (81/101)\u001b[K\rremote: Counting objects:  81% (82/101)\u001b[K\rremote: Counting objects:  82% (83/101)\u001b[K\rremote: Counting objects:  83% (84/101)\u001b[K\rremote: Counting objects:  84% (85/101)\u001b[K\rremote: Counting objects:  85% (86/101)\u001b[K\rremote: Counting objects:  86% (87/101)\u001b[K\rremote: Counting objects:  87% (88/101)\u001b[K\rremote: Counting objects:  88% (89/101)\u001b[K\rremote: Counting objects:  89% (90/101)\u001b[K\rremote: Counting objects:  90% (91/101)\u001b[K\rremote: Counting objects:  91% (92/101)\u001b[K\rremote: Counting objects:  92% (93/101)\u001b[K\rremote: Counting objects:  93% (94/101)\u001b[K\rremote: Counting objects:  94% (95/101)\u001b[K\rremote: Counting objects:  95% (96/101)\u001b[K\rremote: Counting objects:  96% (97/101)\u001b[K\rremote: Counting objects:  97% (98/101)\u001b[K\rremote: Counting objects:  98% (99/101)\u001b[K\rremote: Counting objects:  99% (100/101)\u001b[K\rremote: Counting objects: 100% (101/101)\u001b[K\rremote: Counting objects: 100% (101/101), done.\u001b[K\n",
            "remote: Compressing objects: 100% (101/101), done.\u001b[K\n",
            "remote: Total 501 (delta 59), reused 0 (delta 0), pack-reused 400\u001b[K\n",
            "Receiving objects: 100% (501/501), 8.70 MiB | 11.25 MiB/s, done.\n",
            "Resolving deltas: 100% (253/253), done.\n",
            "Cloned the repository\n",
            "cal_recall  install_dependencies.sh  PSENet.ipynb\t     train.py\n",
            "config.py   LICENSE\t\t     PSENet_predict.ipynb    utils\n",
            "dataset     models\t\t     PSENet_training.ipynb\n",
            "eval.py     predict.py\t\t     PSENet_trial_run.ipynb\n",
            "imgs\t    pse\t\t\t     README.md\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PrOmfo6_3X2I",
        "colab_type": "text"
      },
      "source": [
        "## Extracting the data and setting up the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dGLMxTujlm0w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## unzipping the files\n",
        "from zipfile import ZipFile\n",
        "\n",
        "def unzip_files(file,output_dir):\n",
        "  with ZipFile(file, 'r') as zipObj:\n",
        "    # Extract all the contents of zip file in current directory\n",
        "    zipObj.extractall(output_dir)\n",
        "  print('Extracted to',output_dir)\n",
        "\n",
        "def make_directory(directory):\n",
        "  if os.path.isdir(directory):\n",
        "    shutil.rmtree(directory)\n",
        "  \n",
        "  os.mkdir(directory)\n",
        "  print('Created a new directory')\n",
        "\n",
        "training_data_zip = '/content/drive/My Drive/Colab Notebooks/padh.ai.notebooks/15. Object Detection/AI4Bharat Dataset/Training Set/Random 5000.zip'\n",
        "test_data_zip = '/content/drive/My Drive/Colab Notebooks/padh.ai.notebooks/15. Object Detection/AI4Bharat Dataset/Test Set/real_Image_dataset_Detection.zip'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FqJ6xb5u2dgg",
        "colab_type": "code",
        "outputId": "87359cdd-7763-4ba9-91ba-a6a695c8171e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# make directories\n",
        "make_directory('Training Set')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Created a new directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FFSaSXr-m-K4",
        "colab_type": "code",
        "outputId": "524ad4e6-8b8d-499a-be64-1c670741e9b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "%%time\n",
        "unzip_files(test_data_zip,'Training Set')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracted to Training Set\n",
            "CPU times: user 425 ms, sys: 91.9 ms, total: 517 ms\n",
            "Wall time: 7.25 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YRSeyluknVH4",
        "colab_type": "code",
        "outputId": "6339067f-1c77-4e9f-e6de-d0c454f7a464",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(len(os.listdir('Training Set/Images')))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "428\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MOp4JsZ6nXvn",
        "colab_type": "code",
        "outputId": "106cde26-165a-405e-bcfa-948159e27b4f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(len(os.listdir('Training Set/Annotations')))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "428\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mhvg89mXn8zG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from dataset.data_utils import *\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader, Dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4JeHTz3G6Lyj",
        "colab_type": "code",
        "outputId": "e70d2fff-4c24-4c62-b455-1d66871f5ed6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_data = MyDataset('Training Set',transform=transforms.ToTensor())\n",
        "len(train_data)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "428"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "odLKMn-5SBAz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device(\"cuda:0\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9a9PsyF3dqo",
        "colab_type": "text"
      },
      "source": [
        "## Training "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9x221Ya0RBGO",
        "outputId": "e3e65bb4-641c-49aa-8c84-e7ddbbb8b927",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        }
      },
      "source": [
        "!chmod +x install_dependencies.sh # make shell script executable\n",
        "!./install_dependencies.sh # run the shell script"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyclipper in /usr/local/lib/python3.6/dist-packages (1.1.0.post1)\n",
            "Collecting Polygon3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/73/a0/d07a4f3e80ed7020a33f3111db217f54ac44a485ff45da3c21ce49f65041/Polygon3-3.0.8.tar.gz (71kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 5.3MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: Polygon3\n",
            "  Building wheel for Polygon3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for Polygon3: filename=Polygon3-3.0.8-cp36-cp36m-linux_x86_64.whl size=101489 sha256=fe45214b90436b9c93f91660c35e62c8d66575879b175342af6dec949f021618\n",
            "  Stored in directory: /root/.cache/pip/wheels/95/32/f1/5525b233996d9d99cbce2f0a8da60d137ddddc555d3e8b0e2a\n",
            "Successfully built Polygon3\n",
            "Installing collected packages: Polygon3\n",
            "Successfully installed Polygon3-3.0.8\n",
            "Collecting colorlog\n",
            "  Downloading https://files.pythonhosted.org/packages/68/4d/892728b0c14547224f0ac40884e722a3d00cb54e7a146aea0b3186806c9e/colorlog-4.0.2-py2.py3-none-any.whl\n",
            "Installing collected packages: colorlog\n",
            "Successfully installed colorlog-4.0.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xD4GuTKC3wXY",
        "colab_type": "code",
        "outputId": "026ef6fd-06a5-42dc-97af-f2d8e9e8c988",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!python3 train.py"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "make: Entering directory '/content/cloned-repo/pse'\n",
            "make: 'pse.so' is up to date.\n",
            "make: Leaving directory '/content/cloned-repo/pse'\n",
            "2019-11-26 14:34:57 \u001b[32mINFO     \u001b[0m utils.py: logger init finished\u001b[0m\n",
            "2019-11-26 14:34:57 \u001b[32mINFO     \u001b[0m train.py: {'Lambda': 0.7,\n",
            " 'OHEM_ratio': 3,\n",
            " 'backbone': 'resnet50',\n",
            " 'checkpoint': '',\n",
            " 'data_shape': 640,\n",
            " 'display_input_images': False,\n",
            " 'display_interval': 10,\n",
            " 'display_output_images': False,\n",
            " 'end_lr': 1e-07,\n",
            " 'epochs': 50,\n",
            " 'gpu_id': '0',\n",
            " 'lr': 0.001,\n",
            " 'lr_decay_step': [200, 400],\n",
            " 'lr_gamma': 0.1,\n",
            " 'm': 0.5,\n",
            " 'n': 6,\n",
            " 'output_dir': '/content/drive/My Drive/PSENet_2',\n",
            " 'pretrained': True,\n",
            " 'pretrained_path': '/content/drive/My Drive/PSENet_2/PSENet_resnet50.pth',\n",
            " 'restart_training': True,\n",
            " 'scale': 1,\n",
            " 'seed': 2,\n",
            " 'show_images_interval': 50,\n",
            " 'start_epoch': 0,\n",
            " 'testroot': 'Test Set',\n",
            " 'train_batch_size': 4,\n",
            " 'trainroot': 'Training Set',\n",
            " 'warm_up_epoch': 6,\n",
            " 'warm_up_lr': 0.0001,\n",
            " 'weight_decay': 0.0005,\n",
            " 'workers': 12}\u001b[0m\n",
            "2019-11-26 14:34:57 \u001b[32mINFO     \u001b[0m train.py: train with gpu 0 and pytorch 1.3.1\u001b[0m\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-19c8e357.pth\" to /root/.cache/torch/checkpoints/resnet50-19c8e357.pth\n",
            "100% 97.8M/97.8M [00:03<00:00, 33.7MB/s]\n",
            "2019-11-26 14:35:02 \u001b[32mINFO     \u001b[0m resnet.py: load pretrained models from imagenet\u001b[0m\n",
            "2019-11-26 14:35:08 \u001b[32mINFO     \u001b[0m train.py: train dataset has 428 samples,107 in dataloader\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:100: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule.See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "2019-11-26 14:35:30 \u001b[32mINFO     \u001b[0m train.py: [0/50], [0/107], step: 0, 1.823 samples/sec, batch_loss: 0.6204, batch_loss_c: 0.5415, batch_loss_s: 0.8046, time:21.9399, lr:0.001\u001b[0m\n",
            "2019-11-26 14:35:36 \u001b[32mINFO     \u001b[0m train.py: [0/50], [10/107], step: 10, 6.770 samples/sec, batch_loss: 0.4286, batch_loss_c: 0.4159, batch_loss_s: 0.4582, time:5.9087, lr:0.001\u001b[0m\n",
            "2019-11-26 14:35:43 \u001b[32mINFO     \u001b[0m train.py: [0/50], [20/107], step: 20, 5.900 samples/sec, batch_loss: 0.3457, batch_loss_c: 0.3631, batch_loss_s: 0.3051, time:6.7802, lr:0.001\u001b[0m\n",
            "2019-11-26 14:35:48 \u001b[32mINFO     \u001b[0m train.py: [0/50], [30/107], step: 30, 6.906 samples/sec, batch_loss: 0.3397, batch_loss_c: 0.3298, batch_loss_s: 0.3628, time:5.7924, lr:0.001\u001b[0m\n",
            "2019-11-26 14:35:55 \u001b[32mINFO     \u001b[0m train.py: [0/50], [40/107], step: 40, 6.295 samples/sec, batch_loss: 0.2665, batch_loss_c: 0.2426, batch_loss_s: 0.3223, time:6.3547, lr:0.001\u001b[0m\n",
            "2019-11-26 14:36:01 \u001b[32mINFO     \u001b[0m train.py: [0/50], [50/107], step: 50, 6.337 samples/sec, batch_loss: 0.2778, batch_loss_c: 0.2604, batch_loss_s: 0.3186, time:6.3119, lr:0.001\u001b[0m\n",
            "2019-11-26 14:36:11 \u001b[32mINFO     \u001b[0m train.py: [0/50], [60/107], step: 60, 4.083 samples/sec, batch_loss: 0.3677, batch_loss_c: 0.3659, batch_loss_s: 0.3720, time:9.7959, lr:0.001\u001b[0m\n",
            "tcmalloc: large alloc 1173168128 bytes == 0x9dc98000 @  0x7fed666bc1e7 0x7fed5bd8ff71 0x7fed5bdf355d 0x7fed5bdf6e28 0x7fed5bdf73e5 0x7fed5be8dfc2 0x50abc5 0x50d320 0x5081d5 0x50a020 0x50aa1d 0x50d320 0x5093e5 0x5951c1 0x5a04ce 0x557878 0x50c81e 0x5081d5 0x50a020 0x50aa1d 0x50c549 0x5081d5 0x50a020 0x50aa1d 0x50c549 0x5081d5 0x5895e1 0x5a04ce 0x50d8f5 0x509ce8 0x50aa1d\n",
            "2019-11-26 14:36:21 \u001b[32mINFO     \u001b[0m train.py: [0/50], [70/107], step: 70, 3.883 samples/sec, batch_loss: 0.2010, batch_loss_c: 0.1861, batch_loss_s: 0.2358, time:10.3001, lr:0.001\u001b[0m\n",
            "2019-11-26 14:36:30 \u001b[32mINFO     \u001b[0m train.py: [0/50], [80/107], step: 80, 4.822 samples/sec, batch_loss: 0.3016, batch_loss_c: 0.2892, batch_loss_s: 0.3307, time:8.2948, lr:0.001\u001b[0m\n",
            "2019-11-26 14:36:36 \u001b[32mINFO     \u001b[0m train.py: [0/50], [90/107], step: 90, 6.720 samples/sec, batch_loss: 0.3000, batch_loss_c: 0.2859, batch_loss_s: 0.3331, time:5.9527, lr:0.001\u001b[0m\n",
            "2019-11-26 14:36:40 \u001b[32mINFO     \u001b[0m train.py: [0/50], [100/107], step: 100, 8.227 samples/sec, batch_loss: 0.4256, batch_loss_c: 0.4310, batch_loss_s: 0.4132, time:4.8621, lr:0.001\u001b[0m\n",
            "2019-11-26 14:36:44 \u001b[32mINFO     \u001b[0m train.py: [0/50], train_loss: 0.3300, time: 95.6421, lr: 0.001\u001b[0m\n",
            "2019-11-26 14:36:51 \u001b[32mINFO     \u001b[0m train.py: [1/50], [0/107], step: 107, 5.643 samples/sec, batch_loss: 0.3741, batch_loss_c: 0.3646, batch_loss_s: 0.3962, time:7.0889, lr:0.001\u001b[0m\n",
            "2019-11-26 14:37:06 \u001b[32mINFO     \u001b[0m train.py: [1/50], [10/107], step: 117, 2.762 samples/sec, batch_loss: 0.3320, batch_loss_c: 0.3419, batch_loss_s: 0.3090, time:14.4806, lr:0.001\u001b[0m\n",
            "2019-11-26 14:37:12 \u001b[32mINFO     \u001b[0m train.py: [1/50], [20/107], step: 127, 6.749 samples/sec, batch_loss: 0.2333, batch_loss_c: 0.2184, batch_loss_s: 0.2681, time:5.9266, lr:0.001\u001b[0m\n",
            "2019-11-26 14:37:17 \u001b[32mINFO     \u001b[0m train.py: [1/50], [30/107], step: 137, 6.848 samples/sec, batch_loss: 0.1847, batch_loss_c: 0.1704, batch_loss_s: 0.2182, time:5.8413, lr:0.001\u001b[0m\n",
            "2019-11-26 14:37:23 \u001b[32mINFO     \u001b[0m train.py: [1/50], [40/107], step: 147, 6.881 samples/sec, batch_loss: 0.2342, batch_loss_c: 0.2582, batch_loss_s: 0.1782, time:5.8127, lr:0.001\u001b[0m\n",
            "2019-11-26 14:37:31 \u001b[32mINFO     \u001b[0m train.py: [1/50], [50/107], step: 157, 4.967 samples/sec, batch_loss: 0.2760, batch_loss_c: 0.3018, batch_loss_s: 0.2159, time:8.0524, lr:0.001\u001b[0m\n",
            "2019-11-26 14:37:39 \u001b[32mINFO     \u001b[0m train.py: [1/50], [60/107], step: 167, 5.139 samples/sec, batch_loss: 0.3213, batch_loss_c: 0.3006, batch_loss_s: 0.3698, time:7.7833, lr:0.001\u001b[0m\n",
            "2019-11-26 14:37:48 \u001b[32mINFO     \u001b[0m train.py: [1/50], [70/107], step: 177, 4.615 samples/sec, batch_loss: 0.2549, batch_loss_c: 0.2602, batch_loss_s: 0.2426, time:8.6671, lr:0.001\u001b[0m\n",
            "2019-11-26 14:37:54 \u001b[32mINFO     \u001b[0m train.py: [1/50], [80/107], step: 187, 6.900 samples/sec, batch_loss: 0.1527, batch_loss_c: 0.1385, batch_loss_s: 0.1860, time:5.7971, lr:0.001\u001b[0m\n",
            "2019-11-26 14:37:59 \u001b[32mINFO     \u001b[0m train.py: [1/50], [90/107], step: 197, 7.236 samples/sec, batch_loss: 0.1962, batch_loss_c: 0.1733, batch_loss_s: 0.2495, time:5.5279, lr:0.001\u001b[0m\n",
            "2019-11-26 14:38:04 \u001b[32mINFO     \u001b[0m train.py: [1/50], [100/107], step: 207, 8.148 samples/sec, batch_loss: 0.2077, batch_loss_c: 0.1950, batch_loss_s: 0.2374, time:4.9092, lr:0.001\u001b[0m\n",
            "2019-11-26 14:38:07 \u001b[32mINFO     \u001b[0m train.py: [1/50], train_loss: 0.2716, time: 83.0623, lr: 0.001\u001b[0m\n",
            "2019-11-26 14:38:14 \u001b[32mINFO     \u001b[0m train.py: [2/50], [0/107], step: 214, 6.620 samples/sec, batch_loss: 0.1585, batch_loss_c: 0.1447, batch_loss_s: 0.1905, time:6.0426, lr:0.001\u001b[0m\n",
            "2019-11-26 14:38:26 \u001b[32mINFO     \u001b[0m train.py: [2/50], [10/107], step: 224, 3.139 samples/sec, batch_loss: 0.1641, batch_loss_c: 0.1437, batch_loss_s: 0.2119, time:12.7432, lr:0.001\u001b[0m\n",
            "2019-11-26 14:38:33 \u001b[32mINFO     \u001b[0m train.py: [2/50], [20/107], step: 234, 6.377 samples/sec, batch_loss: 0.2686, batch_loss_c: 0.2725, batch_loss_s: 0.2594, time:6.2726, lr:0.001\u001b[0m\n",
            "2019-11-26 14:38:38 \u001b[32mINFO     \u001b[0m train.py: [2/50], [30/107], step: 244, 7.042 samples/sec, batch_loss: 0.1781, batch_loss_c: 0.1650, batch_loss_s: 0.2086, time:5.6801, lr:0.001\u001b[0m\n",
            "2019-11-26 14:38:47 \u001b[32mINFO     \u001b[0m train.py: [2/50], [40/107], step: 254, 4.477 samples/sec, batch_loss: 0.4016, batch_loss_c: 0.3888, batch_loss_s: 0.4314, time:8.9345, lr:0.001\u001b[0m\n",
            "tcmalloc: large alloc 1214889984 bytes == 0x9b73a000 @  0x7fed666bc1e7 0x7fed5bd8ff71 0x7fed5bdf355d 0x7fed5bdf6e28 0x7fed5bdf73e5 0x7fed5be8dfc2 0x50abc5 0x50d320 0x5081d5 0x50a020 0x50aa1d 0x50d320 0x5093e5 0x5951c1 0x5a04ce 0x557878 0x50c81e 0x5081d5 0x50a020 0x50aa1d 0x50c549 0x5081d5 0x50a020 0x50aa1d 0x50c549 0x5081d5 0x5895e1 0x5a04ce 0x50d8f5 0x509ce8 0x50aa1d\n",
            "2019-11-26 14:38:57 \u001b[32mINFO     \u001b[0m train.py: [2/50], [50/107], step: 264, 3.940 samples/sec, batch_loss: 0.2988, batch_loss_c: 0.3030, batch_loss_s: 0.2892, time:10.1513, lr:0.001\u001b[0m\n",
            "tcmalloc: large alloc 1083916288 bytes == 0x97196000 @  0x7fed666bc1e7 0x7fed5bd8ff71 0x7fed5bdf355d 0x7fed5bdf6e28 0x7fed5bdf73e5 0x7fed5be8dfc2 0x50abc5 0x50d320 0x5081d5 0x50a020 0x50aa1d 0x50d320 0x5093e5 0x5951c1 0x5a04ce 0x557878 0x50c81e 0x5081d5 0x50a020 0x50aa1d 0x50c549 0x5081d5 0x50a020 0x50aa1d 0x50c549 0x5081d5 0x5895e1 0x5a04ce 0x50d8f5 0x509ce8 0x50aa1d\n",
            "2019-11-26 14:39:08 \u001b[32mINFO     \u001b[0m train.py: [2/50], [60/107], step: 274, 3.832 samples/sec, batch_loss: 0.1719, batch_loss_c: 0.1726, batch_loss_s: 0.1701, time:10.4376, lr:0.001\u001b[0m\n",
            "2019-11-26 14:39:16 \u001b[32mINFO     \u001b[0m train.py: [2/50], [70/107], step: 284, 5.116 samples/sec, batch_loss: 0.2665, batch_loss_c: 0.2784, batch_loss_s: 0.2387, time:7.8182, lr:0.001\u001b[0m\n",
            "2019-11-26 14:39:23 \u001b[32mINFO     \u001b[0m train.py: [2/50], [80/107], step: 294, 5.682 samples/sec, batch_loss: 0.3133, batch_loss_c: 0.3246, batch_loss_s: 0.2868, time:7.0394, lr:0.001\u001b[0m\n",
            "2019-11-26 14:39:28 \u001b[32mINFO     \u001b[0m train.py: [2/50], [90/107], step: 304, 8.087 samples/sec, batch_loss: 0.1388, batch_loss_c: 0.1319, batch_loss_s: 0.1549, time:4.9463, lr:0.001\u001b[0m\n",
            "2019-11-26 14:39:33 \u001b[32mINFO     \u001b[0m train.py: [2/50], [100/107], step: 314, 8.150 samples/sec, batch_loss: 0.1511, batch_loss_c: 0.1481, batch_loss_s: 0.1581, time:4.9077, lr:0.001\u001b[0m\n",
            "2019-11-26 14:39:36 \u001b[32mINFO     \u001b[0m train.py: [2/50], train_loss: 0.2525, time: 88.2880, lr: 0.001\u001b[0m\n",
            "2019-11-26 14:39:42 \u001b[32mINFO     \u001b[0m train.py: [3/50], [0/107], step: 321, 6.749 samples/sec, batch_loss: 0.1622, batch_loss_c: 0.1527, batch_loss_s: 0.1844, time:5.9268, lr:0.001\u001b[0m\n",
            "tcmalloc: large alloc 1083916288 bytes == 0x9c994000 @  0x7fed666bc1e7 0x7fed5bd8ff71 0x7fed5bdf355d 0x7fed5bdf6e28 0x7fed5bdf73e5 0x7fed5be8dfc2 0x50abc5 0x50d320 0x5081d5 0x50a020 0x50aa1d 0x50d320 0x5093e5 0x5951c1 0x5a04ce 0x557878 0x50c81e 0x5081d5 0x50a020 0x50aa1d 0x50c549 0x5081d5 0x50a020 0x50aa1d 0x50c549 0x5081d5 0x5895e1 0x5a04ce 0x50d8f5 0x509ce8 0x50aa1d\n",
            "2019-11-26 14:39:54 \u001b[32mINFO     \u001b[0m train.py: [3/50], [10/107], step: 331, 3.241 samples/sec, batch_loss: 0.2081, batch_loss_c: 0.2072, batch_loss_s: 0.2101, time:12.3433, lr:0.001\u001b[0m\n",
            "2019-11-26 14:40:02 \u001b[32mINFO     \u001b[0m train.py: [3/50], [20/107], step: 341, 5.000 samples/sec, batch_loss: 0.1483, batch_loss_c: 0.1313, batch_loss_s: 0.1881, time:8.0006, lr:0.001\u001b[0m\n",
            "2019-11-26 14:40:11 \u001b[32mINFO     \u001b[0m train.py: [3/50], [30/107], step: 351, 4.743 samples/sec, batch_loss: 0.1463, batch_loss_c: 0.1346, batch_loss_s: 0.1737, time:8.4337, lr:0.001\u001b[0m\n",
            "2019-11-26 14:40:18 \u001b[32mINFO     \u001b[0m train.py: [3/50], [40/107], step: 361, 5.289 samples/sec, batch_loss: 0.2311, batch_loss_c: 0.2533, batch_loss_s: 0.1791, time:7.5623, lr:0.001\u001b[0m\n",
            "2019-11-26 14:40:24 \u001b[32mINFO     \u001b[0m train.py: [3/50], [50/107], step: 371, 6.814 samples/sec, batch_loss: 0.4431, batch_loss_c: 0.4202, batch_loss_s: 0.4963, time:5.8702, lr:0.001\u001b[0m\n",
            "2019-11-26 14:40:32 \u001b[32mINFO     \u001b[0m train.py: [3/50], [60/107], step: 381, 5.326 samples/sec, batch_loss: 0.3215, batch_loss_c: 0.2940, batch_loss_s: 0.3858, time:7.5110, lr:0.001\u001b[0m\n",
            "2019-11-26 14:40:40 \u001b[32mINFO     \u001b[0m train.py: [3/50], [70/107], step: 391, 5.056 samples/sec, batch_loss: 0.1979, batch_loss_c: 0.1970, batch_loss_s: 0.1998, time:7.9112, lr:0.001\u001b[0m\n",
            "2019-11-26 14:40:47 \u001b[32mINFO     \u001b[0m train.py: [3/50], [80/107], step: 401, 5.536 samples/sec, batch_loss: 0.2737, batch_loss_c: 0.3029, batch_loss_s: 0.2056, time:7.2248, lr:0.001\u001b[0m\n",
            "2019-11-26 14:40:53 \u001b[32mINFO     \u001b[0m train.py: [3/50], [90/107], step: 411, 7.015 samples/sec, batch_loss: 0.1413, batch_loss_c: 0.1313, batch_loss_s: 0.1646, time:5.7023, lr:0.001\u001b[0m\n",
            "2019-11-26 14:40:58 \u001b[32mINFO     \u001b[0m train.py: [3/50], [100/107], step: 421, 8.054 samples/sec, batch_loss: 0.2770, batch_loss_c: 0.2614, batch_loss_s: 0.3133, time:4.9664, lr:0.001\u001b[0m\n",
            "2019-11-26 14:41:01 \u001b[32mINFO     \u001b[0m train.py: [3/50], train_loss: 0.2392, time: 84.7942, lr: 0.001\u001b[0m\n",
            "2019-11-26 14:41:10 \u001b[32mINFO     \u001b[0m train.py: [4/50], [0/107], step: 428, 4.829 samples/sec, batch_loss: 0.1351, batch_loss_c: 0.1127, batch_loss_s: 0.1875, time:8.2841, lr:0.001\u001b[0m\n",
            "2019-11-26 14:41:20 \u001b[32mINFO     \u001b[0m train.py: [4/50], [10/107], step: 438, 3.825 samples/sec, batch_loss: 0.1869, batch_loss_c: 0.1745, batch_loss_s: 0.2158, time:10.4582, lr:0.001\u001b[0m\n",
            "2019-11-26 14:41:26 \u001b[32mINFO     \u001b[0m train.py: [4/50], [20/107], step: 448, 7.008 samples/sec, batch_loss: 0.2252, batch_loss_c: 0.2207, batch_loss_s: 0.2360, time:5.7078, lr:0.001\u001b[0m\n",
            "2019-11-26 14:41:32 \u001b[32mINFO     \u001b[0m train.py: [4/50], [30/107], step: 458, 6.674 samples/sec, batch_loss: 0.1441, batch_loss_c: 0.1486, batch_loss_s: 0.1335, time:5.9934, lr:0.001\u001b[0m\n",
            "2019-11-26 14:41:38 \u001b[32mINFO     \u001b[0m train.py: [4/50], [40/107], step: 468, 6.044 samples/sec, batch_loss: 0.3908, batch_loss_c: 0.3822, batch_loss_s: 0.4111, time:6.6180, lr:0.001\u001b[0m\n",
            "2019-11-26 14:41:45 \u001b[32mINFO     \u001b[0m train.py: [4/50], [50/107], step: 478, 6.392 samples/sec, batch_loss: 0.1279, batch_loss_c: 0.1299, batch_loss_s: 0.1234, time:6.2579, lr:0.001\u001b[0m\n",
            "2019-11-26 14:41:52 \u001b[32mINFO     \u001b[0m train.py: [4/50], [60/107], step: 488, 5.436 samples/sec, batch_loss: 0.3822, batch_loss_c: 0.3836, batch_loss_s: 0.3790, time:7.3582, lr:0.001\u001b[0m\n",
            "2019-11-26 14:41:59 \u001b[32mINFO     \u001b[0m train.py: [4/50], [70/107], step: 498, 5.431 samples/sec, batch_loss: 0.1496, batch_loss_c: 0.1333, batch_loss_s: 0.1876, time:7.3654, lr:0.001\u001b[0m\n",
            "2019-11-26 14:42:07 \u001b[32mINFO     \u001b[0m train.py: [4/50], [80/107], step: 508, 5.613 samples/sec, batch_loss: 0.2167, batch_loss_c: 0.1867, batch_loss_s: 0.2867, time:7.1257, lr:0.001\u001b[0m\n",
            "2019-11-26 14:42:12 \u001b[32mINFO     \u001b[0m train.py: [4/50], [90/107], step: 518, 7.460 samples/sec, batch_loss: 0.3265, batch_loss_c: 0.2938, batch_loss_s: 0.4028, time:5.3617, lr:0.001\u001b[0m\n",
            "2019-11-26 14:42:17 \u001b[32mINFO     \u001b[0m train.py: [4/50], [100/107], step: 528, 7.996 samples/sec, batch_loss: 0.1396, batch_loss_c: 0.1183, batch_loss_s: 0.1892, time:5.0025, lr:0.001\u001b[0m\n",
            "2019-11-26 14:42:20 \u001b[32mINFO     \u001b[0m train.py: [4/50], train_loss: 0.2375, time: 78.7272, lr: 0.001\u001b[0m\n",
            "2019-11-26 14:42:27 \u001b[32mINFO     \u001b[0m train.py: [5/50], [0/107], step: 535, 5.762 samples/sec, batch_loss: 0.2705, batch_loss_c: 0.2612, batch_loss_s: 0.2920, time:6.9417, lr:0.001\u001b[0m\n",
            "tcmalloc: large alloc 1180852224 bytes == 0x9ac6a000 @  0x7fed666bc1e7 0x7fed5bd8ff71 0x7fed5bdf355d 0x7fed5bdf6e28 0x7fed5bdf73e5 0x7fed5be8dfc2 0x50abc5 0x50d320 0x5081d5 0x50a020 0x50aa1d 0x50d320 0x5093e5 0x5951c1 0x5a04ce 0x557878 0x50c81e 0x5081d5 0x50a020 0x50aa1d 0x50c549 0x5081d5 0x50a020 0x50aa1d 0x50c549 0x5081d5 0x5895e1 0x5a04ce 0x50d8f5 0x509ce8 0x50aa1d\n",
            "2019-11-26 14:42:47 \u001b[32mINFO     \u001b[0m train.py: [5/50], [10/107], step: 545, 2.087 samples/sec, batch_loss: 0.2651, batch_loss_c: 0.2573, batch_loss_s: 0.2833, time:19.1664, lr:0.001\u001b[0m\n",
            "2019-11-26 14:42:54 \u001b[32mINFO     \u001b[0m train.py: [5/50], [20/107], step: 555, 5.448 samples/sec, batch_loss: 0.2273, batch_loss_c: 0.2258, batch_loss_s: 0.2308, time:7.3420, lr:0.001\u001b[0m\n",
            "tcmalloc: large alloc 1101635584 bytes == 0x9a596000 @  0x7fed666bc1e7 0x7fed5bd8ff71 0x7fed5bdf355d 0x7fed5bdf6e28 0x7fed5bdf73e5 0x7fed5be8dfc2 0x50abc5 0x50d320 0x5081d5 0x50a020 0x50aa1d 0x50d320 0x5093e5 0x5951c1 0x5a04ce 0x557878 0x50c81e 0x5081d5 0x50a020 0x50aa1d 0x50c549 0x5081d5 0x50a020 0x50aa1d 0x50c549 0x5081d5 0x5895e1 0x5a04ce 0x50d8f5 0x509ce8 0x50aa1d\n",
            "2019-11-26 14:43:02 \u001b[32mINFO     \u001b[0m train.py: [5/50], [30/107], step: 565, 5.042 samples/sec, batch_loss: 0.1310, batch_loss_c: 0.1105, batch_loss_s: 0.1789, time:7.9330, lr:0.001\u001b[0m\n",
            "2019-11-26 14:43:08 \u001b[32mINFO     \u001b[0m train.py: [5/50], [40/107], step: 575, 6.221 samples/sec, batch_loss: 0.2162, batch_loss_c: 0.2038, batch_loss_s: 0.2449, time:6.4301, lr:0.001\u001b[0m\n",
            "2019-11-26 14:43:15 \u001b[32mINFO     \u001b[0m train.py: [5/50], [50/107], step: 585, 5.745 samples/sec, batch_loss: 0.2865, batch_loss_c: 0.2829, batch_loss_s: 0.2949, time:6.9629, lr:0.001\u001b[0m\n",
            "2019-11-26 14:43:22 \u001b[32mINFO     \u001b[0m train.py: [5/50], [60/107], step: 595, 5.648 samples/sec, batch_loss: 0.1917, batch_loss_c: 0.1853, batch_loss_s: 0.2068, time:7.0815, lr:0.001\u001b[0m\n",
            "2019-11-26 14:43:29 \u001b[32mINFO     \u001b[0m train.py: [5/50], [70/107], step: 605, 5.966 samples/sec, batch_loss: 0.3757, batch_loss_c: 0.3600, batch_loss_s: 0.4123, time:6.7046, lr:0.001\u001b[0m\n",
            "2019-11-26 14:43:37 \u001b[32mINFO     \u001b[0m train.py: [5/50], [80/107], step: 615, 5.248 samples/sec, batch_loss: 0.0969, batch_loss_c: 0.0796, batch_loss_s: 0.1373, time:7.6220, lr:0.001\u001b[0m\n",
            "2019-11-26 14:43:42 \u001b[32mINFO     \u001b[0m train.py: [5/50], [90/107], step: 625, 7.627 samples/sec, batch_loss: 0.1941, batch_loss_c: 0.1850, batch_loss_s: 0.2153, time:5.2449, lr:0.001\u001b[0m\n",
            "2019-11-26 14:43:47 \u001b[32mINFO     \u001b[0m train.py: [5/50], [100/107], step: 635, 8.088 samples/sec, batch_loss: 0.1578, batch_loss_c: 0.1610, batch_loss_s: 0.1505, time:4.9453, lr:0.001\u001b[0m\n",
            "2019-11-26 14:43:50 \u001b[32mINFO     \u001b[0m train.py: [5/50], train_loss: 0.2131, time: 89.6456, lr: 0.001\u001b[0m\n",
            "2019-11-26 14:43:57 \u001b[32mINFO     \u001b[0m train.py: [6/50], [0/107], step: 642, 5.902 samples/sec, batch_loss: 0.2432, batch_loss_c: 0.2517, batch_loss_s: 0.2234, time:6.7773, lr:0.001\u001b[0m\n",
            "tcmalloc: large alloc 1342701568 bytes == 0xa832a000 @  0x7fed666bc1e7 0x7fed5bd8ff71 0x7fed5bdf355d 0x7fed5bdf6e28 0x7fed5bdf73e5 0x7fed5be8dfc2 0x50abc5 0x50d320 0x5081d5 0x50a020 0x50aa1d 0x50d320 0x5093e5 0x5951c1 0x5a04ce 0x557878 0x50c81e 0x5081d5 0x50a020 0x50aa1d 0x50c549 0x5081d5 0x50a020 0x50aa1d 0x50c549 0x5081d5 0x5895e1 0x5a04ce 0x50d8f5 0x509ce8 0x50aa1d\n",
            "2019-11-26 14:44:10 \u001b[32mINFO     \u001b[0m train.py: [6/50], [10/107], step: 652, 3.113 samples/sec, batch_loss: 0.1370, batch_loss_c: 0.1428, batch_loss_s: 0.1233, time:12.8480, lr:0.001\u001b[0m\n",
            "2019-11-26 14:44:17 \u001b[32mINFO     \u001b[0m train.py: [6/50], [20/107], step: 662, 6.185 samples/sec, batch_loss: 0.2640, batch_loss_c: 0.2518, batch_loss_s: 0.2924, time:6.4671, lr:0.001\u001b[0m\n",
            "2019-11-26 14:44:23 \u001b[32mINFO     \u001b[0m train.py: [6/50], [30/107], step: 672, 5.928 samples/sec, batch_loss: 0.1993, batch_loss_c: 0.1924, batch_loss_s: 0.2154, time:6.7471, lr:0.001\u001b[0m\n",
            "2019-11-26 14:44:31 \u001b[32mINFO     \u001b[0m train.py: [6/50], [40/107], step: 682, 5.450 samples/sec, batch_loss: 0.2377, batch_loss_c: 0.2598, batch_loss_s: 0.1861, time:7.3390, lr:0.001\u001b[0m\n",
            "2019-11-26 14:44:37 \u001b[32mINFO     \u001b[0m train.py: [6/50], [50/107], step: 692, 6.065 samples/sec, batch_loss: 0.1202, batch_loss_c: 0.1144, batch_loss_s: 0.1337, time:6.5954, lr:0.001\u001b[0m\n",
            "2019-11-26 14:44:44 \u001b[32mINFO     \u001b[0m train.py: [6/50], [60/107], step: 702, 5.954 samples/sec, batch_loss: 0.1559, batch_loss_c: 0.1594, batch_loss_s: 0.1477, time:6.7180, lr:0.001\u001b[0m\n",
            "2019-11-26 14:44:51 \u001b[32mINFO     \u001b[0m train.py: [6/50], [70/107], step: 712, 6.083 samples/sec, batch_loss: 0.1634, batch_loss_c: 0.1572, batch_loss_s: 0.1777, time:6.5755, lr:0.001\u001b[0m\n",
            "2019-11-26 14:44:58 \u001b[32mINFO     \u001b[0m train.py: [6/50], [80/107], step: 722, 5.688 samples/sec, batch_loss: 0.1326, batch_loss_c: 0.1187, batch_loss_s: 0.1651, time:7.0325, lr:0.001\u001b[0m\n",
            "2019-11-26 14:45:03 \u001b[32mINFO     \u001b[0m train.py: [6/50], [90/107], step: 732, 7.876 samples/sec, batch_loss: 0.2460, batch_loss_c: 0.2722, batch_loss_s: 0.1849, time:5.0785, lr:0.001\u001b[0m\n",
            "2019-11-26 14:45:08 \u001b[32mINFO     \u001b[0m train.py: [6/50], [100/107], step: 742, 8.142 samples/sec, batch_loss: 0.1620, batch_loss_c: 0.1533, batch_loss_s: 0.1822, time:4.9130, lr:0.001\u001b[0m\n",
            "2019-11-26 14:45:11 \u001b[32mINFO     \u001b[0m train.py: [6/50], train_loss: 0.2299, time: 80.3454, lr: 0.001\u001b[0m\n",
            "2019-11-26 14:45:18 \u001b[32mINFO     \u001b[0m train.py: [7/50], [0/107], step: 749, 5.548 samples/sec, batch_loss: 0.1953, batch_loss_c: 0.1841, batch_loss_s: 0.2217, time:7.2095, lr:0.001\u001b[0m\n",
            "2019-11-26 14:45:29 \u001b[32mINFO     \u001b[0m train.py: [7/50], [10/107], step: 759, 3.788 samples/sec, batch_loss: 0.1619, batch_loss_c: 0.1680, batch_loss_s: 0.1476, time:10.5590, lr:0.001\u001b[0m\n",
            "2019-11-26 14:45:35 \u001b[32mINFO     \u001b[0m train.py: [7/50], [20/107], step: 769, 6.279 samples/sec, batch_loss: 0.1131, batch_loss_c: 0.1043, batch_loss_s: 0.1337, time:6.3701, lr:0.001\u001b[0m\n",
            "2019-11-26 14:45:43 \u001b[32mINFO     \u001b[0m train.py: [7/50], [30/107], step: 779, 5.427 samples/sec, batch_loss: 0.2873, batch_loss_c: 0.2781, batch_loss_s: 0.3088, time:7.3712, lr:0.001\u001b[0m\n",
            "2019-11-26 14:45:49 \u001b[32mINFO     \u001b[0m train.py: [7/50], [40/107], step: 789, 6.472 samples/sec, batch_loss: 0.1612, batch_loss_c: 0.1554, batch_loss_s: 0.1748, time:6.1802, lr:0.001\u001b[0m\n",
            "2019-11-26 14:45:56 \u001b[32mINFO     \u001b[0m train.py: [7/50], [50/107], step: 799, 5.886 samples/sec, batch_loss: 0.1495, batch_loss_c: 0.1485, batch_loss_s: 0.1517, time:6.7962, lr:0.001\u001b[0m\n",
            "2019-11-26 14:46:03 \u001b[32mINFO     \u001b[0m train.py: [7/50], [60/107], step: 809, 5.300 samples/sec, batch_loss: 0.3239, batch_loss_c: 0.3025, batch_loss_s: 0.3740, time:7.5468, lr:0.001\u001b[0m\n",
            "2019-11-26 14:46:11 \u001b[32mINFO     \u001b[0m train.py: [7/50], [70/107], step: 819, 5.024 samples/sec, batch_loss: 0.2374, batch_loss_c: 0.2275, batch_loss_s: 0.2606, time:7.9612, lr:0.001\u001b[0m\n",
            "2019-11-26 14:46:19 \u001b[32mINFO     \u001b[0m train.py: [7/50], [80/107], step: 829, 5.093 samples/sec, batch_loss: 0.1711, batch_loss_c: 0.1700, batch_loss_s: 0.1734, time:7.8536, lr:0.001\u001b[0m\n",
            "2019-11-26 14:46:24 \u001b[32mINFO     \u001b[0m train.py: [7/50], [90/107], step: 839, 7.843 samples/sec, batch_loss: 0.2696, batch_loss_c: 0.2559, batch_loss_s: 0.3015, time:5.1002, lr:0.001\u001b[0m\n",
            "2019-11-26 14:46:29 \u001b[32mINFO     \u001b[0m train.py: [7/50], [100/107], step: 849, 8.125 samples/sec, batch_loss: 0.2138, batch_loss_c: 0.2015, batch_loss_s: 0.2425, time:4.9233, lr:0.001\u001b[0m\n",
            "2019-11-26 14:46:32 \u001b[32mINFO     \u001b[0m train.py: [7/50], train_loss: 0.2400, time: 81.1381, lr: 0.001\u001b[0m\n",
            "2019-11-26 14:46:40 \u001b[32mINFO     \u001b[0m train.py: [8/50], [0/107], step: 856, 5.710 samples/sec, batch_loss: 0.1905, batch_loss_c: 0.1894, batch_loss_s: 0.1930, time:7.0051, lr:0.001\u001b[0m\n",
            "2019-11-26 14:46:51 \u001b[32mINFO     \u001b[0m train.py: [8/50], [10/107], step: 866, 3.525 samples/sec, batch_loss: 0.1710, batch_loss_c: 0.1791, batch_loss_s: 0.1521, time:11.3474, lr:0.001\u001b[0m\n",
            "2019-11-26 14:46:58 \u001b[32mINFO     \u001b[0m train.py: [8/50], [20/107], step: 876, 5.825 samples/sec, batch_loss: 0.1788, batch_loss_c: 0.1666, batch_loss_s: 0.2074, time:6.8671, lr:0.001\u001b[0m\n",
            "2019-11-26 14:47:05 \u001b[32mINFO     \u001b[0m train.py: [8/50], [30/107], step: 886, 5.956 samples/sec, batch_loss: 0.1201, batch_loss_c: 0.1164, batch_loss_s: 0.1287, time:6.7155, lr:0.001\u001b[0m\n",
            "tcmalloc: large alloc 1083916288 bytes == 0x97ff2000 @  0x7fed666bc1e7 0x7fed5bd8ff71 0x7fed5bdf355d 0x7fed5bdf6e28 0x7fed5bdf73e5 0x7fed5be8dfc2 0x50abc5 0x50d320 0x5081d5 0x50a020 0x50aa1d 0x50d320 0x5093e5 0x5951c1 0x5a04ce 0x557878 0x50c81e 0x5081d5 0x50a020 0x50aa1d 0x50c549 0x5081d5 0x50a020 0x50aa1d 0x50c549 0x5081d5 0x5895e1 0x5a04ce 0x50d8f5 0x509ce8 0x50aa1d\n",
            "2019-11-26 14:47:10 \u001b[32mINFO     \u001b[0m train.py: [8/50], [40/107], step: 896, 7.049 samples/sec, batch_loss: 0.1827, batch_loss_c: 0.1783, batch_loss_s: 0.1928, time:5.6745, lr:0.001\u001b[0m\n",
            "2019-11-26 14:47:17 \u001b[32mINFO     \u001b[0m train.py: [8/50], [50/107], step: 906, 5.998 samples/sec, batch_loss: 0.1386, batch_loss_c: 0.1246, batch_loss_s: 0.1711, time:6.6689, lr:0.001\u001b[0m\n",
            "2019-11-26 14:47:24 \u001b[32mINFO     \u001b[0m train.py: [8/50], [60/107], step: 916, 5.404 samples/sec, batch_loss: 0.2740, batch_loss_c: 0.2627, batch_loss_s: 0.3004, time:7.4018, lr:0.001\u001b[0m\n",
            "2019-11-26 14:47:33 \u001b[32mINFO     \u001b[0m train.py: [8/50], [70/107], step: 926, 4.881 samples/sec, batch_loss: 0.3163, batch_loss_c: 0.3019, batch_loss_s: 0.3499, time:8.1956, lr:0.001\u001b[0m\n",
            "2019-11-26 14:47:40 \u001b[32mINFO     \u001b[0m train.py: [8/50], [80/107], step: 936, 5.315 samples/sec, batch_loss: 0.2300, batch_loss_c: 0.2198, batch_loss_s: 0.2538, time:7.5252, lr:0.001\u001b[0m\n",
            "2019-11-26 14:47:45 \u001b[32mINFO     \u001b[0m train.py: [8/50], [90/107], step: 946, 7.622 samples/sec, batch_loss: 0.2219, batch_loss_c: 0.2067, batch_loss_s: 0.2575, time:5.2477, lr:0.001\u001b[0m\n",
            "2019-11-26 14:47:50 \u001b[32mINFO     \u001b[0m train.py: [8/50], [100/107], step: 956, 8.099 samples/sec, batch_loss: 0.1595, batch_loss_c: 0.1603, batch_loss_s: 0.1578, time:4.9390, lr:0.001\u001b[0m\n",
            "2019-11-26 14:47:54 \u001b[32mINFO     \u001b[0m train.py: [8/50], train_loss: 0.2200, time: 80.8776, lr: 0.001\u001b[0m\n",
            "2019-11-26 14:48:01 \u001b[32mINFO     \u001b[0m train.py: [9/50], [0/107], step: 963, 5.741 samples/sec, batch_loss: 0.2043, batch_loss_c: 0.2071, batch_loss_s: 0.1976, time:6.9670, lr:0.001\u001b[0m\n",
            "2019-11-26 14:48:12 \u001b[32mINFO     \u001b[0m train.py: [9/50], [10/107], step: 973, 3.696 samples/sec, batch_loss: 0.2502, batch_loss_c: 0.2185, batch_loss_s: 0.3240, time:10.8219, lr:0.001\u001b[0m\n",
            "2019-11-26 14:48:18 \u001b[32mINFO     \u001b[0m train.py: [9/50], [20/107], step: 983, 6.406 samples/sec, batch_loss: 0.1933, batch_loss_c: 0.1873, batch_loss_s: 0.2073, time:6.2438, lr:0.001\u001b[0m\n",
            "2019-11-26 14:48:25 \u001b[32mINFO     \u001b[0m train.py: [9/50], [30/107], step: 993, 6.053 samples/sec, batch_loss: 0.1317, batch_loss_c: 0.1361, batch_loss_s: 0.1214, time:6.6079, lr:0.001\u001b[0m\n",
            "2019-11-26 14:48:31 \u001b[32mINFO     \u001b[0m train.py: [9/50], [40/107], step: 1003, 5.907 samples/sec, batch_loss: 0.2164, batch_loss_c: 0.2113, batch_loss_s: 0.2281, time:6.7720, lr:0.001\u001b[0m\n",
            "2019-11-26 14:48:37 \u001b[32mINFO     \u001b[0m train.py: [9/50], [50/107], step: 1013, 6.748 samples/sec, batch_loss: 0.1841, batch_loss_c: 0.1718, batch_loss_s: 0.2127, time:5.9277, lr:0.001\u001b[0m\n",
            "2019-11-26 14:48:45 \u001b[32mINFO     \u001b[0m train.py: [9/50], [60/107], step: 1023, 5.551 samples/sec, batch_loss: 0.1133, batch_loss_c: 0.0939, batch_loss_s: 0.1587, time:7.2056, lr:0.001\u001b[0m\n",
            "2019-11-26 14:48:55 \u001b[32mINFO     \u001b[0m train.py: [9/50], [70/107], step: 1033, 3.743 samples/sec, batch_loss: 0.1683, batch_loss_c: 0.1693, batch_loss_s: 0.1661, time:10.6852, lr:0.001\u001b[0m\n",
            "2019-11-26 14:49:04 \u001b[32mINFO     \u001b[0m train.py: [9/50], [80/107], step: 1043, 4.588 samples/sec, batch_loss: 0.4992, batch_loss_c: 0.4462, batch_loss_s: 0.6227, time:8.7182, lr:0.001\u001b[0m\n",
            "2019-11-26 14:49:09 \u001b[32mINFO     \u001b[0m train.py: [9/50], [90/107], step: 1053, 7.776 samples/sec, batch_loss: 0.1795, batch_loss_c: 0.1721, batch_loss_s: 0.1967, time:5.1440, lr:0.001\u001b[0m\n",
            "2019-11-26 14:49:14 \u001b[32mINFO     \u001b[0m train.py: [9/50], [100/107], step: 1063, 8.223 samples/sec, batch_loss: 0.1989, batch_loss_c: 0.1887, batch_loss_s: 0.2227, time:4.8646, lr:0.001\u001b[0m\n",
            "2019-11-26 14:49:17 \u001b[32mINFO     \u001b[0m train.py: [9/50], train_loss: 0.2121, time: 83.1982, lr: 0.001\u001b[0m\n",
            "2019-11-26 14:49:22 \u001b[32mINFO     \u001b[0m train.py: [10/50], [0/107], step: 1070, 8.573 samples/sec, batch_loss: 0.3374, batch_loss_c: 0.3229, batch_loss_s: 0.3713, time:4.6659, lr:0.001\u001b[0m\n",
            "2019-11-26 14:49:34 \u001b[32mINFO     \u001b[0m train.py: [10/50], [10/107], step: 1080, 3.282 samples/sec, batch_loss: 0.1051, batch_loss_c: 0.0937, batch_loss_s: 0.1317, time:12.1877, lr:0.001\u001b[0m\n",
            "2019-11-26 14:49:41 \u001b[32mINFO     \u001b[0m train.py: [10/50], [20/107], step: 1090, 6.030 samples/sec, batch_loss: 0.2073, batch_loss_c: 0.2205, batch_loss_s: 0.1763, time:6.6340, lr:0.001\u001b[0m\n",
            "2019-11-26 14:49:47 \u001b[32mINFO     \u001b[0m train.py: [10/50], [30/107], step: 1100, 6.283 samples/sec, batch_loss: 0.1416, batch_loss_c: 0.1270, batch_loss_s: 0.1755, time:6.3664, lr:0.001\u001b[0m\n",
            "2019-11-26 14:49:54 \u001b[32mINFO     \u001b[0m train.py: [10/50], [40/107], step: 1110, 6.369 samples/sec, batch_loss: 0.1136, batch_loss_c: 0.1079, batch_loss_s: 0.1268, time:6.2801, lr:0.001\u001b[0m\n",
            "2019-11-26 14:50:00 \u001b[32mINFO     \u001b[0m train.py: [10/50], [50/107], step: 1120, 6.224 samples/sec, batch_loss: 0.3451, batch_loss_c: 0.3307, batch_loss_s: 0.3787, time:6.4266, lr:0.001\u001b[0m\n",
            "2019-11-26 14:50:10 \u001b[32mINFO     \u001b[0m train.py: [10/50], [60/107], step: 1130, 3.900 samples/sec, batch_loss: 0.1693, batch_loss_c: 0.1568, batch_loss_s: 0.1984, time:10.2558, lr:0.001\u001b[0m\n",
            "tcmalloc: large alloc 1143881728 bytes == 0x98a74000 @  0x7fed666bc1e7 0x7fed5bd8ff71 0x7fed5bdf355d 0x7fed5bdf6e28 0x7fed5bdf73e5 0x7fed5be8dfc2 0x50abc5 0x50d320 0x5081d5 0x50a020 0x50aa1d 0x50d320 0x5093e5 0x5951c1 0x5a04ce 0x557878 0x50c81e 0x5081d5 0x50a020 0x50aa1d 0x50c549 0x5081d5 0x50a020 0x50aa1d 0x50c549 0x5081d5 0x5895e1 0x5a04ce 0x50d8f5 0x509ce8 0x50aa1d\n",
            "2019-11-26 14:50:20 \u001b[32mINFO     \u001b[0m train.py: [10/50], [70/107], step: 1140, 4.037 samples/sec, batch_loss: 0.2042, batch_loss_c: 0.1907, batch_loss_s: 0.2356, time:9.9083, lr:0.001\u001b[0m\n",
            "2019-11-26 14:50:27 \u001b[32mINFO     \u001b[0m train.py: [10/50], [80/107], step: 1150, 5.598 samples/sec, batch_loss: 0.1362, batch_loss_c: 0.1225, batch_loss_s: 0.1680, time:7.1455, lr:0.001\u001b[0m\n",
            "2019-11-26 14:50:33 \u001b[32mINFO     \u001b[0m train.py: [10/50], [90/107], step: 1160, 6.929 samples/sec, batch_loss: 0.1424, batch_loss_c: 0.1344, batch_loss_s: 0.1611, time:5.7729, lr:0.001\u001b[0m\n",
            "2019-11-26 14:50:38 \u001b[32mINFO     \u001b[0m train.py: [10/50], [100/107], step: 1170, 8.216 samples/sec, batch_loss: 0.1012, batch_loss_c: 0.0853, batch_loss_s: 0.1383, time:4.8686, lr:0.001\u001b[0m\n",
            "2019-11-26 14:50:41 \u001b[32mINFO     \u001b[0m train.py: [10/50], train_loss: 0.1924, time: 83.7711, lr: 0.001\u001b[0m\n",
            "2019-11-26 14:50:48 \u001b[32mINFO     \u001b[0m train.py: [11/50], [0/107], step: 1177, 6.395 samples/sec, batch_loss: 0.1354, batch_loss_c: 0.1262, batch_loss_s: 0.1570, time:6.2552, lr:0.001\u001b[0m\n",
            "2019-11-26 14:51:02 \u001b[32mINFO     \u001b[0m train.py: [11/50], [10/107], step: 1187, 2.945 samples/sec, batch_loss: 0.2955, batch_loss_c: 0.2841, batch_loss_s: 0.3223, time:13.5804, lr:0.001\u001b[0m\n",
            "2019-11-26 14:51:08 \u001b[32mINFO     \u001b[0m train.py: [11/50], [20/107], step: 1197, 6.315 samples/sec, batch_loss: 0.1605, batch_loss_c: 0.1513, batch_loss_s: 0.1820, time:6.3346, lr:0.001\u001b[0m\n",
            "2019-11-26 14:51:15 \u001b[32mINFO     \u001b[0m train.py: [11/50], [30/107], step: 1207, 5.805 samples/sec, batch_loss: 0.3072, batch_loss_c: 0.3119, batch_loss_s: 0.2962, time:6.8907, lr:0.001\u001b[0m\n",
            "2019-11-26 14:51:22 \u001b[32mINFO     \u001b[0m train.py: [11/50], [40/107], step: 1217, 5.823 samples/sec, batch_loss: 0.1591, batch_loss_c: 0.1482, batch_loss_s: 0.1846, time:6.8694, lr:0.001\u001b[0m\n",
            "2019-11-26 14:51:28 \u001b[32mINFO     \u001b[0m train.py: [11/50], [50/107], step: 1227, 6.196 samples/sec, batch_loss: 0.1034, batch_loss_c: 0.0844, batch_loss_s: 0.1477, time:6.4561, lr:0.001\u001b[0m\n",
            "2019-11-26 14:51:36 \u001b[32mINFO     \u001b[0m train.py: [11/50], [60/107], step: 1237, 4.848 samples/sec, batch_loss: 0.1562, batch_loss_c: 0.1715, batch_loss_s: 0.1203, time:8.2513, lr:0.001\u001b[0m\n",
            "2019-11-26 14:51:44 \u001b[32mINFO     \u001b[0m train.py: [11/50], [70/107], step: 1247, 5.363 samples/sec, batch_loss: 0.0855, batch_loss_c: 0.0758, batch_loss_s: 0.1081, time:7.4585, lr:0.001\u001b[0m\n",
            "2019-11-26 14:51:50 \u001b[32mINFO     \u001b[0m train.py: [11/50], [80/107], step: 1257, 6.020 samples/sec, batch_loss: 0.1177, batch_loss_c: 0.1051, batch_loss_s: 0.1470, time:6.6440, lr:0.001\u001b[0m\n",
            "2019-11-26 14:51:56 \u001b[32mINFO     \u001b[0m train.py: [11/50], [90/107], step: 1267, 6.849 samples/sec, batch_loss: 0.1788, batch_loss_c: 0.2042, batch_loss_s: 0.1193, time:5.8405, lr:0.001\u001b[0m\n",
            "2019-11-26 14:52:01 \u001b[32mINFO     \u001b[0m train.py: [11/50], [100/107], step: 1277, 8.089 samples/sec, batch_loss: 0.3276, batch_loss_c: 0.3070, batch_loss_s: 0.3758, time:4.9451, lr:0.001\u001b[0m\n",
            "2019-11-26 14:52:04 \u001b[32mINFO     \u001b[0m train.py: [11/50], train_loss: 0.2138, time: 82.8047, lr: 0.001\u001b[0m\n",
            "2019-11-26 14:52:13 \u001b[32mINFO     \u001b[0m train.py: [12/50], [0/107], step: 1284, 5.001 samples/sec, batch_loss: 0.1134, batch_loss_c: 0.0962, batch_loss_s: 0.1536, time:7.9978, lr:0.001\u001b[0m\n",
            "2019-11-26 14:52:22 \u001b[32mINFO     \u001b[0m train.py: [12/50], [10/107], step: 1294, 4.546 samples/sec, batch_loss: 0.2051, batch_loss_c: 0.2025, batch_loss_s: 0.2113, time:8.7997, lr:0.001\u001b[0m\n",
            "2019-11-26 14:52:29 \u001b[32mINFO     \u001b[0m train.py: [12/50], [20/107], step: 1304, 5.260 samples/sec, batch_loss: 0.1380, batch_loss_c: 0.1245, batch_loss_s: 0.1697, time:7.6041, lr:0.001\u001b[0m\n",
            "2019-11-26 14:52:35 \u001b[32mINFO     \u001b[0m train.py: [12/50], [30/107], step: 1314, 6.771 samples/sec, batch_loss: 0.3402, batch_loss_c: 0.3211, batch_loss_s: 0.3850, time:5.9072, lr:0.001\u001b[0m\n",
            "2019-11-26 14:52:42 \u001b[32mINFO     \u001b[0m train.py: [12/50], [40/107], step: 1324, 6.320 samples/sec, batch_loss: 0.0944, batch_loss_c: 0.0878, batch_loss_s: 0.1099, time:6.3286, lr:0.001\u001b[0m\n",
            "2019-11-26 14:52:48 \u001b[32mINFO     \u001b[0m train.py: [12/50], [50/107], step: 1334, 6.456 samples/sec, batch_loss: 0.1528, batch_loss_c: 0.1466, batch_loss_s: 0.1673, time:6.1963, lr:0.001\u001b[0m\n",
            "2019-11-26 14:52:55 \u001b[32mINFO     \u001b[0m train.py: [12/50], [60/107], step: 1344, 5.663 samples/sec, batch_loss: 0.1254, batch_loss_c: 0.1225, batch_loss_s: 0.1322, time:7.0639, lr:0.001\u001b[0m\n",
            "2019-11-26 14:53:02 \u001b[32mINFO     \u001b[0m train.py: [12/50], [70/107], step: 1354, 5.570 samples/sec, batch_loss: 0.1225, batch_loss_c: 0.1111, batch_loss_s: 0.1489, time:7.1818, lr:0.001\u001b[0m\n",
            "2019-11-26 14:53:08 \u001b[32mINFO     \u001b[0m train.py: [12/50], [80/107], step: 1364, 6.292 samples/sec, batch_loss: 0.1915, batch_loss_c: 0.2073, batch_loss_s: 0.1548, time:6.3571, lr:0.001\u001b[0m\n",
            "2019-11-26 14:53:14 \u001b[32mINFO     \u001b[0m train.py: [12/50], [90/107], step: 1374, 7.543 samples/sec, batch_loss: 0.1873, batch_loss_c: 0.1761, batch_loss_s: 0.2135, time:5.3028, lr:0.001\u001b[0m\n",
            "2019-11-26 14:53:19 \u001b[32mINFO     \u001b[0m train.py: [12/50], [100/107], step: 1384, 8.091 samples/sec, batch_loss: 0.2556, batch_loss_c: 0.2348, batch_loss_s: 0.3042, time:4.9437, lr:0.001\u001b[0m\n",
            "2019-11-26 14:53:22 \u001b[32mINFO     \u001b[0m train.py: [12/50], train_loss: 0.1997, time: 76.9422, lr: 0.001\u001b[0m\n",
            "2019-11-26 14:53:29 \u001b[32mINFO     \u001b[0m train.py: [13/50], [0/107], step: 1391, 6.260 samples/sec, batch_loss: 0.1066, batch_loss_c: 0.0941, batch_loss_s: 0.1360, time:6.3897, lr:0.001\u001b[0m\n",
            "2019-11-26 14:53:45 \u001b[32mINFO     \u001b[0m train.py: [13/50], [10/107], step: 1401, 2.464 samples/sec, batch_loss: 0.1211, batch_loss_c: 0.1046, batch_loss_s: 0.1596, time:16.2306, lr:0.001\u001b[0m\n",
            "tcmalloc: large alloc 1192763392 bytes == 0xa08e0000 @  0x7fed666bc1e7 0x7fed5bd8ff71 0x7fed5bdf355d 0x7fed5bdf6e28 0x7fed5bdf73e5 0x7fed5be8dfc2 0x50abc5 0x50d320 0x5081d5 0x50a020 0x50aa1d 0x50d320 0x5093e5 0x5951c1 0x5a04ce 0x557878 0x50c81e 0x5081d5 0x50a020 0x50aa1d 0x50c549 0x5081d5 0x50a020 0x50aa1d 0x50c549 0x5081d5 0x5895e1 0x5a04ce 0x50d8f5 0x509ce8 0x50aa1d\n",
            "2019-11-26 14:53:54 \u001b[32mINFO     \u001b[0m train.py: [13/50], [20/107], step: 1411, 4.582 samples/sec, batch_loss: 0.1490, batch_loss_c: 0.1382, batch_loss_s: 0.1741, time:8.7291, lr:0.001\u001b[0m\n",
            "2019-11-26 14:54:00 \u001b[32mINFO     \u001b[0m train.py: [13/50], [30/107], step: 1421, 6.173 samples/sec, batch_loss: 0.1035, batch_loss_c: 0.0964, batch_loss_s: 0.1200, time:6.4803, lr:0.001\u001b[0m\n",
            "2019-11-26 14:54:07 \u001b[32mINFO     \u001b[0m train.py: [13/50], [40/107], step: 1431, 5.853 samples/sec, batch_loss: 0.1484, batch_loss_c: 0.1479, batch_loss_s: 0.1497, time:6.8339, lr:0.001\u001b[0m\n",
            "2019-11-26 14:54:13 \u001b[32mINFO     \u001b[0m train.py: [13/50], [50/107], step: 1441, 6.092 samples/sec, batch_loss: 0.1703, batch_loss_c: 0.1818, batch_loss_s: 0.1435, time:6.5662, lr:0.001\u001b[0m\n",
            "2019-11-26 14:54:21 \u001b[32mINFO     \u001b[0m train.py: [13/50], [60/107], step: 1451, 5.056 samples/sec, batch_loss: 0.2196, batch_loss_c: 0.2154, batch_loss_s: 0.2292, time:7.9109, lr:0.001\u001b[0m\n",
            "2019-11-26 14:54:30 \u001b[32mINFO     \u001b[0m train.py: [13/50], [70/107], step: 1461, 4.453 samples/sec, batch_loss: 0.2302, batch_loss_c: 0.2367, batch_loss_s: 0.2150, time:8.9827, lr:0.001\u001b[0m\n",
            "2019-11-26 14:54:37 \u001b[32mINFO     \u001b[0m train.py: [13/50], [80/107], step: 1471, 6.418 samples/sec, batch_loss: 0.2353, batch_loss_c: 0.2285, batch_loss_s: 0.2514, time:6.2326, lr:0.001\u001b[0m\n",
            "2019-11-26 14:54:42 \u001b[32mINFO     \u001b[0m train.py: [13/50], [90/107], step: 1481, 7.662 samples/sec, batch_loss: 0.2686, batch_loss_c: 0.2597, batch_loss_s: 0.2894, time:5.2209, lr:0.001\u001b[0m\n",
            "2019-11-26 14:54:47 \u001b[32mINFO     \u001b[0m train.py: [13/50], [100/107], step: 1491, 7.970 samples/sec, batch_loss: 0.2380, batch_loss_c: 0.2801, batch_loss_s: 0.1399, time:5.0185, lr:0.001\u001b[0m\n",
            "2019-11-26 14:54:50 \u001b[32mINFO     \u001b[0m train.py: [13/50], train_loss: 0.2000, time: 87.9385, lr: 0.001\u001b[0m\n",
            "2019-11-26 14:54:58 \u001b[32mINFO     \u001b[0m train.py: [14/50], [0/107], step: 1498, 5.400 samples/sec, batch_loss: 0.3377, batch_loss_c: 0.3320, batch_loss_s: 0.3511, time:7.4075, lr:0.001\u001b[0m\n",
            "2019-11-26 14:55:13 \u001b[32mINFO     \u001b[0m train.py: [14/50], [10/107], step: 1508, 2.592 samples/sec, batch_loss: 0.1443, batch_loss_c: 0.1343, batch_loss_s: 0.1675, time:15.4320, lr:0.001\u001b[0m\n",
            "2019-11-26 14:55:21 \u001b[32mINFO     \u001b[0m train.py: [14/50], [20/107], step: 1518, 5.165 samples/sec, batch_loss: 0.1522, batch_loss_c: 0.1280, batch_loss_s: 0.2086, time:7.7452, lr:0.001\u001b[0m\n",
            "2019-11-26 14:55:27 \u001b[32mINFO     \u001b[0m train.py: [14/50], [30/107], step: 1528, 6.911 samples/sec, batch_loss: 0.1564, batch_loss_c: 0.1604, batch_loss_s: 0.1472, time:5.7878, lr:0.001\u001b[0m\n",
            "2019-11-26 14:55:33 \u001b[32mINFO     \u001b[0m train.py: [14/50], [40/107], step: 1538, 6.800 samples/sec, batch_loss: 0.1378, batch_loss_c: 0.1375, batch_loss_s: 0.1385, time:5.8822, lr:0.001\u001b[0m\n",
            "2019-11-26 14:55:41 \u001b[32mINFO     \u001b[0m train.py: [14/50], [50/107], step: 1548, 5.092 samples/sec, batch_loss: 0.2550, batch_loss_c: 0.2544, batch_loss_s: 0.2566, time:7.8551, lr:0.001\u001b[0m\n",
            "2019-11-26 14:55:49 \u001b[32mINFO     \u001b[0m train.py: [14/50], [60/107], step: 1558, 4.913 samples/sec, batch_loss: 0.1875, batch_loss_c: 0.1787, batch_loss_s: 0.2078, time:8.1411, lr:0.001\u001b[0m\n",
            "2019-11-26 14:55:57 \u001b[32mINFO     \u001b[0m train.py: [14/50], [70/107], step: 1568, 5.059 samples/sec, batch_loss: 0.1032, batch_loss_c: 0.0981, batch_loss_s: 0.1151, time:7.9065, lr:0.001\u001b[0m\n",
            "2019-11-26 14:56:04 \u001b[32mINFO     \u001b[0m train.py: [14/50], [80/107], step: 1578, 5.602 samples/sec, batch_loss: 0.4058, batch_loss_c: 0.3960, batch_loss_s: 0.4287, time:7.1409, lr:0.001\u001b[0m\n",
            "2019-11-26 14:56:09 \u001b[32mINFO     \u001b[0m train.py: [14/50], [90/107], step: 1588, 7.836 samples/sec, batch_loss: 0.2089, batch_loss_c: 0.1842, batch_loss_s: 0.2665, time:5.1047, lr:0.001\u001b[0m\n",
            "2019-11-26 14:56:14 \u001b[32mINFO     \u001b[0m train.py: [14/50], [100/107], step: 1598, 8.121 samples/sec, batch_loss: 0.1580, batch_loss_c: 0.1565, batch_loss_s: 0.1613, time:4.9254, lr:0.001\u001b[0m\n",
            "2019-11-26 14:56:17 \u001b[32mINFO     \u001b[0m train.py: [14/50], train_loss: 0.2113, time: 86.5990, lr: 0.001\u001b[0m\n",
            "2019-11-26 14:56:28 \u001b[32mINFO     \u001b[0m train.py: [15/50], [0/107], step: 1605, 3.645 samples/sec, batch_loss: 0.1353, batch_loss_c: 0.1354, batch_loss_s: 0.1351, time:10.9748, lr:0.001\u001b[0m\n",
            "2019-11-26 14:56:42 \u001b[32mINFO     \u001b[0m train.py: [15/50], [10/107], step: 1615, 2.937 samples/sec, batch_loss: 0.0997, batch_loss_c: 0.0967, batch_loss_s: 0.1069, time:13.6202, lr:0.001\u001b[0m\n",
            "tcmalloc: large alloc 1116094464 bytes == 0xaa972000 @  0x7fed666bc1e7 0x7fed5bd8ff71 0x7fed5bdf355d 0x7fed5bdf6e28 0x7fed5bdf73e5 0x7fed5be8dfc2 0x50abc5 0x50d320 0x5081d5 0x50a020 0x50aa1d 0x50d320 0x5093e5 0x5951c1 0x5a04ce 0x557878 0x50c81e 0x5081d5 0x50a020 0x50aa1d 0x50c549 0x5081d5 0x50a020 0x50aa1d 0x50c549 0x5081d5 0x5895e1 0x5a04ce 0x50d8f5 0x509ce8 0x50aa1d\n",
            "2019-11-26 14:56:49 \u001b[32mINFO     \u001b[0m train.py: [15/50], [20/107], step: 1625, 5.941 samples/sec, batch_loss: 0.2962, batch_loss_c: 0.2805, batch_loss_s: 0.3328, time:6.7326, lr:0.001\u001b[0m\n",
            "2019-11-26 14:56:57 \u001b[32mINFO     \u001b[0m train.py: [15/50], [30/107], step: 1635, 4.957 samples/sec, batch_loss: 0.1405, batch_loss_c: 0.1363, batch_loss_s: 0.1502, time:8.0694, lr:0.001\u001b[0m\n",
            "2019-11-26 14:57:03 \u001b[32mINFO     \u001b[0m train.py: [15/50], [40/107], step: 1645, 6.256 samples/sec, batch_loss: 0.1970, batch_loss_c: 0.2023, batch_loss_s: 0.1846, time:6.3935, lr:0.001\u001b[0m\n",
            "2019-11-26 14:57:10 \u001b[32mINFO     \u001b[0m train.py: [15/50], [50/107], step: 1655, 5.691 samples/sec, batch_loss: 0.0824, batch_loss_c: 0.0783, batch_loss_s: 0.0920, time:7.0287, lr:0.001\u001b[0m\n",
            "2019-11-26 14:57:18 \u001b[32mINFO     \u001b[0m train.py: [15/50], [60/107], step: 1665, 4.868 samples/sec, batch_loss: 0.1206, batch_loss_c: 0.1113, batch_loss_s: 0.1423, time:8.2173, lr:0.001\u001b[0m\n",
            "2019-11-26 14:57:27 \u001b[32mINFO     \u001b[0m train.py: [15/50], [70/107], step: 1675, 4.637 samples/sec, batch_loss: 0.1524, batch_loss_c: 0.1650, batch_loss_s: 0.1231, time:8.6257, lr:0.001\u001b[0m\n",
            "2019-11-26 14:57:34 \u001b[32mINFO     \u001b[0m train.py: [15/50], [80/107], step: 1685, 6.043 samples/sec, batch_loss: 0.7948, batch_loss_c: 0.7644, batch_loss_s: 0.8658, time:6.6194, lr:0.001\u001b[0m\n",
            "2019-11-26 14:57:39 \u001b[32mINFO     \u001b[0m train.py: [15/50], [90/107], step: 1695, 7.967 samples/sec, batch_loss: 0.1119, batch_loss_c: 0.1055, batch_loss_s: 0.1269, time:5.0208, lr:0.001\u001b[0m\n",
            "2019-11-26 14:57:44 \u001b[32mINFO     \u001b[0m train.py: [15/50], [100/107], step: 1705, 8.043 samples/sec, batch_loss: 0.1154, batch_loss_c: 0.1176, batch_loss_s: 0.1102, time:4.9735, lr:0.001\u001b[0m\n",
            "2019-11-26 14:57:47 \u001b[32mINFO     \u001b[0m train.py: [15/50], train_loss: 0.1810, time: 89.5754, lr: 0.001\u001b[0m\n",
            "2019-11-26 14:57:55 \u001b[32mINFO     \u001b[0m train.py: [16/50], [0/107], step: 1712, 5.422 samples/sec, batch_loss: 0.1607, batch_loss_c: 0.1581, batch_loss_s: 0.1669, time:7.3772, lr:0.001\u001b[0m\n",
            "2019-11-26 14:58:07 \u001b[32mINFO     \u001b[0m train.py: [16/50], [10/107], step: 1722, 3.149 samples/sec, batch_loss: 0.1209, batch_loss_c: 0.1065, batch_loss_s: 0.1544, time:12.7024, lr:0.001\u001b[0m\n",
            "2019-11-26 14:58:14 \u001b[32mINFO     \u001b[0m train.py: [16/50], [20/107], step: 1732, 6.632 samples/sec, batch_loss: 0.1566, batch_loss_c: 0.1456, batch_loss_s: 0.1824, time:6.0309, lr:0.001\u001b[0m\n",
            "2019-11-26 14:58:19 \u001b[32mINFO     \u001b[0m train.py: [16/50], [30/107], step: 1742, 7.076 samples/sec, batch_loss: 0.1245, batch_loss_c: 0.1128, batch_loss_s: 0.1517, time:5.6529, lr:0.001\u001b[0m\n",
            "2019-11-26 14:58:25 \u001b[32mINFO     \u001b[0m train.py: [16/50], [40/107], step: 1752, 6.727 samples/sec, batch_loss: 0.4205, batch_loss_c: 0.4223, batch_loss_s: 0.4163, time:5.9464, lr:0.001\u001b[0m\n",
            "2019-11-26 14:58:32 \u001b[32mINFO     \u001b[0m train.py: [16/50], [50/107], step: 1762, 5.761 samples/sec, batch_loss: 0.3171, batch_loss_c: 0.3103, batch_loss_s: 0.3328, time:6.9434, lr:0.001\u001b[0m\n",
            "2019-11-26 14:58:40 \u001b[32mINFO     \u001b[0m train.py: [16/50], [60/107], step: 1772, 4.889 samples/sec, batch_loss: 0.1172, batch_loss_c: 0.1183, batch_loss_s: 0.1146, time:8.1810, lr:0.001\u001b[0m\n",
            "2019-11-26 14:58:48 \u001b[32mINFO     \u001b[0m train.py: [16/50], [70/107], step: 1782, 5.203 samples/sec, batch_loss: 0.5813, batch_loss_c: 0.5813, batch_loss_s: 0.5813, time:7.6880, lr:0.001\u001b[0m\n",
            "2019-11-26 14:58:55 \u001b[32mINFO     \u001b[0m train.py: [16/50], [80/107], step: 1792, 6.046 samples/sec, batch_loss: 0.4524, batch_loss_c: 0.4563, batch_loss_s: 0.4433, time:6.6155, lr:0.001\u001b[0m\n",
            "2019-11-26 14:59:00 \u001b[32mINFO     \u001b[0m train.py: [16/50], [90/107], step: 1802, 7.342 samples/sec, batch_loss: 0.1271, batch_loss_c: 0.1168, batch_loss_s: 0.1512, time:5.4477, lr:0.001\u001b[0m\n",
            "2019-11-26 14:59:05 \u001b[32mINFO     \u001b[0m train.py: [16/50], [100/107], step: 1812, 7.939 samples/sec, batch_loss: 0.1183, batch_loss_c: 0.1110, batch_loss_s: 0.1352, time:5.0382, lr:0.001\u001b[0m\n",
            "2019-11-26 14:59:08 \u001b[32mINFO     \u001b[0m train.py: [16/50], train_loss: 0.1996, time: 80.8772, lr: 0.001\u001b[0m\n",
            "2019-11-26 14:59:14 \u001b[32mINFO     \u001b[0m train.py: [17/50], [0/107], step: 1819, 7.173 samples/sec, batch_loss: 0.2915, batch_loss_c: 0.2706, batch_loss_s: 0.3402, time:5.5766, lr:0.001\u001b[0m\n",
            "2019-11-26 14:59:27 \u001b[32mINFO     \u001b[0m train.py: [17/50], [10/107], step: 1829, 3.222 samples/sec, batch_loss: 0.1573, batch_loss_c: 0.1422, batch_loss_s: 0.1924, time:12.4140, lr:0.001\u001b[0m\n",
            "2019-11-26 14:59:34 \u001b[32mINFO     \u001b[0m train.py: [17/50], [20/107], step: 1839, 5.231 samples/sec, batch_loss: 0.2197, batch_loss_c: 0.2095, batch_loss_s: 0.2435, time:7.6471, lr:0.001\u001b[0m\n",
            "2019-11-26 14:59:40 \u001b[32mINFO     \u001b[0m train.py: [17/50], [30/107], step: 1849, 6.469 samples/sec, batch_loss: 0.1810, batch_loss_c: 0.1844, batch_loss_s: 0.1728, time:6.1834, lr:0.001\u001b[0m\n",
            "2019-11-26 14:59:47 \u001b[32mINFO     \u001b[0m train.py: [17/50], [40/107], step: 1859, 6.358 samples/sec, batch_loss: 0.1403, batch_loss_c: 0.1380, batch_loss_s: 0.1457, time:6.2918, lr:0.001\u001b[0m\n",
            "2019-11-26 14:59:53 \u001b[32mINFO     \u001b[0m train.py: [17/50], [50/107], step: 1869, 6.840 samples/sec, batch_loss: 0.0984, batch_loss_c: 0.0948, batch_loss_s: 0.1067, time:5.8479, lr:0.001\u001b[0m\n",
            "2019-11-26 14:59:59 \u001b[32mINFO     \u001b[0m train.py: [17/50], [60/107], step: 1879, 6.208 samples/sec, batch_loss: 0.1857, batch_loss_c: 0.1525, batch_loss_s: 0.2631, time:6.4428, lr:0.001\u001b[0m\n",
            "2019-11-26 15:00:07 \u001b[32mINFO     \u001b[0m train.py: [17/50], [70/107], step: 1889, 5.138 samples/sec, batch_loss: 0.0741, batch_loss_c: 0.0622, batch_loss_s: 0.1018, time:7.7854, lr:0.001\u001b[0m\n",
            "2019-11-26 15:00:14 \u001b[32mINFO     \u001b[0m train.py: [17/50], [80/107], step: 1899, 5.314 samples/sec, batch_loss: 0.1410, batch_loss_c: 0.1438, batch_loss_s: 0.1345, time:7.5275, lr:0.001\u001b[0m\n",
            "2019-11-26 15:00:20 \u001b[32mINFO     \u001b[0m train.py: [17/50], [90/107], step: 1909, 7.288 samples/sec, batch_loss: 0.3807, batch_loss_c: 0.3694, batch_loss_s: 0.4071, time:5.4885, lr:0.001\u001b[0m\n",
            "2019-11-26 15:00:25 \u001b[32mINFO     \u001b[0m train.py: [17/50], [100/107], step: 1919, 7.940 samples/sec, batch_loss: 0.1521, batch_loss_c: 0.1482, batch_loss_s: 0.1611, time:5.0379, lr:0.001\u001b[0m\n",
            "2019-11-26 15:00:28 \u001b[32mINFO     \u001b[0m train.py: [17/50], train_loss: 0.1825, time: 79.4476, lr: 0.001\u001b[0m\n",
            "2019-11-26 15:00:36 \u001b[32mINFO     \u001b[0m train.py: [18/50], [0/107], step: 1926, 5.447 samples/sec, batch_loss: 0.1522, batch_loss_c: 0.1351, batch_loss_s: 0.1920, time:7.3429, lr:0.001\u001b[0m\n",
            "2019-11-26 15:00:49 \u001b[32mINFO     \u001b[0m train.py: [18/50], [10/107], step: 1936, 3.160 samples/sec, batch_loss: 0.1729, batch_loss_c: 0.1847, batch_loss_s: 0.1455, time:12.6589, lr:0.001\u001b[0m\n",
            "2019-11-26 15:00:55 \u001b[32mINFO     \u001b[0m train.py: [18/50], [20/107], step: 1946, 6.622 samples/sec, batch_loss: 0.1981, batch_loss_c: 0.1648, batch_loss_s: 0.2760, time:6.0403, lr:0.001\u001b[0m\n",
            "2019-11-26 15:01:00 \u001b[32mINFO     \u001b[0m train.py: [18/50], [30/107], step: 1956, 7.521 samples/sec, batch_loss: 0.3706, batch_loss_c: 0.3541, batch_loss_s: 0.4091, time:5.3181, lr:0.001\u001b[0m\n",
            "2019-11-26 15:01:06 \u001b[32mINFO     \u001b[0m train.py: [18/50], [40/107], step: 1966, 6.100 samples/sec, batch_loss: 0.3685, batch_loss_c: 0.3708, batch_loss_s: 0.3632, time:6.5578, lr:0.001\u001b[0m\n",
            "2019-11-26 15:01:13 \u001b[32mINFO     \u001b[0m train.py: [18/50], [50/107], step: 1976, 6.488 samples/sec, batch_loss: 0.1550, batch_loss_c: 0.1522, batch_loss_s: 0.1615, time:6.1651, lr:0.001\u001b[0m\n",
            "2019-11-26 15:01:24 \u001b[32mINFO     \u001b[0m train.py: [18/50], [60/107], step: 1986, 3.437 samples/sec, batch_loss: 0.3016, batch_loss_c: 0.2747, batch_loss_s: 0.3643, time:11.6365, lr:0.001\u001b[0m\n",
            "2019-11-26 15:01:31 \u001b[32mINFO     \u001b[0m train.py: [18/50], [70/107], step: 1996, 5.964 samples/sec, batch_loss: 0.1775, batch_loss_c: 0.1735, batch_loss_s: 0.1869, time:6.7067, lr:0.001\u001b[0m\n",
            "2019-11-26 15:01:37 \u001b[32mINFO     \u001b[0m train.py: [18/50], [80/107], step: 2006, 6.494 samples/sec, batch_loss: 0.1187, batch_loss_c: 0.1074, batch_loss_s: 0.1451, time:6.1597, lr:0.001\u001b[0m\n",
            "2019-11-26 15:01:42 \u001b[32mINFO     \u001b[0m train.py: [18/50], [90/107], step: 2016, 7.477 samples/sec, batch_loss: 0.1500, batch_loss_c: 0.1533, batch_loss_s: 0.1422, time:5.3500, lr:0.001\u001b[0m\n",
            "2019-11-26 15:01:48 \u001b[32mINFO     \u001b[0m train.py: [18/50], [100/107], step: 2026, 7.932 samples/sec, batch_loss: 0.1816, batch_loss_c: 0.1798, batch_loss_s: 0.1858, time:5.0426, lr:0.001\u001b[0m\n",
            "2019-11-26 15:01:51 \u001b[32mINFO     \u001b[0m train.py: [18/50], train_loss: 0.1912, time: 82.2565, lr: 0.001\u001b[0m\n",
            "2019-11-26 15:01:58 \u001b[32mINFO     \u001b[0m train.py: [19/50], [0/107], step: 2033, 6.067 samples/sec, batch_loss: 0.3492, batch_loss_c: 0.3305, batch_loss_s: 0.3929, time:6.5930, lr:0.001\u001b[0m\n",
            "2019-11-26 15:02:10 \u001b[32mINFO     \u001b[0m train.py: [19/50], [10/107], step: 2043, 3.286 samples/sec, batch_loss: 0.0985, batch_loss_c: 0.0869, batch_loss_s: 0.1255, time:12.1713, lr:0.001\u001b[0m\n",
            "2019-11-26 15:02:17 \u001b[32mINFO     \u001b[0m train.py: [19/50], [20/107], step: 2053, 5.855 samples/sec, batch_loss: 0.1070, batch_loss_c: 0.0994, batch_loss_s: 0.1248, time:6.8318, lr:0.001\u001b[0m\n",
            "tcmalloc: large alloc 1342701568 bytes == 0xa0dbe000 @  0x7fed666bc1e7 0x7fed5bd8ff71 0x7fed5bdf355d 0x7fed5bdf6e28 0x7fed5bdf73e5 0x7fed5be8dfc2 0x50abc5 0x50d320 0x5081d5 0x50a020 0x50aa1d 0x50d320 0x5093e5 0x5951c1 0x5a04ce 0x557878 0x50c81e 0x5081d5 0x50a020 0x50aa1d 0x50c549 0x5081d5 0x50a020 0x50aa1d 0x50c549 0x5081d5 0x5895e1 0x5a04ce 0x50d8f5 0x509ce8 0x50aa1d\n",
            "2019-11-26 15:02:24 \u001b[32mINFO     \u001b[0m train.py: [19/50], [30/107], step: 2063, 5.359 samples/sec, batch_loss: 0.0793, batch_loss_c: 0.0662, batch_loss_s: 0.1098, time:7.4639, lr:0.001\u001b[0m\n",
            "2019-11-26 15:02:30 \u001b[32mINFO     \u001b[0m train.py: [19/50], [40/107], step: 2073, 7.270 samples/sec, batch_loss: 0.3629, batch_loss_c: 0.3560, batch_loss_s: 0.3790, time:5.5017, lr:0.001\u001b[0m\n",
            "2019-11-26 15:02:36 \u001b[32mINFO     \u001b[0m train.py: [19/50], [50/107], step: 2083, 6.421 samples/sec, batch_loss: 0.3634, batch_loss_c: 0.3469, batch_loss_s: 0.4017, time:6.2295, lr:0.001\u001b[0m\n",
            "2019-11-26 15:02:44 \u001b[32mINFO     \u001b[0m train.py: [19/50], [60/107], step: 2093, 5.108 samples/sec, batch_loss: 0.1250, batch_loss_c: 0.1114, batch_loss_s: 0.1565, time:7.8305, lr:0.001\u001b[0m\n",
            "2019-11-26 15:02:51 \u001b[32mINFO     \u001b[0m train.py: [19/50], [70/107], step: 2103, 5.492 samples/sec, batch_loss: 0.1153, batch_loss_c: 0.1093, batch_loss_s: 0.1293, time:7.2836, lr:0.001\u001b[0m\n",
            "2019-11-26 15:02:57 \u001b[32mINFO     \u001b[0m train.py: [19/50], [80/107], step: 2113, 6.434 samples/sec, batch_loss: 0.1427, batch_loss_c: 0.1136, batch_loss_s: 0.2105, time:6.2171, lr:0.001\u001b[0m\n",
            "2019-11-26 15:03:02 \u001b[32mINFO     \u001b[0m train.py: [19/50], [90/107], step: 2123, 7.780 samples/sec, batch_loss: 0.3298, batch_loss_c: 0.3157, batch_loss_s: 0.3629, time:5.1417, lr:0.001\u001b[0m\n",
            "2019-11-26 15:03:07 \u001b[32mINFO     \u001b[0m train.py: [19/50], [100/107], step: 2133, 8.100 samples/sec, batch_loss: 0.0848, batch_loss_c: 0.0677, batch_loss_s: 0.1248, time:4.9384, lr:0.001\u001b[0m\n",
            "2019-11-26 15:03:11 \u001b[32mINFO     \u001b[0m train.py: [19/50], train_loss: 0.1853, time: 79.4500, lr: 0.001\u001b[0m\n",
            "2019-11-26 15:03:20 \u001b[32mINFO     \u001b[0m train.py: [20/50], [0/107], step: 2140, 4.345 samples/sec, batch_loss: 0.1112, batch_loss_c: 0.1031, batch_loss_s: 0.1303, time:9.2070, lr:0.001\u001b[0m\n",
            "2019-11-26 15:03:29 \u001b[32mINFO     \u001b[0m train.py: [20/50], [10/107], step: 2150, 4.359 samples/sec, batch_loss: 0.0755, batch_loss_c: 0.0606, batch_loss_s: 0.1104, time:9.1775, lr:0.001\u001b[0m\n",
            "2019-11-26 15:03:35 \u001b[32mINFO     \u001b[0m train.py: [20/50], [20/107], step: 2160, 6.799 samples/sec, batch_loss: 0.3005, batch_loss_c: 0.2758, batch_loss_s: 0.3582, time:5.8832, lr:0.001\u001b[0m\n",
            "2019-11-26 15:03:43 \u001b[32mINFO     \u001b[0m train.py: [20/50], [30/107], step: 2170, 4.947 samples/sec, batch_loss: 0.1581, batch_loss_c: 0.1742, batch_loss_s: 0.1207, time:8.0853, lr:0.001\u001b[0m\n",
            "2019-11-26 15:03:49 \u001b[32mINFO     \u001b[0m train.py: [20/50], [40/107], step: 2180, 6.693 samples/sec, batch_loss: 0.3403, batch_loss_c: 0.3384, batch_loss_s: 0.3446, time:5.9766, lr:0.001\u001b[0m\n",
            "2019-11-26 15:03:57 \u001b[32mINFO     \u001b[0m train.py: [20/50], [50/107], step: 2190, 5.531 samples/sec, batch_loss: 0.1517, batch_loss_c: 0.1440, batch_loss_s: 0.1698, time:7.2315, lr:0.001\u001b[0m\n",
            "2019-11-26 15:04:04 \u001b[32mINFO     \u001b[0m train.py: [20/50], [60/107], step: 2200, 5.240 samples/sec, batch_loss: 0.0937, batch_loss_c: 0.0855, batch_loss_s: 0.1129, time:7.6342, lr:0.001\u001b[0m\n",
            "2019-11-26 15:04:11 \u001b[32mINFO     \u001b[0m train.py: [20/50], [70/107], step: 2210, 6.067 samples/sec, batch_loss: 0.3612, batch_loss_c: 0.3534, batch_loss_s: 0.3794, time:6.5930, lr:0.001\u001b[0m\n",
            "2019-11-26 15:04:17 \u001b[32mINFO     \u001b[0m train.py: [20/50], [80/107], step: 2220, 5.987 samples/sec, batch_loss: 0.5505, batch_loss_c: 0.5294, batch_loss_s: 0.5998, time:6.6814, lr:0.001\u001b[0m\n",
            "2019-11-26 15:04:23 \u001b[32mINFO     \u001b[0m train.py: [20/50], [90/107], step: 2230, 7.704 samples/sec, batch_loss: 0.1635, batch_loss_c: 0.1838, batch_loss_s: 0.1161, time:5.1924, lr:0.001\u001b[0m\n",
            "2019-11-26 15:04:28 \u001b[32mINFO     \u001b[0m train.py: [20/50], [100/107], step: 2240, 7.963 samples/sec, batch_loss: 0.1863, batch_loss_c: 0.2062, batch_loss_s: 0.1398, time:5.0231, lr:0.001\u001b[0m\n",
            "2019-11-26 15:04:31 \u001b[32mINFO     \u001b[0m train.py: [20/50], train_loss: 0.1850, time: 79.9083, lr: 0.001\u001b[0m\n",
            "2019-11-26 15:04:41 \u001b[32mINFO     \u001b[0m train.py: [21/50], [0/107], step: 2247, 4.200 samples/sec, batch_loss: 0.0786, batch_loss_c: 0.0685, batch_loss_s: 0.1023, time:9.5249, lr:0.001\u001b[0m\n",
            "tcmalloc: large alloc 1342701568 bytes == 0xa90b8000 @  0x7fed666bc1e7 0x7fed5bd8ff71 0x7fed5bdf355d 0x7fed5bdf6e28 0x7fed5bdf73e5 0x7fed5be8dfc2 0x50abc5 0x50d320 0x5081d5 0x50a020 0x50aa1d 0x50d320 0x5093e5 0x5951c1 0x5a04ce 0x557878 0x50c81e 0x5081d5 0x50a020 0x50aa1d 0x50c549 0x5081d5 0x50a020 0x50aa1d 0x50c549 0x5081d5 0x5895e1 0x5a04ce 0x50d8f5 0x509ce8 0x50aa1d\n",
            "2019-11-26 15:04:52 \u001b[32mINFO     \u001b[0m train.py: [21/50], [10/107], step: 2257, 3.693 samples/sec, batch_loss: 0.1092, batch_loss_c: 0.1082, batch_loss_s: 0.1115, time:10.8316, lr:0.001\u001b[0m\n",
            "2019-11-26 15:04:58 \u001b[32mINFO     \u001b[0m train.py: [21/50], [20/107], step: 2267, 5.911 samples/sec, batch_loss: 0.3285, batch_loss_c: 0.3103, batch_loss_s: 0.3709, time:6.7672, lr:0.001\u001b[0m\n",
            "2019-11-26 15:05:05 \u001b[32mINFO     \u001b[0m train.py: [21/50], [30/107], step: 2277, 6.545 samples/sec, batch_loss: 0.1750, batch_loss_c: 0.1614, batch_loss_s: 0.2067, time:6.1115, lr:0.001\u001b[0m\n",
            "2019-11-26 15:05:10 \u001b[32mINFO     \u001b[0m train.py: [21/50], [40/107], step: 2287, 6.877 samples/sec, batch_loss: 0.2899, batch_loss_c: 0.2690, batch_loss_s: 0.3387, time:5.8163, lr:0.001\u001b[0m\n",
            "2019-11-26 15:05:18 \u001b[32mINFO     \u001b[0m train.py: [21/50], [50/107], step: 2297, 5.569 samples/sec, batch_loss: 0.1206, batch_loss_c: 0.1032, batch_loss_s: 0.1613, time:7.1828, lr:0.001\u001b[0m\n",
            "2019-11-26 15:05:25 \u001b[32mINFO     \u001b[0m train.py: [21/50], [60/107], step: 2307, 5.064 samples/sec, batch_loss: 0.1089, batch_loss_c: 0.0940, batch_loss_s: 0.1438, time:7.8984, lr:0.001\u001b[0m\n",
            "2019-11-26 15:05:34 \u001b[32mINFO     \u001b[0m train.py: [21/50], [70/107], step: 2317, 4.430 samples/sec, batch_loss: 0.1705, batch_loss_c: 0.1740, batch_loss_s: 0.1624, time:9.0285, lr:0.001\u001b[0m\n",
            "2019-11-26 15:05:42 \u001b[32mINFO     \u001b[0m train.py: [21/50], [80/107], step: 2327, 5.380 samples/sec, batch_loss: 0.1526, batch_loss_c: 0.1240, batch_loss_s: 0.2194, time:7.4354, lr:0.001\u001b[0m\n",
            "2019-11-26 15:05:47 \u001b[32mINFO     \u001b[0m train.py: [21/50], [90/107], step: 2337, 7.664 samples/sec, batch_loss: 0.0924, batch_loss_c: 0.0783, batch_loss_s: 0.1254, time:5.2193, lr:0.001\u001b[0m\n",
            "2019-11-26 15:05:52 \u001b[32mINFO     \u001b[0m train.py: [21/50], [100/107], step: 2347, 8.096 samples/sec, batch_loss: 0.1474, batch_loss_c: 0.1392, batch_loss_s: 0.1665, time:4.9408, lr:0.001\u001b[0m\n",
            "2019-11-26 15:05:55 \u001b[32mINFO     \u001b[0m train.py: [21/50], train_loss: 0.1832, time: 84.0383, lr: 0.001\u001b[0m\n",
            "2019-11-26 15:06:05 \u001b[32mINFO     \u001b[0m train.py: [22/50], [0/107], step: 2354, 4.481 samples/sec, batch_loss: 0.1795, batch_loss_c: 0.2128, batch_loss_s: 0.1017, time:8.9261, lr:0.001\u001b[0m\n",
            "2019-11-26 15:06:14 \u001b[32mINFO     \u001b[0m train.py: [22/50], [10/107], step: 2364, 4.200 samples/sec, batch_loss: 0.2241, batch_loss_c: 0.2025, batch_loss_s: 0.2746, time:9.5244, lr:0.001\u001b[0m\n",
            "tcmalloc: large alloc 1636024320 bytes == 0xa9734000 @  0x7fed666bc1e7 0x7fed5bd8ff71 0x7fed5bdf355d 0x7fed5bdf6e28 0x7fed5bdf73e5 0x7fed5be8dfc2 0x50abc5 0x50d320 0x5081d5 0x50a020 0x50aa1d 0x50d320 0x5093e5 0x5951c1 0x5a04ce 0x557878 0x50c81e 0x5081d5 0x50a020 0x50aa1d 0x50c549 0x5081d5 0x50a020 0x50aa1d 0x50c549 0x5081d5 0x5895e1 0x5a04ce 0x50d8f5 0x509ce8 0x50aa1d\n",
            "2019-11-26 15:06:22 \u001b[32mINFO     \u001b[0m train.py: [22/50], [20/107], step: 2374, 5.126 samples/sec, batch_loss: 0.3522, batch_loss_c: 0.3466, batch_loss_s: 0.3655, time:7.8032, lr:0.001\u001b[0m\n",
            "2019-11-26 15:06:29 \u001b[32mINFO     \u001b[0m train.py: [22/50], [30/107], step: 2384, 5.934 samples/sec, batch_loss: 0.1113, batch_loss_c: 0.1057, batch_loss_s: 0.1243, time:6.7403, lr:0.001\u001b[0m\n",
            "2019-11-26 15:06:35 \u001b[32mINFO     \u001b[0m train.py: [22/50], [40/107], step: 2394, 6.201 samples/sec, batch_loss: 0.1882, batch_loss_c: 0.2064, batch_loss_s: 0.1458, time:6.4503, lr:0.001\u001b[0m\n",
            "2019-11-26 15:06:41 \u001b[32mINFO     \u001b[0m train.py: [22/50], [50/107], step: 2404, 6.526 samples/sec, batch_loss: 0.0796, batch_loss_c: 0.0705, batch_loss_s: 0.1010, time:6.1294, lr:0.001\u001b[0m\n",
            "2019-11-26 15:06:50 \u001b[32mINFO     \u001b[0m train.py: [22/50], [60/107], step: 2414, 4.418 samples/sec, batch_loss: 0.4410, batch_loss_c: 0.4333, batch_loss_s: 0.4592, time:9.0540, lr:0.001\u001b[0m\n",
            "2019-11-26 15:07:01 \u001b[32mINFO     \u001b[0m train.py: [22/50], [70/107], step: 2424, 3.871 samples/sec, batch_loss: 0.0823, batch_loss_c: 0.0702, batch_loss_s: 0.1104, time:10.3340, lr:0.001\u001b[0m\n",
            "tcmalloc: large alloc 1453907968 bytes == 0xa783e000 @  0x7fed666bc1e7 0x7fed5bd8ff71 0x7fed5bdf355d 0x7fed5bdf6e28 0x7fed5bdf73e5 0x7fed5be8dfc2 0x50abc5 0x50d320 0x5081d5 0x50a020 0x50aa1d 0x50d320 0x5093e5 0x5951c1 0x5a04ce 0x557878 0x50c81e 0x5081d5 0x50a020 0x50aa1d 0x50c549 0x5081d5 0x50a020 0x50aa1d 0x50c549 0x5081d5 0x5895e1 0x5a04ce 0x50d8f5 0x509ce8 0x50aa1d\n",
            "2019-11-26 15:07:09 \u001b[32mINFO     \u001b[0m train.py: [22/50], [80/107], step: 2434, 4.662 samples/sec, batch_loss: 0.4848, batch_loss_c: 0.4577, batch_loss_s: 0.5480, time:8.5801, lr:0.001\u001b[0m\n",
            "2019-11-26 15:07:15 \u001b[32mINFO     \u001b[0m train.py: [22/50], [90/107], step: 2444, 6.646 samples/sec, batch_loss: 0.1772, batch_loss_c: 0.1643, batch_loss_s: 0.2075, time:6.0187, lr:0.001\u001b[0m\n",
            "2019-11-26 15:07:20 \u001b[32mINFO     \u001b[0m train.py: [22/50], [100/107], step: 2454, 8.127 samples/sec, batch_loss: 0.2878, batch_loss_c: 0.2480, batch_loss_s: 0.3806, time:4.9216, lr:0.001\u001b[0m\n",
            "2019-11-26 15:07:24 \u001b[32mINFO     \u001b[0m train.py: [22/50], train_loss: 0.1792, time: 87.8489, lr: 0.001\u001b[0m\n",
            "2019-11-26 15:07:31 \u001b[32mINFO     \u001b[0m train.py: [23/50], [0/107], step: 2461, 5.807 samples/sec, batch_loss: 0.2995, batch_loss_c: 0.2798, batch_loss_s: 0.3455, time:6.8877, lr:0.001\u001b[0m\n",
            "2019-11-26 15:07:42 \u001b[32mINFO     \u001b[0m train.py: [23/50], [10/107], step: 2471, 3.723 samples/sec, batch_loss: 0.2256, batch_loss_c: 0.2223, batch_loss_s: 0.2334, time:10.7450, lr:0.001\u001b[0m\n",
            "2019-11-26 15:07:48 \u001b[32mINFO     \u001b[0m train.py: [23/50], [20/107], step: 2481, 6.087 samples/sec, batch_loss: 0.1713, batch_loss_c: 0.1593, batch_loss_s: 0.1992, time:6.5710, lr:0.001\u001b[0m\n",
            "2019-11-26 15:07:55 \u001b[32mINFO     \u001b[0m train.py: [23/50], [30/107], step: 2491, 5.778 samples/sec, batch_loss: 0.1023, batch_loss_c: 0.0907, batch_loss_s: 0.1294, time:6.9229, lr:0.001\u001b[0m\n",
            "2019-11-26 15:08:02 \u001b[32mINFO     \u001b[0m train.py: [23/50], [40/107], step: 2501, 6.005 samples/sec, batch_loss: 0.1226, batch_loss_c: 0.1250, batch_loss_s: 0.1172, time:6.6614, lr:0.001\u001b[0m\n",
            "2019-11-26 15:08:08 \u001b[32mINFO     \u001b[0m train.py: [23/50], [50/107], step: 2511, 6.393 samples/sec, batch_loss: 0.0753, batch_loss_c: 0.0675, batch_loss_s: 0.0934, time:6.2569, lr:0.001\u001b[0m\n",
            "2019-11-26 15:08:15 \u001b[32mINFO     \u001b[0m train.py: [23/50], [60/107], step: 2521, 5.593 samples/sec, batch_loss: 0.3405, batch_loss_c: 0.3372, batch_loss_s: 0.3483, time:7.1517, lr:0.001\u001b[0m\n",
            "2019-11-26 15:08:22 \u001b[32mINFO     \u001b[0m train.py: [23/50], [70/107], step: 2531, 5.599 samples/sec, batch_loss: 0.1560, batch_loss_c: 0.1595, batch_loss_s: 0.1480, time:7.1437, lr:0.001\u001b[0m\n",
            "2019-11-26 15:08:29 \u001b[32mINFO     \u001b[0m train.py: [23/50], [80/107], step: 2541, 6.233 samples/sec, batch_loss: 0.1062, batch_loss_c: 0.0792, batch_loss_s: 0.1691, time:6.4173, lr:0.001\u001b[0m\n",
            "2019-11-26 15:08:34 \u001b[32mINFO     \u001b[0m train.py: [23/50], [90/107], step: 2551, 7.772 samples/sec, batch_loss: 0.1484, batch_loss_c: 0.1531, batch_loss_s: 0.1376, time:5.1467, lr:0.001\u001b[0m\n",
            "2019-11-26 15:08:39 \u001b[32mINFO     \u001b[0m train.py: [23/50], [100/107], step: 2561, 8.032 samples/sec, batch_loss: 0.1745, batch_loss_c: 0.1838, batch_loss_s: 0.1529, time:4.9802, lr:0.001\u001b[0m\n",
            "2019-11-26 15:08:42 \u001b[32mINFO     \u001b[0m train.py: [23/50], train_loss: 0.1924, time: 78.1372, lr: 0.001\u001b[0m\n",
            "2019-11-26 15:08:51 \u001b[32mINFO     \u001b[0m train.py: [24/50], [0/107], step: 2568, 4.987 samples/sec, batch_loss: 0.0831, batch_loss_c: 0.0685, batch_loss_s: 0.1171, time:8.0206, lr:0.001\u001b[0m\n",
            "2019-11-26 15:08:59 \u001b[32mINFO     \u001b[0m train.py: [24/50], [10/107], step: 2578, 4.507 samples/sec, batch_loss: 0.3553, batch_loss_c: 0.3606, batch_loss_s: 0.3431, time:8.8746, lr:0.001\u001b[0m\n",
            "2019-11-26 15:09:05 \u001b[32mINFO     \u001b[0m train.py: [24/50], [20/107], step: 2588, 6.978 samples/sec, batch_loss: 0.1473, batch_loss_c: 0.1381, batch_loss_s: 0.1687, time:5.7326, lr:0.001\u001b[0m\n",
            "2019-11-26 15:09:11 \u001b[32mINFO     \u001b[0m train.py: [24/50], [30/107], step: 2598, 6.747 samples/sec, batch_loss: 0.1836, batch_loss_c: 0.1896, batch_loss_s: 0.1694, time:5.9286, lr:0.001\u001b[0m\n",
            "2019-11-26 15:09:17 \u001b[32mINFO     \u001b[0m train.py: [24/50], [40/107], step: 2608, 6.306 samples/sec, batch_loss: 0.1418, batch_loss_c: 0.1257, batch_loss_s: 0.1796, time:6.3433, lr:0.001\u001b[0m\n",
            "2019-11-26 15:09:24 \u001b[32mINFO     \u001b[0m train.py: [24/50], [50/107], step: 2618, 6.126 samples/sec, batch_loss: 0.1085, batch_loss_c: 0.0925, batch_loss_s: 0.1456, time:6.5292, lr:0.001\u001b[0m\n",
            "2019-11-26 15:09:32 \u001b[32mINFO     \u001b[0m train.py: [24/50], [60/107], step: 2628, 4.988 samples/sec, batch_loss: 0.2218, batch_loss_c: 0.2185, batch_loss_s: 0.2296, time:8.0191, lr:0.001\u001b[0m\n",
            "2019-11-26 15:09:39 \u001b[32mINFO     \u001b[0m train.py: [24/50], [70/107], step: 2638, 5.596 samples/sec, batch_loss: 0.1047, batch_loss_c: 0.1003, batch_loss_s: 0.1149, time:7.1481, lr:0.001\u001b[0m\n",
            "2019-11-26 15:09:46 \u001b[32mINFO     \u001b[0m train.py: [24/50], [80/107], step: 2648, 6.100 samples/sec, batch_loss: 0.3296, batch_loss_c: 0.3157, batch_loss_s: 0.3620, time:6.5574, lr:0.001\u001b[0m\n",
            "2019-11-26 15:09:51 \u001b[32mINFO     \u001b[0m train.py: [24/50], [90/107], step: 2658, 7.717 samples/sec, batch_loss: 0.1059, batch_loss_c: 0.0802, batch_loss_s: 0.1660, time:5.1832, lr:0.001\u001b[0m\n",
            "2019-11-26 15:09:56 \u001b[32mINFO     \u001b[0m train.py: [24/50], [100/107], step: 2668, 8.069 samples/sec, batch_loss: 0.0879, batch_loss_c: 0.0739, batch_loss_s: 0.1207, time:4.9575, lr:0.001\u001b[0m\n",
            "2019-11-26 15:09:59 \u001b[32mINFO     \u001b[0m train.py: [24/50], train_loss: 0.1766, time: 76.5302, lr: 0.001\u001b[0m\n",
            "2019-11-26 15:10:04 \u001b[32mINFO     \u001b[0m train.py: [25/50], [0/107], step: 2675, 8.499 samples/sec, batch_loss: 0.0806, batch_loss_c: 0.0689, batch_loss_s: 0.1080, time:4.7063, lr:0.001\u001b[0m\n",
            "2019-11-26 15:10:19 \u001b[32mINFO     \u001b[0m train.py: [25/50], [10/107], step: 2685, 2.680 samples/sec, batch_loss: 0.1078, batch_loss_c: 0.0944, batch_loss_s: 0.1392, time:14.9281, lr:0.001\u001b[0m\n",
            "tcmalloc: large alloc 1337573376 bytes == 0x9fba2000 @  0x7fed666bc1e7 0x7fed5bd8ff71 0x7fed5bdf355d 0x7fed5bdf6e28 0x7fed5bdf73e5 0x7fed5be8dfc2 0x50abc5 0x50d320 0x5081d5 0x50a020 0x50aa1d 0x50d320 0x5093e5 0x5951c1 0x5a04ce 0x557878 0x50c81e 0x5081d5 0x50a020 0x50aa1d 0x50c549 0x5081d5 0x50a020 0x50aa1d 0x50c549 0x5081d5 0x5895e1 0x5a04ce 0x50d8f5 0x509ce8 0x50aa1d\n",
            "2019-11-26 15:10:27 \u001b[32mINFO     \u001b[0m train.py: [25/50], [20/107], step: 2695, 4.974 samples/sec, batch_loss: 0.1221, batch_loss_c: 0.1204, batch_loss_s: 0.1261, time:8.0419, lr:0.001\u001b[0m\n",
            "2019-11-26 15:10:33 \u001b[32mINFO     \u001b[0m train.py: [25/50], [30/107], step: 2705, 6.971 samples/sec, batch_loss: 0.1962, batch_loss_c: 0.1966, batch_loss_s: 0.1951, time:5.7384, lr:0.001\u001b[0m\n",
            "2019-11-26 15:10:39 \u001b[32mINFO     \u001b[0m train.py: [25/50], [40/107], step: 2715, 6.527 samples/sec, batch_loss: 0.0857, batch_loss_c: 0.0672, batch_loss_s: 0.1288, time:6.1285, lr:0.001\u001b[0m\n",
            "2019-11-26 15:10:45 \u001b[32mINFO     \u001b[0m train.py: [25/50], [50/107], step: 2725, 6.437 samples/sec, batch_loss: 0.1099, batch_loss_c: 0.0946, batch_loss_s: 0.1454, time:6.2136, lr:0.001\u001b[0m\n",
            "2019-11-26 15:10:53 \u001b[32mINFO     \u001b[0m train.py: [25/50], [60/107], step: 2735, 5.337 samples/sec, batch_loss: 0.1433, batch_loss_c: 0.1363, batch_loss_s: 0.1598, time:7.4946, lr:0.001\u001b[0m\n",
            "2019-11-26 15:11:00 \u001b[32mINFO     \u001b[0m train.py: [25/50], [70/107], step: 2745, 5.538 samples/sec, batch_loss: 0.1942, batch_loss_c: 0.2191, batch_loss_s: 0.1360, time:7.2226, lr:0.001\u001b[0m\n",
            "2019-11-26 15:11:07 \u001b[32mINFO     \u001b[0m train.py: [25/50], [80/107], step: 2755, 5.944 samples/sec, batch_loss: 0.0885, batch_loss_c: 0.0807, batch_loss_s: 0.1067, time:6.7297, lr:0.001\u001b[0m\n",
            "2019-11-26 15:11:12 \u001b[32mINFO     \u001b[0m train.py: [25/50], [90/107], step: 2765, 7.854 samples/sec, batch_loss: 0.2016, batch_loss_c: 0.1851, batch_loss_s: 0.2400, time:5.0928, lr:0.001\u001b[0m\n",
            "2019-11-26 15:11:17 \u001b[32mINFO     \u001b[0m train.py: [25/50], [100/107], step: 2775, 7.980 samples/sec, batch_loss: 0.3243, batch_loss_c: 0.3111, batch_loss_s: 0.3552, time:5.0128, lr:0.001\u001b[0m\n",
            "2019-11-26 15:11:20 \u001b[32mINFO     \u001b[0m train.py: [25/50], train_loss: 0.1618, time: 80.5771, lr: 0.001\u001b[0m\n",
            "2019-11-26 15:11:26 \u001b[32mINFO     \u001b[0m train.py: [26/50], [0/107], step: 2782, 7.048 samples/sec, batch_loss: 0.0963, batch_loss_c: 0.0776, batch_loss_s: 0.1397, time:5.6757, lr:0.001\u001b[0m\n",
            "2019-11-26 15:11:40 \u001b[32mINFO     \u001b[0m train.py: [26/50], [10/107], step: 2792, 2.954 samples/sec, batch_loss: 0.1341, batch_loss_c: 0.1217, batch_loss_s: 0.1629, time:13.5404, lr:0.001\u001b[0m\n",
            "2019-11-26 15:11:47 \u001b[32mINFO     \u001b[0m train.py: [26/50], [20/107], step: 2802, 5.316 samples/sec, batch_loss: 0.2952, batch_loss_c: 0.2880, batch_loss_s: 0.3122, time:7.5250, lr:0.001\u001b[0m\n",
            "2019-11-26 15:11:54 \u001b[32mINFO     \u001b[0m train.py: [26/50], [30/107], step: 2812, 6.119 samples/sec, batch_loss: 0.0816, batch_loss_c: 0.0749, batch_loss_s: 0.0972, time:6.5370, lr:0.001\u001b[0m\n",
            "2019-11-26 15:11:59 \u001b[32mINFO     \u001b[0m train.py: [26/50], [40/107], step: 2822, 7.148 samples/sec, batch_loss: 0.1436, batch_loss_c: 0.1266, batch_loss_s: 0.1831, time:5.5963, lr:0.001\u001b[0m\n",
            "2019-11-26 15:12:05 \u001b[32mINFO     \u001b[0m train.py: [26/50], [50/107], step: 2832, 6.985 samples/sec, batch_loss: 0.1112, batch_loss_c: 0.1130, batch_loss_s: 0.1069, time:5.7263, lr:0.001\u001b[0m\n",
            "2019-11-26 15:12:13 \u001b[32mINFO     \u001b[0m train.py: [26/50], [60/107], step: 2842, 5.103 samples/sec, batch_loss: 0.1158, batch_loss_c: 0.1165, batch_loss_s: 0.1142, time:7.8388, lr:0.001\u001b[0m\n",
            "2019-11-26 15:12:21 \u001b[32mINFO     \u001b[0m train.py: [26/50], [70/107], step: 2852, 5.018 samples/sec, batch_loss: 0.3214, batch_loss_c: 0.3147, batch_loss_s: 0.3371, time:7.9707, lr:0.001\u001b[0m\n",
            "2019-11-26 15:12:29 \u001b[32mINFO     \u001b[0m train.py: [26/50], [80/107], step: 2862, 4.981 samples/sec, batch_loss: 0.1318, batch_loss_c: 0.1260, batch_loss_s: 0.1452, time:8.0298, lr:0.001\u001b[0m\n",
            "2019-11-26 15:12:34 \u001b[32mINFO     \u001b[0m train.py: [26/50], [90/107], step: 2872, 7.584 samples/sec, batch_loss: 0.1150, batch_loss_c: 0.1159, batch_loss_s: 0.1130, time:5.2740, lr:0.001\u001b[0m\n",
            "2019-11-26 15:12:39 \u001b[32mINFO     \u001b[0m train.py: [26/50], [100/107], step: 2882, 8.110 samples/sec, batch_loss: 0.1217, batch_loss_c: 0.1045, batch_loss_s: 0.1618, time:4.9321, lr:0.001\u001b[0m\n",
            "2019-11-26 15:12:42 \u001b[32mINFO     \u001b[0m train.py: [26/50], train_loss: 0.1761, time: 81.8368, lr: 0.001\u001b[0m\n",
            "2019-11-26 15:12:49 \u001b[32mINFO     \u001b[0m train.py: [27/50], [0/107], step: 2889, 6.629 samples/sec, batch_loss: 0.1039, batch_loss_c: 0.0877, batch_loss_s: 0.1416, time:6.0343, lr:0.001\u001b[0m\n",
            "2019-11-26 15:12:59 \u001b[32mINFO     \u001b[0m train.py: [27/50], [10/107], step: 2899, 3.786 samples/sec, batch_loss: 0.3081, batch_loss_c: 0.2915, batch_loss_s: 0.3468, time:10.5659, lr:0.001\u001b[0m\n",
            "2019-11-26 15:13:05 \u001b[32mINFO     \u001b[0m train.py: [27/50], [20/107], step: 2909, 6.888 samples/sec, batch_loss: 0.1487, batch_loss_c: 0.1501, batch_loss_s: 0.1457, time:5.8069, lr:0.001\u001b[0m\n",
            "2019-11-26 15:13:13 \u001b[32mINFO     \u001b[0m train.py: [27/50], [30/107], step: 2919, 5.293 samples/sec, batch_loss: 0.2406, batch_loss_c: 0.2268, batch_loss_s: 0.2728, time:7.5566, lr:0.001\u001b[0m\n",
            "2019-11-26 15:13:19 \u001b[32mINFO     \u001b[0m train.py: [27/50], [40/107], step: 2929, 6.547 samples/sec, batch_loss: 0.0866, batch_loss_c: 0.0781, batch_loss_s: 0.1065, time:6.1095, lr:0.001\u001b[0m\n",
            "2019-11-26 15:13:26 \u001b[32mINFO     \u001b[0m train.py: [27/50], [50/107], step: 2939, 5.159 samples/sec, batch_loss: 0.1150, batch_loss_c: 0.1092, batch_loss_s: 0.1286, time:7.7536, lr:0.001\u001b[0m\n",
            "2019-11-26 15:13:34 \u001b[32mINFO     \u001b[0m train.py: [27/50], [60/107], step: 2949, 5.472 samples/sec, batch_loss: 0.3629, batch_loss_c: 0.3186, batch_loss_s: 0.4662, time:7.3102, lr:0.001\u001b[0m\n",
            "2019-11-26 15:13:41 \u001b[32mINFO     \u001b[0m train.py: [27/50], [70/107], step: 2959, 5.337 samples/sec, batch_loss: 0.3298, batch_loss_c: 0.3173, batch_loss_s: 0.3589, time:7.4951, lr:0.001\u001b[0m\n",
            "2019-11-26 15:13:48 \u001b[32mINFO     \u001b[0m train.py: [27/50], [80/107], step: 2969, 6.388 samples/sec, batch_loss: 0.0825, batch_loss_c: 0.0711, batch_loss_s: 0.1089, time:6.2615, lr:0.001\u001b[0m\n",
            "2019-11-26 15:13:53 \u001b[32mINFO     \u001b[0m train.py: [27/50], [90/107], step: 2979, 7.474 samples/sec, batch_loss: 0.0886, batch_loss_c: 0.0747, batch_loss_s: 0.1210, time:5.3519, lr:0.001\u001b[0m\n",
            "2019-11-26 15:13:58 \u001b[32mINFO     \u001b[0m train.py: [27/50], [100/107], step: 2989, 8.074 samples/sec, batch_loss: 0.1353, batch_loss_c: 0.1365, batch_loss_s: 0.1325, time:4.9540, lr:0.001\u001b[0m\n",
            "2019-11-26 15:14:01 \u001b[32mINFO     \u001b[0m train.py: [27/50], train_loss: 0.1671, time: 78.5227, lr: 0.001\u001b[0m\n",
            "2019-11-26 15:14:13 \u001b[32mINFO     \u001b[0m train.py: [28/50], [0/107], step: 2996, 3.533 samples/sec, batch_loss: 0.1130, batch_loss_c: 0.1117, batch_loss_s: 0.1159, time:11.3213, lr:0.001\u001b[0m\n",
            "2019-11-26 15:14:22 \u001b[32mINFO     \u001b[0m train.py: [28/50], [10/107], step: 3006, 4.251 samples/sec, batch_loss: 0.1181, batch_loss_c: 0.1188, batch_loss_s: 0.1165, time:9.4094, lr:0.001\u001b[0m\n",
            "tcmalloc: large alloc 1337573376 bytes == 0xa4dd4000 @  0x7fed666bc1e7 0x7fed5bd8ff71 0x7fed5bdf355d 0x7fed5bdf6e28 0x7fed5bdf73e5 0x7fed5be8dfc2 0x50abc5 0x50d320 0x5081d5 0x50a020 0x50aa1d 0x50d320 0x5093e5 0x5951c1 0x5a04ce 0x557878 0x50c81e 0x5081d5 0x50a020 0x50aa1d 0x50c549 0x5081d5 0x50a020 0x50aa1d 0x50c549 0x5081d5 0x5895e1 0x5a04ce 0x50d8f5 0x509ce8 0x50aa1d\n",
            "2019-11-26 15:14:31 \u001b[32mINFO     \u001b[0m train.py: [28/50], [20/107], step: 3016, 4.654 samples/sec, batch_loss: 0.2844, batch_loss_c: 0.2665, batch_loss_s: 0.3261, time:8.5949, lr:0.001\u001b[0m\n",
            "2019-11-26 15:14:37 \u001b[32mINFO     \u001b[0m train.py: [28/50], [30/107], step: 3026, 6.235 samples/sec, batch_loss: 0.0812, batch_loss_c: 0.0733, batch_loss_s: 0.0996, time:6.4154, lr:0.001\u001b[0m\n",
            "2019-11-26 15:14:43 \u001b[32mINFO     \u001b[0m train.py: [28/50], [40/107], step: 3036, 7.026 samples/sec, batch_loss: 0.1063, batch_loss_c: 0.0949, batch_loss_s: 0.1329, time:5.6931, lr:0.001\u001b[0m\n",
            "2019-11-26 15:14:53 \u001b[32mINFO     \u001b[0m train.py: [28/50], [50/107], step: 3046, 4.031 samples/sec, batch_loss: 0.1015, batch_loss_c: 0.0943, batch_loss_s: 0.1182, time:9.9231, lr:0.001\u001b[0m\n",
            "2019-11-26 15:15:03 \u001b[32mINFO     \u001b[0m train.py: [28/50], [60/107], step: 3056, 3.927 samples/sec, batch_loss: 0.3117, batch_loss_c: 0.3112, batch_loss_s: 0.3128, time:10.1859, lr:0.001\u001b[0m\n",
            "2019-11-26 15:15:13 \u001b[32mINFO     \u001b[0m train.py: [28/50], [70/107], step: 3066, 4.226 samples/sec, batch_loss: 0.1092, batch_loss_c: 0.0988, batch_loss_s: 0.1336, time:9.4649, lr:0.001\u001b[0m\n",
            "2019-11-26 15:15:19 \u001b[32mINFO     \u001b[0m train.py: [28/50], [80/107], step: 3076, 6.491 samples/sec, batch_loss: 0.3345, batch_loss_c: 0.3251, batch_loss_s: 0.3565, time:6.1628, lr:0.001\u001b[0m\n",
            "2019-11-26 15:15:24 \u001b[32mINFO     \u001b[0m train.py: [28/50], [90/107], step: 3086, 7.607 samples/sec, batch_loss: 0.1157, batch_loss_c: 0.0994, batch_loss_s: 0.1537, time:5.2585, lr:0.001\u001b[0m\n",
            "2019-11-26 15:15:29 \u001b[32mINFO     \u001b[0m train.py: [28/50], [100/107], step: 3096, 8.132 samples/sec, batch_loss: 0.0749, batch_loss_c: 0.0635, batch_loss_s: 0.1014, time:4.9189, lr:0.001\u001b[0m\n",
            "2019-11-26 15:15:32 \u001b[32mINFO     \u001b[0m train.py: [28/50], train_loss: 0.1767, time: 90.6453, lr: 0.001\u001b[0m\n",
            "2019-11-26 15:15:40 \u001b[32mINFO     \u001b[0m train.py: [29/50], [0/107], step: 3103, 5.131 samples/sec, batch_loss: 0.0726, batch_loss_c: 0.0580, batch_loss_s: 0.1068, time:7.7961, lr:0.001\u001b[0m\n",
            "2019-11-26 15:15:52 \u001b[32mINFO     \u001b[0m train.py: [29/50], [10/107], step: 3113, 3.435 samples/sec, batch_loss: 0.0893, batch_loss_c: 0.0808, batch_loss_s: 0.1091, time:11.6448, lr:0.001\u001b[0m\n",
            "2019-11-26 15:15:58 \u001b[32mINFO     \u001b[0m train.py: [29/50], [20/107], step: 3123, 6.264 samples/sec, batch_loss: 0.0822, batch_loss_c: 0.0695, batch_loss_s: 0.1118, time:6.3858, lr:0.001\u001b[0m\n",
            "2019-11-26 15:16:04 \u001b[32mINFO     \u001b[0m train.py: [29/50], [30/107], step: 3133, 6.655 samples/sec, batch_loss: 0.1464, batch_loss_c: 0.1436, batch_loss_s: 0.1530, time:6.0105, lr:0.001\u001b[0m\n",
            "2019-11-26 15:16:11 \u001b[32mINFO     \u001b[0m train.py: [29/50], [40/107], step: 3143, 6.247 samples/sec, batch_loss: 0.2468, batch_loss_c: 0.2250, batch_loss_s: 0.2976, time:6.4026, lr:0.001\u001b[0m\n",
            "2019-11-26 15:16:17 \u001b[32mINFO     \u001b[0m train.py: [29/50], [50/107], step: 3153, 6.449 samples/sec, batch_loss: 0.0945, batch_loss_c: 0.0885, batch_loss_s: 0.1085, time:6.2025, lr:0.001\u001b[0m\n",
            "2019-11-26 15:16:24 \u001b[32mINFO     \u001b[0m train.py: [29/50], [60/107], step: 3163, 5.941 samples/sec, batch_loss: 0.1392, batch_loss_c: 0.1235, batch_loss_s: 0.1760, time:6.7326, lr:0.001\u001b[0m\n",
            "2019-11-26 15:16:31 \u001b[32mINFO     \u001b[0m train.py: [29/50], [70/107], step: 3173, 5.532 samples/sec, batch_loss: 0.3258, batch_loss_c: 0.3235, batch_loss_s: 0.3310, time:7.2311, lr:0.001\u001b[0m\n",
            "2019-11-26 15:16:40 \u001b[32mINFO     \u001b[0m train.py: [29/50], [80/107], step: 3183, 4.621 samples/sec, batch_loss: 0.1799, batch_loss_c: 0.1772, batch_loss_s: 0.1862, time:8.6560, lr:0.001\u001b[0m\n",
            "2019-11-26 15:16:45 \u001b[32mINFO     \u001b[0m train.py: [29/50], [90/107], step: 3193, 7.020 samples/sec, batch_loss: 0.3114, batch_loss_c: 0.2997, batch_loss_s: 0.3387, time:5.6981, lr:0.001\u001b[0m\n",
            "2019-11-26 15:16:50 \u001b[32mINFO     \u001b[0m train.py: [29/50], [100/107], step: 3203, 8.161 samples/sec, batch_loss: 0.1291, batch_loss_c: 0.1001, batch_loss_s: 0.1969, time:4.9015, lr:0.001\u001b[0m\n",
            "2019-11-26 15:16:53 \u001b[32mINFO     \u001b[0m train.py: [29/50], train_loss: 0.1771, time: 80.8902, lr: 0.001\u001b[0m\n",
            "2019-11-26 15:17:01 \u001b[32mINFO     \u001b[0m train.py: [30/50], [0/107], step: 3210, 5.899 samples/sec, batch_loss: 0.2420, batch_loss_c: 0.2358, batch_loss_s: 0.2567, time:6.7803, lr:0.001\u001b[0m\n",
            "tcmalloc: large alloc 1342701568 bytes == 0xa7598000 @  0x7fed666bc1e7 0x7fed5bd8ff71 0x7fed5bdf355d 0x7fed5bdf6e28 0x7fed5bdf73e5 0x7fed5be8dfc2 0x50abc5 0x50d320 0x5081d5 0x50a020 0x50aa1d 0x50d320 0x5093e5 0x5951c1 0x5a04ce 0x557878 0x50c81e 0x5081d5 0x50a020 0x50aa1d 0x50c549 0x5081d5 0x50a020 0x50aa1d 0x50c549 0x5081d5 0x5895e1 0x5a04ce 0x50d8f5 0x509ce8 0x50aa1d\n",
            "2019-11-26 15:17:12 \u001b[32mINFO     \u001b[0m train.py: [30/50], [10/107], step: 3220, 3.596 samples/sec, batch_loss: 0.0934, batch_loss_c: 0.0829, batch_loss_s: 0.1178, time:11.1241, lr:0.001\u001b[0m\n",
            "2019-11-26 15:17:18 \u001b[32mINFO     \u001b[0m train.py: [30/50], [20/107], step: 3230, 6.253 samples/sec, batch_loss: 0.2007, batch_loss_c: 0.1909, batch_loss_s: 0.2235, time:6.3971, lr:0.001\u001b[0m\n",
            "2019-11-26 15:17:24 \u001b[32mINFO     \u001b[0m train.py: [30/50], [30/107], step: 3240, 6.812 samples/sec, batch_loss: 0.0895, batch_loss_c: 0.0837, batch_loss_s: 0.1031, time:5.8722, lr:0.001\u001b[0m\n",
            "2019-11-26 15:17:30 \u001b[32mINFO     \u001b[0m train.py: [30/50], [40/107], step: 3250, 6.438 samples/sec, batch_loss: 0.1043, batch_loss_c: 0.0949, batch_loss_s: 0.1262, time:6.2132, lr:0.001\u001b[0m\n",
            "2019-11-26 15:17:38 \u001b[32mINFO     \u001b[0m train.py: [30/50], [50/107], step: 3260, 5.349 samples/sec, batch_loss: 0.1004, batch_loss_c: 0.0943, batch_loss_s: 0.1148, time:7.4786, lr:0.001\u001b[0m\n",
            "2019-11-26 15:17:47 \u001b[32mINFO     \u001b[0m train.py: [30/50], [60/107], step: 3270, 4.501 samples/sec, batch_loss: 0.1418, batch_loss_c: 0.1281, batch_loss_s: 0.1738, time:8.8863, lr:0.001\u001b[0m\n",
            "2019-11-26 15:17:55 \u001b[32mINFO     \u001b[0m train.py: [30/50], [70/107], step: 3280, 4.778 samples/sec, batch_loss: 0.1579, batch_loss_c: 0.1382, batch_loss_s: 0.2040, time:8.3722, lr:0.001\u001b[0m\n",
            "2019-11-26 15:18:02 \u001b[32mINFO     \u001b[0m train.py: [30/50], [80/107], step: 3290, 5.847 samples/sec, batch_loss: 0.3102, batch_loss_c: 0.3025, batch_loss_s: 0.3280, time:6.8414, lr:0.001\u001b[0m\n",
            "2019-11-26 15:18:07 \u001b[32mINFO     \u001b[0m train.py: [30/50], [90/107], step: 3300, 7.705 samples/sec, batch_loss: 0.3282, batch_loss_c: 0.3204, batch_loss_s: 0.3464, time:5.1913, lr:0.001\u001b[0m\n",
            "2019-11-26 15:18:12 \u001b[32mINFO     \u001b[0m train.py: [30/50], [100/107], step: 3310, 8.119 samples/sec, batch_loss: 0.0970, batch_loss_c: 0.0894, batch_loss_s: 0.1147, time:4.9265, lr:0.001\u001b[0m\n",
            "2019-11-26 15:18:15 \u001b[32mINFO     \u001b[0m train.py: [30/50], train_loss: 0.1737, time: 81.3607, lr: 0.001\u001b[0m\n",
            "2019-11-26 15:18:26 \u001b[32mINFO     \u001b[0m train.py: [31/50], [0/107], step: 3317, 3.952 samples/sec, batch_loss: 0.1859, batch_loss_c: 0.1676, batch_loss_s: 0.2288, time:10.1226, lr:0.001\u001b[0m\n",
            "2019-11-26 15:18:38 \u001b[32mINFO     \u001b[0m train.py: [31/50], [10/107], step: 3327, 3.237 samples/sec, batch_loss: 0.1054, batch_loss_c: 0.0915, batch_loss_s: 0.1378, time:12.3553, lr:0.001\u001b[0m\n",
            "2019-11-26 15:18:46 \u001b[32mINFO     \u001b[0m train.py: [31/50], [20/107], step: 3337, 5.334 samples/sec, batch_loss: 0.8152, batch_loss_c: 0.8112, batch_loss_s: 0.8245, time:7.4993, lr:0.001\u001b[0m\n",
            "2019-11-26 15:18:52 \u001b[32mINFO     \u001b[0m train.py: [31/50], [30/107], step: 3347, 6.135 samples/sec, batch_loss: 0.1358, batch_loss_c: 0.1212, batch_loss_s: 0.1699, time:6.5200, lr:0.001\u001b[0m\n",
            "2019-11-26 15:18:58 \u001b[32mINFO     \u001b[0m train.py: [31/50], [40/107], step: 3357, 6.982 samples/sec, batch_loss: 0.0737, batch_loss_c: 0.0634, batch_loss_s: 0.0977, time:5.7289, lr:0.001\u001b[0m\n",
            "2019-11-26 15:19:05 \u001b[32mINFO     \u001b[0m train.py: [31/50], [50/107], step: 3367, 5.682 samples/sec, batch_loss: 0.0700, batch_loss_c: 0.0573, batch_loss_s: 0.0998, time:7.0393, lr:0.001\u001b[0m\n",
            "2019-11-26 15:19:11 \u001b[32mINFO     \u001b[0m train.py: [31/50], [60/107], step: 3377, 6.112 samples/sec, batch_loss: 0.0827, batch_loss_c: 0.0647, batch_loss_s: 0.1249, time:6.5445, lr:0.001\u001b[0m\n",
            "2019-11-26 15:19:19 \u001b[32mINFO     \u001b[0m train.py: [31/50], [70/107], step: 3387, 5.346 samples/sec, batch_loss: 0.0770, batch_loss_c: 0.0612, batch_loss_s: 0.1138, time:7.4817, lr:0.001\u001b[0m\n",
            "2019-11-26 15:19:26 \u001b[32mINFO     \u001b[0m train.py: [31/50], [80/107], step: 3397, 5.914 samples/sec, batch_loss: 0.3237, batch_loss_c: 0.3070, batch_loss_s: 0.3627, time:6.7634, lr:0.001\u001b[0m\n",
            "2019-11-26 15:19:31 \u001b[32mINFO     \u001b[0m train.py: [31/50], [90/107], step: 3407, 7.381 samples/sec, batch_loss: 0.1242, batch_loss_c: 0.1145, batch_loss_s: 0.1469, time:5.4192, lr:0.001\u001b[0m\n",
            "2019-11-26 15:19:36 \u001b[32mINFO     \u001b[0m train.py: [31/50], [100/107], step: 3417, 8.187 samples/sec, batch_loss: 0.1285, batch_loss_c: 0.0956, batch_loss_s: 0.2053, time:4.8855, lr:0.001\u001b[0m\n",
            "2019-11-26 15:19:39 \u001b[32mINFO     \u001b[0m train.py: [31/50], train_loss: 0.1676, time: 83.5490, lr: 0.001\u001b[0m\n",
            "2019-11-26 15:19:46 \u001b[32mINFO     \u001b[0m train.py: [32/50], [0/107], step: 3424, 6.378 samples/sec, batch_loss: 0.1621, batch_loss_c: 0.1681, batch_loss_s: 0.1480, time:6.2719, lr:0.001\u001b[0m\n",
            "2019-11-26 15:19:56 \u001b[32mINFO     \u001b[0m train.py: [32/50], [10/107], step: 3434, 3.811 samples/sec, batch_loss: 0.1682, batch_loss_c: 0.1689, batch_loss_s: 0.1666, time:10.4953, lr:0.001\u001b[0m\n",
            "2019-11-26 15:20:03 \u001b[32mINFO     \u001b[0m train.py: [32/50], [20/107], step: 3444, 6.144 samples/sec, batch_loss: 0.2701, batch_loss_c: 0.3256, batch_loss_s: 0.1407, time:6.5103, lr:0.001\u001b[0m\n",
            "2019-11-26 15:20:09 \u001b[32mINFO     \u001b[0m train.py: [32/50], [30/107], step: 3454, 6.342 samples/sec, batch_loss: 0.1198, batch_loss_c: 0.1036, batch_loss_s: 0.1576, time:6.3074, lr:0.001\u001b[0m\n",
            "2019-11-26 15:20:17 \u001b[32mINFO     \u001b[0m train.py: [32/50], [40/107], step: 3464, 5.015 samples/sec, batch_loss: 0.1805, batch_loss_c: 0.2003, batch_loss_s: 0.1342, time:7.9763, lr:0.001\u001b[0m\n",
            "2019-11-26 15:20:26 \u001b[32mINFO     \u001b[0m train.py: [32/50], [50/107], step: 3474, 4.553 samples/sec, batch_loss: 0.1263, batch_loss_c: 0.1099, batch_loss_s: 0.1645, time:8.7845, lr:0.001\u001b[0m\n",
            "2019-11-26 15:20:33 \u001b[32mINFO     \u001b[0m train.py: [32/50], [60/107], step: 3484, 5.553 samples/sec, batch_loss: 0.2994, batch_loss_c: 0.2816, batch_loss_s: 0.3408, time:7.2034, lr:0.001\u001b[0m\n",
            "2019-11-26 15:20:39 \u001b[32mINFO     \u001b[0m train.py: [32/50], [70/107], step: 3494, 6.224 samples/sec, batch_loss: 0.1090, batch_loss_c: 0.0974, batch_loss_s: 0.1361, time:6.4265, lr:0.001\u001b[0m\n",
            "2019-11-26 15:20:47 \u001b[32mINFO     \u001b[0m train.py: [32/50], [80/107], step: 3504, 5.669 samples/sec, batch_loss: 0.2471, batch_loss_c: 0.2787, batch_loss_s: 0.1732, time:7.0560, lr:0.001\u001b[0m\n",
            "2019-11-26 15:20:52 \u001b[32mINFO     \u001b[0m train.py: [32/50], [90/107], step: 3514, 7.248 samples/sec, batch_loss: 0.1369, batch_loss_c: 0.1301, batch_loss_s: 0.1527, time:5.5190, lr:0.001\u001b[0m\n",
            "2019-11-26 15:20:57 \u001b[32mINFO     \u001b[0m train.py: [32/50], [100/107], step: 3524, 7.980 samples/sec, batch_loss: 0.2683, batch_loss_c: 0.2441, batch_loss_s: 0.3248, time:5.0125, lr:0.001\u001b[0m\n",
            "2019-11-26 15:21:00 \u001b[32mINFO     \u001b[0m train.py: [32/50], train_loss: 0.1771, time: 80.7399, lr: 0.001\u001b[0m\n",
            "2019-11-26 15:21:06 \u001b[32mINFO     \u001b[0m train.py: [33/50], [0/107], step: 3531, 7.278 samples/sec, batch_loss: 0.1514, batch_loss_c: 0.1448, batch_loss_s: 0.1668, time:5.4958, lr:0.001\u001b[0m\n",
            "2019-11-26 15:21:21 \u001b[32mINFO     \u001b[0m train.py: [33/50], [10/107], step: 3541, 2.595 samples/sec, batch_loss: 0.0690, batch_loss_c: 0.0608, batch_loss_s: 0.0882, time:15.4160, lr:0.001\u001b[0m\n",
            "2019-11-26 15:21:28 \u001b[32mINFO     \u001b[0m train.py: [33/50], [20/107], step: 3551, 5.842 samples/sec, batch_loss: 0.0947, batch_loss_c: 0.0816, batch_loss_s: 0.1252, time:6.8468, lr:0.001\u001b[0m\n",
            "2019-11-26 15:21:34 \u001b[32mINFO     \u001b[0m train.py: [33/50], [30/107], step: 3561, 7.011 samples/sec, batch_loss: 0.0740, batch_loss_c: 0.0650, batch_loss_s: 0.0950, time:5.7052, lr:0.001\u001b[0m\n",
            "2019-11-26 15:21:39 \u001b[32mINFO     \u001b[0m train.py: [33/50], [40/107], step: 3571, 7.440 samples/sec, batch_loss: 0.0827, batch_loss_c: 0.0711, batch_loss_s: 0.1100, time:5.3766, lr:0.001\u001b[0m\n",
            "2019-11-26 15:21:46 \u001b[32mINFO     \u001b[0m train.py: [33/50], [50/107], step: 3581, 6.224 samples/sec, batch_loss: 0.1170, batch_loss_c: 0.1181, batch_loss_s: 0.1146, time:6.4266, lr:0.001\u001b[0m\n",
            "2019-11-26 15:21:53 \u001b[32mINFO     \u001b[0m train.py: [33/50], [60/107], step: 3591, 5.371 samples/sec, batch_loss: 0.3351, batch_loss_c: 0.3254, batch_loss_s: 0.3578, time:7.4476, lr:0.001\u001b[0m\n",
            "2019-11-26 15:22:01 \u001b[32mINFO     \u001b[0m train.py: [33/50], [70/107], step: 3601, 5.018 samples/sec, batch_loss: 0.1201, batch_loss_c: 0.1054, batch_loss_s: 0.1545, time:7.9709, lr:0.001\u001b[0m\n",
            "2019-11-26 15:22:10 \u001b[32mINFO     \u001b[0m train.py: [33/50], [80/107], step: 3611, 4.641 samples/sec, batch_loss: 0.1913, batch_loss_c: 0.1784, batch_loss_s: 0.2213, time:8.6197, lr:0.001\u001b[0m\n",
            "2019-11-26 15:22:15 \u001b[32mINFO     \u001b[0m train.py: [33/50], [90/107], step: 3621, 7.852 samples/sec, batch_loss: 0.1833, batch_loss_c: 0.1785, batch_loss_s: 0.1945, time:5.0944, lr:0.001\u001b[0m\n",
            "2019-11-26 15:22:20 \u001b[32mINFO     \u001b[0m train.py: [33/50], [100/107], step: 3631, 8.239 samples/sec, batch_loss: 0.4506, batch_loss_c: 0.4603, batch_loss_s: 0.4279, time:4.8551, lr:0.001\u001b[0m\n",
            "2019-11-26 15:22:23 \u001b[32mINFO     \u001b[0m train.py: [33/50], train_loss: 0.1571, time: 82.4941, lr: 0.001\u001b[0m\n",
            "2019-11-26 15:22:30 \u001b[32mINFO     \u001b[0m train.py: [34/50], [0/107], step: 3638, 5.955 samples/sec, batch_loss: 0.0826, batch_loss_c: 0.0770, batch_loss_s: 0.0957, time:6.7171, lr:0.001\u001b[0m\n",
            "2019-11-26 15:22:44 \u001b[32mINFO     \u001b[0m train.py: [34/50], [10/107], step: 3648, 2.911 samples/sec, batch_loss: 0.2477, batch_loss_c: 0.2692, batch_loss_s: 0.1973, time:13.7416, lr:0.001\u001b[0m\n",
            "2019-11-26 15:22:51 \u001b[32mINFO     \u001b[0m train.py: [34/50], [20/107], step: 3658, 5.292 samples/sec, batch_loss: 0.1371, batch_loss_c: 0.1247, batch_loss_s: 0.1659, time:7.5588, lr:0.001\u001b[0m\n",
            "2019-11-26 15:22:58 \u001b[32mINFO     \u001b[0m train.py: [34/50], [30/107], step: 3668, 5.853 samples/sec, batch_loss: 0.5004, batch_loss_c: 0.4812, batch_loss_s: 0.5454, time:6.8343, lr:0.001\u001b[0m\n",
            "2019-11-26 15:23:04 \u001b[32mINFO     \u001b[0m train.py: [34/50], [40/107], step: 3678, 6.744 samples/sec, batch_loss: 0.3300, batch_loss_c: 0.3158, batch_loss_s: 0.3632, time:5.9308, lr:0.001\u001b[0m\n",
            "2019-11-26 15:23:10 \u001b[32mINFO     \u001b[0m train.py: [34/50], [50/107], step: 3688, 6.645 samples/sec, batch_loss: 0.3098, batch_loss_c: 0.3035, batch_loss_s: 0.3245, time:6.0200, lr:0.001\u001b[0m\n",
            "tcmalloc: large alloc 1342701568 bytes == 0x9ea30000 @  0x7fed666bc1e7 0x7fed5bd8ff71 0x7fed5bdf355d 0x7fed5bdf6e28 0x7fed5bdf73e5 0x7fed5be8dfc2 0x50abc5 0x50d320 0x5081d5 0x50a020 0x50aa1d 0x50d320 0x5093e5 0x5951c1 0x5a04ce 0x557878 0x50c81e 0x5081d5 0x50a020 0x50aa1d 0x50c549 0x5081d5 0x50a020 0x50aa1d 0x50c549 0x5081d5 0x5895e1 0x5a04ce 0x50d8f5 0x509ce8 0x50aa1d\n",
            "2019-11-26 15:23:17 \u001b[32mINFO     \u001b[0m train.py: [34/50], [60/107], step: 3698, 6.066 samples/sec, batch_loss: 0.1134, batch_loss_c: 0.1059, batch_loss_s: 0.1310, time:6.5937, lr:0.001\u001b[0m\n",
            "2019-11-26 15:23:25 \u001b[32mINFO     \u001b[0m train.py: [34/50], [70/107], step: 3708, 4.687 samples/sec, batch_loss: 0.0765, batch_loss_c: 0.0692, batch_loss_s: 0.0934, time:8.5348, lr:0.001\u001b[0m\n",
            "tcmalloc: large alloc 1167089664 bytes == 0x9ad28000 @  0x7fed666bc1e7 0x7fed5bd8ff71 0x7fed5bdf355d 0x7fed5bdf6e28 0x7fed5bdf73e5 0x7fed5be8dfc2 0x50abc5 0x50d320 0x5081d5 0x50a020 0x50aa1d 0x50d320 0x5093e5 0x5951c1 0x5a04ce 0x557878 0x50c81e 0x5081d5 0x50a020 0x50aa1d 0x50c549 0x5081d5 0x50a020 0x50aa1d 0x50c549 0x5081d5 0x5895e1 0x5a04ce 0x50d8f5 0x509ce8 0x50aa1d\n",
            "2019-11-26 15:23:33 \u001b[32mINFO     \u001b[0m train.py: [34/50], [80/107], step: 3718, 5.192 samples/sec, batch_loss: 0.2799, batch_loss_c: 0.2924, batch_loss_s: 0.2508, time:7.7034, lr:0.001\u001b[0m\n",
            "2019-11-26 15:23:39 \u001b[32mINFO     \u001b[0m train.py: [34/50], [90/107], step: 3728, 6.793 samples/sec, batch_loss: 0.1164, batch_loss_c: 0.1116, batch_loss_s: 0.1276, time:5.8883, lr:0.001\u001b[0m\n",
            "2019-11-26 15:23:44 \u001b[32mINFO     \u001b[0m train.py: [34/50], [100/107], step: 3738, 8.178 samples/sec, batch_loss: 0.0901, batch_loss_c: 0.0719, batch_loss_s: 0.1325, time:4.8910, lr:0.001\u001b[0m\n",
            "2019-11-26 15:23:47 \u001b[32mINFO     \u001b[0m train.py: [34/50], train_loss: 0.1566, time: 83.7323, lr: 0.001\u001b[0m\n",
            "2019-11-26 15:23:55 \u001b[32mINFO     \u001b[0m train.py: [35/50], [0/107], step: 3745, 5.673 samples/sec, batch_loss: 0.1003, batch_loss_c: 0.1078, batch_loss_s: 0.0828, time:7.0507, lr:0.001\u001b[0m\n",
            "2019-11-26 15:24:04 \u001b[32mINFO     \u001b[0m train.py: [35/50], [10/107], step: 3755, 4.038 samples/sec, batch_loss: 0.1373, batch_loss_c: 0.1607, batch_loss_s: 0.0827, time:9.9052, lr:0.001\u001b[0m\n",
            "2019-11-26 15:24:11 \u001b[32mINFO     \u001b[0m train.py: [35/50], [20/107], step: 3765, 5.726 samples/sec, batch_loss: 0.1150, batch_loss_c: 0.1015, batch_loss_s: 0.1466, time:6.9859, lr:0.001\u001b[0m\n",
            "2019-11-26 15:24:18 \u001b[32mINFO     \u001b[0m train.py: [35/50], [30/107], step: 3775, 6.163 samples/sec, batch_loss: 0.1001, batch_loss_c: 0.0950, batch_loss_s: 0.1120, time:6.4907, lr:0.001\u001b[0m\n",
            "2019-11-26 15:24:24 \u001b[32mINFO     \u001b[0m train.py: [35/50], [40/107], step: 3785, 6.468 samples/sec, batch_loss: 0.0809, batch_loss_c: 0.0704, batch_loss_s: 0.1053, time:6.1840, lr:0.001\u001b[0m\n",
            "2019-11-26 15:24:30 \u001b[32mINFO     \u001b[0m train.py: [35/50], [50/107], step: 3795, 6.278 samples/sec, batch_loss: 0.1097, batch_loss_c: 0.1022, batch_loss_s: 0.1273, time:6.3715, lr:0.001\u001b[0m\n",
            "2019-11-26 15:24:39 \u001b[32mINFO     \u001b[0m train.py: [35/50], [60/107], step: 3805, 4.712 samples/sec, batch_loss: 0.0971, batch_loss_c: 0.0868, batch_loss_s: 0.1212, time:8.4887, lr:0.001\u001b[0m\n",
            "2019-11-26 15:24:46 \u001b[32mINFO     \u001b[0m train.py: [35/50], [70/107], step: 3815, 5.329 samples/sec, batch_loss: 0.3668, batch_loss_c: 0.3587, batch_loss_s: 0.3859, time:7.5058, lr:0.001\u001b[0m\n",
            "2019-11-26 15:24:52 \u001b[32mINFO     \u001b[0m train.py: [35/50], [80/107], step: 3825, 6.770 samples/sec, batch_loss: 0.1698, batch_loss_c: 0.1598, batch_loss_s: 0.1931, time:5.9088, lr:0.001\u001b[0m\n",
            "2019-11-26 15:24:58 \u001b[32mINFO     \u001b[0m train.py: [35/50], [90/107], step: 3835, 7.539 samples/sec, batch_loss: 0.1679, batch_loss_c: 0.1818, batch_loss_s: 0.1352, time:5.3056, lr:0.001\u001b[0m\n",
            "2019-11-26 15:25:03 \u001b[32mINFO     \u001b[0m train.py: [35/50], [100/107], step: 3845, 8.131 samples/sec, batch_loss: 0.0659, batch_loss_c: 0.0590, batch_loss_s: 0.0821, time:4.9193, lr:0.001\u001b[0m\n",
            "2019-11-26 15:25:06 \u001b[32mINFO     \u001b[0m train.py: [35/50], train_loss: 0.1611, time: 78.3539, lr: 0.001\u001b[0m\n",
            "2019-11-26 15:25:12 \u001b[32mINFO     \u001b[0m train.py: [36/50], [0/107], step: 3852, 6.656 samples/sec, batch_loss: 0.1640, batch_loss_c: 0.1404, batch_loss_s: 0.2191, time:6.0096, lr:0.001\u001b[0m\n",
            "2019-11-26 15:25:22 \u001b[32mINFO     \u001b[0m train.py: [36/50], [10/107], step: 3862, 3.937 samples/sec, batch_loss: 0.1233, batch_loss_c: 0.1179, batch_loss_s: 0.1360, time:10.1608, lr:0.001\u001b[0m\n",
            "2019-11-26 15:25:29 \u001b[32mINFO     \u001b[0m train.py: [36/50], [20/107], step: 3872, 6.541 samples/sec, batch_loss: 0.2056, batch_loss_c: 0.1767, batch_loss_s: 0.2729, time:6.1151, lr:0.001\u001b[0m\n",
            "2019-11-26 15:25:35 \u001b[32mINFO     \u001b[0m train.py: [36/50], [30/107], step: 3882, 6.561 samples/sec, batch_loss: 0.1164, batch_loss_c: 0.1244, batch_loss_s: 0.0977, time:6.0968, lr:0.001\u001b[0m\n",
            "2019-11-26 15:25:41 \u001b[32mINFO     \u001b[0m train.py: [36/50], [40/107], step: 3892, 6.177 samples/sec, batch_loss: 0.2509, batch_loss_c: 0.2367, batch_loss_s: 0.2841, time:6.4754, lr:0.001\u001b[0m\n",
            "2019-11-26 15:25:48 \u001b[32mINFO     \u001b[0m train.py: [36/50], [50/107], step: 3902, 5.646 samples/sec, batch_loss: 0.2256, batch_loss_c: 0.2104, batch_loss_s: 0.2612, time:7.0849, lr:0.001\u001b[0m\n",
            "2019-11-26 15:25:56 \u001b[32mINFO     \u001b[0m train.py: [36/50], [60/107], step: 3912, 5.164 samples/sec, batch_loss: 0.0729, batch_loss_c: 0.0697, batch_loss_s: 0.0804, time:7.7463, lr:0.001\u001b[0m\n",
            "2019-11-26 15:26:03 \u001b[32mINFO     \u001b[0m train.py: [36/50], [70/107], step: 3922, 5.774 samples/sec, batch_loss: 0.2758, batch_loss_c: 0.3214, batch_loss_s: 0.1693, time:6.9280, lr:0.001\u001b[0m\n",
            "2019-11-26 15:26:09 \u001b[32mINFO     \u001b[0m train.py: [36/50], [80/107], step: 3932, 6.189 samples/sec, batch_loss: 0.4385, batch_loss_c: 0.4366, batch_loss_s: 0.4427, time:6.4632, lr:0.001\u001b[0m\n",
            "2019-11-26 15:26:15 \u001b[32mINFO     \u001b[0m train.py: [36/50], [90/107], step: 3942, 7.047 samples/sec, batch_loss: 0.1115, batch_loss_c: 0.0987, batch_loss_s: 0.1413, time:5.6763, lr:0.001\u001b[0m\n",
            "2019-11-26 15:26:20 \u001b[32mINFO     \u001b[0m train.py: [36/50], [100/107], step: 3952, 8.265 samples/sec, batch_loss: 0.1625, batch_loss_c: 0.1673, batch_loss_s: 0.1513, time:4.8396, lr:0.001\u001b[0m\n",
            "2019-11-26 15:26:23 \u001b[32mINFO     \u001b[0m train.py: [36/50], train_loss: 0.1632, time: 76.7546, lr: 0.001\u001b[0m\n",
            "2019-11-26 15:26:30 \u001b[32mINFO     \u001b[0m train.py: [37/50], [0/107], step: 3959, 6.389 samples/sec, batch_loss: 0.1590, batch_loss_c: 0.1687, batch_loss_s: 0.1365, time:6.2611, lr:0.001\u001b[0m\n",
            "2019-11-26 15:26:41 \u001b[32mINFO     \u001b[0m train.py: [37/50], [10/107], step: 3969, 3.508 samples/sec, batch_loss: 0.1005, batch_loss_c: 0.0892, batch_loss_s: 0.1268, time:11.4015, lr:0.001\u001b[0m\n",
            "2019-11-26 15:26:47 \u001b[32mINFO     \u001b[0m train.py: [37/50], [20/107], step: 3979, 6.490 samples/sec, batch_loss: 0.0927, batch_loss_c: 0.0866, batch_loss_s: 0.1069, time:6.1634, lr:0.001\u001b[0m\n",
            "2019-11-26 15:26:53 \u001b[32mINFO     \u001b[0m train.py: [37/50], [30/107], step: 3989, 6.937 samples/sec, batch_loss: 0.1751, batch_loss_c: 0.2013, batch_loss_s: 0.1138, time:5.7660, lr:0.001\u001b[0m\n",
            "2019-11-26 15:26:59 \u001b[32mINFO     \u001b[0m train.py: [37/50], [40/107], step: 3999, 6.184 samples/sec, batch_loss: 0.1039, batch_loss_c: 0.0964, batch_loss_s: 0.1215, time:6.4683, lr:0.001\u001b[0m\n",
            "2019-11-26 15:27:07 \u001b[32mINFO     \u001b[0m train.py: [37/50], [50/107], step: 4009, 5.340 samples/sec, batch_loss: 0.0887, batch_loss_c: 0.0797, batch_loss_s: 0.1097, time:7.4906, lr:0.001\u001b[0m\n",
            "2019-11-26 15:27:14 \u001b[32mINFO     \u001b[0m train.py: [37/50], [60/107], step: 4019, 6.039 samples/sec, batch_loss: 0.0722, batch_loss_c: 0.0555, batch_loss_s: 0.1112, time:6.6239, lr:0.001\u001b[0m\n",
            "2019-11-26 15:27:21 \u001b[32mINFO     \u001b[0m train.py: [37/50], [70/107], step: 4029, 5.238 samples/sec, batch_loss: 0.3087, batch_loss_c: 0.3040, batch_loss_s: 0.3197, time:7.6362, lr:0.001\u001b[0m\n",
            "tcmalloc: large alloc 1587216384 bytes == 0xab13c000 @  0x7fed666bc1e7 0x7fed5bd8ff71 0x7fed5bdf355d 0x7fed5bdf6e28 0x7fed5bdf73e5 0x7fed5be8dfc2 0x50abc5 0x50d320 0x5081d5 0x50a020 0x50aa1d 0x50d320 0x5093e5 0x5951c1 0x5a04ce 0x557878 0x50c81e 0x5081d5 0x50a020 0x50aa1d 0x50c549 0x5081d5 0x50a020 0x50aa1d 0x50c549 0x5081d5 0x5895e1 0x5a04ce 0x50d8f5 0x509ce8 0x50aa1d\n",
            "2019-11-26 15:27:29 \u001b[32mINFO     \u001b[0m train.py: [37/50], [80/107], step: 4039, 4.869 samples/sec, batch_loss: 0.0914, batch_loss_c: 0.0795, batch_loss_s: 0.1192, time:8.2161, lr:0.001\u001b[0m\n",
            "2019-11-26 15:27:35 \u001b[32mINFO     \u001b[0m train.py: [37/50], [90/107], step: 4049, 6.905 samples/sec, batch_loss: 0.0919, batch_loss_c: 0.0690, batch_loss_s: 0.1454, time:5.7932, lr:0.001\u001b[0m\n",
            "2019-11-26 15:27:40 \u001b[32mINFO     \u001b[0m train.py: [37/50], [100/107], step: 4059, 8.164 samples/sec, batch_loss: 0.1280, batch_loss_c: 0.1221, batch_loss_s: 0.1420, time:4.8994, lr:0.001\u001b[0m\n",
            "2019-11-26 15:27:43 \u001b[32mINFO     \u001b[0m train.py: [37/50], train_loss: 0.1518, time: 79.9570, lr: 0.001\u001b[0m\n",
            "2019-11-26 15:27:50 \u001b[32mINFO     \u001b[0m train.py: [38/50], [0/107], step: 4066, 6.564 samples/sec, batch_loss: 0.0916, batch_loss_c: 0.0609, batch_loss_s: 0.1634, time:6.0943, lr:0.001\u001b[0m\n",
            "2019-11-26 15:28:02 \u001b[32mINFO     \u001b[0m train.py: [38/50], [10/107], step: 4076, 3.211 samples/sec, batch_loss: 0.1117, batch_loss_c: 0.1060, batch_loss_s: 0.1249, time:12.4587, lr:0.001\u001b[0m\n",
            "2019-11-26 15:28:09 \u001b[32mINFO     \u001b[0m train.py: [38/50], [20/107], step: 4086, 6.379 samples/sec, batch_loss: 0.0882, batch_loss_c: 0.0756, batch_loss_s: 0.1176, time:6.2701, lr:0.001\u001b[0m\n",
            "2019-11-26 15:28:17 \u001b[32mINFO     \u001b[0m train.py: [38/50], [30/107], step: 4096, 4.758 samples/sec, batch_loss: 0.1971, batch_loss_c: 0.2184, batch_loss_s: 0.1473, time:8.4068, lr:0.001\u001b[0m\n",
            "2019-11-26 15:28:23 \u001b[32mINFO     \u001b[0m train.py: [38/50], [40/107], step: 4106, 6.183 samples/sec, batch_loss: 0.0951, batch_loss_c: 0.0856, batch_loss_s: 0.1173, time:6.4691, lr:0.001\u001b[0m\n",
            "2019-11-26 15:28:29 \u001b[32mINFO     \u001b[0m train.py: [38/50], [50/107], step: 4116, 7.155 samples/sec, batch_loss: 0.0970, batch_loss_c: 0.0898, batch_loss_s: 0.1138, time:5.5907, lr:0.001\u001b[0m\n",
            "2019-11-26 15:28:35 \u001b[32mINFO     \u001b[0m train.py: [38/50], [60/107], step: 4126, 6.512 samples/sec, batch_loss: 0.1407, batch_loss_c: 0.1335, batch_loss_s: 0.1575, time:6.1422, lr:0.001\u001b[0m\n",
            "2019-11-26 15:28:42 \u001b[32mINFO     \u001b[0m train.py: [38/50], [70/107], step: 4136, 6.164 samples/sec, batch_loss: 0.1332, batch_loss_c: 0.1379, batch_loss_s: 0.1224, time:6.4896, lr:0.001\u001b[0m\n",
            "2019-11-26 15:28:48 \u001b[32mINFO     \u001b[0m train.py: [38/50], [80/107], step: 4146, 6.407 samples/sec, batch_loss: 0.1105, batch_loss_c: 0.1030, batch_loss_s: 0.1281, time:6.2431, lr:0.001\u001b[0m\n",
            "2019-11-26 15:28:53 \u001b[32mINFO     \u001b[0m train.py: [38/50], [90/107], step: 4156, 7.748 samples/sec, batch_loss: 0.2073, batch_loss_c: 0.2032, batch_loss_s: 0.2170, time:5.1627, lr:0.001\u001b[0m\n",
            "2019-11-26 15:28:58 \u001b[32mINFO     \u001b[0m train.py: [38/50], [100/107], step: 4166, 8.108 samples/sec, batch_loss: 0.0990, batch_loss_c: 0.0803, batch_loss_s: 0.1427, time:4.9333, lr:0.001\u001b[0m\n",
            "2019-11-26 15:29:01 \u001b[32mINFO     \u001b[0m train.py: [38/50], train_loss: 0.1584, time: 77.5017, lr: 0.001\u001b[0m\n",
            "2019-11-26 15:29:07 \u001b[32mINFO     \u001b[0m train.py: [39/50], [0/107], step: 4173, 7.230 samples/sec, batch_loss: 0.1924, batch_loss_c: 0.2132, batch_loss_s: 0.1439, time:5.5327, lr:0.001\u001b[0m\n",
            "tcmalloc: large alloc 1460150272 bytes == 0xa5ce0000 @  0x7fed666bc1e7 0x7fed5bd8ff71 0x7fed5bdf355d 0x7fed5bdf6e28 0x7fed5bdf73e5 0x7fed5be8dfc2 0x50abc5 0x50d320 0x5081d5 0x50a020 0x50aa1d 0x50d320 0x5093e5 0x5951c1 0x5a04ce 0x557878 0x50c81e 0x5081d5 0x50a020 0x50aa1d 0x50c549 0x5081d5 0x50a020 0x50aa1d 0x50c549 0x5081d5 0x5895e1 0x5a04ce 0x50d8f5 0x509ce8 0x50aa1d\n",
            "2019-11-26 15:29:23 \u001b[32mINFO     \u001b[0m train.py: [39/50], [10/107], step: 4183, 2.551 samples/sec, batch_loss: 0.3014, batch_loss_c: 0.2928, batch_loss_s: 0.3213, time:15.6805, lr:0.001\u001b[0m\n",
            "2019-11-26 15:29:29 \u001b[32mINFO     \u001b[0m train.py: [39/50], [20/107], step: 4193, 6.494 samples/sec, batch_loss: 0.0769, batch_loss_c: 0.0552, batch_loss_s: 0.1277, time:6.1594, lr:0.001\u001b[0m\n",
            "2019-11-26 15:29:35 \u001b[32mINFO     \u001b[0m train.py: [39/50], [30/107], step: 4203, 6.823 samples/sec, batch_loss: 0.0811, batch_loss_c: 0.0721, batch_loss_s: 0.1021, time:5.8621, lr:0.001\u001b[0m\n",
            "2019-11-26 15:29:41 \u001b[32mINFO     \u001b[0m train.py: [39/50], [40/107], step: 4213, 6.733 samples/sec, batch_loss: 0.0667, batch_loss_c: 0.0570, batch_loss_s: 0.0894, time:5.9409, lr:0.001\u001b[0m\n",
            "2019-11-26 15:29:49 \u001b[32mINFO     \u001b[0m train.py: [39/50], [50/107], step: 4223, 5.118 samples/sec, batch_loss: 0.1178, batch_loss_c: 0.1174, batch_loss_s: 0.1187, time:7.8155, lr:0.001\u001b[0m\n",
            "2019-11-26 15:29:55 \u001b[32mINFO     \u001b[0m train.py: [39/50], [60/107], step: 4233, 6.101 samples/sec, batch_loss: 0.0805, batch_loss_c: 0.0678, batch_loss_s: 0.1104, time:6.5561, lr:0.001\u001b[0m\n",
            "2019-11-26 15:30:02 \u001b[32mINFO     \u001b[0m train.py: [39/50], [70/107], step: 4243, 5.518 samples/sec, batch_loss: 0.4051, batch_loss_c: 0.4141, batch_loss_s: 0.3841, time:7.2489, lr:0.001\u001b[0m\n",
            "2019-11-26 15:30:10 \u001b[32mINFO     \u001b[0m train.py: [39/50], [80/107], step: 4253, 5.092 samples/sec, batch_loss: 0.1948, batch_loss_c: 0.1616, batch_loss_s: 0.2722, time:7.8562, lr:0.001\u001b[0m\n",
            "2019-11-26 15:30:16 \u001b[32mINFO     \u001b[0m train.py: [39/50], [90/107], step: 4263, 6.677 samples/sec, batch_loss: 0.2814, batch_loss_c: 0.2744, batch_loss_s: 0.2976, time:5.9905, lr:0.001\u001b[0m\n",
            "2019-11-26 15:30:21 \u001b[32mINFO     \u001b[0m train.py: [39/50], [100/107], step: 4273, 8.200 samples/sec, batch_loss: 0.1090, batch_loss_c: 0.1037, batch_loss_s: 0.1214, time:4.8778, lr:0.001\u001b[0m\n",
            "2019-11-26 15:30:24 \u001b[32mINFO     \u001b[0m train.py: [39/50], train_loss: 0.1514, time: 82.7716, lr: 0.001\u001b[0m\n",
            "2019-11-26 15:30:33 \u001b[32mINFO     \u001b[0m train.py: [40/50], [0/107], step: 4280, 4.768 samples/sec, batch_loss: 0.1047, batch_loss_c: 0.0939, batch_loss_s: 0.1296, time:8.3889, lr:0.001\u001b[0m\n",
            "2019-11-26 15:30:44 \u001b[32mINFO     \u001b[0m train.py: [40/50], [10/107], step: 4290, 3.610 samples/sec, batch_loss: 0.0983, batch_loss_c: 0.0920, batch_loss_s: 0.1129, time:11.0800, lr:0.001\u001b[0m\n",
            "2019-11-26 15:30:51 \u001b[32mINFO     \u001b[0m train.py: [40/50], [20/107], step: 4300, 5.512 samples/sec, batch_loss: 0.0744, batch_loss_c: 0.0728, batch_loss_s: 0.0781, time:7.2569, lr:0.001\u001b[0m\n",
            "2019-11-26 15:30:58 \u001b[32mINFO     \u001b[0m train.py: [40/50], [30/107], step: 4310, 5.745 samples/sec, batch_loss: 0.1024, batch_loss_c: 0.0994, batch_loss_s: 0.1096, time:6.9621, lr:0.001\u001b[0m\n",
            "2019-11-26 15:31:04 \u001b[32mINFO     \u001b[0m train.py: [40/50], [40/107], step: 4320, 7.085 samples/sec, batch_loss: 0.1421, batch_loss_c: 0.1201, batch_loss_s: 0.1934, time:5.6459, lr:0.001\u001b[0m\n",
            "2019-11-26 15:31:11 \u001b[32mINFO     \u001b[0m train.py: [40/50], [50/107], step: 4330, 5.601 samples/sec, batch_loss: 0.0777, batch_loss_c: 0.0537, batch_loss_s: 0.1338, time:7.1414, lr:0.001\u001b[0m\n",
            "2019-11-26 15:31:19 \u001b[32mINFO     \u001b[0m train.py: [40/50], [60/107], step: 4340, 4.979 samples/sec, batch_loss: 0.0621, batch_loss_c: 0.0513, batch_loss_s: 0.0873, time:8.0335, lr:0.001\u001b[0m\n",
            "2019-11-26 15:31:26 \u001b[32mINFO     \u001b[0m train.py: [40/50], [70/107], step: 4350, 5.930 samples/sec, batch_loss: 0.2074, batch_loss_c: 0.1664, batch_loss_s: 0.3031, time:6.7453, lr:0.001\u001b[0m\n",
            "2019-11-26 15:31:33 \u001b[32mINFO     \u001b[0m train.py: [40/50], [80/107], step: 4360, 5.897 samples/sec, batch_loss: 0.3060, batch_loss_c: 0.3002, batch_loss_s: 0.3194, time:6.7834, lr:0.001\u001b[0m\n",
            "2019-11-26 15:31:38 \u001b[32mINFO     \u001b[0m train.py: [40/50], [90/107], step: 4370, 7.528 samples/sec, batch_loss: 0.1623, batch_loss_c: 0.1805, batch_loss_s: 0.1197, time:5.3132, lr:0.001\u001b[0m\n",
            "2019-11-26 15:31:43 \u001b[32mINFO     \u001b[0m train.py: [40/50], [100/107], step: 4380, 8.240 samples/sec, batch_loss: 0.3527, batch_loss_c: 0.3294, batch_loss_s: 0.4071, time:4.8541, lr:0.001\u001b[0m\n",
            "2019-11-26 15:31:46 \u001b[32mINFO     \u001b[0m train.py: [40/50], train_loss: 0.1520, time: 81.3657, lr: 0.001\u001b[0m\n",
            "2019-11-26 15:31:56 \u001b[32mINFO     \u001b[0m train.py: [41/50], [0/107], step: 4387, 4.252 samples/sec, batch_loss: 0.0873, batch_loss_c: 0.0778, batch_loss_s: 0.1095, time:9.4075, lr:0.001\u001b[0m\n",
            "2019-11-26 15:32:08 \u001b[32mINFO     \u001b[0m train.py: [41/50], [10/107], step: 4397, 3.195 samples/sec, batch_loss: 0.0618, batch_loss_c: 0.0508, batch_loss_s: 0.0875, time:12.5210, lr:0.001\u001b[0m\n",
            "2019-11-26 15:32:14 \u001b[32mINFO     \u001b[0m train.py: [41/50], [20/107], step: 4407, 7.044 samples/sec, batch_loss: 0.1242, batch_loss_c: 0.1071, batch_loss_s: 0.1641, time:5.6782, lr:0.001\u001b[0m\n",
            "2019-11-26 15:32:20 \u001b[32mINFO     \u001b[0m train.py: [41/50], [30/107], step: 4417, 6.696 samples/sec, batch_loss: 0.0969, batch_loss_c: 0.0878, batch_loss_s: 0.1181, time:5.9734, lr:0.001\u001b[0m\n",
            "2019-11-26 15:32:26 \u001b[32mINFO     \u001b[0m train.py: [41/50], [40/107], step: 4427, 7.056 samples/sec, batch_loss: 0.1373, batch_loss_c: 0.1393, batch_loss_s: 0.1327, time:5.6688, lr:0.001\u001b[0m\n",
            "2019-11-26 15:32:32 \u001b[32mINFO     \u001b[0m train.py: [41/50], [50/107], step: 4437, 6.245 samples/sec, batch_loss: 0.1185, batch_loss_c: 0.1111, batch_loss_s: 0.1358, time:6.4047, lr:0.001\u001b[0m\n",
            "2019-11-26 15:32:39 \u001b[32mINFO     \u001b[0m train.py: [41/50], [60/107], step: 4447, 5.746 samples/sec, batch_loss: 0.0670, batch_loss_c: 0.0556, batch_loss_s: 0.0937, time:6.9615, lr:0.001\u001b[0m\n",
            "2019-11-26 15:32:45 \u001b[32mINFO     \u001b[0m train.py: [41/50], [70/107], step: 4457, 6.325 samples/sec, batch_loss: 0.1496, batch_loss_c: 0.1495, batch_loss_s: 0.1501, time:6.3245, lr:0.001\u001b[0m\n",
            "2019-11-26 15:32:52 \u001b[32mINFO     \u001b[0m train.py: [41/50], [80/107], step: 4467, 5.806 samples/sec, batch_loss: 0.3250, batch_loss_c: 0.3186, batch_loss_s: 0.3399, time:6.8894, lr:0.001\u001b[0m\n",
            "2019-11-26 15:32:59 \u001b[32mINFO     \u001b[0m train.py: [41/50], [90/107], step: 4477, 5.795 samples/sec, batch_loss: 0.0936, batch_loss_c: 0.0895, batch_loss_s: 0.1031, time:6.9028, lr:0.001\u001b[0m\n",
            "2019-11-26 15:33:04 \u001b[32mINFO     \u001b[0m train.py: [41/50], [100/107], step: 4487, 7.937 samples/sec, batch_loss: 0.1143, batch_loss_c: 0.1084, batch_loss_s: 0.1282, time:5.0396, lr:0.001\u001b[0m\n",
            "2019-11-26 15:33:07 \u001b[32mINFO     \u001b[0m train.py: [41/50], train_loss: 0.1734, time: 80.9945, lr: 0.001\u001b[0m\n",
            "2019-11-26 15:33:17 \u001b[32mINFO     \u001b[0m train.py: [42/50], [0/107], step: 4494, 4.555 samples/sec, batch_loss: 0.1198, batch_loss_c: 0.1209, batch_loss_s: 0.1174, time:8.7811, lr:0.001\u001b[0m\n",
            "2019-11-26 15:33:29 \u001b[32mINFO     \u001b[0m train.py: [42/50], [10/107], step: 4504, 3.321 samples/sec, batch_loss: 0.1144, batch_loss_c: 0.0967, batch_loss_s: 0.1556, time:12.0459, lr:0.001\u001b[0m\n",
            "2019-11-26 15:33:34 \u001b[32mINFO     \u001b[0m train.py: [42/50], [20/107], step: 4514, 6.941 samples/sec, batch_loss: 0.3839, batch_loss_c: 0.3765, batch_loss_s: 0.4012, time:5.7625, lr:0.001\u001b[0m\n",
            "2019-11-26 15:33:41 \u001b[32mINFO     \u001b[0m train.py: [42/50], [30/107], step: 4524, 5.871 samples/sec, batch_loss: 0.1169, batch_loss_c: 0.0842, batch_loss_s: 0.1933, time:6.8134, lr:0.001\u001b[0m\n",
            "2019-11-26 15:33:47 \u001b[32mINFO     \u001b[0m train.py: [42/50], [40/107], step: 4534, 6.781 samples/sec, batch_loss: 0.0871, batch_loss_c: 0.0820, batch_loss_s: 0.0990, time:5.8984, lr:0.001\u001b[0m\n",
            "2019-11-26 15:33:53 \u001b[32mINFO     \u001b[0m train.py: [42/50], [50/107], step: 4544, 6.729 samples/sec, batch_loss: 0.1083, batch_loss_c: 0.0948, batch_loss_s: 0.1397, time:5.9446, lr:0.001\u001b[0m\n",
            "2019-11-26 15:34:01 \u001b[32mINFO     \u001b[0m train.py: [42/50], [60/107], step: 4554, 5.044 samples/sec, batch_loss: 0.0892, batch_loss_c: 0.0780, batch_loss_s: 0.1154, time:7.9306, lr:0.001\u001b[0m\n",
            "tcmalloc: large alloc 1453907968 bytes == 0xa6a92000 @  0x7fed666bc1e7 0x7fed5bd8ff71 0x7fed5bdf355d 0x7fed5bdf6e28 0x7fed5bdf73e5 0x7fed5be8dfc2 0x50abc5 0x50d320 0x5081d5 0x50a020 0x50aa1d 0x50d320 0x5093e5 0x5951c1 0x5a04ce 0x557878 0x50c81e 0x5081d5 0x50a020 0x50aa1d 0x50c549 0x5081d5 0x50a020 0x50aa1d 0x50c549 0x5081d5 0x5895e1 0x5a04ce 0x50d8f5 0x509ce8 0x50aa1d\n",
            "2019-11-26 15:34:10 \u001b[32mINFO     \u001b[0m train.py: [42/50], [70/107], step: 4564, 4.540 samples/sec, batch_loss: 0.0698, batch_loss_c: 0.0647, batch_loss_s: 0.0816, time:8.8103, lr:0.001\u001b[0m\n",
            "2019-11-26 15:34:18 \u001b[32mINFO     \u001b[0m train.py: [42/50], [80/107], step: 4574, 5.025 samples/sec, batch_loss: 0.2502, batch_loss_c: 0.2781, batch_loss_s: 0.1852, time:7.9596, lr:0.001\u001b[0m\n",
            "2019-11-26 15:34:24 \u001b[32mINFO     \u001b[0m train.py: [42/50], [90/107], step: 4584, 6.710 samples/sec, batch_loss: 0.1184, batch_loss_c: 0.1210, batch_loss_s: 0.1123, time:5.9610, lr:0.001\u001b[0m\n",
            "2019-11-26 15:34:29 \u001b[32mINFO     \u001b[0m train.py: [42/50], [100/107], step: 4594, 8.124 samples/sec, batch_loss: 0.1011, batch_loss_c: 0.0920, batch_loss_s: 0.1225, time:4.9239, lr:0.001\u001b[0m\n",
            "2019-11-26 15:34:32 \u001b[32mINFO     \u001b[0m train.py: [42/50], train_loss: 0.1402, time: 84.0297, lr: 0.001\u001b[0m\n",
            "2019-11-26 15:34:39 \u001b[32mINFO     \u001b[0m train.py: [43/50], [0/107], step: 4601, 5.776 samples/sec, batch_loss: 0.1657, batch_loss_c: 0.1435, batch_loss_s: 0.2175, time:6.9256, lr:0.001\u001b[0m\n",
            "2019-11-26 15:34:50 \u001b[32mINFO     \u001b[0m train.py: [43/50], [10/107], step: 4611, 3.612 samples/sec, batch_loss: 0.0632, batch_loss_c: 0.0534, batch_loss_s: 0.0861, time:11.0755, lr:0.001\u001b[0m\n",
            "2019-11-26 15:34:56 \u001b[32mINFO     \u001b[0m train.py: [43/50], [20/107], step: 4621, 6.750 samples/sec, batch_loss: 0.0758, batch_loss_c: 0.0579, batch_loss_s: 0.1175, time:5.9262, lr:0.001\u001b[0m\n",
            "2019-11-26 15:35:02 \u001b[32mINFO     \u001b[0m train.py: [43/50], [30/107], step: 4631, 6.313 samples/sec, batch_loss: 0.0851, batch_loss_c: 0.0808, batch_loss_s: 0.0951, time:6.3358, lr:0.001\u001b[0m\n",
            "2019-11-26 15:35:10 \u001b[32mINFO     \u001b[0m train.py: [43/50], [40/107], step: 4641, 5.246 samples/sec, batch_loss: 0.0939, batch_loss_c: 0.0894, batch_loss_s: 0.1043, time:7.6246, lr:0.001\u001b[0m\n",
            "2019-11-26 15:35:16 \u001b[32mINFO     \u001b[0m train.py: [43/50], [50/107], step: 4651, 6.268 samples/sec, batch_loss: 0.3797, batch_loss_c: 0.3872, batch_loss_s: 0.3623, time:6.3820, lr:0.001\u001b[0m\n",
            "2019-11-26 15:35:23 \u001b[32mINFO     \u001b[0m train.py: [43/50], [60/107], step: 4661, 6.193 samples/sec, batch_loss: 0.0909, batch_loss_c: 0.0809, batch_loss_s: 0.1141, time:6.4594, lr:0.001\u001b[0m\n",
            "2019-11-26 15:35:31 \u001b[32mINFO     \u001b[0m train.py: [43/50], [70/107], step: 4671, 4.829 samples/sec, batch_loss: 0.1230, batch_loss_c: 0.1092, batch_loss_s: 0.1553, time:8.2824, lr:0.001\u001b[0m\n",
            "2019-11-26 15:35:38 \u001b[32mINFO     \u001b[0m train.py: [43/50], [80/107], step: 4681, 6.157 samples/sec, batch_loss: 0.4332, batch_loss_c: 0.4593, batch_loss_s: 0.3725, time:6.4971, lr:0.001\u001b[0m\n",
            "2019-11-26 15:35:43 \u001b[32mINFO     \u001b[0m train.py: [43/50], [90/107], step: 4691, 7.944 samples/sec, batch_loss: 0.0731, batch_loss_c: 0.0627, batch_loss_s: 0.0976, time:5.0352, lr:0.001\u001b[0m\n",
            "2019-11-26 15:35:48 \u001b[32mINFO     \u001b[0m train.py: [43/50], [100/107], step: 4701, 8.153 samples/sec, batch_loss: 0.0972, batch_loss_c: 0.0909, batch_loss_s: 0.1120, time:4.9060, lr:0.001\u001b[0m\n",
            "2019-11-26 15:35:51 \u001b[32mINFO     \u001b[0m train.py: [43/50], train_loss: 0.1551, time: 78.6361, lr: 0.001\u001b[0m\n",
            "2019-11-26 15:35:58 \u001b[32mINFO     \u001b[0m train.py: [44/50], [0/107], step: 4708, 5.671 samples/sec, batch_loss: 0.3019, batch_loss_c: 0.2912, batch_loss_s: 0.3269, time:7.0537, lr:0.001\u001b[0m\n",
            "2019-11-26 15:36:11 \u001b[32mINFO     \u001b[0m train.py: [44/50], [10/107], step: 4718, 3.221 samples/sec, batch_loss: 0.2255, batch_loss_c: 0.2249, batch_loss_s: 0.2269, time:12.4169, lr:0.001\u001b[0m\n",
            "2019-11-26 15:36:16 \u001b[32mINFO     \u001b[0m train.py: [44/50], [20/107], step: 4728, 7.328 samples/sec, batch_loss: 0.2504, batch_loss_c: 0.2359, batch_loss_s: 0.2841, time:5.4585, lr:0.001\u001b[0m\n",
            "2019-11-26 15:36:22 \u001b[32mINFO     \u001b[0m train.py: [44/50], [30/107], step: 4738, 7.347 samples/sec, batch_loss: 0.4028, batch_loss_c: 0.4081, batch_loss_s: 0.3902, time:5.4447, lr:0.001\u001b[0m\n",
            "2019-11-26 15:36:29 \u001b[32mINFO     \u001b[0m train.py: [44/50], [40/107], step: 4748, 5.604 samples/sec, batch_loss: 0.0887, batch_loss_c: 0.0887, batch_loss_s: 0.0885, time:7.1376, lr:0.001\u001b[0m\n",
            "2019-11-26 15:36:36 \u001b[32mINFO     \u001b[0m train.py: [44/50], [50/107], step: 4758, 5.781 samples/sec, batch_loss: 0.1133, batch_loss_c: 0.0995, batch_loss_s: 0.1456, time:6.9191, lr:0.001\u001b[0m\n",
            "tcmalloc: large alloc 1083916288 bytes == 0x97fee000 @  0x7fed666bc1e7 0x7fed5bd8ff71 0x7fed5bdf355d 0x7fed5bdf6e28 0x7fed5bdf73e5 0x7fed5be8dfc2 0x50abc5 0x50d320 0x5081d5 0x50a020 0x50aa1d 0x50d320 0x5093e5 0x5951c1 0x5a04ce 0x557878 0x50c81e 0x5081d5 0x50a020 0x50aa1d 0x50c549 0x5081d5 0x50a020 0x50aa1d 0x50c549 0x5081d5 0x5895e1 0x5a04ce 0x50d8f5 0x509ce8 0x50aa1d\n",
            "2019-11-26 15:36:42 \u001b[32mINFO     \u001b[0m train.py: [44/50], [60/107], step: 4768, 6.218 samples/sec, batch_loss: 0.1732, batch_loss_c: 0.1924, batch_loss_s: 0.1284, time:6.4333, lr:0.001\u001b[0m\n",
            "2019-11-26 15:36:50 \u001b[32mINFO     \u001b[0m train.py: [44/50], [70/107], step: 4778, 5.149 samples/sec, batch_loss: 0.0621, batch_loss_c: 0.0493, batch_loss_s: 0.0918, time:7.7684, lr:0.001\u001b[0m\n",
            "2019-11-26 15:36:59 \u001b[32mINFO     \u001b[0m train.py: [44/50], [80/107], step: 4788, 4.263 samples/sec, batch_loss: 0.5528, batch_loss_c: 0.5485, batch_loss_s: 0.5626, time:9.3837, lr:0.001\u001b[0m\n",
            "tcmalloc: large alloc 1461092352 bytes == 0xa56bc000 @  0x7fed666bc1e7 0x7fed5bd8ff71 0x7fed5bdf355d 0x7fed5bdf6e28 0x7fed5bdf73e5 0x7fed5be8dfc2 0x50abc5 0x50d320 0x5081d5 0x50a020 0x50aa1d 0x50d320 0x5093e5 0x5951c1 0x5a04ce 0x557878 0x50c81e 0x5081d5 0x50a020 0x50aa1d 0x50c549 0x5081d5 0x50a020 0x50aa1d 0x50c549 0x5081d5 0x5895e1 0x5a04ce 0x50d8f5 0x509ce8 0x50aa1d\n",
            "2019-11-26 15:37:06 \u001b[32mINFO     \u001b[0m train.py: [44/50], [90/107], step: 4798, 6.101 samples/sec, batch_loss: 0.3523, batch_loss_c: 0.3545, batch_loss_s: 0.3473, time:6.5568, lr:0.001\u001b[0m\n",
            "2019-11-26 15:37:11 \u001b[32mINFO     \u001b[0m train.py: [44/50], [100/107], step: 4808, 8.204 samples/sec, batch_loss: 0.3031, batch_loss_c: 0.2949, batch_loss_s: 0.3221, time:4.8756, lr:0.001\u001b[0m\n",
            "2019-11-26 15:37:14 \u001b[32mINFO     \u001b[0m train.py: [44/50], train_loss: 0.1667, time: 82.6485, lr: 0.001\u001b[0m\n",
            "2019-11-26 15:37:20 \u001b[32mINFO     \u001b[0m train.py: [45/50], [0/107], step: 4815, 6.680 samples/sec, batch_loss: 0.2604, batch_loss_c: 0.2503, batch_loss_s: 0.2839, time:5.9882, lr:0.001\u001b[0m\n",
            "2019-11-26 15:37:34 \u001b[32mINFO     \u001b[0m train.py: [45/50], [10/107], step: 4825, 2.851 samples/sec, batch_loss: 0.2958, batch_loss_c: 0.2888, batch_loss_s: 0.3121, time:14.0321, lr:0.001\u001b[0m\n",
            "2019-11-26 15:37:40 \u001b[32mINFO     \u001b[0m train.py: [45/50], [20/107], step: 4835, 6.577 samples/sec, batch_loss: 0.1404, batch_loss_c: 0.1164, batch_loss_s: 0.1964, time:6.0815, lr:0.001\u001b[0m\n",
            "2019-11-26 15:37:46 \u001b[32mINFO     \u001b[0m train.py: [45/50], [30/107], step: 4845, 6.789 samples/sec, batch_loss: 0.0752, batch_loss_c: 0.0664, batch_loss_s: 0.0957, time:5.8915, lr:0.001\u001b[0m\n",
            "2019-11-26 15:37:52 \u001b[32mINFO     \u001b[0m train.py: [45/50], [40/107], step: 4855, 6.740 samples/sec, batch_loss: 0.3549, batch_loss_c: 0.3405, batch_loss_s: 0.3886, time:5.9343, lr:0.001\u001b[0m\n",
            "tcmalloc: large alloc 1342701568 bytes == 0xa2846000 @  0x7fed666bc1e7 0x7fed5bd8ff71 0x7fed5bdf355d 0x7fed5bdf6e28 0x7fed5bdf73e5 0x7fed5be8dfc2 0x50abc5 0x50d320 0x5081d5 0x50a020 0x50aa1d 0x50d320 0x5093e5 0x5951c1 0x5a04ce 0x557878 0x50c81e 0x5081d5 0x50a020 0x50aa1d 0x50c549 0x5081d5 0x50a020 0x50aa1d 0x50c549 0x5081d5 0x5895e1 0x5a04ce 0x50d8f5 0x509ce8 0x50aa1d\n",
            "2019-11-26 15:37:59 \u001b[32mINFO     \u001b[0m train.py: [45/50], [50/107], step: 4865, 5.519 samples/sec, batch_loss: 0.0968, batch_loss_c: 0.0890, batch_loss_s: 0.1152, time:7.2479, lr:0.001\u001b[0m\n",
            "2019-11-26 15:38:08 \u001b[32mINFO     \u001b[0m train.py: [45/50], [60/107], step: 4875, 4.858 samples/sec, batch_loss: 0.2617, batch_loss_c: 0.2952, batch_loss_s: 0.1836, time:8.2336, lr:0.001\u001b[0m\n",
            "2019-11-26 15:38:14 \u001b[32mINFO     \u001b[0m train.py: [45/50], [70/107], step: 4885, 5.948 samples/sec, batch_loss: 0.1610, batch_loss_c: 0.1489, batch_loss_s: 0.1892, time:6.7253, lr:0.001\u001b[0m\n",
            "tcmalloc: large alloc 1083916288 bytes == 0x99ca6000 @  0x7fed666bc1e7 0x7fed5bd8ff71 0x7fed5bdf355d 0x7fed5bdf6e28 0x7fed5bdf73e5 0x7fed5be8dfc2 0x50abc5 0x50d320 0x5081d5 0x50a020 0x50aa1d 0x50d320 0x5093e5 0x5951c1 0x5a04ce 0x557878 0x50c81e 0x5081d5 0x50a020 0x50aa1d 0x50c549 0x5081d5 0x50a020 0x50aa1d 0x50c549 0x5081d5 0x5895e1 0x5a04ce 0x50d8f5 0x509ce8 0x50aa1d\n",
            "2019-11-26 15:38:22 \u001b[32mINFO     \u001b[0m train.py: [45/50], [80/107], step: 4895, 5.421 samples/sec, batch_loss: 0.1371, batch_loss_c: 0.1173, batch_loss_s: 0.1832, time:7.3790, lr:0.001\u001b[0m\n",
            "2019-11-26 15:38:27 \u001b[32mINFO     \u001b[0m train.py: [45/50], [90/107], step: 4905, 8.030 samples/sec, batch_loss: 0.0822, batch_loss_c: 0.0723, batch_loss_s: 0.1051, time:4.9812, lr:0.001\u001b[0m\n",
            "2019-11-26 15:38:31 \u001b[32mINFO     \u001b[0m train.py: [45/50], [100/107], step: 4915, 8.238 samples/sec, batch_loss: 0.3307, batch_loss_c: 0.3269, batch_loss_s: 0.3396, time:4.8556, lr:0.001\u001b[0m\n",
            "2019-11-26 15:38:35 \u001b[32mINFO     \u001b[0m train.py: [45/50], train_loss: 0.1590, time: 80.6067, lr: 0.001\u001b[0m\n",
            "2019-11-26 15:38:44 \u001b[32mINFO     \u001b[0m train.py: [46/50], [0/107], step: 4922, 4.592 samples/sec, batch_loss: 0.0845, batch_loss_c: 0.0720, batch_loss_s: 0.1134, time:8.7110, lr:0.001\u001b[0m\n",
            "2019-11-26 15:38:53 \u001b[32mINFO     \u001b[0m train.py: [46/50], [10/107], step: 4932, 4.282 samples/sec, batch_loss: 0.1205, batch_loss_c: 0.1051, batch_loss_s: 0.1565, time:9.3410, lr:0.001\u001b[0m\n",
            "tcmalloc: large alloc 1636024320 bytes == 0xa6afe000 @  0x7fed666bc1e7 0x7fed5bd8ff71 0x7fed5bdf355d 0x7fed5bdf6e28 0x7fed5bdf73e5 0x7fed5be8dfc2 0x50abc5 0x50d320 0x5081d5 0x50a020 0x50aa1d 0x50d320 0x5093e5 0x5951c1 0x5a04ce 0x557878 0x50c81e 0x5081d5 0x50a020 0x50aa1d 0x50c549 0x5081d5 0x50a020 0x50aa1d 0x50c549 0x5081d5 0x5895e1 0x5a04ce 0x50d8f5 0x509ce8 0x50aa1d\n",
            "2019-11-26 15:39:00 \u001b[32mINFO     \u001b[0m train.py: [46/50], [20/107], step: 4942, 5.659 samples/sec, batch_loss: 0.1056, batch_loss_c: 0.0879, batch_loss_s: 0.1467, time:7.0680, lr:0.001\u001b[0m\n",
            "2019-11-26 15:39:08 \u001b[32mINFO     \u001b[0m train.py: [46/50], [30/107], step: 4952, 5.364 samples/sec, batch_loss: 0.2949, batch_loss_c: 0.2904, batch_loss_s: 0.3056, time:7.4565, lr:0.001\u001b[0m\n",
            "2019-11-26 15:39:16 \u001b[32mINFO     \u001b[0m train.py: [46/50], [40/107], step: 4962, 4.949 samples/sec, batch_loss: 0.4986, batch_loss_c: 0.4664, batch_loss_s: 0.5737, time:8.0817, lr:0.001\u001b[0m\n",
            "2019-11-26 15:39:22 \u001b[32mINFO     \u001b[0m train.py: [46/50], [50/107], step: 4972, 5.872 samples/sec, batch_loss: 0.1549, batch_loss_c: 0.1317, batch_loss_s: 0.2091, time:6.8120, lr:0.001\u001b[0m\n",
            "2019-11-26 15:39:31 \u001b[32mINFO     \u001b[0m train.py: [46/50], [60/107], step: 4982, 4.878 samples/sec, batch_loss: 0.2328, batch_loss_c: 0.1841, batch_loss_s: 0.3464, time:8.2001, lr:0.001\u001b[0m\n",
            "2019-11-26 15:39:38 \u001b[32mINFO     \u001b[0m train.py: [46/50], [70/107], step: 4992, 5.241 samples/sec, batch_loss: 0.1061, batch_loss_c: 0.0937, batch_loss_s: 0.1351, time:7.6319, lr:0.001\u001b[0m\n",
            "2019-11-26 15:39:45 \u001b[32mINFO     \u001b[0m train.py: [46/50], [80/107], step: 5002, 6.473 samples/sec, batch_loss: 0.1543, batch_loss_c: 0.1399, batch_loss_s: 0.1880, time:6.1796, lr:0.001\u001b[0m\n",
            "2019-11-26 15:39:50 \u001b[32mINFO     \u001b[0m train.py: [46/50], [90/107], step: 5012, 7.900 samples/sec, batch_loss: 0.0894, batch_loss_c: 0.0784, batch_loss_s: 0.1149, time:5.0632, lr:0.001\u001b[0m\n",
            "2019-11-26 15:39:54 \u001b[32mINFO     \u001b[0m train.py: [46/50], [100/107], step: 5022, 8.324 samples/sec, batch_loss: 0.3627, batch_loss_c: 0.3576, batch_loss_s: 0.3745, time:4.8056, lr:0.001\u001b[0m\n",
            "2019-11-26 15:39:58 \u001b[32mINFO     \u001b[0m train.py: [46/50], train_loss: 0.1462, time: 82.6095, lr: 0.001\u001b[0m\n",
            "2019-11-26 15:40:04 \u001b[32mINFO     \u001b[0m train.py: [47/50], [0/107], step: 5029, 6.340 samples/sec, batch_loss: 0.2521, batch_loss_c: 0.2365, batch_loss_s: 0.2887, time:6.3089, lr:0.001\u001b[0m\n",
            "2019-11-26 15:40:16 \u001b[32mINFO     \u001b[0m train.py: [47/50], [10/107], step: 5039, 3.514 samples/sec, batch_loss: 0.1370, batch_loss_c: 0.1245, batch_loss_s: 0.1661, time:11.3814, lr:0.001\u001b[0m\n",
            "2019-11-26 15:40:24 \u001b[32mINFO     \u001b[0m train.py: [47/50], [20/107], step: 5049, 5.013 samples/sec, batch_loss: 0.0851, batch_loss_c: 0.0543, batch_loss_s: 0.1568, time:7.9791, lr:0.001\u001b[0m\n",
            "tcmalloc: large alloc 1342701568 bytes == 0x9ea30000 @  0x7fed666bc1e7 0x7fed5bd8ff71 0x7fed5bdf355d 0x7fed5bdf6e28 0x7fed5bdf73e5 0x7fed5be8dfc2 0x50abc5 0x50d320 0x5081d5 0x50a020 0x50aa1d 0x50d320 0x5093e5 0x5951c1 0x5a04ce 0x557878 0x50c81e 0x5081d5 0x50a020 0x50aa1d 0x50c549 0x5081d5 0x50a020 0x50aa1d 0x50c549 0x5081d5 0x5895e1 0x5a04ce 0x50d8f5 0x509ce8 0x50aa1d\n",
            "2019-11-26 15:40:30 \u001b[32mINFO     \u001b[0m train.py: [47/50], [30/107], step: 5059, 6.038 samples/sec, batch_loss: 0.0602, batch_loss_c: 0.0579, batch_loss_s: 0.0656, time:6.6250, lr:0.001\u001b[0m\n",
            "2019-11-26 15:40:36 \u001b[32mINFO     \u001b[0m train.py: [47/50], [40/107], step: 5069, 6.879 samples/sec, batch_loss: 0.1085, batch_loss_c: 0.0993, batch_loss_s: 0.1298, time:5.8148, lr:0.001\u001b[0m\n",
            "2019-11-26 15:40:44 \u001b[32mINFO     \u001b[0m train.py: [47/50], [50/107], step: 5079, 4.860 samples/sec, batch_loss: 0.1021, batch_loss_c: 0.0846, batch_loss_s: 0.1429, time:8.2311, lr:0.001\u001b[0m\n",
            "tcmalloc: large alloc 1083916288 bytes == 0x9d4fa000 @  0x7fed666bc1e7 0x7fed5bd8ff71 0x7fed5bdf355d 0x7fed5bdf6e28 0x7fed5bdf73e5 0x7fed5be8dfc2 0x50abc5 0x50d320 0x5081d5 0x50a020 0x50aa1d 0x50d320 0x5093e5 0x5951c1 0x5a04ce 0x557878 0x50c81e 0x5081d5 0x50a020 0x50aa1d 0x50c549 0x5081d5 0x50a020 0x50aa1d 0x50c549 0x5081d5 0x5895e1 0x5a04ce 0x50d8f5 0x509ce8 0x50aa1d\n",
            "2019-11-26 15:40:52 \u001b[32mINFO     \u001b[0m train.py: [47/50], [60/107], step: 5089, 4.930 samples/sec, batch_loss: 0.1146, batch_loss_c: 0.1140, batch_loss_s: 0.1161, time:8.1128, lr:0.001\u001b[0m\n",
            "2019-11-26 15:41:00 \u001b[32mINFO     \u001b[0m train.py: [47/50], [70/107], step: 5099, 5.022 samples/sec, batch_loss: 0.0807, batch_loss_c: 0.0782, batch_loss_s: 0.0864, time:7.9656, lr:0.001\u001b[0m\n",
            "2019-11-26 15:41:06 \u001b[32mINFO     \u001b[0m train.py: [47/50], [80/107], step: 5109, 6.954 samples/sec, batch_loss: 0.0701, batch_loss_c: 0.0632, batch_loss_s: 0.0861, time:5.7524, lr:0.001\u001b[0m\n",
            "2019-11-26 15:41:12 \u001b[32mINFO     \u001b[0m train.py: [47/50], [90/107], step: 5119, 6.871 samples/sec, batch_loss: 0.1660, batch_loss_c: 0.1460, batch_loss_s: 0.2125, time:5.8218, lr:0.001\u001b[0m\n",
            "2019-11-26 15:41:17 \u001b[32mINFO     \u001b[0m train.py: [47/50], [100/107], step: 5129, 8.106 samples/sec, batch_loss: 0.1009, batch_loss_c: 0.0861, batch_loss_s: 0.1355, time:4.9348, lr:0.001\u001b[0m\n",
            "2019-11-26 15:41:20 \u001b[32mINFO     \u001b[0m train.py: [47/50], train_loss: 0.1521, time: 82.1568, lr: 0.001\u001b[0m\n",
            "2019-11-26 15:41:30 \u001b[32mINFO     \u001b[0m train.py: [48/50], [0/107], step: 5136, 4.243 samples/sec, batch_loss: 0.0823, batch_loss_c: 0.0766, batch_loss_s: 0.0956, time:9.4279, lr:0.001\u001b[0m\n",
            "2019-11-26 15:41:41 \u001b[32mINFO     \u001b[0m train.py: [48/50], [10/107], step: 5146, 3.757 samples/sec, batch_loss: 0.1097, batch_loss_c: 0.1052, batch_loss_s: 0.1202, time:10.6476, lr:0.001\u001b[0m\n",
            "2019-11-26 15:41:47 \u001b[32mINFO     \u001b[0m train.py: [48/50], [20/107], step: 5156, 6.521 samples/sec, batch_loss: 0.0866, batch_loss_c: 0.0742, batch_loss_s: 0.1154, time:6.1338, lr:0.001\u001b[0m\n",
            "2019-11-26 15:41:53 \u001b[32mINFO     \u001b[0m train.py: [48/50], [30/107], step: 5166, 6.378 samples/sec, batch_loss: 0.0915, batch_loss_c: 0.0915, batch_loss_s: 0.0916, time:6.2711, lr:0.001\u001b[0m\n",
            "2019-11-26 15:41:59 \u001b[32mINFO     \u001b[0m train.py: [48/50], [40/107], step: 5176, 7.069 samples/sec, batch_loss: 0.2737, batch_loss_c: 0.2501, batch_loss_s: 0.3288, time:5.6582, lr:0.001\u001b[0m\n",
            "2019-11-26 15:42:08 \u001b[32mINFO     \u001b[0m train.py: [48/50], [50/107], step: 5186, 4.191 samples/sec, batch_loss: 0.0789, batch_loss_c: 0.0671, batch_loss_s: 0.1064, time:9.5449, lr:0.001\u001b[0m\n",
            "tcmalloc: large alloc 1432739840 bytes == 0xa7404000 @  0x7fed666bc1e7 0x7fed5bd8ff71 0x7fed5bdf355d 0x7fed5bdf6e28 0x7fed5bdf73e5 0x7fed5be8dfc2 0x50abc5 0x50d320 0x5081d5 0x50a020 0x50aa1d 0x50d320 0x5093e5 0x5951c1 0x5a04ce 0x557878 0x50c81e 0x5081d5 0x50a020 0x50aa1d 0x50c549 0x5081d5 0x50a020 0x50aa1d 0x50c549 0x5081d5 0x5895e1 0x5a04ce 0x50d8f5 0x509ce8 0x50aa1d\n",
            "2019-11-26 15:42:16 \u001b[32mINFO     \u001b[0m train.py: [48/50], [60/107], step: 5196, 5.323 samples/sec, batch_loss: 0.1282, batch_loss_c: 0.1315, batch_loss_s: 0.1206, time:7.5151, lr:0.001\u001b[0m\n",
            "2019-11-26 15:42:24 \u001b[32mINFO     \u001b[0m train.py: [48/50], [70/107], step: 5206, 4.897 samples/sec, batch_loss: 0.3212, batch_loss_c: 0.3136, batch_loss_s: 0.3388, time:8.1688, lr:0.001\u001b[0m\n",
            "2019-11-26 15:42:33 \u001b[32mINFO     \u001b[0m train.py: [48/50], [80/107], step: 5216, 4.629 samples/sec, batch_loss: 0.3832, batch_loss_c: 0.3432, batch_loss_s: 0.4765, time:8.6412, lr:0.001\u001b[0m\n",
            "2019-11-26 15:42:38 \u001b[32mINFO     \u001b[0m train.py: [48/50], [90/107], step: 5226, 7.329 samples/sec, batch_loss: 0.3426, batch_loss_c: 0.3374, batch_loss_s: 0.3546, time:5.4580, lr:0.001\u001b[0m\n",
            "2019-11-26 15:42:43 \u001b[32mINFO     \u001b[0m train.py: [48/50], [100/107], step: 5236, 8.343 samples/sec, batch_loss: 0.0721, batch_loss_c: 0.0578, batch_loss_s: 0.1052, time:4.7945, lr:0.001\u001b[0m\n",
            "2019-11-26 15:42:46 \u001b[32mINFO     \u001b[0m train.py: [48/50], train_loss: 0.1559, time: 85.4584, lr: 0.001\u001b[0m\n",
            "2019-11-26 15:42:54 \u001b[32mINFO     \u001b[0m train.py: [49/50], [0/107], step: 5243, 5.436 samples/sec, batch_loss: 0.0850, batch_loss_c: 0.0836, batch_loss_s: 0.0882, time:7.3587, lr:0.001\u001b[0m\n",
            "2019-11-26 15:43:06 \u001b[32mINFO     \u001b[0m train.py: [49/50], [10/107], step: 5253, 3.146 samples/sec, batch_loss: 0.0980, batch_loss_c: 0.0844, batch_loss_s: 0.1297, time:12.7158, lr:0.001\u001b[0m\n",
            "2019-11-26 15:43:13 \u001b[32mINFO     \u001b[0m train.py: [49/50], [20/107], step: 5263, 5.671 samples/sec, batch_loss: 0.1347, batch_loss_c: 0.1308, batch_loss_s: 0.1438, time:7.0540, lr:0.001\u001b[0m\n",
            "2019-11-26 15:43:19 \u001b[32mINFO     \u001b[0m train.py: [49/50], [30/107], step: 5273, 6.865 samples/sec, batch_loss: 0.0879, batch_loss_c: 0.0732, batch_loss_s: 0.1221, time:5.8265, lr:0.001\u001b[0m\n",
            "2019-11-26 15:43:25 \u001b[32mINFO     \u001b[0m train.py: [49/50], [40/107], step: 5283, 7.267 samples/sec, batch_loss: 0.2900, batch_loss_c: 0.2744, batch_loss_s: 0.3265, time:5.5046, lr:0.001\u001b[0m\n",
            "2019-11-26 15:43:32 \u001b[32mINFO     \u001b[0m train.py: [49/50], [50/107], step: 5293, 5.744 samples/sec, batch_loss: 0.0906, batch_loss_c: 0.0904, batch_loss_s: 0.0909, time:6.9634, lr:0.001\u001b[0m\n",
            "2019-11-26 15:43:39 \u001b[32mINFO     \u001b[0m train.py: [49/50], [60/107], step: 5303, 5.310 samples/sec, batch_loss: 0.1230, batch_loss_c: 0.1177, batch_loss_s: 0.1356, time:7.5332, lr:0.001\u001b[0m\n",
            "tcmalloc: large alloc 1295515648 bytes == 0x9f814000 @  0x7fed666bc1e7 0x7fed5bd8ff71 0x7fed5bdf355d 0x7fed5bdf6e28 0x7fed5bdf73e5 0x7fed5be8dfc2 0x50abc5 0x50d320 0x5081d5 0x50a020 0x50aa1d 0x50d320 0x5093e5 0x5951c1 0x5a04ce 0x557878 0x50c81e 0x5081d5 0x50a020 0x50aa1d 0x50c549 0x5081d5 0x50a020 0x50aa1d 0x50c549 0x5081d5 0x5895e1 0x5a04ce 0x50d8f5 0x509ce8 0x50aa1d\n",
            "2019-11-26 15:43:49 \u001b[32mINFO     \u001b[0m train.py: [49/50], [70/107], step: 5313, 4.335 samples/sec, batch_loss: 0.0557, batch_loss_c: 0.0508, batch_loss_s: 0.0673, time:9.2267, lr:0.001\u001b[0m\n",
            "2019-11-26 15:43:56 \u001b[32mINFO     \u001b[0m train.py: [49/50], [80/107], step: 5323, 5.554 samples/sec, batch_loss: 0.2213, batch_loss_c: 0.1866, batch_loss_s: 0.3024, time:7.2017, lr:0.001\u001b[0m\n",
            "2019-11-26 15:44:02 \u001b[32mINFO     \u001b[0m train.py: [49/50], [90/107], step: 5333, 6.740 samples/sec, batch_loss: 0.0836, batch_loss_c: 0.0636, batch_loss_s: 0.1302, time:5.9347, lr:0.001\u001b[0m\n",
            "2019-11-26 15:44:07 \u001b[32mINFO     \u001b[0m train.py: [49/50], [100/107], step: 5343, 8.297 samples/sec, batch_loss: 0.0654, batch_loss_c: 0.0556, batch_loss_s: 0.0884, time:4.8212, lr:0.001\u001b[0m\n",
            "2019-11-26 15:44:10 \u001b[32mINFO     \u001b[0m train.py: [49/50], train_loss: 0.1551, time: 83.2983, lr: 0.001\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P47Jx7_DPeO7",
        "colab_type": "text"
      },
      "source": [
        "[The IIIT Scene Text Retrieval (STR) Dataset](https://cvit.iiit.ac.in/research/projects/cvit-projects/the-iiit-scene-text-retrieval-str-dataset)"
      ]
    }
  ]
}