{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PSENet_trial_run.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kavyajeetbora/PSENet.pytorch/blob/master/PSENet_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UByxwp0F3QUp",
        "colab_type": "text"
      },
      "source": [
        "## Installing softwares and other dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GSEHnmCxK_o9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, Dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y99kqKlHoYkz",
        "colab_type": "code",
        "outputId": "99be9c31-f64d-439c-b53d-d8f402d782b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        }
      },
      "source": [
        "!pip install pyclipper"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyclipper\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e1/40/57a0d54a1c696d58253c88a95677e50ab2b305a15af0ac64b70db4320562/pyclipper-1.1.0.post3-cp36-cp36m-manylinux1_x86_64.whl (131kB)\n",
            "\r\u001b[K     |██▌                             | 10kB 22.4MB/s eta 0:00:01\r\u001b[K     |█████                           | 20kB 2.1MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 30kB 3.1MB/s eta 0:00:01\r\u001b[K     |██████████                      | 40kB 1.9MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 51kB 2.3MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 61kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 71kB 3.2MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 81kB 3.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 92kB 4.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 102kB 3.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 112kB 3.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 122kB 3.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 133kB 3.2MB/s \n",
            "\u001b[?25hInstalling collected packages: pyclipper\n",
            "Successfully installed pyclipper-1.1.0.post3\n",
            "Requirement already satisfied: pyclipper in /usr/local/lib/python3.6/dist-packages (1.1.0.post3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ncoynOlfnIW1",
        "colab_type": "code",
        "outputId": "6913e237-68a5-4c42-ab63-a7e40cace3c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hhJ-6yyXcnRx",
        "colab_type": "code",
        "outputId": "387ec6ed-5fd5-458a-bbd9-2192ff490c3f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478
        }
      },
      "source": [
        "import shutil, os\n",
        "os.chdir('/content')\n",
        "directory = '/content/cloned-repo'\n",
        "if os.path.exists(directory):\n",
        "  shutil.rmtree(directory)\n",
        "\n",
        "!git clone https://github.com/kavyajeetbora/PSENet.pytorch.git /content/cloned-repo\n",
        "print(\"Cloned the repository\")\n",
        "os.chdir('/content/cloned-repo')\n",
        "!ls"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into '/content/cloned-repo'...\n",
            "remote: Enumerating objects: 42, done.\u001b[K\n",
            "remote: Counting objects:   2% (1/42)\u001b[K\rremote: Counting objects:   4% (2/42)\u001b[K\rremote: Counting objects:   7% (3/42)\u001b[K\rremote: Counting objects:   9% (4/42)\u001b[K\rremote: Counting objects:  11% (5/42)\u001b[K\rremote: Counting objects:  14% (6/42)\u001b[K\rremote: Counting objects:  16% (7/42)\u001b[K\rremote: Counting objects:  19% (8/42)\u001b[K\rremote: Counting objects:  21% (9/42)\u001b[K\rremote: Counting objects:  23% (10/42)\u001b[K\rremote: Counting objects:  26% (11/42)\u001b[K\rremote: Counting objects:  28% (12/42)\u001b[K\rremote: Counting objects:  30% (13/42)\u001b[K\rremote: Counting objects:  33% (14/42)\u001b[K\rremote: Counting objects:  35% (15/42)\u001b[K\rremote: Counting objects:  38% (16/42)\u001b[K\rremote: Counting objects:  40% (17/42)\u001b[K\rremote: Counting objects:  42% (18/42)\u001b[K\rremote: Counting objects:  45% (19/42)\u001b[K\rremote: Counting objects:  47% (20/42)\u001b[K\rremote: Counting objects:  50% (21/42)\u001b[K\rremote: Counting objects:  52% (22/42)\u001b[K\rremote: Counting objects:  54% (23/42)\u001b[K\rremote: Counting objects:  57% (24/42)\u001b[K\rremote: Counting objects:  59% (25/42)\u001b[K\rremote: Counting objects:  61% (26/42)\u001b[K\rremote: Counting objects:  64% (27/42)\u001b[K\rremote: Counting objects:  66% (28/42)\u001b[K\rremote: Counting objects:  69% (29/42)\u001b[K\rremote: Counting objects:  71% (30/42)\u001b[K\rremote: Counting objects:  73% (31/42)\u001b[K\rremote: Counting objects:  76% (32/42)\u001b[K\rremote: Counting objects:  78% (33/42)\u001b[K\rremote: Counting objects:  80% (34/42)\u001b[K\rremote: Counting objects:  83% (35/42)\u001b[K\rremote: Counting objects:  85% (36/42)\u001b[K\rremote: Counting objects:  88% (37/42)\u001b[K\rremote: Counting objects:  90% (38/42)\u001b[K\rremote: Counting objects:  92% (39/42)\u001b[K\rremote: Counting objects:  95% (40/42)\u001b[K\rremote: Counting objects:  97% (41/42)\u001b[K\rremote: Counting objects: 100% (42/42)\u001b[K\rremote: Counting objects: 100% (42/42), done.\u001b[K\n",
            "remote: Compressing objects: 100% (42/42), done.\u001b[K\n",
            "remote: Total 605 (delta 20), reused 0 (delta 0), pack-reused 563\u001b[K\n",
            "Receiving objects: 100% (605/605), 33.34 MiB | 24.42 MiB/s, done.\n",
            "Resolving deltas: 100% (317/317), done.\n",
            "Cloned the repository\n",
            "cal_recall  install_dependencies.sh  PSENet.ipynb\t     train.py\n",
            "config.py   LICENSE\t\t     PSENet_predict.ipynb    utils\n",
            "dataset     models\t\t     PSENet_training.ipynb\n",
            "eval.py     predict.py\t\t     PSENet_trial_run.ipynb\n",
            "imgs\t    pse\t\t\t     README.md\n",
            "Cloning into '/content/cloned-repo'...\n",
            "remote: Enumerating objects: 42, done.\u001b[K\n",
            "remote: Counting objects: 100% (42/42), done.\u001b[K\n",
            "remote: Compressing objects: 100% (42/42), done.\u001b[K\n",
            "remote: Total 605 (delta 20), reused 0 (delta 0), pack-reused 563\u001b[K\n",
            "Receiving objects: 100% (605/605), 33.34 MiB | 23.81 MiB/s, done.\n",
            "Resolving deltas: 100% (317/317), done.\n",
            "Cloned the repository\n",
            "cal_recall  install_dependencies.sh  PSENet.ipynb\t     train.py\n",
            "config.py   LICENSE\t\t     PSENet_predict.ipynb    utils\n",
            "dataset     models\t\t     PSENet_training.ipynb\n",
            "eval.py     predict.py\t\t     PSENet_trial_run.ipynb\n",
            "imgs\t    pse\t\t\t     README.md\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PrOmfo6_3X2I",
        "colab_type": "text"
      },
      "source": [
        "## Extracting the data and setting up the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dGLMxTujlm0w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## unzipping the files\n",
        "from zipfile import ZipFile\n",
        "\n",
        "def unzip_files(file,output_dir):\n",
        "  with ZipFile(file, 'r') as zipObj:\n",
        "    # Extract all the contents of zip file in current directory\n",
        "    zipObj.extractall(output_dir)\n",
        "  print('Extracted to',output_dir)\n",
        "\n",
        "def make_directory(directory):\n",
        "  if os.path.isdir(directory):\n",
        "    shutil.rmtree(directory)\n",
        "  \n",
        "  os.mkdir(directory)\n",
        "  print('Created a new directory')\n",
        "\n",
        "training_data_zip = '/content/drive/My Drive/Colab Notebooks/padh.ai.notebooks/15. Object Detection/Scene Text Detection Dataset/English and Hindi MLT 2019.zip'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FqJ6xb5u2dgg",
        "colab_type": "code",
        "outputId": "3bf4121d-365d-4604-e27b-26932dbbd529",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "# make directories\n",
        "make_directory('Training Set')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Created a new directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FFSaSXr-m-K4",
        "colab_type": "code",
        "outputId": "0c523a18-7b98-485e-bc64-92e8dfe9477e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "%%time\n",
        "unzip_files(training_data_zip,'Training Set')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracted to Training Set\n",
            "CPU times: user 2.94 s, sys: 1.18 s, total: 4.12 s\n",
            "Wall time: 11.7 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YRSeyluknVH4",
        "colab_type": "code",
        "outputId": "4d6014ea-b76f-43c3-a5b7-9eed4f8a1bb8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "print(len(os.listdir('Training Set/Images')))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MOp4JsZ6nXvn",
        "colab_type": "code",
        "outputId": "41c02adc-5048-47ca-d6d3-964ba2ee8031",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "print(len(os.listdir('Training Set/Annotations')))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mhvg89mXn8zG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from dataset.data_utils import *\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader, Dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4JeHTz3G6Lyj",
        "colab_type": "code",
        "outputId": "560f5c67-c139-4240-ee70-85d20edcfe3e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "train_data = MyDataset('Training Set',transform=transforms.ToTensor())\n",
        "len(train_data)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1938"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "odLKMn-5SBAz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device(\"cuda:0\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9a9PsyF3dqo",
        "colab_type": "text"
      },
      "source": [
        "## Training "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9x221Ya0RBGO",
        "outputId": "e0e04c55-3c8a-4f7b-8628-949d97cf5052",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "source": [
        "!chmod +x install_dependencies.sh # make shell script executable\n",
        "!./install_dependencies.sh # run the shell script"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyclipper in /usr/local/lib/python3.6/dist-packages (1.1.0.post3)\n",
            "Collecting Polygon3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/73/a0/d07a4f3e80ed7020a33f3111db217f54ac44a485ff45da3c21ce49f65041/Polygon3-3.0.8.tar.gz (71kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 2.7MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: Polygon3\n",
            "  Building wheel for Polygon3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for Polygon3: filename=Polygon3-3.0.8-cp36-cp36m-linux_x86_64.whl size=101493 sha256=2be239a5ec9f3a0a477d9831a00430a7817554075fb8a79efa160425d753c1a4\n",
            "  Stored in directory: /root/.cache/pip/wheels/95/32/f1/5525b233996d9d99cbce2f0a8da60d137ddddc555d3e8b0e2a\n",
            "Successfully built Polygon3\n",
            "Installing collected packages: Polygon3\n",
            "Successfully installed Polygon3-3.0.8\n",
            "Collecting colorlog\n",
            "  Downloading https://files.pythonhosted.org/packages/68/4d/892728b0c14547224f0ac40884e722a3d00cb54e7a146aea0b3186806c9e/colorlog-4.0.2-py2.py3-none-any.whl\n",
            "Installing collected packages: colorlog\n",
            "Successfully installed colorlog-4.0.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xD4GuTKC3wXY",
        "colab_type": "code",
        "outputId": "ff81e28d-b38b-4459-eb57-dce56fa55bf1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!python3 train.py"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "make: Entering directory '/content/cloned-repo/pse'\n",
            "make: 'pse.so' is up to date.\n",
            "make: Leaving directory '/content/cloned-repo/pse'\n",
            "2019-12-19 06:23:54 \u001b[32mINFO     \u001b[0m utils.py: logger init finished\u001b[0m\n",
            "2019-12-19 06:23:55 \u001b[32mINFO     \u001b[0m train.py: {'Lambda': 0.7,\n",
            " 'OHEM_ratio': 3,\n",
            " 'backbone': 'resnet50',\n",
            " 'checkpoint': '',\n",
            " 'data_shape': 640,\n",
            " 'display_input_images': False,\n",
            " 'display_interval': 10,\n",
            " 'display_output_images': False,\n",
            " 'end_lr': 1e-06,\n",
            " 'epochs': 300,\n",
            " 'gpu_id': '0',\n",
            " 'lr': 1e-06,\n",
            " 'lr_decay_step': [100, 200],\n",
            " 'lr_gamma': 0.1,\n",
            " 'm': 0.5,\n",
            " 'n': 6,\n",
            " 'output_dir': '/content/drive/My Drive/PSENet_2',\n",
            " 'pretrained': False,\n",
            " 'pretrained_path': '/content/drive/My Drive/PSENet_2/PSENet_resnet50.pth',\n",
            " 'restart_training': False,\n",
            " 'scale': 1,\n",
            " 'seed': 2,\n",
            " 'show_images_interval': 50,\n",
            " 'start_epoch': 0,\n",
            " 'testroot': 'Test Set',\n",
            " 'train_batch_size': 4,\n",
            " 'trainroot': 'Training Set',\n",
            " 'warm_up_epoch': 6,\n",
            " 'warm_up_lr': 1e-07,\n",
            " 'weight_decay': 0.0005,\n",
            " 'workers': 0}\u001b[0m\n",
            "2019-12-19 06:23:55 \u001b[32mINFO     \u001b[0m train.py: train with gpu 0 and pytorch 1.3.1\u001b[0m\n",
            "2019-12-19 06:24:05 \u001b[32mINFO     \u001b[0m train.py: train dataset has 1938 samples,484 in dataloader\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:100: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule.See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "2019-12-19 06:24:29 \u001b[32mINFO     \u001b[0m train.py: [0/300], [0/484], step: 0, 1.727 samples/sec, batch_loss: 0.3060, batch_loss_c: 0.2984, batch_loss_s: 0.3238, time:23.1646, lr:1e-06\u001b[0m\n",
            "tcmalloc: large alloc 1725898752 bytes == 0x8dce8000 @  0x7efcc32221e7 0x7efcb7806f71 0x7efcb786a55d 0x7efcb786de28 0x7efcb786e3e5 0x7efcb7904fc2 0x50ac25 0x50d390 0x508245 0x50a080 0x50aa7d 0x50d390 0x509455 0x595311 0x5a067e 0x557d28 0x50c88b 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x509455 0x595311 0x5a522c 0x557f7e 0x4b5eff 0x50c810\n",
            "2019-12-19 06:24:53 \u001b[32mINFO     \u001b[0m train.py: [0/300], [10/484], step: 10, 1.659 samples/sec, batch_loss: 0.0500, batch_loss_c: 0.0366, batch_loss_s: 0.0813, time:24.1080, lr:1e-06\u001b[0m\n",
            "tcmalloc: large alloc 3437101056 bytes == 0xeeec6000 @  0x7efcc32221e7 0x7efcb7806f71 0x7efcb786a55d 0x7efcb786de28 0x7efcb786e3e5 0x7efcb7904fc2 0x50ac25 0x50d390 0x508245 0x50a080 0x50aa7d 0x50d390 0x509455 0x595311 0x5a067e 0x557d28 0x50c88b 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x509455 0x595311 0x5a522c 0x557f7e 0x4b5eff 0x50c810\n",
            "tcmalloc: large alloc 1848999936 bytes == 0x941ce000 @  0x7efcc32221e7 0x7efcb7806f71 0x7efcb786a55d 0x7efcb786de28 0x7efcb786e3e5 0x7efcb7904fc2 0x50ac25 0x50d390 0x508245 0x50a080 0x50aa7d 0x50d390 0x509455 0x595311 0x5a067e 0x557d28 0x50c88b 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x509455 0x595311 0x5a522c 0x557f7e 0x4b5eff 0x50c810\n",
            "2019-12-19 06:25:40 \u001b[32mINFO     \u001b[0m train.py: [0/300], [20/484], step: 20, 0.840 samples/sec, batch_loss: 0.1164, batch_loss_c: 0.0972, batch_loss_s: 0.1614, time:47.6176, lr:1e-06\u001b[0m\n",
            "2019-12-19 06:26:15 \u001b[32mINFO     \u001b[0m train.py: [0/300], [30/484], step: 30, 1.147 samples/sec, batch_loss: 0.0617, batch_loss_c: 0.0537, batch_loss_s: 0.0805, time:34.8860, lr:1e-06\u001b[0m\n",
            "2019-12-19 06:26:40 \u001b[32mINFO     \u001b[0m train.py: [0/300], [40/484], step: 40, 1.591 samples/sec, batch_loss: 0.0986, batch_loss_c: 0.0915, batch_loss_s: 0.1153, time:25.1347, lr:1e-06\u001b[0m\n",
            "tcmalloc: large alloc 2592006144 bytes == 0xa8284000 @  0x7efcc32221e7 0x7efcb7806f71 0x7efcb786a55d 0x7efcb786de28 0x7efcb786e3e5 0x7efcb7904fc2 0x50ac25 0x50d390 0x508245 0x50a080 0x50aa7d 0x50d390 0x509455 0x595311 0x5a067e 0x557d28 0x50c88b 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x509455 0x595311 0x5a522c 0x557f7e 0x4b5eff 0x50c810\n",
            "2019-12-19 06:27:06 \u001b[32mINFO     \u001b[0m train.py: [0/300], [50/484], step: 50, 1.542 samples/sec, batch_loss: 0.0686, batch_loss_c: 0.0607, batch_loss_s: 0.0871, time:25.9363, lr:1e-06\u001b[0m\n",
            "tcmalloc: large alloc 2633637888 bytes == 0xa3ef0000 @  0x7efcc32221e7 0x7efcb7806f71 0x7efcb786a55d 0x7efcb786de28 0x7efcb786e3e5 0x7efcb7904fc2 0x50ac25 0x50d390 0x508245 0x50a080 0x50aa7d 0x50d390 0x509455 0x595311 0x5a067e 0x557d28 0x50c88b 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x509455 0x595311 0x5a522c 0x557f7e 0x4b5eff 0x50c810\n",
            "2019-12-19 06:27:31 \u001b[32mINFO     \u001b[0m train.py: [0/300], [60/484], step: 60, 1.586 samples/sec, batch_loss: 0.0469, batch_loss_c: 0.0406, batch_loss_s: 0.0616, time:25.2148, lr:1e-06\u001b[0m\n",
            "2019-12-19 06:27:58 \u001b[32mINFO     \u001b[0m train.py: [0/300], [70/484], step: 70, 1.491 samples/sec, batch_loss: 0.1493, batch_loss_c: 0.1500, batch_loss_s: 0.1477, time:26.8324, lr:1e-06\u001b[0m\n",
            "2019-12-19 06:28:21 \u001b[32mINFO     \u001b[0m train.py: [0/300], [80/484], step: 80, 1.763 samples/sec, batch_loss: 0.0813, batch_loss_c: 0.0708, batch_loss_s: 0.1057, time:22.6835, lr:1e-06\u001b[0m\n",
            "2019-12-19 06:28:53 \u001b[32mINFO     \u001b[0m train.py: [0/300], [90/484], step: 90, 1.248 samples/sec, batch_loss: 0.0542, batch_loss_c: 0.0433, batch_loss_s: 0.0796, time:32.0640, lr:1e-06\u001b[0m\n",
            "2019-12-19 06:29:27 \u001b[32mINFO     \u001b[0m train.py: [0/300], [100/484], step: 100, 1.179 samples/sec, batch_loss: 0.3007, batch_loss_c: 0.2945, batch_loss_s: 0.3150, time:33.9168, lr:1e-06\u001b[0m\n",
            "tcmalloc: large alloc 2123186176 bytes == 0x9a920000 @  0x7efcc32221e7 0x7efcb7806f71 0x7efcb786a55d 0x7efcb786de28 0x7efcb786e3e5 0x7efcb7904fc2 0x50ac25 0x50d390 0x508245 0x50a080 0x50aa7d 0x50d390 0x509455 0x595311 0x5a067e 0x557d28 0x50c88b 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x509455 0x595311 0x5a522c 0x557f7e 0x4b5eff 0x50c810\n",
            "2019-12-19 06:30:15 \u001b[32mINFO     \u001b[0m train.py: [0/300], [110/484], step: 110, 0.827 samples/sec, batch_loss: 0.0626, batch_loss_c: 0.0548, batch_loss_s: 0.0809, time:48.3786, lr:1e-06\u001b[0m\n",
            "2019-12-19 06:30:41 \u001b[32mINFO     \u001b[0m train.py: [0/300], [120/484], step: 120, 1.552 samples/sec, batch_loss: 0.3192, batch_loss_c: 0.3133, batch_loss_s: 0.3330, time:25.7663, lr:1e-06\u001b[0m\n",
            "2019-12-19 06:31:06 \u001b[32mINFO     \u001b[0m train.py: [0/300], [130/484], step: 130, 1.600 samples/sec, batch_loss: 0.2879, batch_loss_c: 0.3116, batch_loss_s: 0.2326, time:24.9932, lr:1e-06\u001b[0m\n",
            "2019-12-19 06:31:40 \u001b[32mINFO     \u001b[0m train.py: [0/300], [140/484], step: 140, 1.194 samples/sec, batch_loss: 0.0649, batch_loss_c: 0.0541, batch_loss_s: 0.0901, time:33.5029, lr:1e-06\u001b[0m\n",
            "tcmalloc: large alloc 3486769152 bytes == 0xbf0c8000 @  0x7efcc32221e7 0x7efcb7806f71 0x7efcb786a55d 0x7efcb786de28 0x7efcb786e3e5 0x7efcb7904fc2 0x50ac25 0x50d390 0x508245 0x50a080 0x50aa7d 0x50d390 0x509455 0x595311 0x5a067e 0x557d28 0x50c88b 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x509455 0x595311 0x5a522c 0x557f7e 0x4b5eff 0x50c810\n",
            "2019-12-19 06:32:16 \u001b[32mINFO     \u001b[0m train.py: [0/300], [150/484], step: 150, 1.094 samples/sec, batch_loss: 0.0652, batch_loss_c: 0.0564, batch_loss_s: 0.0858, time:36.5494, lr:1e-06\u001b[0m\n",
            "2019-12-19 06:32:43 \u001b[32mINFO     \u001b[0m train.py: [0/300], [160/484], step: 160, 1.488 samples/sec, batch_loss: 0.3335, batch_loss_c: 0.3204, batch_loss_s: 0.3639, time:26.8803, lr:1e-06\u001b[0m\n",
            "tcmalloc: large alloc 3063201792 bytes == 0xaf090000 @  0x7efcc32221e7 0x7efcb7806f71 0x7efcb786a55d 0x7efcb786de28 0x7efcb786e3e5 0x7efcb7904fc2 0x50ac25 0x50d390 0x508245 0x50a080 0x50aa7d 0x50d390 0x509455 0x595311 0x5a067e 0x557d28 0x50c88b 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x509455 0x595311 0x5a522c 0x557f7e 0x4b5eff 0x50c810\n",
            "2019-12-19 06:33:19 \u001b[32mINFO     \u001b[0m train.py: [0/300], [170/484], step: 170, 1.098 samples/sec, batch_loss: 0.0697, batch_loss_c: 0.0603, batch_loss_s: 0.0917, time:36.4195, lr:1e-06\u001b[0m\n",
            "2019-12-19 06:33:45 \u001b[32mINFO     \u001b[0m train.py: [0/300], [180/484], step: 180, 1.589 samples/sec, batch_loss: 0.1386, batch_loss_c: 0.1523, batch_loss_s: 0.1067, time:25.1760, lr:1e-06\u001b[0m\n",
            "2019-12-19 06:34:27 \u001b[32mINFO     \u001b[0m train.py: [0/300], [190/484], step: 190, 0.948 samples/sec, batch_loss: 0.0544, batch_loss_c: 0.0503, batch_loss_s: 0.0640, time:42.1768, lr:1e-06\u001b[0m\n",
            "tcmalloc: large alloc 3376373760 bytes == 0xbd8ee000 @  0x7efcc32221e7 0x7efcb7806f71 0x7efcb786a55d 0x7efcb786de28 0x7efcb786e3e5 0x7efcb7904fc2 0x50ac25 0x50d390 0x508245 0x50a080 0x50aa7d 0x50d390 0x509455 0x595311 0x5a067e 0x557d28 0x50c88b 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x509455 0x595311 0x5a522c 0x557f7e 0x4b5eff 0x50c810\n",
            "2019-12-19 06:35:19 \u001b[32mINFO     \u001b[0m train.py: [0/300], [200/484], step: 200, 0.772 samples/sec, batch_loss: 0.1392, batch_loss_c: 0.1582, batch_loss_s: 0.0949, time:51.7910, lr:1e-06\u001b[0m\n",
            "2019-12-19 06:35:42 \u001b[32mINFO     \u001b[0m train.py: [0/300], [210/484], step: 210, 1.688 samples/sec, batch_loss: 0.0331, batch_loss_c: 0.0244, batch_loss_s: 0.0533, time:23.6985, lr:1e-06\u001b[0m\n",
            "2019-12-19 06:36:08 \u001b[32mINFO     \u001b[0m train.py: [0/300], [220/484], step: 220, 1.558 samples/sec, batch_loss: 0.0403, batch_loss_c: 0.0314, batch_loss_s: 0.0613, time:25.6747, lr:1e-06\u001b[0m\n",
            "2019-12-19 06:36:31 \u001b[32mINFO     \u001b[0m train.py: [0/300], [230/484], step: 230, 1.751 samples/sec, batch_loss: 0.1983, batch_loss_c: 0.2037, batch_loss_s: 0.1858, time:22.8485, lr:1e-06\u001b[0m\n",
            "2019-12-19 06:37:08 \u001b[32mINFO     \u001b[0m train.py: [0/300], [240/484], step: 240, 1.070 samples/sec, batch_loss: 0.0423, batch_loss_c: 0.0347, batch_loss_s: 0.0601, time:37.3894, lr:1e-06\u001b[0m\n",
            "tcmalloc: large alloc 4695465984 bytes == 0x7efae0b56000 @  0x7efcc32221e7 0x7efcb7806f71 0x7efcb786a55d 0x7efcb786de28 0x7efcb786e3e5 0x7efcb7904fc2 0x50ac25 0x50d390 0x508245 0x50a080 0x50aa7d 0x50d390 0x509455 0x595311 0x5a067e 0x557d28 0x50c88b 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x509455 0x595311 0x5a522c 0x557f7e 0x4b5eff 0x50c810\n",
            "2019-12-19 06:37:44 \u001b[32mINFO     \u001b[0m train.py: [0/300], [250/484], step: 250, 1.124 samples/sec, batch_loss: 0.0513, batch_loss_c: 0.0450, batch_loss_s: 0.0660, time:35.5977, lr:1e-06\u001b[0m\n",
            "2019-12-19 06:38:18 \u001b[32mINFO     \u001b[0m train.py: [0/300], [260/484], step: 260, 1.171 samples/sec, batch_loss: 0.2935, batch_loss_c: 0.2867, batch_loss_s: 0.3095, time:34.1679, lr:1e-06\u001b[0m\n",
            "2019-12-19 06:38:45 \u001b[32mINFO     \u001b[0m train.py: [0/300], [270/484], step: 270, 1.487 samples/sec, batch_loss: 0.0650, batch_loss_c: 0.0553, batch_loss_s: 0.0876, time:26.9082, lr:1e-06\u001b[0m\n",
            "2019-12-19 06:39:11 \u001b[32mINFO     \u001b[0m train.py: [0/300], [280/484], step: 280, 1.524 samples/sec, batch_loss: 0.0676, batch_loss_c: 0.0633, batch_loss_s: 0.0776, time:26.2476, lr:1e-06\u001b[0m\n",
            "2019-12-19 06:39:44 \u001b[32mINFO     \u001b[0m train.py: [0/300], [290/484], step: 290, 1.202 samples/sec, batch_loss: 0.3891, batch_loss_c: 0.3856, batch_loss_s: 0.3973, time:33.2683, lr:1e-06\u001b[0m\n",
            "2019-12-19 06:40:17 \u001b[32mINFO     \u001b[0m train.py: [0/300], [300/484], step: 300, 1.237 samples/sec, batch_loss: 0.0527, batch_loss_c: 0.0438, batch_loss_s: 0.0736, time:32.3314, lr:1e-06\u001b[0m\n",
            "2019-12-19 06:40:43 \u001b[32mINFO     \u001b[0m train.py: [0/300], [310/484], step: 310, 1.496 samples/sec, batch_loss: 0.0705, batch_loss_c: 0.0674, batch_loss_s: 0.0776, time:26.7439, lr:1e-06\u001b[0m\n",
            "2019-12-19 06:41:06 \u001b[32mINFO     \u001b[0m train.py: [0/300], [320/484], step: 320, 1.783 samples/sec, batch_loss: 0.0729, batch_loss_c: 0.0703, batch_loss_s: 0.0791, time:22.4352, lr:1e-06\u001b[0m\n",
            "2019-12-19 06:41:35 \u001b[32mINFO     \u001b[0m train.py: [0/300], [330/484], step: 330, 1.379 samples/sec, batch_loss: 0.0478, batch_loss_c: 0.0413, batch_loss_s: 0.0630, time:29.0091, lr:1e-06\u001b[0m\n",
            "2019-12-19 06:42:07 \u001b[32mINFO     \u001b[0m train.py: [0/300], [340/484], step: 340, 1.245 samples/sec, batch_loss: 0.4671, batch_loss_c: 0.4575, batch_loss_s: 0.4894, time:32.1329, lr:1e-06\u001b[0m\n",
            "2019-12-19 06:42:44 \u001b[32mINFO     \u001b[0m train.py: [0/300], [350/484], step: 350, 1.073 samples/sec, batch_loss: 0.0690, batch_loss_c: 0.0597, batch_loss_s: 0.0907, time:37.2799, lr:1e-06\u001b[0m\n",
            "2019-12-19 06:43:11 \u001b[32mINFO     \u001b[0m train.py: [0/300], [360/484], step: 360, 1.481 samples/sec, batch_loss: 0.0719, batch_loss_c: 0.0669, batch_loss_s: 0.0834, time:27.0114, lr:1e-06\u001b[0m\n",
            "2019-12-19 06:43:47 \u001b[32mINFO     \u001b[0m train.py: [0/300], [370/484], step: 370, 1.107 samples/sec, batch_loss: 0.1054, batch_loss_c: 0.0909, batch_loss_s: 0.1393, time:36.1363, lr:1e-06\u001b[0m\n",
            "2019-12-19 06:44:11 \u001b[32mINFO     \u001b[0m train.py: [0/300], [380/484], step: 380, 1.687 samples/sec, batch_loss: 0.0734, batch_loss_c: 0.0688, batch_loss_s: 0.0842, time:23.7128, lr:1e-06\u001b[0m\n",
            "2019-12-19 06:44:43 \u001b[32mINFO     \u001b[0m train.py: [0/300], [390/484], step: 390, 1.252 samples/sec, batch_loss: 0.0852, batch_loss_c: 0.0708, batch_loss_s: 0.1188, time:31.9505, lr:1e-06\u001b[0m\n",
            "2019-12-19 06:45:23 \u001b[32mINFO     \u001b[0m train.py: [0/300], [400/484], step: 400, 1.000 samples/sec, batch_loss: 0.2861, batch_loss_c: 0.2842, batch_loss_s: 0.2906, time:39.9889, lr:1e-06\u001b[0m\n",
            "2019-12-19 06:45:53 \u001b[32mINFO     \u001b[0m train.py: [0/300], [410/484], step: 410, 1.319 samples/sec, batch_loss: 0.1331, batch_loss_c: 0.1363, batch_loss_s: 0.1254, time:30.3172, lr:1e-06\u001b[0m\n",
            "2019-12-19 06:46:22 \u001b[32mINFO     \u001b[0m train.py: [0/300], [420/484], step: 420, 1.409 samples/sec, batch_loss: 0.1081, batch_loss_c: 0.1225, batch_loss_s: 0.0743, time:28.3800, lr:1e-06\u001b[0m\n",
            "2019-12-19 06:46:46 \u001b[32mINFO     \u001b[0m train.py: [0/300], [430/484], step: 430, 1.660 samples/sec, batch_loss: 0.0571, batch_loss_c: 0.0466, batch_loss_s: 0.0816, time:24.1018, lr:1e-06\u001b[0m\n",
            "2019-12-19 06:47:12 \u001b[32mINFO     \u001b[0m train.py: [0/300], [440/484], step: 440, 1.563 samples/sec, batch_loss: 0.0723, batch_loss_c: 0.0698, batch_loss_s: 0.0783, time:25.5991, lr:1e-06\u001b[0m\n",
            "2019-12-19 06:47:40 \u001b[32mINFO     \u001b[0m train.py: [0/300], [450/484], step: 450, 1.400 samples/sec, batch_loss: 0.0474, batch_loss_c: 0.0365, batch_loss_s: 0.0727, time:28.5785, lr:1e-06\u001b[0m\n",
            "2019-12-19 06:48:10 \u001b[32mINFO     \u001b[0m train.py: [0/300], [460/484], step: 460, 1.344 samples/sec, batch_loss: 0.3575, batch_loss_c: 0.3652, batch_loss_s: 0.3395, time:29.7598, lr:1e-06\u001b[0m\n",
            "2019-12-19 06:48:36 \u001b[32mINFO     \u001b[0m train.py: [0/300], [470/484], step: 470, 1.511 samples/sec, batch_loss: 0.0623, batch_loss_c: 0.0517, batch_loss_s: 0.0873, time:26.4647, lr:1e-06\u001b[0m\n",
            "2019-12-19 06:49:03 \u001b[32mINFO     \u001b[0m train.py: [0/300], [480/484], step: 480, 1.479 samples/sec, batch_loss: 0.0413, batch_loss_c: 0.0343, batch_loss_s: 0.0578, time:27.0473, lr:1e-06\u001b[0m\n",
            "2019-12-19 06:49:22 \u001b[32mINFO     \u001b[0m train.py: [0/300], train_loss: 0.1365, time: 1517.0876, lr: 1e-06\u001b[0m\n",
            "2019-12-19 06:49:26 \u001b[32mINFO     \u001b[0m train.py: [1/300], [0/484], step: 484, 14.435 samples/sec, batch_loss: 0.1865, batch_loss_c: 0.1879, batch_loss_s: 0.1833, time:2.7711, lr:1e-06\u001b[0m\n",
            "2019-12-19 06:50:03 \u001b[32mINFO     \u001b[0m train.py: [1/300], [10/484], step: 494, 1.070 samples/sec, batch_loss: 0.1018, batch_loss_c: 0.0928, batch_loss_s: 0.1227, time:37.3889, lr:1e-06\u001b[0m\n",
            "2019-12-19 06:50:29 \u001b[32mINFO     \u001b[0m train.py: [1/300], [20/484], step: 504, 1.573 samples/sec, batch_loss: 0.0630, batch_loss_c: 0.0534, batch_loss_s: 0.0853, time:25.4355, lr:1e-06\u001b[0m\n",
            "2019-12-19 06:50:53 \u001b[32mINFO     \u001b[0m train.py: [1/300], [30/484], step: 514, 1.659 samples/sec, batch_loss: 0.1376, batch_loss_c: 0.1227, batch_loss_s: 0.1721, time:24.1103, lr:1e-06\u001b[0m\n",
            "2019-12-19 06:51:19 \u001b[32mINFO     \u001b[0m train.py: [1/300], [40/484], step: 524, 1.526 samples/sec, batch_loss: 0.0535, batch_loss_c: 0.0475, batch_loss_s: 0.0673, time:26.2075, lr:1e-06\u001b[0m\n",
            "2019-12-19 06:51:43 \u001b[32mINFO     \u001b[0m train.py: [1/300], [50/484], step: 534, 1.663 samples/sec, batch_loss: 0.3103, batch_loss_c: 0.3075, batch_loss_s: 0.3169, time:24.0548, lr:1e-06\u001b[0m\n",
            "2019-12-19 06:52:06 \u001b[32mINFO     \u001b[0m train.py: [1/300], [60/484], step: 544, 1.728 samples/sec, batch_loss: 0.0369, batch_loss_c: 0.0305, batch_loss_s: 0.0518, time:23.1433, lr:1e-06\u001b[0m\n",
            "2019-12-19 06:52:28 \u001b[32mINFO     \u001b[0m train.py: [1/300], [70/484], step: 554, 1.825 samples/sec, batch_loss: 0.0734, batch_loss_c: 0.0642, batch_loss_s: 0.0949, time:21.9235, lr:1e-06\u001b[0m\n",
            "2019-12-19 06:52:56 \u001b[32mINFO     \u001b[0m train.py: [1/300], [80/484], step: 564, 1.445 samples/sec, batch_loss: 0.0712, batch_loss_c: 0.0688, batch_loss_s: 0.0767, time:27.6867, lr:1e-06\u001b[0m\n",
            "2019-12-19 06:53:28 \u001b[32mINFO     \u001b[0m train.py: [1/300], [90/484], step: 574, 1.233 samples/sec, batch_loss: 0.0597, batch_loss_c: 0.0485, batch_loss_s: 0.0858, time:32.4305, lr:1e-06\u001b[0m\n",
            "2019-12-19 06:53:51 \u001b[32mINFO     \u001b[0m train.py: [1/300], [100/484], step: 584, 1.722 samples/sec, batch_loss: 0.0629, batch_loss_c: 0.0515, batch_loss_s: 0.0893, time:23.2286, lr:1e-06\u001b[0m\n",
            "2019-12-19 06:54:14 \u001b[32mINFO     \u001b[0m train.py: [1/300], [110/484], step: 594, 1.740 samples/sec, batch_loss: 0.1228, batch_loss_c: 0.1240, batch_loss_s: 0.1199, time:22.9885, lr:1e-06\u001b[0m\n",
            "2019-12-19 06:54:45 \u001b[32mINFO     \u001b[0m train.py: [1/300], [120/484], step: 604, 1.297 samples/sec, batch_loss: 0.0714, batch_loss_c: 0.0660, batch_loss_s: 0.0841, time:30.8433, lr:1e-06\u001b[0m\n",
            "2019-12-19 06:55:14 \u001b[32mINFO     \u001b[0m train.py: [1/300], [130/484], step: 614, 1.371 samples/sec, batch_loss: 0.0498, batch_loss_c: 0.0466, batch_loss_s: 0.0571, time:29.1674, lr:1e-06\u001b[0m\n",
            "2019-12-19 06:55:51 \u001b[32mINFO     \u001b[0m train.py: [1/300], [140/484], step: 624, 1.078 samples/sec, batch_loss: 0.3629, batch_loss_c: 0.3700, batch_loss_s: 0.3462, time:37.1147, lr:1e-06\u001b[0m\n",
            "2019-12-19 06:56:19 \u001b[32mINFO     \u001b[0m train.py: [1/300], [150/484], step: 634, 1.433 samples/sec, batch_loss: 0.0546, batch_loss_c: 0.0502, batch_loss_s: 0.0647, time:27.9163, lr:1e-06\u001b[0m\n",
            "2019-12-19 06:56:45 \u001b[32mINFO     \u001b[0m train.py: [1/300], [160/484], step: 644, 1.588 samples/sec, batch_loss: 0.0516, batch_loss_c: 0.0440, batch_loss_s: 0.0693, time:25.1963, lr:1e-06\u001b[0m\n",
            "2019-12-19 06:57:13 \u001b[32mINFO     \u001b[0m train.py: [1/300], [170/484], step: 654, 1.421 samples/sec, batch_loss: 0.1229, batch_loss_c: 0.1139, batch_loss_s: 0.1441, time:28.1433, lr:1e-06\u001b[0m\n",
            "2019-12-19 06:57:52 \u001b[32mINFO     \u001b[0m train.py: [1/300], [180/484], step: 664, 1.021 samples/sec, batch_loss: 0.1151, batch_loss_c: 0.1166, batch_loss_s: 0.1116, time:39.1948, lr:1e-06\u001b[0m\n",
            "2019-12-19 06:58:22 \u001b[32mINFO     \u001b[0m train.py: [1/300], [190/484], step: 674, 1.348 samples/sec, batch_loss: 0.0525, batch_loss_c: 0.0472, batch_loss_s: 0.0650, time:29.6661, lr:1e-06\u001b[0m\n",
            "2019-12-19 06:58:52 \u001b[32mINFO     \u001b[0m train.py: [1/300], [200/484], step: 684, 1.313 samples/sec, batch_loss: 0.0562, batch_loss_c: 0.0441, batch_loss_s: 0.0844, time:30.4709, lr:1e-06\u001b[0m\n",
            "2019-12-19 06:59:23 \u001b[32mINFO     \u001b[0m train.py: [1/300], [210/484], step: 694, 1.284 samples/sec, batch_loss: 0.1129, batch_loss_c: 0.0886, batch_loss_s: 0.1696, time:31.1441, lr:1e-06\u001b[0m\n",
            "2019-12-19 06:59:51 \u001b[32mINFO     \u001b[0m train.py: [1/300], [220/484], step: 704, 1.439 samples/sec, batch_loss: 0.0520, batch_loss_c: 0.0472, batch_loss_s: 0.0632, time:27.7954, lr:1e-06\u001b[0m\n",
            "2019-12-19 07:00:18 \u001b[32mINFO     \u001b[0m train.py: [1/300], [230/484], step: 714, 1.507 samples/sec, batch_loss: 0.0617, batch_loss_c: 0.0534, batch_loss_s: 0.0811, time:26.5423, lr:1e-06\u001b[0m\n",
            "2019-12-19 07:00:43 \u001b[32mINFO     \u001b[0m train.py: [1/300], [240/484], step: 724, 1.558 samples/sec, batch_loss: 0.0745, batch_loss_c: 0.0744, batch_loss_s: 0.0747, time:25.6789, lr:1e-06\u001b[0m\n",
            "2019-12-19 07:01:24 \u001b[32mINFO     \u001b[0m train.py: [1/300], [250/484], step: 734, 0.978 samples/sec, batch_loss: 0.0701, batch_loss_c: 0.0613, batch_loss_s: 0.0908, time:40.8989, lr:1e-06\u001b[0m\n",
            "2019-12-19 07:01:48 \u001b[32mINFO     \u001b[0m train.py: [1/300], [260/484], step: 744, 1.662 samples/sec, batch_loss: 0.2279, batch_loss_c: 0.2126, batch_loss_s: 0.2636, time:24.0734, lr:1e-06\u001b[0m\n",
            "2019-12-19 07:02:21 \u001b[32mINFO     \u001b[0m train.py: [1/300], [270/484], step: 754, 1.231 samples/sec, batch_loss: 0.0543, batch_loss_c: 0.0434, batch_loss_s: 0.0799, time:32.5057, lr:1e-06\u001b[0m\n",
            "2019-12-19 07:02:47 \u001b[32mINFO     \u001b[0m train.py: [1/300], [280/484], step: 764, 1.505 samples/sec, batch_loss: 0.0891, batch_loss_c: 0.0870, batch_loss_s: 0.0940, time:26.5694, lr:1e-06\u001b[0m\n",
            "2019-12-19 07:03:30 \u001b[32mINFO     \u001b[0m train.py: [1/300], [290/484], step: 774, 0.941 samples/sec, batch_loss: 0.0567, batch_loss_c: 0.0523, batch_loss_s: 0.0670, time:42.5290, lr:1e-06\u001b[0m\n",
            "tcmalloc: large alloc 4204503040 bytes == 0x5bc86000 @  0x7efcc32221e7 0x7efcb7806f71 0x7efcb786a55d 0x7efcb786de28 0x7efcb786e3e5 0x7efcb7904fc2 0x50ac25 0x50d390 0x508245 0x50a080 0x50aa7d 0x50d390 0x509455 0x595311 0x5a067e 0x557d28 0x50c88b 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x509455 0x595311 0x5a522c 0x557f7e 0x4b5eff 0x50c810\n",
            "2019-12-19 07:04:27 \u001b[32mINFO     \u001b[0m train.py: [1/300], [300/484], step: 784, 0.701 samples/sec, batch_loss: 0.2866, batch_loss_c: 0.2855, batch_loss_s: 0.2891, time:57.0987, lr:1e-06\u001b[0m\n",
            "2019-12-19 07:04:52 \u001b[32mINFO     \u001b[0m train.py: [1/300], [310/484], step: 794, 1.579 samples/sec, batch_loss: 0.1486, batch_loss_c: 0.1204, batch_loss_s: 0.2145, time:25.3348, lr:1e-06\u001b[0m\n",
            "2019-12-19 07:05:22 \u001b[32mINFO     \u001b[0m train.py: [1/300], [320/484], step: 804, 1.361 samples/sec, batch_loss: 0.1544, batch_loss_c: 0.1632, batch_loss_s: 0.1338, time:29.3924, lr:1e-06\u001b[0m\n",
            "tcmalloc: large alloc 5787049984 bytes == 0x5bc86000 @  0x7efcc32221e7 0x7efcb7806f71 0x7efcb786a55d 0x7efcb786de28 0x7efcb786e3e5 0x7efcb7904fc2 0x50ac25 0x50d390 0x508245 0x50a080 0x50aa7d 0x50d390 0x509455 0x595311 0x5a067e 0x557d28 0x50c88b 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x509455 0x595311 0x5a522c 0x557f7e 0x4b5eff 0x50c810\n",
            "2019-12-19 07:06:12 \u001b[32mINFO     \u001b[0m train.py: [1/300], [330/484], step: 814, 0.796 samples/sec, batch_loss: 0.3920, batch_loss_c: 0.3802, batch_loss_s: 0.4196, time:50.2461, lr:1e-06\u001b[0m\n",
            "2019-12-19 07:06:35 \u001b[32mINFO     \u001b[0m train.py: [1/300], [340/484], step: 824, 1.755 samples/sec, batch_loss: 0.0986, batch_loss_c: 0.0938, batch_loss_s: 0.1097, time:22.7865, lr:1e-06\u001b[0m\n",
            "2019-12-19 07:06:58 \u001b[32mINFO     \u001b[0m train.py: [1/300], [350/484], step: 834, 1.705 samples/sec, batch_loss: 0.0721, batch_loss_c: 0.0593, batch_loss_s: 0.1018, time:23.4577, lr:1e-06\u001b[0m\n",
            "2019-12-19 07:07:48 \u001b[32mINFO     \u001b[0m train.py: [1/300], [360/484], step: 844, 0.803 samples/sec, batch_loss: 0.0707, batch_loss_c: 0.0677, batch_loss_s: 0.0775, time:49.7899, lr:1e-06\u001b[0m\n",
            "2019-12-19 07:08:14 \u001b[32mINFO     \u001b[0m train.py: [1/300], [370/484], step: 854, 1.552 samples/sec, batch_loss: 0.3543, batch_loss_c: 0.3065, batch_loss_s: 0.4657, time:25.7739, lr:1e-06\u001b[0m\n",
            "2019-12-19 07:08:38 \u001b[32mINFO     \u001b[0m train.py: [1/300], [380/484], step: 864, 1.646 samples/sec, batch_loss: 0.0687, batch_loss_c: 0.0689, batch_loss_s: 0.0683, time:24.3027, lr:1e-06\u001b[0m\n",
            "2019-12-19 07:09:00 \u001b[32mINFO     \u001b[0m train.py: [1/300], [390/484], step: 874, 1.818 samples/sec, batch_loss: 0.0658, batch_loss_c: 0.0532, batch_loss_s: 0.0951, time:21.9988, lr:1e-06\u001b[0m\n",
            "2019-12-19 07:09:31 \u001b[32mINFO     \u001b[0m train.py: [1/300], [400/484], step: 884, 1.299 samples/sec, batch_loss: 0.0446, batch_loss_c: 0.0380, batch_loss_s: 0.0602, time:30.7877, lr:1e-06\u001b[0m\n",
            "2019-12-19 07:10:10 \u001b[32mINFO     \u001b[0m train.py: [1/300], [410/484], step: 894, 1.024 samples/sec, batch_loss: 0.0530, batch_loss_c: 0.0490, batch_loss_s: 0.0625, time:39.0562, lr:1e-06\u001b[0m\n",
            "2019-12-19 07:10:42 \u001b[32mINFO     \u001b[0m train.py: [1/300], [420/484], step: 904, 1.259 samples/sec, batch_loss: 0.4404, batch_loss_c: 0.4078, batch_loss_s: 0.5163, time:31.7838, lr:1e-06\u001b[0m\n",
            "2019-12-19 07:11:07 \u001b[32mINFO     \u001b[0m train.py: [1/300], [430/484], step: 914, 1.579 samples/sec, batch_loss: 0.0599, batch_loss_c: 0.0571, batch_loss_s: 0.0665, time:25.3303, lr:1e-06\u001b[0m\n",
            "2019-12-19 07:11:33 \u001b[32mINFO     \u001b[0m train.py: [1/300], [440/484], step: 924, 1.507 samples/sec, batch_loss: 0.1030, batch_loss_c: 0.0990, batch_loss_s: 0.1125, time:26.5502, lr:1e-06\u001b[0m\n",
            "2019-12-19 07:12:00 \u001b[32mINFO     \u001b[0m train.py: [1/300], [450/484], step: 934, 1.531 samples/sec, batch_loss: 0.0740, batch_loss_c: 0.0653, batch_loss_s: 0.0944, time:26.1212, lr:1e-06\u001b[0m\n",
            "2019-12-19 07:12:28 \u001b[32mINFO     \u001b[0m train.py: [1/300], [460/484], step: 944, 1.388 samples/sec, batch_loss: 0.0795, batch_loss_c: 0.0792, batch_loss_s: 0.0800, time:28.8089, lr:1e-06\u001b[0m\n",
            "2019-12-19 07:12:55 \u001b[32mINFO     \u001b[0m train.py: [1/300], [470/484], step: 954, 1.484 samples/sec, batch_loss: 0.0741, batch_loss_c: 0.0683, batch_loss_s: 0.0876, time:26.9551, lr:1e-06\u001b[0m\n",
            "2019-12-19 07:13:18 \u001b[32mINFO     \u001b[0m train.py: [1/300], [480/484], step: 964, 1.803 samples/sec, batch_loss: 0.0541, batch_loss_c: 0.0489, batch_loss_s: 0.0663, time:22.1835, lr:1e-06\u001b[0m\n",
            "2019-12-19 07:13:25 \u001b[32mINFO     \u001b[0m train.py: [1/300], train_loss: 0.1377, time: 1441.9176, lr: 1e-06\u001b[0m\n",
            "2019-12-19 07:13:27 \u001b[32mINFO     \u001b[0m train.py: [2/300], [0/484], step: 968, 20.435 samples/sec, batch_loss: 0.0463, batch_loss_c: 0.0412, batch_loss_s: 0.0583, time:1.9574, lr:1e-06\u001b[0m\n",
            "2019-12-19 07:14:01 \u001b[32mINFO     \u001b[0m train.py: [2/300], [10/484], step: 978, 1.192 samples/sec, batch_loss: 0.2860, batch_loss_c: 0.2731, batch_loss_s: 0.3159, time:33.5444, lr:1e-06\u001b[0m\n",
            "2019-12-19 07:14:37 \u001b[32mINFO     \u001b[0m train.py: [2/300], [20/484], step: 988, 1.099 samples/sec, batch_loss: 0.0784, batch_loss_c: 0.0728, batch_loss_s: 0.0914, time:36.4027, lr:1e-06\u001b[0m\n",
            "2019-12-19 07:15:01 \u001b[32mINFO     \u001b[0m train.py: [2/300], [30/484], step: 998, 1.693 samples/sec, batch_loss: 0.1035, batch_loss_c: 0.0910, batch_loss_s: 0.1327, time:23.6273, lr:1e-06\u001b[0m\n",
            "2019-12-19 07:15:23 \u001b[32mINFO     \u001b[0m train.py: [2/300], [40/484], step: 1008, 1.771 samples/sec, batch_loss: 0.0548, batch_loss_c: 0.0506, batch_loss_s: 0.0645, time:22.5871, lr:1e-06\u001b[0m\n",
            "2019-12-19 07:15:46 \u001b[32mINFO     \u001b[0m train.py: [2/300], [50/484], step: 1018, 1.770 samples/sec, batch_loss: 0.0776, batch_loss_c: 0.0714, batch_loss_s: 0.0923, time:22.6051, lr:1e-06\u001b[0m\n",
            "2019-12-19 07:16:13 \u001b[32mINFO     \u001b[0m train.py: [2/300], [60/484], step: 1028, 1.473 samples/sec, batch_loss: 0.1015, batch_loss_c: 0.1004, batch_loss_s: 0.1042, time:27.1628, lr:1e-06\u001b[0m\n",
            "2019-12-19 07:16:54 \u001b[32mINFO     \u001b[0m train.py: [2/300], [70/484], step: 1038, 0.976 samples/sec, batch_loss: 0.0894, batch_loss_c: 0.0991, batch_loss_s: 0.0667, time:40.9804, lr:1e-06\u001b[0m\n",
            "2019-12-19 07:17:27 \u001b[32mINFO     \u001b[0m train.py: [2/300], [80/484], step: 1048, 1.209 samples/sec, batch_loss: 0.0501, batch_loss_c: 0.0418, batch_loss_s: 0.0695, time:33.0788, lr:1e-06\u001b[0m\n",
            "2019-12-19 07:18:02 \u001b[32mINFO     \u001b[0m train.py: [2/300], [90/484], step: 1058, 1.164 samples/sec, batch_loss: 0.1446, batch_loss_c: 0.1598, batch_loss_s: 0.1092, time:34.3696, lr:1e-06\u001b[0m\n",
            "2019-12-19 07:18:29 \u001b[32mINFO     \u001b[0m train.py: [2/300], [100/484], step: 1068, 1.460 samples/sec, batch_loss: 0.0517, batch_loss_c: 0.0404, batch_loss_s: 0.0781, time:27.4030, lr:1e-06\u001b[0m\n",
            "2019-12-19 07:18:53 \u001b[32mINFO     \u001b[0m train.py: [2/300], [110/484], step: 1078, 1.662 samples/sec, batch_loss: 0.5253, batch_loss_c: 0.5229, batch_loss_s: 0.5311, time:24.0645, lr:1e-06\u001b[0m\n",
            "2019-12-19 07:19:23 \u001b[32mINFO     \u001b[0m train.py: [2/300], [120/484], step: 1088, 1.326 samples/sec, batch_loss: 0.0648, batch_loss_c: 0.0576, batch_loss_s: 0.0817, time:30.1648, lr:1e-06\u001b[0m\n",
            "2019-12-19 07:19:49 \u001b[32mINFO     \u001b[0m train.py: [2/300], [130/484], step: 1098, 1.564 samples/sec, batch_loss: 0.0675, batch_loss_c: 0.0599, batch_loss_s: 0.0853, time:25.5765, lr:1e-06\u001b[0m\n",
            "2019-12-19 07:20:19 \u001b[32mINFO     \u001b[0m train.py: [2/300], [140/484], step: 1108, 1.322 samples/sec, batch_loss: 0.0499, batch_loss_c: 0.0429, batch_loss_s: 0.0663, time:30.2680, lr:1e-06\u001b[0m\n",
            "2019-12-19 07:20:53 \u001b[32mINFO     \u001b[0m train.py: [2/300], [150/484], step: 1118, 1.169 samples/sec, batch_loss: 0.0476, batch_loss_c: 0.0396, batch_loss_s: 0.0664, time:34.2243, lr:1e-06\u001b[0m\n",
            "2019-12-19 07:21:26 \u001b[32mINFO     \u001b[0m train.py: [2/300], [160/484], step: 1128, 1.215 samples/sec, batch_loss: 0.0606, batch_loss_c: 0.0535, batch_loss_s: 0.0773, time:32.9084, lr:1e-06\u001b[0m\n",
            "2019-12-19 07:21:52 \u001b[32mINFO     \u001b[0m train.py: [2/300], [170/484], step: 1138, 1.556 samples/sec, batch_loss: 0.0761, batch_loss_c: 0.0605, batch_loss_s: 0.1124, time:25.7039, lr:1e-06\u001b[0m\n",
            "2019-12-19 07:22:17 \u001b[32mINFO     \u001b[0m train.py: [2/300], [180/484], step: 1148, 1.571 samples/sec, batch_loss: 0.0470, batch_loss_c: 0.0426, batch_loss_s: 0.0573, time:25.4616, lr:1e-06\u001b[0m\n",
            "2019-12-19 07:22:58 \u001b[32mINFO     \u001b[0m train.py: [2/300], [190/484], step: 1158, 0.977 samples/sec, batch_loss: 0.2883, batch_loss_c: 0.2866, batch_loss_s: 0.2921, time:40.9624, lr:1e-06\u001b[0m\n",
            "2019-12-19 07:23:25 \u001b[32mINFO     \u001b[0m train.py: [2/300], [200/484], step: 1168, 1.485 samples/sec, batch_loss: 0.1129, batch_loss_c: 0.1008, batch_loss_s: 0.1410, time:26.9387, lr:1e-06\u001b[0m\n",
            "2019-12-19 07:24:07 \u001b[32mINFO     \u001b[0m train.py: [2/300], [210/484], step: 1178, 0.962 samples/sec, batch_loss: 0.1036, batch_loss_c: 0.0943, batch_loss_s: 0.1254, time:41.5590, lr:1e-06\u001b[0m\n",
            "2019-12-19 07:24:30 \u001b[32mINFO     \u001b[0m train.py: [2/300], [220/484], step: 1188, 1.758 samples/sec, batch_loss: 0.4062, batch_loss_c: 0.3995, batch_loss_s: 0.4219, time:22.7560, lr:1e-06\u001b[0m\n",
            "2019-12-19 07:24:55 \u001b[32mINFO     \u001b[0m train.py: [2/300], [230/484], step: 1198, 1.580 samples/sec, batch_loss: 0.1152, batch_loss_c: 0.1089, batch_loss_s: 0.1297, time:25.3245, lr:1e-06\u001b[0m\n",
            "2019-12-19 07:25:36 \u001b[32mINFO     \u001b[0m train.py: [2/300], [240/484], step: 1208, 0.965 samples/sec, batch_loss: 0.2194, batch_loss_c: 0.2518, batch_loss_s: 0.1439, time:41.4488, lr:1e-06\u001b[0m\n",
            "2019-12-19 07:26:11 \u001b[32mINFO     \u001b[0m train.py: [2/300], [250/484], step: 1218, 1.156 samples/sec, batch_loss: 0.0449, batch_loss_c: 0.0311, batch_loss_s: 0.0773, time:34.6028, lr:1e-06\u001b[0m\n",
            "2019-12-19 07:26:47 \u001b[32mINFO     \u001b[0m train.py: [2/300], [260/484], step: 1228, 1.098 samples/sec, batch_loss: 0.0391, batch_loss_c: 0.0323, batch_loss_s: 0.0549, time:36.4153, lr:1e-06\u001b[0m\n",
            "2019-12-19 07:27:26 \u001b[32mINFO     \u001b[0m train.py: [2/300], [270/484], step: 1238, 1.027 samples/sec, batch_loss: 0.1009, batch_loss_c: 0.0986, batch_loss_s: 0.1062, time:38.9338, lr:1e-06\u001b[0m\n",
            "2019-12-19 07:27:56 \u001b[32mINFO     \u001b[0m train.py: [2/300], [280/484], step: 1248, 1.332 samples/sec, batch_loss: 0.0577, batch_loss_c: 0.0470, batch_loss_s: 0.0827, time:30.0328, lr:1e-06\u001b[0m\n",
            "2019-12-19 07:28:23 \u001b[32mINFO     \u001b[0m train.py: [2/300], [290/484], step: 1258, 1.507 samples/sec, batch_loss: 0.0927, batch_loss_c: 0.0933, batch_loss_s: 0.0915, time:26.5469, lr:1e-06\u001b[0m\n",
            "2019-12-19 07:29:00 \u001b[32mINFO     \u001b[0m train.py: [2/300], [300/484], step: 1268, 1.076 samples/sec, batch_loss: 0.2398, batch_loss_c: 0.1996, batch_loss_s: 0.3334, time:37.1630, lr:1e-06\u001b[0m\n",
            "2019-12-19 07:29:24 \u001b[32mINFO     \u001b[0m train.py: [2/300], [310/484], step: 1278, 1.651 samples/sec, batch_loss: 0.0415, batch_loss_c: 0.0358, batch_loss_s: 0.0547, time:24.2296, lr:1e-06\u001b[0m\n",
            "2019-12-19 07:29:51 \u001b[32mINFO     \u001b[0m train.py: [2/300], [320/484], step: 1288, 1.509 samples/sec, batch_loss: 0.0990, batch_loss_c: 0.0968, batch_loss_s: 0.1041, time:26.5103, lr:1e-06\u001b[0m\n",
            "2019-12-19 07:30:20 \u001b[32mINFO     \u001b[0m train.py: [2/300], [330/484], step: 1298, 1.365 samples/sec, batch_loss: 0.0672, batch_loss_c: 0.0641, batch_loss_s: 0.0745, time:29.3134, lr:1e-06\u001b[0m\n",
            "2019-12-19 07:31:07 \u001b[32mINFO     \u001b[0m train.py: [2/300], [340/484], step: 1308, 0.849 samples/sec, batch_loss: 0.0834, batch_loss_c: 0.0765, batch_loss_s: 0.0995, time:47.0922, lr:1e-06\u001b[0m\n",
            "2019-12-19 07:31:36 \u001b[32mINFO     \u001b[0m train.py: [2/300], [350/484], step: 1318, 1.385 samples/sec, batch_loss: 0.0544, batch_loss_c: 0.0476, batch_loss_s: 0.0702, time:28.8819, lr:1e-06\u001b[0m\n",
            "2019-12-19 07:32:17 \u001b[32mINFO     \u001b[0m train.py: [2/300], [360/484], step: 1328, 0.986 samples/sec, batch_loss: 0.0845, batch_loss_c: 0.0783, batch_loss_s: 0.0988, time:40.5638, lr:1e-06\u001b[0m\n",
            "2019-12-19 07:32:48 \u001b[32mINFO     \u001b[0m train.py: [2/300], [370/484], step: 1338, 1.269 samples/sec, batch_loss: 0.0501, batch_loss_c: 0.0418, batch_loss_s: 0.0696, time:31.5092, lr:1e-06\u001b[0m\n",
            "2019-12-19 07:33:14 \u001b[32mINFO     \u001b[0m train.py: [2/300], [380/484], step: 1348, 1.561 samples/sec, batch_loss: 0.2206, batch_loss_c: 0.2092, batch_loss_s: 0.2470, time:25.6207, lr:1e-06\u001b[0m\n",
            "2019-12-19 07:33:41 \u001b[32mINFO     \u001b[0m train.py: [2/300], [390/484], step: 1358, 1.482 samples/sec, batch_loss: 0.2846, batch_loss_c: 0.2819, batch_loss_s: 0.2908, time:26.9955, lr:1e-06\u001b[0m\n",
            "2019-12-19 07:34:18 \u001b[32mINFO     \u001b[0m train.py: [2/300], [400/484], step: 1368, 1.086 samples/sec, batch_loss: 0.0577, batch_loss_c: 0.0485, batch_loss_s: 0.0792, time:36.8465, lr:1e-06\u001b[0m\n",
            "2019-12-19 07:34:44 \u001b[32mINFO     \u001b[0m train.py: [2/300], [410/484], step: 1378, 1.527 samples/sec, batch_loss: 0.1491, batch_loss_c: 0.1426, batch_loss_s: 0.1644, time:26.2011, lr:1e-06\u001b[0m\n",
            "2019-12-19 07:35:07 \u001b[32mINFO     \u001b[0m train.py: [2/300], [420/484], step: 1388, 1.717 samples/sec, batch_loss: 0.2188, batch_loss_c: 0.1793, batch_loss_s: 0.3111, time:23.2934, lr:1e-06\u001b[0m\n",
            "2019-12-19 07:35:37 \u001b[32mINFO     \u001b[0m train.py: [2/300], [430/484], step: 1398, 1.348 samples/sec, batch_loss: 0.0881, batch_loss_c: 0.0765, batch_loss_s: 0.1152, time:29.6683, lr:1e-06\u001b[0m\n",
            "2019-12-19 07:36:00 \u001b[32mINFO     \u001b[0m train.py: [2/300], [440/484], step: 1408, 1.731 samples/sec, batch_loss: 0.1332, batch_loss_c: 0.1420, batch_loss_s: 0.1127, time:23.1031, lr:1e-06\u001b[0m\n",
            "2019-12-19 07:36:31 \u001b[32mINFO     \u001b[0m train.py: [2/300], [450/484], step: 1418, 1.273 samples/sec, batch_loss: 0.3157, batch_loss_c: 0.3155, batch_loss_s: 0.3161, time:31.4208, lr:1e-06\u001b[0m\n",
            "2019-12-19 07:37:05 \u001b[32mINFO     \u001b[0m train.py: [2/300], [460/484], step: 1428, 1.178 samples/sec, batch_loss: 0.1046, batch_loss_c: 0.0947, batch_loss_s: 0.1278, time:33.9606, lr:1e-06\u001b[0m\n",
            "2019-12-19 07:37:37 \u001b[32mINFO     \u001b[0m train.py: [2/300], [470/484], step: 1438, 1.284 samples/sec, batch_loss: 0.0807, batch_loss_c: 0.0778, batch_loss_s: 0.0872, time:31.1474, lr:1e-06\u001b[0m\n",
            "2019-12-19 07:38:04 \u001b[32mINFO     \u001b[0m train.py: [2/300], [480/484], step: 1448, 1.477 samples/sec, batch_loss: 0.1424, batch_loss_c: 0.1419, batch_loss_s: 0.1436, time:27.0864, lr:1e-06\u001b[0m\n",
            "2019-12-19 07:38:14 \u001b[32mINFO     \u001b[0m train.py: [2/300], train_loss: 0.1378, time: 1489.0967, lr: 1e-06\u001b[0m\n",
            "2019-12-19 07:38:18 \u001b[32mINFO     \u001b[0m train.py: [3/300], [0/484], step: 1452, 15.078 samples/sec, batch_loss: 0.0437, batch_loss_c: 0.0369, batch_loss_s: 0.0595, time:2.6528, lr:1e-06\u001b[0m\n",
            "2019-12-19 07:38:52 \u001b[32mINFO     \u001b[0m train.py: [3/300], [10/484], step: 1462, 1.152 samples/sec, batch_loss: 0.0815, batch_loss_c: 0.0724, batch_loss_s: 0.1027, time:34.7108, lr:1e-06\u001b[0m\n",
            "2019-12-19 07:39:21 \u001b[32mINFO     \u001b[0m train.py: [3/300], [20/484], step: 1472, 1.373 samples/sec, batch_loss: 0.0606, batch_loss_c: 0.0560, batch_loss_s: 0.0714, time:29.1368, lr:1e-06\u001b[0m\n",
            "2019-12-19 07:39:56 \u001b[32mINFO     \u001b[0m train.py: [3/300], [30/484], step: 1482, 1.164 samples/sec, batch_loss: 0.0599, batch_loss_c: 0.0429, batch_loss_s: 0.0996, time:34.3539, lr:1e-06\u001b[0m\n",
            "2019-12-19 07:40:22 \u001b[32mINFO     \u001b[0m train.py: [3/300], [40/484], step: 1492, 1.511 samples/sec, batch_loss: 0.0742, batch_loss_c: 0.0608, batch_loss_s: 0.1054, time:26.4776, lr:1e-06\u001b[0m\n",
            "2019-12-19 07:40:49 \u001b[32mINFO     \u001b[0m train.py: [3/300], [50/484], step: 1502, 1.482 samples/sec, batch_loss: 0.2894, batch_loss_c: 0.2847, batch_loss_s: 0.3002, time:26.9885, lr:1e-06\u001b[0m\n",
            "2019-12-19 07:41:13 \u001b[32mINFO     \u001b[0m train.py: [3/300], [60/484], step: 1512, 1.707 samples/sec, batch_loss: 0.0595, batch_loss_c: 0.0517, batch_loss_s: 0.0776, time:23.4299, lr:1e-06\u001b[0m\n",
            "2019-12-19 07:41:41 \u001b[32mINFO     \u001b[0m train.py: [3/300], [70/484], step: 1522, 1.428 samples/sec, batch_loss: 0.0932, batch_loss_c: 0.0868, batch_loss_s: 0.1081, time:28.0097, lr:1e-06\u001b[0m\n",
            "2019-12-19 07:42:04 \u001b[32mINFO     \u001b[0m train.py: [3/300], [80/484], step: 1532, 1.732 samples/sec, batch_loss: 0.0606, batch_loss_c: 0.0535, batch_loss_s: 0.0771, time:23.1012, lr:1e-06\u001b[0m\n",
            "2019-12-19 07:42:32 \u001b[32mINFO     \u001b[0m train.py: [3/300], [90/484], step: 1542, 1.405 samples/sec, batch_loss: 0.3516, batch_loss_c: 0.3625, batch_loss_s: 0.3263, time:28.4650, lr:1e-06\u001b[0m\n",
            "2019-12-19 07:43:08 \u001b[32mINFO     \u001b[0m train.py: [3/300], [100/484], step: 1552, 1.120 samples/sec, batch_loss: 0.1623, batch_loss_c: 0.1174, batch_loss_s: 0.2672, time:35.7162, lr:1e-06\u001b[0m\n",
            "2019-12-19 07:43:33 \u001b[32mINFO     \u001b[0m train.py: [3/300], [110/484], step: 1562, 1.621 samples/sec, batch_loss: 0.0601, batch_loss_c: 0.0545, batch_loss_s: 0.0732, time:24.6827, lr:1e-06\u001b[0m\n",
            "2019-12-19 07:44:06 \u001b[32mINFO     \u001b[0m train.py: [3/300], [120/484], step: 1572, 1.204 samples/sec, batch_loss: 0.2788, batch_loss_c: 0.2759, batch_loss_s: 0.2855, time:33.2107, lr:1e-06\u001b[0m\n",
            "2019-12-19 07:44:37 \u001b[32mINFO     \u001b[0m train.py: [3/300], [130/484], step: 1582, 1.296 samples/sec, batch_loss: 0.1406, batch_loss_c: 0.1434, batch_loss_s: 0.1341, time:30.8636, lr:1e-06\u001b[0m\n",
            "2019-12-19 07:45:03 \u001b[32mINFO     \u001b[0m train.py: [3/300], [140/484], step: 1592, 1.499 samples/sec, batch_loss: 0.2348, batch_loss_c: 0.2247, batch_loss_s: 0.2583, time:26.6829, lr:1e-06\u001b[0m\n",
            "2019-12-19 07:45:44 \u001b[32mINFO     \u001b[0m train.py: [3/300], [150/484], step: 1602, 0.987 samples/sec, batch_loss: 0.0578, batch_loss_c: 0.0537, batch_loss_s: 0.0673, time:40.5105, lr:1e-06\u001b[0m\n",
            "2019-12-19 07:46:07 \u001b[32mINFO     \u001b[0m train.py: [3/300], [160/484], step: 1612, 1.717 samples/sec, batch_loss: 0.1591, batch_loss_c: 0.1663, batch_loss_s: 0.1423, time:23.3018, lr:1e-06\u001b[0m\n",
            "2019-12-19 07:46:38 \u001b[32mINFO     \u001b[0m train.py: [3/300], [170/484], step: 1622, 1.281 samples/sec, batch_loss: 0.1551, batch_loss_c: 0.0994, batch_loss_s: 0.2851, time:31.2243, lr:1e-06\u001b[0m\n",
            "2019-12-19 07:47:12 \u001b[32mINFO     \u001b[0m train.py: [3/300], [180/484], step: 1632, 1.183 samples/sec, batch_loss: 0.0578, batch_loss_c: 0.0511, batch_loss_s: 0.0735, time:33.8190, lr:1e-06\u001b[0m\n",
            "2019-12-19 07:48:03 \u001b[32mINFO     \u001b[0m train.py: [3/300], [190/484], step: 1642, 0.792 samples/sec, batch_loss: 0.1271, batch_loss_c: 0.1121, batch_loss_s: 0.1621, time:50.5140, lr:1e-06\u001b[0m\n",
            "2019-12-19 07:48:35 \u001b[32mINFO     \u001b[0m train.py: [3/300], [200/484], step: 1652, 1.232 samples/sec, batch_loss: 0.0715, batch_loss_c: 0.0652, batch_loss_s: 0.0863, time:32.4753, lr:1e-06\u001b[0m\n",
            "2019-12-19 07:49:16 \u001b[32mINFO     \u001b[0m train.py: [3/300], [210/484], step: 1662, 0.974 samples/sec, batch_loss: 0.0737, batch_loss_c: 0.0596, batch_loss_s: 0.1067, time:41.0613, lr:1e-06\u001b[0m\n",
            "2019-12-19 07:49:51 \u001b[32mINFO     \u001b[0m train.py: [3/300], [220/484], step: 1672, 1.163 samples/sec, batch_loss: 0.3316, batch_loss_c: 0.3361, batch_loss_s: 0.3211, time:34.3867, lr:1e-06\u001b[0m\n",
            "2019-12-19 07:50:33 \u001b[32mINFO     \u001b[0m train.py: [3/300], [230/484], step: 1682, 0.940 samples/sec, batch_loss: 0.0803, batch_loss_c: 0.0624, batch_loss_s: 0.1220, time:42.5717, lr:1e-06\u001b[0m\n",
            "2019-12-19 07:51:10 \u001b[32mINFO     \u001b[0m train.py: [3/300], [240/484], step: 1692, 1.088 samples/sec, batch_loss: 0.0658, batch_loss_c: 0.0589, batch_loss_s: 0.0817, time:36.7548, lr:1e-06\u001b[0m\n",
            "2019-12-19 07:51:33 \u001b[32mINFO     \u001b[0m train.py: [3/300], [250/484], step: 1702, 1.718 samples/sec, batch_loss: 0.0620, batch_loss_c: 0.0605, batch_loss_s: 0.0653, time:23.2891, lr:1e-06\u001b[0m\n",
            "2019-12-19 07:52:03 \u001b[32mINFO     \u001b[0m train.py: [3/300], [260/484], step: 1712, 1.344 samples/sec, batch_loss: 0.0682, batch_loss_c: 0.0550, batch_loss_s: 0.0989, time:29.7712, lr:1e-06\u001b[0m\n",
            "2019-12-19 07:52:31 \u001b[32mINFO     \u001b[0m train.py: [3/300], [270/484], step: 1722, 1.435 samples/sec, batch_loss: 0.0709, batch_loss_c: 0.0613, batch_loss_s: 0.0934, time:27.8764, lr:1e-06\u001b[0m\n",
            "2019-12-19 07:52:59 \u001b[32mINFO     \u001b[0m train.py: [3/300], [280/484], step: 1732, 1.443 samples/sec, batch_loss: 0.1311, batch_loss_c: 0.1220, batch_loss_s: 0.1523, time:27.7116, lr:1e-06\u001b[0m\n",
            "2019-12-19 07:53:30 \u001b[32mINFO     \u001b[0m train.py: [3/300], [290/484], step: 1742, 1.274 samples/sec, batch_loss: 0.0665, batch_loss_c: 0.0498, batch_loss_s: 0.1054, time:31.3940, lr:1e-06\u001b[0m\n",
            "2019-12-19 07:54:04 \u001b[32mINFO     \u001b[0m train.py: [3/300], [300/484], step: 1752, 1.189 samples/sec, batch_loss: 0.1182, batch_loss_c: 0.1043, batch_loss_s: 0.1506, time:33.6469, lr:1e-06\u001b[0m\n",
            "2019-12-19 07:54:38 \u001b[32mINFO     \u001b[0m train.py: [3/300], [310/484], step: 1762, 1.158 samples/sec, batch_loss: 0.2676, batch_loss_c: 0.2482, batch_loss_s: 0.3129, time:34.5417, lr:1e-06\u001b[0m\n",
            "2019-12-19 07:55:01 \u001b[32mINFO     \u001b[0m train.py: [3/300], [320/484], step: 1772, 1.741 samples/sec, batch_loss: 0.0657, batch_loss_c: 0.0563, batch_loss_s: 0.0876, time:22.9744, lr:1e-06\u001b[0m\n",
            "2019-12-19 07:55:30 \u001b[32mINFO     \u001b[0m train.py: [3/300], [330/484], step: 1782, 1.413 samples/sec, batch_loss: 0.1221, batch_loss_c: 0.1124, batch_loss_s: 0.1447, time:28.3041, lr:1e-06\u001b[0m\n",
            "2019-12-19 07:56:07 \u001b[32mINFO     \u001b[0m train.py: [3/300], [340/484], step: 1792, 1.057 samples/sec, batch_loss: 0.0929, batch_loss_c: 0.0910, batch_loss_s: 0.0974, time:37.8392, lr:1e-06\u001b[0m\n",
            "2019-12-19 07:56:41 \u001b[32mINFO     \u001b[0m train.py: [3/300], [350/484], step: 1802, 1.206 samples/sec, batch_loss: 0.2806, batch_loss_c: 0.2780, batch_loss_s: 0.2869, time:33.1540, lr:1e-06\u001b[0m\n",
            "2019-12-19 07:57:06 \u001b[32mINFO     \u001b[0m train.py: [3/300], [360/484], step: 1812, 1.578 samples/sec, batch_loss: 0.0511, batch_loss_c: 0.0463, batch_loss_s: 0.0624, time:25.3501, lr:1e-06\u001b[0m\n",
            "2019-12-19 07:57:33 \u001b[32mINFO     \u001b[0m train.py: [3/300], [370/484], step: 1822, 1.497 samples/sec, batch_loss: 0.2010, batch_loss_c: 0.1984, batch_loss_s: 0.2069, time:26.7162, lr:1e-06\u001b[0m\n",
            "2019-12-19 07:58:06 \u001b[32mINFO     \u001b[0m train.py: [3/300], [380/484], step: 1832, 1.215 samples/sec, batch_loss: 0.0411, batch_loss_c: 0.0367, batch_loss_s: 0.0515, time:32.9160, lr:1e-06\u001b[0m\n",
            "2019-12-19 07:58:28 \u001b[32mINFO     \u001b[0m train.py: [3/300], [390/484], step: 1842, 1.756 samples/sec, batch_loss: 0.1258, batch_loss_c: 0.1312, batch_loss_s: 0.1131, time:22.7824, lr:1e-06\u001b[0m\n",
            "2019-12-19 07:58:52 \u001b[32mINFO     \u001b[0m train.py: [3/300], [400/484], step: 1852, 1.723 samples/sec, batch_loss: 0.1985, batch_loss_c: 0.1888, batch_loss_s: 0.2210, time:23.2157, lr:1e-06\u001b[0m\n",
            "2019-12-19 07:59:16 \u001b[32mINFO     \u001b[0m train.py: [3/300], [410/484], step: 1862, 1.611 samples/sec, batch_loss: 0.2928, batch_loss_c: 0.2856, batch_loss_s: 0.3097, time:24.8325, lr:1e-06\u001b[0m\n",
            "2019-12-19 07:59:40 \u001b[32mINFO     \u001b[0m train.py: [3/300], [420/484], step: 1872, 1.664 samples/sec, batch_loss: 0.0863, batch_loss_c: 0.0785, batch_loss_s: 0.1046, time:24.0358, lr:1e-06\u001b[0m\n",
            "2019-12-19 08:00:06 \u001b[32mINFO     \u001b[0m train.py: [3/300], [430/484], step: 1882, 1.575 samples/sec, batch_loss: 0.0865, batch_loss_c: 0.0751, batch_loss_s: 0.1131, time:25.3923, lr:1e-06\u001b[0m\n",
            "2019-12-19 08:00:34 \u001b[32mINFO     \u001b[0m train.py: [3/300], [440/484], step: 1892, 1.424 samples/sec, batch_loss: 0.0708, batch_loss_c: 0.0587, batch_loss_s: 0.0990, time:28.0988, lr:1e-06\u001b[0m\n",
            "2019-12-19 08:01:13 \u001b[32mINFO     \u001b[0m train.py: [3/300], [450/484], step: 1902, 1.015 samples/sec, batch_loss: 0.0950, batch_loss_c: 0.0946, batch_loss_s: 0.0960, time:39.4144, lr:1e-06\u001b[0m\n",
            "2019-12-19 08:01:36 \u001b[32mINFO     \u001b[0m train.py: [3/300], [460/484], step: 1912, 1.762 samples/sec, batch_loss: 0.0950, batch_loss_c: 0.0846, batch_loss_s: 0.1195, time:22.6989, lr:1e-06\u001b[0m\n",
            "2019-12-19 08:02:04 \u001b[32mINFO     \u001b[0m train.py: [3/300], [470/484], step: 1922, 1.411 samples/sec, batch_loss: 0.0892, batch_loss_c: 0.0867, batch_loss_s: 0.0952, time:28.3500, lr:1e-06\u001b[0m\n",
            "2019-12-19 08:02:45 \u001b[32mINFO     \u001b[0m train.py: [3/300], [480/484], step: 1932, 0.982 samples/sec, batch_loss: 0.0771, batch_loss_c: 0.0817, batch_loss_s: 0.0664, time:40.7219, lr:1e-06\u001b[0m\n",
            "2019-12-19 08:02:52 \u001b[32mINFO     \u001b[0m train.py: [3/300], train_loss: 0.1419, time: 1477.1748, lr: 1e-06\u001b[0m\n",
            "2019-12-19 08:02:55 \u001b[32mINFO     \u001b[0m train.py: [4/300], [0/484], step: 1936, 20.154 samples/sec, batch_loss: 0.0568, batch_loss_c: 0.0486, batch_loss_s: 0.0757, time:1.9847, lr:1e-06\u001b[0m\n",
            "2019-12-19 08:03:22 \u001b[32mINFO     \u001b[0m train.py: [4/300], [10/484], step: 1946, 1.444 samples/sec, batch_loss: 0.0534, batch_loss_c: 0.0477, batch_loss_s: 0.0669, time:27.7083, lr:1e-06\u001b[0m\n",
            "2019-12-19 08:04:00 \u001b[32mINFO     \u001b[0m train.py: [4/300], [20/484], step: 1956, 1.070 samples/sec, batch_loss: 0.0563, batch_loss_c: 0.0458, batch_loss_s: 0.0808, time:37.3663, lr:1e-06\u001b[0m\n",
            "2019-12-19 08:04:31 \u001b[32mINFO     \u001b[0m train.py: [4/300], [30/484], step: 1966, 1.268 samples/sec, batch_loss: 0.3189, batch_loss_c: 0.3055, batch_loss_s: 0.3501, time:31.5487, lr:1e-06\u001b[0m\n",
            "2019-12-19 08:04:57 \u001b[32mINFO     \u001b[0m train.py: [4/300], [40/484], step: 1976, 1.526 samples/sec, batch_loss: 0.0650, batch_loss_c: 0.0586, batch_loss_s: 0.0799, time:26.2057, lr:1e-06\u001b[0m\n",
            "2019-12-19 08:05:20 \u001b[32mINFO     \u001b[0m train.py: [4/300], [50/484], step: 1986, 1.747 samples/sec, batch_loss: 0.0572, batch_loss_c: 0.0531, batch_loss_s: 0.0667, time:22.8938, lr:1e-06\u001b[0m\n",
            "2019-12-19 08:05:49 \u001b[32mINFO     \u001b[0m train.py: [4/300], [60/484], step: 1996, 1.403 samples/sec, batch_loss: 0.0541, batch_loss_c: 0.0470, batch_loss_s: 0.0706, time:28.5005, lr:1e-06\u001b[0m\n",
            "tcmalloc: large alloc 6396469248 bytes == 0x7ef963730000 @  0x7efcc32221e7 0x7efcb7806f71 0x7efcb786a55d 0x7efcb786de28 0x7efcb786e3e5 0x7efcb7904fc2 0x50ac25 0x50d390 0x508245 0x50a080 0x50aa7d 0x50d390 0x509455 0x595311 0x5a067e 0x557d28 0x50c88b 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x509455 0x595311 0x5a522c 0x557f7e 0x4b5eff 0x50c810\n",
            "2019-12-19 08:06:54 \u001b[32mINFO     \u001b[0m train.py: [4/300], [70/484], step: 2006, 0.614 samples/sec, batch_loss: 0.0501, batch_loss_c: 0.0416, batch_loss_s: 0.0699, time:65.1607, lr:1e-06\u001b[0m\n",
            "2019-12-19 08:07:21 \u001b[32mINFO     \u001b[0m train.py: [4/300], [80/484], step: 2016, 1.468 samples/sec, batch_loss: 0.0798, batch_loss_c: 0.0670, batch_loss_s: 0.1095, time:27.2568, lr:1e-06\u001b[0m\n",
            "2019-12-19 08:07:47 \u001b[32mINFO     \u001b[0m train.py: [4/300], [90/484], step: 2026, 1.549 samples/sec, batch_loss: 0.0735, batch_loss_c: 0.0613, batch_loss_s: 0.1022, time:25.8223, lr:1e-06\u001b[0m\n",
            "2019-12-19 08:08:19 \u001b[32mINFO     \u001b[0m train.py: [4/300], [100/484], step: 2036, 1.258 samples/sec, batch_loss: 0.5229, batch_loss_c: 0.5179, batch_loss_s: 0.5346, time:31.7862, lr:1e-06\u001b[0m\n",
            "2019-12-19 08:08:48 \u001b[32mINFO     \u001b[0m train.py: [4/300], [110/484], step: 2046, 1.378 samples/sec, batch_loss: 0.0760, batch_loss_c: 0.0724, batch_loss_s: 0.0844, time:29.0269, lr:1e-06\u001b[0m\n",
            "2019-12-19 08:09:12 \u001b[32mINFO     \u001b[0m train.py: [4/300], [120/484], step: 2056, 1.681 samples/sec, batch_loss: 0.3013, batch_loss_c: 0.2941, batch_loss_s: 0.3181, time:23.7919, lr:1e-06\u001b[0m\n",
            "2019-12-19 08:09:36 \u001b[32mINFO     \u001b[0m train.py: [4/300], [130/484], step: 2066, 1.683 samples/sec, batch_loss: 0.0797, batch_loss_c: 0.0748, batch_loss_s: 0.0910, time:23.7621, lr:1e-06\u001b[0m\n",
            "2019-12-19 08:10:04 \u001b[32mINFO     \u001b[0m train.py: [4/300], [140/484], step: 2076, 1.405 samples/sec, batch_loss: 0.1387, batch_loss_c: 0.1368, batch_loss_s: 0.1430, time:28.4626, lr:1e-06\u001b[0m\n",
            "2019-12-19 08:10:40 \u001b[32mINFO     \u001b[0m train.py: [4/300], [150/484], step: 2086, 1.126 samples/sec, batch_loss: 0.0896, batch_loss_c: 0.0797, batch_loss_s: 0.1126, time:35.5342, lr:1e-06\u001b[0m\n",
            "2019-12-19 08:11:04 \u001b[32mINFO     \u001b[0m train.py: [4/300], [160/484], step: 2096, 1.639 samples/sec, batch_loss: 0.0549, batch_loss_c: 0.0430, batch_loss_s: 0.0826, time:24.4116, lr:1e-06\u001b[0m\n",
            "2019-12-19 08:11:41 \u001b[32mINFO     \u001b[0m train.py: [4/300], [170/484], step: 2106, 1.071 samples/sec, batch_loss: 0.1055, batch_loss_c: 0.1050, batch_loss_s: 0.1068, time:37.3467, lr:1e-06\u001b[0m\n",
            "2019-12-19 08:12:14 \u001b[32mINFO     \u001b[0m train.py: [4/300], [180/484], step: 2116, 1.219 samples/sec, batch_loss: 0.3012, batch_loss_c: 0.2986, batch_loss_s: 0.3073, time:32.8271, lr:1e-06\u001b[0m\n",
            "2019-12-19 08:12:48 \u001b[32mINFO     \u001b[0m train.py: [4/300], [190/484], step: 2126, 1.183 samples/sec, batch_loss: 0.3093, batch_loss_c: 0.2990, batch_loss_s: 0.3332, time:33.8069, lr:1e-06\u001b[0m\n",
            "2019-12-19 08:13:13 \u001b[32mINFO     \u001b[0m train.py: [4/300], [200/484], step: 2136, 1.597 samples/sec, batch_loss: 0.0554, batch_loss_c: 0.0450, batch_loss_s: 0.0797, time:25.0538, lr:1e-06\u001b[0m\n",
            "2019-12-19 08:13:37 \u001b[32mINFO     \u001b[0m train.py: [4/300], [210/484], step: 2146, 1.635 samples/sec, batch_loss: 0.0520, batch_loss_c: 0.0432, batch_loss_s: 0.0723, time:24.4709, lr:1e-06\u001b[0m\n",
            "2019-12-19 08:14:04 \u001b[32mINFO     \u001b[0m train.py: [4/300], [220/484], step: 2156, 1.490 samples/sec, batch_loss: 0.0686, batch_loss_c: 0.0610, batch_loss_s: 0.0862, time:26.8515, lr:1e-06\u001b[0m\n",
            "2019-12-19 08:14:42 \u001b[32mINFO     \u001b[0m train.py: [4/300], [230/484], step: 2166, 1.069 samples/sec, batch_loss: 0.0840, batch_loss_c: 0.0758, batch_loss_s: 0.1033, time:37.4293, lr:1e-06\u001b[0m\n",
            "2019-12-19 08:15:23 \u001b[32mINFO     \u001b[0m train.py: [4/300], [240/484], step: 2176, 0.975 samples/sec, batch_loss: 0.0582, batch_loss_c: 0.0546, batch_loss_s: 0.0668, time:41.0253, lr:1e-06\u001b[0m\n",
            "2019-12-19 08:15:48 \u001b[32mINFO     \u001b[0m train.py: [4/300], [250/484], step: 2186, 1.555 samples/sec, batch_loss: 0.0543, batch_loss_c: 0.0476, batch_loss_s: 0.0699, time:25.7273, lr:1e-06\u001b[0m\n",
            "2019-12-19 08:16:21 \u001b[32mINFO     \u001b[0m train.py: [4/300], [260/484], step: 2196, 1.241 samples/sec, batch_loss: 0.0680, batch_loss_c: 0.0629, batch_loss_s: 0.0798, time:32.2289, lr:1e-06\u001b[0m\n",
            "2019-12-19 08:16:46 \u001b[32mINFO     \u001b[0m train.py: [4/300], [270/484], step: 2206, 1.570 samples/sec, batch_loss: 0.0876, batch_loss_c: 0.0867, batch_loss_s: 0.0898, time:25.4730, lr:1e-06\u001b[0m\n",
            "2019-12-19 08:17:08 \u001b[32mINFO     \u001b[0m train.py: [4/300], [280/484], step: 2216, 1.818 samples/sec, batch_loss: 0.0476, batch_loss_c: 0.0377, batch_loss_s: 0.0707, time:22.0027, lr:1e-06\u001b[0m\n",
            "2019-12-19 08:17:41 \u001b[32mINFO     \u001b[0m train.py: [4/300], [290/484], step: 2226, 1.213 samples/sec, batch_loss: 0.1447, batch_loss_c: 0.1257, batch_loss_s: 0.1889, time:32.9647, lr:1e-06\u001b[0m\n",
            "2019-12-19 08:18:14 \u001b[32mINFO     \u001b[0m train.py: [4/300], [300/484], step: 2236, 1.209 samples/sec, batch_loss: 0.3251, batch_loss_c: 0.3100, batch_loss_s: 0.3602, time:33.0968, lr:1e-06\u001b[0m\n",
            "2019-12-19 08:18:43 \u001b[32mINFO     \u001b[0m train.py: [4/300], [310/484], step: 2246, 1.411 samples/sec, batch_loss: 0.2464, batch_loss_c: 0.2229, batch_loss_s: 0.3012, time:28.3393, lr:1e-06\u001b[0m\n",
            "2019-12-19 08:19:19 \u001b[32mINFO     \u001b[0m train.py: [4/300], [320/484], step: 2256, 1.112 samples/sec, batch_loss: 0.0606, batch_loss_c: 0.0606, batch_loss_s: 0.0605, time:35.9829, lr:1e-06\u001b[0m\n",
            "2019-12-19 08:19:51 \u001b[32mINFO     \u001b[0m train.py: [4/300], [330/484], step: 2266, 1.225 samples/sec, batch_loss: 0.1633, batch_loss_c: 0.1224, batch_loss_s: 0.2586, time:32.6544, lr:1e-06\u001b[0m\n",
            "2019-12-19 08:20:20 \u001b[32mINFO     \u001b[0m train.py: [4/300], [340/484], step: 2276, 1.401 samples/sec, batch_loss: 0.1712, batch_loss_c: 0.1556, batch_loss_s: 0.2076, time:28.5542, lr:1e-06\u001b[0m\n",
            "2019-12-19 08:20:47 \u001b[32mINFO     \u001b[0m train.py: [4/300], [350/484], step: 2286, 1.493 samples/sec, batch_loss: 0.0594, batch_loss_c: 0.0625, batch_loss_s: 0.0522, time:26.7995, lr:1e-06\u001b[0m\n",
            "2019-12-19 08:21:14 \u001b[32mINFO     \u001b[0m train.py: [4/300], [360/484], step: 2296, 1.444 samples/sec, batch_loss: 0.0648, batch_loss_c: 0.0614, batch_loss_s: 0.0726, time:27.7038, lr:1e-06\u001b[0m\n",
            "2019-12-19 08:21:53 \u001b[32mINFO     \u001b[0m train.py: [4/300], [370/484], step: 2306, 1.031 samples/sec, batch_loss: 0.0667, batch_loss_c: 0.0566, batch_loss_s: 0.0904, time:38.7815, lr:1e-06\u001b[0m\n",
            "2019-12-19 08:22:40 \u001b[32mINFO     \u001b[0m train.py: [4/300], [380/484], step: 2316, 0.859 samples/sec, batch_loss: 0.1348, batch_loss_c: 0.0961, batch_loss_s: 0.2251, time:46.5822, lr:1e-06\u001b[0m\n",
            "2019-12-19 08:23:09 \u001b[32mINFO     \u001b[0m train.py: [4/300], [390/484], step: 2326, 1.373 samples/sec, batch_loss: 0.0669, batch_loss_c: 0.0546, batch_loss_s: 0.0958, time:29.1228, lr:1e-06\u001b[0m\n",
            "2019-12-19 08:23:37 \u001b[32mINFO     \u001b[0m train.py: [4/300], [400/484], step: 2336, 1.405 samples/sec, batch_loss: 0.0355, batch_loss_c: 0.0295, batch_loss_s: 0.0497, time:28.4777, lr:1e-06\u001b[0m\n",
            "2019-12-19 08:24:05 \u001b[32mINFO     \u001b[0m train.py: [4/300], [410/484], step: 2346, 1.416 samples/sec, batch_loss: 0.0686, batch_loss_c: 0.0622, batch_loss_s: 0.0835, time:28.2474, lr:1e-06\u001b[0m\n",
            "2019-12-19 08:24:33 \u001b[32mINFO     \u001b[0m train.py: [4/300], [420/484], step: 2356, 1.463 samples/sec, batch_loss: 0.2290, batch_loss_c: 0.2464, batch_loss_s: 0.1882, time:27.3378, lr:1e-06\u001b[0m\n",
            "2019-12-19 08:25:04 \u001b[32mINFO     \u001b[0m train.py: [4/300], [430/484], step: 2366, 1.268 samples/sec, batch_loss: 0.0756, batch_loss_c: 0.0698, batch_loss_s: 0.0892, time:31.5346, lr:1e-06\u001b[0m\n",
            "2019-12-19 08:25:42 \u001b[32mINFO     \u001b[0m train.py: [4/300], [440/484], step: 2376, 1.062 samples/sec, batch_loss: 0.2936, batch_loss_c: 0.2811, batch_loss_s: 0.3228, time:37.6658, lr:1e-06\u001b[0m\n",
            "2019-12-19 08:26:16 \u001b[32mINFO     \u001b[0m train.py: [4/300], [450/484], step: 2386, 1.192 samples/sec, batch_loss: 0.1318, batch_loss_c: 0.1237, batch_loss_s: 0.1506, time:33.5560, lr:1e-06\u001b[0m\n",
            "2019-12-19 08:26:45 \u001b[32mINFO     \u001b[0m train.py: [4/300], [460/484], step: 2396, 1.359 samples/sec, batch_loss: 0.0636, batch_loss_c: 0.0637, batch_loss_s: 0.0634, time:29.4363, lr:1e-06\u001b[0m\n",
            "2019-12-19 08:27:13 \u001b[32mINFO     \u001b[0m train.py: [4/300], [470/484], step: 2406, 1.437 samples/sec, batch_loss: 0.2897, batch_loss_c: 0.2858, batch_loss_s: 0.2989, time:27.8279, lr:1e-06\u001b[0m\n",
            "2019-12-19 08:27:39 \u001b[32mINFO     \u001b[0m train.py: [4/300], [480/484], step: 2416, 1.531 samples/sec, batch_loss: 0.0567, batch_loss_c: 0.0459, batch_loss_s: 0.0819, time:26.1349, lr:1e-06\u001b[0m\n",
            "2019-12-19 08:27:45 \u001b[32mINFO     \u001b[0m train.py: [4/300], train_loss: 0.1456, time: 1492.7153, lr: 1e-06\u001b[0m\n",
            "2019-12-19 08:27:48 \u001b[32mINFO     \u001b[0m train.py: [5/300], [0/484], step: 2420, 17.111 samples/sec, batch_loss: 0.0875, batch_loss_c: 0.0748, batch_loss_s: 0.1170, time:2.3377, lr:1e-06\u001b[0m\n",
            "2019-12-19 08:28:27 \u001b[32mINFO     \u001b[0m train.py: [5/300], [10/484], step: 2430, 1.023 samples/sec, batch_loss: 0.0617, batch_loss_c: 0.0536, batch_loss_s: 0.0809, time:39.0997, lr:1e-06\u001b[0m\n",
            "2019-12-19 08:28:51 \u001b[32mINFO     \u001b[0m train.py: [5/300], [20/484], step: 2440, 1.677 samples/sec, batch_loss: 0.1537, batch_loss_c: 0.1298, batch_loss_s: 0.2093, time:23.8523, lr:1e-06\u001b[0m\n",
            "2019-12-19 08:29:18 \u001b[32mINFO     \u001b[0m train.py: [5/300], [30/484], step: 2450, 1.486 samples/sec, batch_loss: 0.0817, batch_loss_c: 0.0755, batch_loss_s: 0.0963, time:26.9205, lr:1e-06\u001b[0m\n",
            "2019-12-19 08:29:44 \u001b[32mINFO     \u001b[0m train.py: [5/300], [40/484], step: 2460, 1.521 samples/sec, batch_loss: 0.4149, batch_loss_c: 0.3712, batch_loss_s: 0.5170, time:26.3062, lr:1e-06\u001b[0m\n",
            "2019-12-19 08:30:10 \u001b[32mINFO     \u001b[0m train.py: [5/300], [50/484], step: 2470, 1.578 samples/sec, batch_loss: 0.1205, batch_loss_c: 0.1094, batch_loss_s: 0.1464, time:25.3469, lr:1e-06\u001b[0m\n",
            "2019-12-19 08:30:39 \u001b[32mINFO     \u001b[0m train.py: [5/300], [60/484], step: 2480, 1.364 samples/sec, batch_loss: 0.2847, batch_loss_c: 0.2815, batch_loss_s: 0.2922, time:29.3254, lr:1e-06\u001b[0m\n",
            "2019-12-19 08:31:16 \u001b[32mINFO     \u001b[0m train.py: [5/300], [70/484], step: 2490, 1.087 samples/sec, batch_loss: 0.3021, batch_loss_c: 0.3054, batch_loss_s: 0.2943, time:36.8033, lr:1e-06\u001b[0m\n",
            "2019-12-19 08:31:49 \u001b[32mINFO     \u001b[0m train.py: [5/300], [80/484], step: 2500, 1.195 samples/sec, batch_loss: 0.1010, batch_loss_c: 0.0980, batch_loss_s: 0.1079, time:33.4696, lr:1e-06\u001b[0m\n",
            "2019-12-19 08:32:25 \u001b[32mINFO     \u001b[0m train.py: [5/300], [90/484], step: 2510, 1.120 samples/sec, batch_loss: 0.0840, batch_loss_c: 0.0729, batch_loss_s: 0.1098, time:35.7276, lr:1e-06\u001b[0m\n",
            "2019-12-19 08:32:49 \u001b[32mINFO     \u001b[0m train.py: [5/300], [100/484], step: 2520, 1.670 samples/sec, batch_loss: 0.0803, batch_loss_c: 0.0697, batch_loss_s: 0.1051, time:23.9540, lr:1e-06\u001b[0m\n",
            "2019-12-19 08:33:15 \u001b[32mINFO     \u001b[0m train.py: [5/300], [110/484], step: 2530, 1.566 samples/sec, batch_loss: 0.0630, batch_loss_c: 0.0581, batch_loss_s: 0.0742, time:25.5372, lr:1e-06\u001b[0m\n",
            "2019-12-19 08:33:56 \u001b[32mINFO     \u001b[0m train.py: [5/300], [120/484], step: 2540, 0.970 samples/sec, batch_loss: 0.2642, batch_loss_c: 0.2593, batch_loss_s: 0.2756, time:41.2230, lr:1e-06\u001b[0m\n",
            "2019-12-19 08:34:21 \u001b[32mINFO     \u001b[0m train.py: [5/300], [130/484], step: 2550, 1.587 samples/sec, batch_loss: 0.2088, batch_loss_c: 0.2452, batch_loss_s: 0.1239, time:25.2022, lr:1e-06\u001b[0m\n",
            "2019-12-19 08:34:46 \u001b[32mINFO     \u001b[0m train.py: [5/300], [140/484], step: 2560, 1.608 samples/sec, batch_loss: 0.3095, batch_loss_c: 0.2971, batch_loss_s: 0.3383, time:24.8703, lr:1e-06\u001b[0m\n",
            "2019-12-19 08:35:22 \u001b[32mINFO     \u001b[0m train.py: [5/300], [150/484], step: 2570, 1.114 samples/sec, batch_loss: 0.0547, batch_loss_c: 0.0494, batch_loss_s: 0.0670, time:35.9123, lr:1e-06\u001b[0m\n",
            "2019-12-19 08:35:59 \u001b[32mINFO     \u001b[0m train.py: [5/300], [160/484], step: 2580, 1.075 samples/sec, batch_loss: 0.0858, batch_loss_c: 0.0883, batch_loss_s: 0.0801, time:37.1984, lr:1e-06\u001b[0m\n",
            "2019-12-19 08:36:26 \u001b[32mINFO     \u001b[0m train.py: [5/300], [170/484], step: 2590, 1.485 samples/sec, batch_loss: 0.0919, batch_loss_c: 0.0933, batch_loss_s: 0.0885, time:26.9388, lr:1e-06\u001b[0m\n",
            "2019-12-19 08:37:01 \u001b[32mINFO     \u001b[0m train.py: [5/300], [180/484], step: 2600, 1.150 samples/sec, batch_loss: 0.0411, batch_loss_c: 0.0348, batch_loss_s: 0.0558, time:34.7854, lr:1e-06\u001b[0m\n",
            "2019-12-19 08:37:28 \u001b[32mINFO     \u001b[0m train.py: [5/300], [190/484], step: 2610, 1.491 samples/sec, batch_loss: 0.0782, batch_loss_c: 0.0740, batch_loss_s: 0.0879, time:26.8286, lr:1e-06\u001b[0m\n",
            "2019-12-19 08:37:54 \u001b[32mINFO     \u001b[0m train.py: [5/300], [200/484], step: 2620, 1.494 samples/sec, batch_loss: 0.1844, batch_loss_c: 0.1797, batch_loss_s: 0.1952, time:26.7748, lr:1e-06\u001b[0m\n",
            "2019-12-19 08:38:23 \u001b[32mINFO     \u001b[0m train.py: [5/300], [210/484], step: 2630, 1.385 samples/sec, batch_loss: 0.2303, batch_loss_c: 0.2070, batch_loss_s: 0.2848, time:28.8747, lr:1e-06\u001b[0m\n",
            "2019-12-19 08:38:56 \u001b[32mINFO     \u001b[0m train.py: [5/300], [220/484], step: 2640, 1.236 samples/sec, batch_loss: 0.0779, batch_loss_c: 0.0765, batch_loss_s: 0.0812, time:32.3523, lr:1e-06\u001b[0m\n",
            "2019-12-19 08:39:18 \u001b[32mINFO     \u001b[0m train.py: [5/300], [230/484], step: 2650, 1.774 samples/sec, batch_loss: 0.1463, batch_loss_c: 0.1301, batch_loss_s: 0.1841, time:22.5529, lr:1e-06\u001b[0m\n",
            "2019-12-19 08:39:50 \u001b[32mINFO     \u001b[0m train.py: [5/300], [240/484], step: 2660, 1.271 samples/sec, batch_loss: 0.0740, batch_loss_c: 0.0664, batch_loss_s: 0.0920, time:31.4815, lr:1e-06\u001b[0m\n",
            "2019-12-19 08:40:15 \u001b[32mINFO     \u001b[0m train.py: [5/300], [250/484], step: 2670, 1.562 samples/sec, batch_loss: 0.0915, batch_loss_c: 0.0702, batch_loss_s: 0.1412, time:25.6151, lr:1e-06\u001b[0m\n",
            "2019-12-19 08:40:45 \u001b[32mINFO     \u001b[0m train.py: [5/300], [260/484], step: 2680, 1.365 samples/sec, batch_loss: 0.0417, batch_loss_c: 0.0335, batch_loss_s: 0.0607, time:29.3091, lr:1e-06\u001b[0m\n",
            "2019-12-19 08:41:06 \u001b[32mINFO     \u001b[0m train.py: [5/300], [270/484], step: 2690, 1.852 samples/sec, batch_loss: 0.2393, batch_loss_c: 0.2289, batch_loss_s: 0.2638, time:21.5946, lr:1e-06\u001b[0m\n",
            "2019-12-19 08:41:29 \u001b[32mINFO     \u001b[0m train.py: [5/300], [280/484], step: 2700, 1.747 samples/sec, batch_loss: 0.0521, batch_loss_c: 0.0483, batch_loss_s: 0.0612, time:22.8937, lr:1e-06\u001b[0m\n",
            "2019-12-19 08:41:58 \u001b[32mINFO     \u001b[0m train.py: [5/300], [290/484], step: 2710, 1.406 samples/sec, batch_loss: 0.0524, batch_loss_c: 0.0400, batch_loss_s: 0.0814, time:28.4397, lr:1e-06\u001b[0m\n",
            "2019-12-19 08:42:25 \u001b[32mINFO     \u001b[0m train.py: [5/300], [300/484], step: 2720, 1.475 samples/sec, batch_loss: 0.1000, batch_loss_c: 0.0983, batch_loss_s: 0.1038, time:27.1232, lr:1e-06\u001b[0m\n",
            "2019-12-19 08:42:57 \u001b[32mINFO     \u001b[0m train.py: [5/300], [310/484], step: 2730, 1.252 samples/sec, batch_loss: 0.1854, batch_loss_c: 0.1823, batch_loss_s: 0.1926, time:31.9496, lr:1e-06\u001b[0m\n",
            "2019-12-19 08:43:22 \u001b[32mINFO     \u001b[0m train.py: [5/300], [320/484], step: 2740, 1.548 samples/sec, batch_loss: 0.2085, batch_loss_c: 0.2464, batch_loss_s: 0.1201, time:25.8421, lr:1e-06\u001b[0m\n",
            "2019-12-19 08:43:54 \u001b[32mINFO     \u001b[0m train.py: [5/300], [330/484], step: 2750, 1.264 samples/sec, batch_loss: 0.1264, batch_loss_c: 0.1072, batch_loss_s: 0.1711, time:31.6453, lr:1e-06\u001b[0m\n",
            "2019-12-19 08:44:34 \u001b[32mINFO     \u001b[0m train.py: [5/300], [340/484], step: 2760, 0.999 samples/sec, batch_loss: 0.0801, batch_loss_c: 0.0701, batch_loss_s: 0.1034, time:40.0209, lr:1e-06\u001b[0m\n",
            "2019-12-19 08:44:57 \u001b[32mINFO     \u001b[0m train.py: [5/300], [350/484], step: 2770, 1.714 samples/sec, batch_loss: 0.0479, batch_loss_c: 0.0404, batch_loss_s: 0.0655, time:23.3410, lr:1e-06\u001b[0m\n",
            "2019-12-19 08:45:37 \u001b[32mINFO     \u001b[0m train.py: [5/300], [360/484], step: 2780, 1.005 samples/sec, batch_loss: 0.0492, batch_loss_c: 0.0432, batch_loss_s: 0.0634, time:39.8173, lr:1e-06\u001b[0m\n",
            "2019-12-19 08:46:00 \u001b[32mINFO     \u001b[0m train.py: [5/300], [370/484], step: 2790, 1.770 samples/sec, batch_loss: 0.0630, batch_loss_c: 0.0598, batch_loss_s: 0.0703, time:22.6003, lr:1e-06\u001b[0m\n",
            "2019-12-19 08:46:27 \u001b[32mINFO     \u001b[0m train.py: [5/300], [380/484], step: 2800, 1.477 samples/sec, batch_loss: 0.0619, batch_loss_c: 0.0471, batch_loss_s: 0.0964, time:27.0793, lr:1e-06\u001b[0m\n",
            "2019-12-19 08:46:54 \u001b[32mINFO     \u001b[0m train.py: [5/300], [390/484], step: 2810, 1.483 samples/sec, batch_loss: 0.0787, batch_loss_c: 0.0683, batch_loss_s: 0.1030, time:26.9636, lr:1e-06\u001b[0m\n",
            "2019-12-19 08:47:21 \u001b[32mINFO     \u001b[0m train.py: [5/300], [400/484], step: 2820, 1.458 samples/sec, batch_loss: 0.3094, batch_loss_c: 0.3097, batch_loss_s: 0.3087, time:27.4264, lr:1e-06\u001b[0m\n",
            "2019-12-19 08:47:56 \u001b[32mINFO     \u001b[0m train.py: [5/300], [410/484], step: 2830, 1.151 samples/sec, batch_loss: 0.1605, batch_loss_c: 0.1827, batch_loss_s: 0.1084, time:34.7576, lr:1e-06\u001b[0m\n",
            "2019-12-19 08:48:19 \u001b[32mINFO     \u001b[0m train.py: [5/300], [420/484], step: 2840, 1.733 samples/sec, batch_loss: 0.5241, batch_loss_c: 0.5194, batch_loss_s: 0.5350, time:23.0777, lr:1e-06\u001b[0m\n",
            "2019-12-19 08:48:48 \u001b[32mINFO     \u001b[0m train.py: [5/300], [430/484], step: 2850, 1.370 samples/sec, batch_loss: 0.0998, batch_loss_c: 0.0984, batch_loss_s: 0.1033, time:29.1871, lr:1e-06\u001b[0m\n",
            "2019-12-19 08:49:24 \u001b[32mINFO     \u001b[0m train.py: [5/300], [440/484], step: 2860, 1.124 samples/sec, batch_loss: 0.1013, batch_loss_c: 0.1002, batch_loss_s: 0.1037, time:35.5938, lr:1e-06\u001b[0m\n",
            "2019-12-19 08:49:53 \u001b[32mINFO     \u001b[0m train.py: [5/300], [450/484], step: 2870, 1.372 samples/sec, batch_loss: 0.0724, batch_loss_c: 0.0670, batch_loss_s: 0.0850, time:29.1584, lr:1e-06\u001b[0m\n",
            "2019-12-19 08:50:20 \u001b[32mINFO     \u001b[0m train.py: [5/300], [460/484], step: 2880, 1.505 samples/sec, batch_loss: 0.0955, batch_loss_c: 0.0838, batch_loss_s: 0.1226, time:26.5759, lr:1e-06\u001b[0m\n",
            "2019-12-19 08:50:42 \u001b[32mINFO     \u001b[0m train.py: [5/300], [470/484], step: 2890, 1.792 samples/sec, batch_loss: 0.3000, batch_loss_c: 0.2954, batch_loss_s: 0.3108, time:22.3224, lr:1e-06\u001b[0m\n",
            "2019-12-19 08:51:10 \u001b[32mINFO     \u001b[0m train.py: [5/300], [480/484], step: 2900, 1.438 samples/sec, batch_loss: 0.0718, batch_loss_c: 0.0683, batch_loss_s: 0.0800, time:27.8198, lr:1e-06\u001b[0m\n",
            "2019-12-19 08:51:16 \u001b[32mINFO     \u001b[0m train.py: [5/300], train_loss: 0.1404, time: 1410.2427, lr: 1e-06\u001b[0m\n",
            "2019-12-19 08:51:19 \u001b[32mINFO     \u001b[0m train.py: [6/300], [0/484], step: 2904, 18.243 samples/sec, batch_loss: 0.3070, batch_loss_c: 0.2986, batch_loss_s: 0.3267, time:2.1927, lr:1e-06\u001b[0m\n",
            "2019-12-19 08:51:43 \u001b[32mINFO     \u001b[0m train.py: [6/300], [10/484], step: 2914, 1.676 samples/sec, batch_loss: 0.1094, batch_loss_c: 0.1067, batch_loss_s: 0.1156, time:23.8695, lr:1e-06\u001b[0m\n",
            "2019-12-19 08:52:12 \u001b[32mINFO     \u001b[0m train.py: [6/300], [20/484], step: 2924, 1.388 samples/sec, batch_loss: 0.0583, batch_loss_c: 0.0522, batch_loss_s: 0.0726, time:28.8114, lr:1e-06\u001b[0m\n",
            "2019-12-19 08:52:51 \u001b[32mINFO     \u001b[0m train.py: [6/300], [30/484], step: 2934, 1.022 samples/sec, batch_loss: 0.1055, batch_loss_c: 0.0927, batch_loss_s: 0.1355, time:39.1408, lr:1e-06\u001b[0m\n",
            "2019-12-19 08:53:19 \u001b[32mINFO     \u001b[0m train.py: [6/300], [40/484], step: 2944, 1.421 samples/sec, batch_loss: 0.1721, batch_loss_c: 0.1557, batch_loss_s: 0.2103, time:28.1499, lr:1e-06\u001b[0m\n",
            "2019-12-19 08:53:55 \u001b[32mINFO     \u001b[0m train.py: [6/300], [50/484], step: 2954, 1.107 samples/sec, batch_loss: 0.2874, batch_loss_c: 0.2834, batch_loss_s: 0.2967, time:36.1329, lr:1e-06\u001b[0m\n",
            "2019-12-19 08:54:20 \u001b[32mINFO     \u001b[0m train.py: [6/300], [60/484], step: 2964, 1.622 samples/sec, batch_loss: 0.1093, batch_loss_c: 0.1000, batch_loss_s: 0.1311, time:24.6561, lr:1e-06\u001b[0m\n",
            "2019-12-19 08:54:46 \u001b[32mINFO     \u001b[0m train.py: [6/300], [70/484], step: 2974, 1.495 samples/sec, batch_loss: 0.0695, batch_loss_c: 0.0659, batch_loss_s: 0.0779, time:26.7524, lr:1e-06\u001b[0m\n",
            "2019-12-19 08:55:11 \u001b[32mINFO     \u001b[0m train.py: [6/300], [80/484], step: 2984, 1.642 samples/sec, batch_loss: 0.0495, batch_loss_c: 0.0383, batch_loss_s: 0.0757, time:24.3562, lr:1e-06\u001b[0m\n",
            "2019-12-19 08:55:42 \u001b[32mINFO     \u001b[0m train.py: [6/300], [90/484], step: 2994, 1.294 samples/sec, batch_loss: 0.1576, batch_loss_c: 0.1394, batch_loss_s: 0.2001, time:30.9205, lr:1e-06\u001b[0m\n",
            "2019-12-19 08:56:25 \u001b[32mINFO     \u001b[0m train.py: [6/300], [100/484], step: 3004, 0.927 samples/sec, batch_loss: 0.2853, batch_loss_c: 0.2814, batch_loss_s: 0.2944, time:43.1300, lr:1e-06\u001b[0m\n",
            "2019-12-19 08:56:49 \u001b[32mINFO     \u001b[0m train.py: [6/300], [110/484], step: 3014, 1.637 samples/sec, batch_loss: 0.0732, batch_loss_c: 0.0587, batch_loss_s: 0.1070, time:24.4387, lr:1e-06\u001b[0m\n",
            "2019-12-19 08:57:22 \u001b[32mINFO     \u001b[0m train.py: [6/300], [120/484], step: 3024, 1.220 samples/sec, batch_loss: 0.0565, batch_loss_c: 0.0431, batch_loss_s: 0.0877, time:32.7999, lr:1e-06\u001b[0m\n",
            "2019-12-19 08:57:52 \u001b[32mINFO     \u001b[0m train.py: [6/300], [130/484], step: 3034, 1.339 samples/sec, batch_loss: 0.0751, batch_loss_c: 0.0655, batch_loss_s: 0.0975, time:29.8688, lr:1e-06\u001b[0m\n",
            "2019-12-19 08:58:16 \u001b[32mINFO     \u001b[0m train.py: [6/300], [140/484], step: 3044, 1.667 samples/sec, batch_loss: 0.1024, batch_loss_c: 0.0997, batch_loss_s: 0.1087, time:23.9885, lr:1e-06\u001b[0m\n",
            "2019-12-19 08:58:37 \u001b[32mINFO     \u001b[0m train.py: [6/300], [150/484], step: 3054, 1.901 samples/sec, batch_loss: 0.0651, batch_loss_c: 0.0581, batch_loss_s: 0.0816, time:21.0396, lr:1e-06\u001b[0m\n",
            "2019-12-19 08:59:01 \u001b[32mINFO     \u001b[0m train.py: [6/300], [160/484], step: 3064, 1.674 samples/sec, batch_loss: 0.0546, batch_loss_c: 0.0406, batch_loss_s: 0.0875, time:23.8935, lr:1e-06\u001b[0m\n",
            "2019-12-19 08:59:35 \u001b[32mINFO     \u001b[0m train.py: [6/300], [170/484], step: 3074, 1.189 samples/sec, batch_loss: 0.3209, batch_loss_c: 0.3195, batch_loss_s: 0.3242, time:33.6479, lr:1e-06\u001b[0m\n",
            "2019-12-19 09:00:03 \u001b[32mINFO     \u001b[0m train.py: [6/300], [180/484], step: 3084, 1.409 samples/sec, batch_loss: 0.0496, batch_loss_c: 0.0421, batch_loss_s: 0.0670, time:28.3971, lr:1e-06\u001b[0m\n",
            "2019-12-19 09:00:28 \u001b[32mINFO     \u001b[0m train.py: [6/300], [190/484], step: 3094, 1.587 samples/sec, batch_loss: 0.0362, batch_loss_c: 0.0296, batch_loss_s: 0.0516, time:25.2055, lr:1e-06\u001b[0m\n",
            "2019-12-19 09:00:57 \u001b[32mINFO     \u001b[0m train.py: [6/300], [200/484], step: 3104, 1.397 samples/sec, batch_loss: 0.0560, batch_loss_c: 0.0587, batch_loss_s: 0.0495, time:28.6312, lr:1e-06\u001b[0m\n",
            "2019-12-19 09:01:29 \u001b[32mINFO     \u001b[0m train.py: [6/300], [210/484], step: 3114, 1.226 samples/sec, batch_loss: 0.0679, batch_loss_c: 0.0614, batch_loss_s: 0.0832, time:32.6175, lr:1e-06\u001b[0m\n",
            "2019-12-19 09:01:56 \u001b[32mINFO     \u001b[0m train.py: [6/300], [220/484], step: 3124, 1.501 samples/sec, batch_loss: 0.0766, batch_loss_c: 0.0710, batch_loss_s: 0.0898, time:26.6408, lr:1e-06\u001b[0m\n",
            "2019-12-19 09:02:26 \u001b[32mINFO     \u001b[0m train.py: [6/300], [230/484], step: 3134, 1.337 samples/sec, batch_loss: 0.3190, batch_loss_c: 0.3119, batch_loss_s: 0.3355, time:29.9172, lr:1e-06\u001b[0m\n",
            "2019-12-19 09:03:01 \u001b[32mINFO     \u001b[0m train.py: [6/300], [240/484], step: 3144, 1.149 samples/sec, batch_loss: 0.0649, batch_loss_c: 0.0508, batch_loss_s: 0.0979, time:34.8156, lr:1e-06\u001b[0m\n",
            "2019-12-19 09:03:25 \u001b[32mINFO     \u001b[0m train.py: [6/300], [250/484], step: 3154, 1.677 samples/sec, batch_loss: 0.0718, batch_loss_c: 0.0648, batch_loss_s: 0.0880, time:23.8524, lr:1e-06\u001b[0m\n",
            "2019-12-19 09:03:59 \u001b[32mINFO     \u001b[0m train.py: [6/300], [260/484], step: 3164, 1.180 samples/sec, batch_loss: 0.0989, batch_loss_c: 0.0941, batch_loss_s: 0.1101, time:33.9002, lr:1e-06\u001b[0m\n",
            "2019-12-19 09:04:21 \u001b[32mINFO     \u001b[0m train.py: [6/300], [270/484], step: 3174, 1.815 samples/sec, batch_loss: 0.0533, batch_loss_c: 0.0459, batch_loss_s: 0.0706, time:22.0407, lr:1e-06\u001b[0m\n",
            "2019-12-19 09:04:48 \u001b[32mINFO     \u001b[0m train.py: [6/300], [280/484], step: 3184, 1.465 samples/sec, batch_loss: 0.2815, batch_loss_c: 0.2780, batch_loss_s: 0.2897, time:27.3017, lr:1e-06\u001b[0m\n",
            "2019-12-19 09:05:11 \u001b[32mINFO     \u001b[0m train.py: [6/300], [290/484], step: 3194, 1.724 samples/sec, batch_loss: 0.1237, batch_loss_c: 0.1280, batch_loss_s: 0.1136, time:23.1964, lr:1e-06\u001b[0m\n",
            "2019-12-19 09:05:44 \u001b[32mINFO     \u001b[0m train.py: [6/300], [300/484], step: 3204, 1.217 samples/sec, batch_loss: 0.3356, batch_loss_c: 0.3363, batch_loss_s: 0.3339, time:32.8707, lr:1e-06\u001b[0m\n",
            "2019-12-19 09:06:13 \u001b[32mINFO     \u001b[0m train.py: [6/300], [310/484], step: 3214, 1.378 samples/sec, batch_loss: 0.1384, batch_loss_c: 0.1639, batch_loss_s: 0.0787, time:29.0217, lr:1e-06\u001b[0m\n",
            "2019-12-19 09:06:38 \u001b[32mINFO     \u001b[0m train.py: [6/300], [320/484], step: 3224, 1.602 samples/sec, batch_loss: 0.1967, batch_loss_c: 0.1589, batch_loss_s: 0.2847, time:24.9633, lr:1e-06\u001b[0m\n",
            "2019-12-19 09:07:27 \u001b[32mINFO     \u001b[0m train.py: [6/300], [330/484], step: 3234, 0.809 samples/sec, batch_loss: 0.7851, batch_loss_c: 0.7825, batch_loss_s: 0.7910, time:49.4515, lr:1e-06\u001b[0m\n",
            "2019-12-19 09:08:06 \u001b[32mINFO     \u001b[0m train.py: [6/300], [340/484], step: 3244, 1.039 samples/sec, batch_loss: 0.0348, batch_loss_c: 0.0282, batch_loss_s: 0.0503, time:38.4811, lr:1e-06\u001b[0m\n",
            "2019-12-19 09:08:43 \u001b[32mINFO     \u001b[0m train.py: [6/300], [350/484], step: 3254, 1.077 samples/sec, batch_loss: 0.0553, batch_loss_c: 0.0449, batch_loss_s: 0.0795, time:37.1401, lr:1e-06\u001b[0m\n",
            "2019-12-19 09:09:09 \u001b[32mINFO     \u001b[0m train.py: [6/300], [360/484], step: 3264, 1.537 samples/sec, batch_loss: 0.0528, batch_loss_c: 0.0428, batch_loss_s: 0.0761, time:26.0300, lr:1e-06\u001b[0m\n",
            "2019-12-19 09:09:34 \u001b[32mINFO     \u001b[0m train.py: [6/300], [370/484], step: 3274, 1.598 samples/sec, batch_loss: 0.3144, batch_loss_c: 0.3065, batch_loss_s: 0.3326, time:25.0376, lr:1e-06\u001b[0m\n",
            "2019-12-19 09:10:17 \u001b[32mINFO     \u001b[0m train.py: [6/300], [380/484], step: 3284, 0.940 samples/sec, batch_loss: 0.3109, batch_loss_c: 0.3081, batch_loss_s: 0.3176, time:42.5340, lr:1e-06\u001b[0m\n",
            "2019-12-19 09:10:42 \u001b[32mINFO     \u001b[0m train.py: [6/300], [390/484], step: 3294, 1.576 samples/sec, batch_loss: 0.0925, batch_loss_c: 0.0931, batch_loss_s: 0.0912, time:25.3812, lr:1e-06\u001b[0m\n",
            "2019-12-19 09:11:07 \u001b[32mINFO     \u001b[0m train.py: [6/300], [400/484], step: 3304, 1.606 samples/sec, batch_loss: 0.0651, batch_loss_c: 0.0602, batch_loss_s: 0.0765, time:24.9078, lr:1e-06\u001b[0m\n",
            "2019-12-19 09:11:34 \u001b[32mINFO     \u001b[0m train.py: [6/300], [410/484], step: 3314, 1.498 samples/sec, batch_loss: 0.0477, batch_loss_c: 0.0453, batch_loss_s: 0.0533, time:26.6935, lr:1e-06\u001b[0m\n",
            "2019-12-19 09:12:01 \u001b[32mINFO     \u001b[0m train.py: [6/300], [420/484], step: 3324, 1.470 samples/sec, batch_loss: 0.0687, batch_loss_c: 0.0604, batch_loss_s: 0.0880, time:27.2061, lr:1e-06\u001b[0m\n",
            "2019-12-19 09:12:28 \u001b[32mINFO     \u001b[0m train.py: [6/300], [430/484], step: 3334, 1.462 samples/sec, batch_loss: 0.1139, batch_loss_c: 0.1091, batch_loss_s: 0.1249, time:27.3527, lr:1e-06\u001b[0m\n",
            "2019-12-19 09:12:52 \u001b[32mINFO     \u001b[0m train.py: [6/300], [440/484], step: 3344, 1.665 samples/sec, batch_loss: 0.5296, batch_loss_c: 0.5263, batch_loss_s: 0.5373, time:24.0303, lr:1e-06\u001b[0m\n",
            "2019-12-19 09:13:21 \u001b[32mINFO     \u001b[0m train.py: [6/300], [450/484], step: 3354, 1.369 samples/sec, batch_loss: 0.0421, batch_loss_c: 0.0351, batch_loss_s: 0.0586, time:29.2151, lr:1e-06\u001b[0m\n",
            "2019-12-19 09:13:45 \u001b[32mINFO     \u001b[0m train.py: [6/300], [460/484], step: 3364, 1.724 samples/sec, batch_loss: 0.1039, batch_loss_c: 0.1070, batch_loss_s: 0.0967, time:23.1959, lr:1e-06\u001b[0m\n",
            "2019-12-19 09:14:13 \u001b[32mINFO     \u001b[0m train.py: [6/300], [470/484], step: 3374, 1.415 samples/sec, batch_loss: 0.0453, batch_loss_c: 0.0387, batch_loss_s: 0.0607, time:28.2737, lr:1e-06\u001b[0m\n",
            "2019-12-19 09:14:40 \u001b[32mINFO     \u001b[0m train.py: [6/300], [480/484], step: 3384, 1.494 samples/sec, batch_loss: 0.0506, batch_loss_c: 0.0431, batch_loss_s: 0.0680, time:26.7670, lr:1e-06\u001b[0m\n",
            "2019-12-19 09:14:46 \u001b[32mINFO     \u001b[0m train.py: [6/300], train_loss: 0.1453, time: 1409.5721, lr: 1e-06\u001b[0m\n",
            "2019-12-19 09:14:49 \u001b[32mINFO     \u001b[0m train.py: [7/300], [0/484], step: 3388, 18.724 samples/sec, batch_loss: 0.0449, batch_loss_c: 0.0417, batch_loss_s: 0.0523, time:2.1362, lr:1e-06\u001b[0m\n",
            "2019-12-19 09:15:20 \u001b[32mINFO     \u001b[0m train.py: [7/300], [10/484], step: 3398, 1.305 samples/sec, batch_loss: 0.0545, batch_loss_c: 0.0490, batch_loss_s: 0.0674, time:30.6479, lr:1e-06\u001b[0m\n",
            "2019-12-19 09:15:47 \u001b[32mINFO     \u001b[0m train.py: [7/300], [20/484], step: 3408, 1.454 samples/sec, batch_loss: 0.2920, batch_loss_c: 0.2884, batch_loss_s: 0.3004, time:27.5116, lr:1e-06\u001b[0m\n",
            "2019-12-19 09:16:27 \u001b[32mINFO     \u001b[0m train.py: [7/300], [30/484], step: 3418, 1.004 samples/sec, batch_loss: 0.0565, batch_loss_c: 0.0435, batch_loss_s: 0.0869, time:39.8322, lr:1e-06\u001b[0m\n",
            "2019-12-19 09:16:51 \u001b[32mINFO     \u001b[0m train.py: [7/300], [40/484], step: 3428, 1.690 samples/sec, batch_loss: 0.2975, batch_loss_c: 0.2893, batch_loss_s: 0.3168, time:23.6724, lr:1e-06\u001b[0m\n",
            "2019-12-19 09:17:25 \u001b[32mINFO     \u001b[0m train.py: [7/300], [50/484], step: 3438, 1.178 samples/sec, batch_loss: 0.3606, batch_loss_c: 0.3719, batch_loss_s: 0.3343, time:33.9418, lr:1e-06\u001b[0m\n",
            "2019-12-19 09:17:53 \u001b[32mINFO     \u001b[0m train.py: [7/300], [60/484], step: 3448, 1.402 samples/sec, batch_loss: 0.1149, batch_loss_c: 0.1088, batch_loss_s: 0.1292, time:28.5246, lr:1e-06\u001b[0m\n",
            "2019-12-19 09:18:23 \u001b[32mINFO     \u001b[0m train.py: [7/300], [70/484], step: 3458, 1.317 samples/sec, batch_loss: 0.0383, batch_loss_c: 0.0314, batch_loss_s: 0.0546, time:30.3827, lr:1e-06\u001b[0m\n",
            "2019-12-19 09:18:54 \u001b[32mINFO     \u001b[0m train.py: [7/300], [80/484], step: 3468, 1.314 samples/sec, batch_loss: 0.0807, batch_loss_c: 0.0796, batch_loss_s: 0.0834, time:30.4332, lr:1e-06\u001b[0m\n",
            "2019-12-19 09:19:20 \u001b[32mINFO     \u001b[0m train.py: [7/300], [90/484], step: 3478, 1.556 samples/sec, batch_loss: 0.0543, batch_loss_c: 0.0468, batch_loss_s: 0.0719, time:25.7097, lr:1e-06\u001b[0m\n",
            "2019-12-19 09:19:43 \u001b[32mINFO     \u001b[0m train.py: [7/300], [100/484], step: 3488, 1.701 samples/sec, batch_loss: 0.0961, batch_loss_c: 0.0981, batch_loss_s: 0.0915, time:23.5096, lr:1e-06\u001b[0m\n",
            "2019-12-19 09:20:21 \u001b[32mINFO     \u001b[0m train.py: [7/300], [110/484], step: 3498, 1.069 samples/sec, batch_loss: 0.0677, batch_loss_c: 0.0683, batch_loss_s: 0.0662, time:37.4105, lr:1e-06\u001b[0m\n",
            "2019-12-19 09:20:44 \u001b[32mINFO     \u001b[0m train.py: [7/300], [120/484], step: 3508, 1.682 samples/sec, batch_loss: 0.0606, batch_loss_c: 0.0589, batch_loss_s: 0.0648, time:23.7833, lr:1e-06\u001b[0m\n",
            "2019-12-19 09:21:28 \u001b[32mINFO     \u001b[0m train.py: [7/300], [130/484], step: 3518, 0.908 samples/sec, batch_loss: 0.0554, batch_loss_c: 0.0476, batch_loss_s: 0.0738, time:44.0534, lr:1e-06\u001b[0m\n",
            "2019-12-19 09:22:02 \u001b[32mINFO     \u001b[0m train.py: [7/300], [140/484], step: 3528, 1.176 samples/sec, batch_loss: 0.1572, batch_loss_c: 0.1464, batch_loss_s: 0.1822, time:34.0121, lr:1e-06\u001b[0m\n",
            "2019-12-19 09:22:26 \u001b[32mINFO     \u001b[0m train.py: [7/300], [150/484], step: 3538, 1.661 samples/sec, batch_loss: 0.2989, batch_loss_c: 0.3006, batch_loss_s: 0.2948, time:24.0770, lr:1e-06\u001b[0m\n",
            "2019-12-19 09:22:52 \u001b[32mINFO     \u001b[0m train.py: [7/300], [160/484], step: 3548, 1.574 samples/sec, batch_loss: 0.0716, batch_loss_c: 0.0777, batch_loss_s: 0.0574, time:25.4177, lr:1e-06\u001b[0m\n",
            "2019-12-19 09:23:17 \u001b[32mINFO     \u001b[0m train.py: [7/300], [170/484], step: 3558, 1.565 samples/sec, batch_loss: 0.0369, batch_loss_c: 0.0303, batch_loss_s: 0.0521, time:25.5563, lr:1e-06\u001b[0m\n",
            "2019-12-19 09:23:45 \u001b[32mINFO     \u001b[0m train.py: [7/300], [180/484], step: 3568, 1.460 samples/sec, batch_loss: 0.2787, batch_loss_c: 0.2717, batch_loss_s: 0.2950, time:27.3923, lr:1e-06\u001b[0m\n",
            "2019-12-19 09:24:06 \u001b[32mINFO     \u001b[0m train.py: [7/300], [190/484], step: 3578, 1.857 samples/sec, batch_loss: 0.1354, batch_loss_c: 0.1423, batch_loss_s: 0.1193, time:21.5360, lr:1e-06\u001b[0m\n",
            "2019-12-19 09:24:31 \u001b[32mINFO     \u001b[0m train.py: [7/300], [200/484], step: 3588, 1.654 samples/sec, batch_loss: 0.1399, batch_loss_c: 0.1511, batch_loss_s: 0.1139, time:24.1827, lr:1e-06\u001b[0m\n",
            "2019-12-19 09:25:10 \u001b[32mINFO     \u001b[0m train.py: [7/300], [210/484], step: 3598, 1.005 samples/sec, batch_loss: 0.1735, batch_loss_c: 0.1168, batch_loss_s: 0.3058, time:39.7966, lr:1e-06\u001b[0m\n",
            "2019-12-19 09:25:43 \u001b[32mINFO     \u001b[0m train.py: [7/300], [220/484], step: 3608, 1.243 samples/sec, batch_loss: 0.2965, batch_loss_c: 0.2916, batch_loss_s: 0.3079, time:32.1754, lr:1e-06\u001b[0m\n",
            "2019-12-19 09:26:20 \u001b[32mINFO     \u001b[0m train.py: [7/300], [230/484], step: 3618, 1.068 samples/sec, batch_loss: 0.0676, batch_loss_c: 0.0572, batch_loss_s: 0.0917, time:37.4596, lr:1e-06\u001b[0m\n",
            "2019-12-19 09:26:44 \u001b[32mINFO     \u001b[0m train.py: [7/300], [240/484], step: 3628, 1.650 samples/sec, batch_loss: 0.1784, batch_loss_c: 0.1884, batch_loss_s: 0.1551, time:24.2491, lr:1e-06\u001b[0m\n",
            "2019-12-19 09:27:10 \u001b[32mINFO     \u001b[0m train.py: [7/300], [250/484], step: 3638, 1.573 samples/sec, batch_loss: 0.1480, batch_loss_c: 0.1446, batch_loss_s: 0.1562, time:25.4278, lr:1e-06\u001b[0m\n",
            "2019-12-19 09:27:39 \u001b[32mINFO     \u001b[0m train.py: [7/300], [260/484], step: 3648, 1.387 samples/sec, batch_loss: 0.5359, batch_loss_c: 0.5335, batch_loss_s: 0.5417, time:28.8334, lr:1e-06\u001b[0m\n",
            "2019-12-19 09:28:12 \u001b[32mINFO     \u001b[0m train.py: [7/300], [270/484], step: 3658, 1.203 samples/sec, batch_loss: 0.0763, batch_loss_c: 0.0636, batch_loss_s: 0.1059, time:33.2606, lr:1e-06\u001b[0m\n",
            "2019-12-19 09:28:50 \u001b[32mINFO     \u001b[0m train.py: [7/300], [280/484], step: 3668, 1.060 samples/sec, batch_loss: 0.0765, batch_loss_c: 0.0712, batch_loss_s: 0.0890, time:37.7483, lr:1e-06\u001b[0m\n",
            "2019-12-19 09:29:21 \u001b[32mINFO     \u001b[0m train.py: [7/300], [290/484], step: 3678, 1.265 samples/sec, batch_loss: 0.0775, batch_loss_c: 0.0707, batch_loss_s: 0.0932, time:31.6103, lr:1e-06\u001b[0m\n",
            "2019-12-19 09:29:51 \u001b[32mINFO     \u001b[0m train.py: [7/300], [300/484], step: 3688, 1.331 samples/sec, batch_loss: 0.3178, batch_loss_c: 0.3049, batch_loss_s: 0.3480, time:30.0416, lr:1e-06\u001b[0m\n",
            "2019-12-19 09:30:31 \u001b[32mINFO     \u001b[0m train.py: [7/300], [310/484], step: 3698, 1.006 samples/sec, batch_loss: 0.0815, batch_loss_c: 0.0720, batch_loss_s: 0.1036, time:39.7586, lr:1e-06\u001b[0m\n",
            "2019-12-19 09:31:07 \u001b[32mINFO     \u001b[0m train.py: [7/300], [320/484], step: 3708, 1.116 samples/sec, batch_loss: 0.2838, batch_loss_c: 0.2772, batch_loss_s: 0.2992, time:35.8535, lr:1e-06\u001b[0m\n",
            "2019-12-19 09:31:44 \u001b[32mINFO     \u001b[0m train.py: [7/300], [330/484], step: 3718, 1.068 samples/sec, batch_loss: 0.0434, batch_loss_c: 0.0367, batch_loss_s: 0.0589, time:37.4679, lr:1e-06\u001b[0m\n",
            "2019-12-19 09:32:16 \u001b[32mINFO     \u001b[0m train.py: [7/300], [340/484], step: 3728, 1.255 samples/sec, batch_loss: 0.0674, batch_loss_c: 0.0585, batch_loss_s: 0.0882, time:31.8724, lr:1e-06\u001b[0m\n",
            "2019-12-19 09:32:44 \u001b[32mINFO     \u001b[0m train.py: [7/300], [350/484], step: 3738, 1.429 samples/sec, batch_loss: 0.3744, batch_loss_c: 0.3721, batch_loss_s: 0.3799, time:27.9867, lr:1e-06\u001b[0m\n",
            "2019-12-19 09:33:06 \u001b[32mINFO     \u001b[0m train.py: [7/300], [360/484], step: 3748, 1.832 samples/sec, batch_loss: 0.0507, batch_loss_c: 0.0439, batch_loss_s: 0.0665, time:21.8357, lr:1e-06\u001b[0m\n",
            "2019-12-19 09:33:41 \u001b[32mINFO     \u001b[0m train.py: [7/300], [370/484], step: 3758, 1.143 samples/sec, batch_loss: 0.3086, batch_loss_c: 0.3054, batch_loss_s: 0.3161, time:34.9837, lr:1e-06\u001b[0m\n",
            "2019-12-19 09:34:14 \u001b[32mINFO     \u001b[0m train.py: [7/300], [380/484], step: 3768, 1.226 samples/sec, batch_loss: 0.2154, batch_loss_c: 0.2011, batch_loss_s: 0.2485, time:32.6375, lr:1e-06\u001b[0m\n",
            "2019-12-19 09:34:39 \u001b[32mINFO     \u001b[0m train.py: [7/300], [390/484], step: 3778, 1.553 samples/sec, batch_loss: 0.0668, batch_loss_c: 0.0556, batch_loss_s: 0.0931, time:25.7534, lr:1e-06\u001b[0m\n",
            "2019-12-19 09:35:02 \u001b[32mINFO     \u001b[0m train.py: [7/300], [400/484], step: 3788, 1.738 samples/sec, batch_loss: 0.0644, batch_loss_c: 0.0551, batch_loss_s: 0.0861, time:23.0105, lr:1e-06\u001b[0m\n",
            "2019-12-19 09:35:29 \u001b[32mINFO     \u001b[0m train.py: [7/300], [410/484], step: 3798, 1.491 samples/sec, batch_loss: 0.1246, batch_loss_c: 0.1196, batch_loss_s: 0.1365, time:26.8292, lr:1e-06\u001b[0m\n",
            "2019-12-19 09:36:00 \u001b[32mINFO     \u001b[0m train.py: [7/300], [420/484], step: 3808, 1.295 samples/sec, batch_loss: 0.0763, batch_loss_c: 0.0641, batch_loss_s: 0.1048, time:30.8932, lr:1e-06\u001b[0m\n",
            "2019-12-19 09:36:24 \u001b[32mINFO     \u001b[0m train.py: [7/300], [430/484], step: 3818, 1.674 samples/sec, batch_loss: 0.3150, batch_loss_c: 0.2915, batch_loss_s: 0.3697, time:23.8931, lr:1e-06\u001b[0m\n",
            "2019-12-19 09:36:50 \u001b[32mINFO     \u001b[0m train.py: [7/300], [440/484], step: 3828, 1.509 samples/sec, batch_loss: 0.2044, batch_loss_c: 0.2414, batch_loss_s: 0.1179, time:26.5061, lr:1e-06\u001b[0m\n",
            "2019-12-19 09:37:15 \u001b[32mINFO     \u001b[0m train.py: [7/300], [450/484], step: 3838, 1.661 samples/sec, batch_loss: 0.0609, batch_loss_c: 0.0583, batch_loss_s: 0.0669, time:24.0832, lr:1e-06\u001b[0m\n",
            "2019-12-19 09:37:39 \u001b[32mINFO     \u001b[0m train.py: [7/300], [460/484], step: 3848, 1.641 samples/sec, batch_loss: 0.0974, batch_loss_c: 0.0929, batch_loss_s: 0.1078, time:24.3775, lr:1e-06\u001b[0m\n",
            "2019-12-19 09:38:06 \u001b[32mINFO     \u001b[0m train.py: [7/300], [470/484], step: 3858, 1.490 samples/sec, batch_loss: 0.3069, batch_loss_c: 0.3082, batch_loss_s: 0.3040, time:26.8379, lr:1e-06\u001b[0m\n",
            "2019-12-19 09:38:39 \u001b[32mINFO     \u001b[0m train.py: [7/300], [480/484], step: 3868, 1.211 samples/sec, batch_loss: 0.0835, batch_loss_c: 0.0731, batch_loss_s: 0.1078, time:33.0253, lr:1e-06\u001b[0m\n",
            "2019-12-19 09:38:50 \u001b[32mINFO     \u001b[0m train.py: [7/300], train_loss: 0.1443, time: 1442.6668, lr: 1e-06\u001b[0m\n",
            "2019-12-19 09:38:52 \u001b[32mINFO     \u001b[0m train.py: [8/300], [0/484], step: 3872, 19.316 samples/sec, batch_loss: 0.1017, batch_loss_c: 0.0991, batch_loss_s: 0.1076, time:2.0708, lr:1e-06\u001b[0m\n",
            "2019-12-19 09:39:21 \u001b[32mINFO     \u001b[0m train.py: [8/300], [10/484], step: 3882, 1.387 samples/sec, batch_loss: 0.5443, batch_loss_c: 0.5348, batch_loss_s: 0.5666, time:28.8349, lr:1e-06\u001b[0m\n",
            "2019-12-19 09:40:03 \u001b[32mINFO     \u001b[0m train.py: [8/300], [20/484], step: 3892, 0.960 samples/sec, batch_loss: 0.1342, batch_loss_c: 0.1198, batch_loss_s: 0.1680, time:41.6640, lr:1e-06\u001b[0m\n",
            "2019-12-19 09:40:27 \u001b[32mINFO     \u001b[0m train.py: [8/300], [30/484], step: 3902, 1.646 samples/sec, batch_loss: 0.5081, batch_loss_c: 0.4921, batch_loss_s: 0.5457, time:24.3073, lr:1e-06\u001b[0m\n",
            "2019-12-19 09:40:50 \u001b[32mINFO     \u001b[0m train.py: [8/300], [40/484], step: 3912, 1.759 samples/sec, batch_loss: 0.0451, batch_loss_c: 0.0348, batch_loss_s: 0.0691, time:22.7418, lr:1e-06\u001b[0m\n",
            "2019-12-19 09:41:52 \u001b[32mINFO     \u001b[0m train.py: [8/300], [50/484], step: 3922, 0.640 samples/sec, batch_loss: 0.0644, batch_loss_c: 0.0639, batch_loss_s: 0.0656, time:62.4531, lr:1e-06\u001b[0m\n",
            "2019-12-19 09:42:16 \u001b[32mINFO     \u001b[0m train.py: [8/300], [60/484], step: 3932, 1.651 samples/sec, batch_loss: 0.2415, batch_loss_c: 0.2200, batch_loss_s: 0.2916, time:24.2231, lr:1e-06\u001b[0m\n",
            "2019-12-19 09:42:44 \u001b[32mINFO     \u001b[0m train.py: [8/300], [70/484], step: 3942, 1.464 samples/sec, batch_loss: 0.0886, batch_loss_c: 0.0798, batch_loss_s: 0.1093, time:27.3155, lr:1e-06\u001b[0m\n",
            "2019-12-19 09:43:11 \u001b[32mINFO     \u001b[0m train.py: [8/300], [80/484], step: 3952, 1.464 samples/sec, batch_loss: 0.0904, batch_loss_c: 0.0798, batch_loss_s: 0.1151, time:27.3296, lr:1e-06\u001b[0m\n",
            "2019-12-19 09:43:35 \u001b[32mINFO     \u001b[0m train.py: [8/300], [90/484], step: 3962, 1.685 samples/sec, batch_loss: 0.0460, batch_loss_c: 0.0397, batch_loss_s: 0.0607, time:23.7361, lr:1e-06\u001b[0m\n",
            "2019-12-19 09:44:06 \u001b[32mINFO     \u001b[0m train.py: [8/300], [100/484], step: 3972, 1.298 samples/sec, batch_loss: 0.0499, batch_loss_c: 0.0391, batch_loss_s: 0.0752, time:30.8229, lr:1e-06\u001b[0m\n",
            "2019-12-19 09:44:29 \u001b[32mINFO     \u001b[0m train.py: [8/300], [110/484], step: 3982, 1.693 samples/sec, batch_loss: 0.1583, batch_loss_c: 0.1801, batch_loss_s: 0.1072, time:23.6328, lr:1e-06\u001b[0m\n",
            "2019-12-19 09:45:08 \u001b[32mINFO     \u001b[0m train.py: [8/300], [120/484], step: 3992, 1.030 samples/sec, batch_loss: 0.5275, batch_loss_c: 0.5252, batch_loss_s: 0.5331, time:38.8437, lr:1e-06\u001b[0m\n",
            "2019-12-19 09:45:39 \u001b[32mINFO     \u001b[0m train.py: [8/300], [130/484], step: 4002, 1.290 samples/sec, batch_loss: 0.1061, batch_loss_c: 0.0986, batch_loss_s: 0.1236, time:31.0105, lr:1e-06\u001b[0m\n",
            "2019-12-19 09:46:12 \u001b[32mINFO     \u001b[0m train.py: [8/300], [140/484], step: 4012, 1.199 samples/sec, batch_loss: 0.0528, batch_loss_c: 0.0493, batch_loss_s: 0.0608, time:33.3547, lr:1e-06\u001b[0m\n",
            "2019-12-19 09:46:40 \u001b[32mINFO     \u001b[0m train.py: [8/300], [150/484], step: 4022, 1.477 samples/sec, batch_loss: 0.1016, batch_loss_c: 0.1136, batch_loss_s: 0.0735, time:27.0895, lr:1e-06\u001b[0m\n",
            "2019-12-19 09:47:03 \u001b[32mINFO     \u001b[0m train.py: [8/300], [160/484], step: 4032, 1.680 samples/sec, batch_loss: 0.0515, batch_loss_c: 0.0429, batch_loss_s: 0.0715, time:23.8141, lr:1e-06\u001b[0m\n",
            "2019-12-19 09:47:26 \u001b[32mINFO     \u001b[0m train.py: [8/300], [170/484], step: 4042, 1.790 samples/sec, batch_loss: 0.1031, batch_loss_c: 0.1071, batch_loss_s: 0.0939, time:22.3466, lr:1e-06\u001b[0m\n",
            "2019-12-19 09:47:56 \u001b[32mINFO     \u001b[0m train.py: [8/300], [180/484], step: 4052, 1.328 samples/sec, batch_loss: 0.0501, batch_loss_c: 0.0472, batch_loss_s: 0.0568, time:30.1298, lr:1e-06\u001b[0m\n",
            "2019-12-19 09:48:22 \u001b[32mINFO     \u001b[0m train.py: [8/300], [190/484], step: 4062, 1.512 samples/sec, batch_loss: 0.1051, batch_loss_c: 0.1036, batch_loss_s: 0.1087, time:26.4547, lr:1e-06\u001b[0m\n",
            "2019-12-19 09:48:54 \u001b[32mINFO     \u001b[0m train.py: [8/300], [200/484], step: 4072, 1.243 samples/sec, batch_loss: 0.0589, batch_loss_c: 0.0506, batch_loss_s: 0.0782, time:32.1854, lr:1e-06\u001b[0m\n",
            "2019-12-19 09:49:30 \u001b[32mINFO     \u001b[0m train.py: [8/300], [210/484], step: 4082, 1.118 samples/sec, batch_loss: 0.0824, batch_loss_c: 0.0612, batch_loss_s: 0.1320, time:35.7659, lr:1e-06\u001b[0m\n",
            "2019-12-19 09:49:54 \u001b[32mINFO     \u001b[0m train.py: [8/300], [220/484], step: 4092, 1.698 samples/sec, batch_loss: 0.0604, batch_loss_c: 0.0565, batch_loss_s: 0.0696, time:23.5543, lr:1e-06\u001b[0m\n",
            "2019-12-19 09:50:20 \u001b[32mINFO     \u001b[0m train.py: [8/300], [230/484], step: 4102, 1.546 samples/sec, batch_loss: 0.0502, batch_loss_c: 0.0441, batch_loss_s: 0.0646, time:25.8707, lr:1e-06\u001b[0m\n",
            "2019-12-19 09:50:42 \u001b[32mINFO     \u001b[0m train.py: [8/300], [240/484], step: 4112, 1.777 samples/sec, batch_loss: 0.1062, batch_loss_c: 0.1003, batch_loss_s: 0.1199, time:22.5069, lr:1e-06\u001b[0m\n",
            "2019-12-19 09:51:08 \u001b[32mINFO     \u001b[0m train.py: [8/300], [250/484], step: 4122, 1.525 samples/sec, batch_loss: 0.0534, batch_loss_c: 0.0481, batch_loss_s: 0.0657, time:26.2220, lr:1e-06\u001b[0m\n",
            "2019-12-19 09:51:32 \u001b[32mINFO     \u001b[0m train.py: [8/300], [260/484], step: 4132, 1.668 samples/sec, batch_loss: 0.0563, batch_loss_c: 0.0534, batch_loss_s: 0.0630, time:23.9761, lr:1e-06\u001b[0m\n",
            "2019-12-19 09:52:10 \u001b[32mINFO     \u001b[0m train.py: [8/300], [270/484], step: 4142, 1.064 samples/sec, batch_loss: 0.3274, batch_loss_c: 0.3113, batch_loss_s: 0.3651, time:37.6030, lr:1e-06\u001b[0m\n",
            "2019-12-19 09:52:34 \u001b[32mINFO     \u001b[0m train.py: [8/300], [280/484], step: 4152, 1.636 samples/sec, batch_loss: 0.0608, batch_loss_c: 0.0622, batch_loss_s: 0.0574, time:24.4498, lr:1e-06\u001b[0m\n",
            "2019-12-19 09:53:06 \u001b[32mINFO     \u001b[0m train.py: [8/300], [290/484], step: 4162, 1.285 samples/sec, batch_loss: 0.2991, batch_loss_c: 0.2958, batch_loss_s: 0.3069, time:31.1203, lr:1e-06\u001b[0m\n",
            "2019-12-19 09:53:31 \u001b[32mINFO     \u001b[0m train.py: [8/300], [300/484], step: 4172, 1.559 samples/sec, batch_loss: 0.1110, batch_loss_c: 0.1084, batch_loss_s: 0.1169, time:25.6648, lr:1e-06\u001b[0m\n",
            "2019-12-19 09:54:02 \u001b[32mINFO     \u001b[0m train.py: [8/300], [310/484], step: 4182, 1.317 samples/sec, batch_loss: 0.0459, batch_loss_c: 0.0378, batch_loss_s: 0.0646, time:30.3736, lr:1e-06\u001b[0m\n",
            "2019-12-19 09:54:40 \u001b[32mINFO     \u001b[0m train.py: [8/300], [320/484], step: 4192, 1.054 samples/sec, batch_loss: 0.0688, batch_loss_c: 0.0634, batch_loss_s: 0.0812, time:37.9631, lr:1e-06\u001b[0m\n",
            "2019-12-19 09:55:04 \u001b[32mINFO     \u001b[0m train.py: [8/300], [330/484], step: 4202, 1.640 samples/sec, batch_loss: 0.2907, batch_loss_c: 0.2846, batch_loss_s: 0.3048, time:24.3970, lr:1e-06\u001b[0m\n",
            "2019-12-19 09:55:33 \u001b[32mINFO     \u001b[0m train.py: [8/300], [340/484], step: 4212, 1.382 samples/sec, batch_loss: 0.0991, batch_loss_c: 0.0983, batch_loss_s: 0.1008, time:28.9381, lr:1e-06\u001b[0m\n",
            "2019-12-19 09:56:10 \u001b[32mINFO     \u001b[0m train.py: [8/300], [350/484], step: 4222, 1.086 samples/sec, batch_loss: 0.3054, batch_loss_c: 0.3004, batch_loss_s: 0.3171, time:36.8181, lr:1e-06\u001b[0m\n",
            "2019-12-19 09:56:46 \u001b[32mINFO     \u001b[0m train.py: [8/300], [360/484], step: 4232, 1.099 samples/sec, batch_loss: 0.1123, batch_loss_c: 0.1051, batch_loss_s: 0.1293, time:36.3999, lr:1e-06\u001b[0m\n",
            "2019-12-19 09:57:19 \u001b[32mINFO     \u001b[0m train.py: [8/300], [370/484], step: 4242, 1.199 samples/sec, batch_loss: 0.0900, batch_loss_c: 0.0719, batch_loss_s: 0.1325, time:33.3597, lr:1e-06\u001b[0m\n",
            "2019-12-19 09:57:48 \u001b[32mINFO     \u001b[0m train.py: [8/300], [380/484], step: 4252, 1.388 samples/sec, batch_loss: 0.0545, batch_loss_c: 0.0502, batch_loss_s: 0.0644, time:28.8125, lr:1e-06\u001b[0m\n",
            "2019-12-19 09:58:14 \u001b[32mINFO     \u001b[0m train.py: [8/300], [390/484], step: 4262, 1.568 samples/sec, batch_loss: 0.0428, batch_loss_c: 0.0327, batch_loss_s: 0.0663, time:25.5178, lr:1e-06\u001b[0m\n",
            "2019-12-19 09:58:46 \u001b[32mINFO     \u001b[0m train.py: [8/300], [400/484], step: 4272, 1.230 samples/sec, batch_loss: 0.0885, batch_loss_c: 0.0889, batch_loss_s: 0.0875, time:32.5096, lr:1e-06\u001b[0m\n",
            "2019-12-19 09:59:21 \u001b[32mINFO     \u001b[0m train.py: [8/300], [410/484], step: 4282, 1.167 samples/sec, batch_loss: 0.1136, batch_loss_c: 0.1007, batch_loss_s: 0.1438, time:34.2678, lr:1e-06\u001b[0m\n",
            "2019-12-19 09:59:51 \u001b[32mINFO     \u001b[0m train.py: [8/300], [420/484], step: 4292, 1.335 samples/sec, batch_loss: 0.3173, batch_loss_c: 0.3129, batch_loss_s: 0.3274, time:29.9616, lr:1e-06\u001b[0m\n",
            "2019-12-19 10:00:40 \u001b[32mINFO     \u001b[0m train.py: [8/300], [430/484], step: 4302, 0.814 samples/sec, batch_loss: 0.3549, batch_loss_c: 0.3553, batch_loss_s: 0.3538, time:49.1217, lr:1e-06\u001b[0m\n",
            "2019-12-19 10:01:06 \u001b[32mINFO     \u001b[0m train.py: [8/300], [440/484], step: 4312, 1.522 samples/sec, batch_loss: 0.0709, batch_loss_c: 0.0624, batch_loss_s: 0.0908, time:26.2800, lr:1e-06\u001b[0m\n",
            "2019-12-19 10:01:29 \u001b[32mINFO     \u001b[0m train.py: [8/300], [450/484], step: 4322, 1.770 samples/sec, batch_loss: 0.0609, batch_loss_c: 0.0566, batch_loss_s: 0.0708, time:22.5986, lr:1e-06\u001b[0m\n",
            "2019-12-19 10:02:03 \u001b[32mINFO     \u001b[0m train.py: [8/300], [460/484], step: 4332, 1.172 samples/sec, batch_loss: 0.0518, batch_loss_c: 0.0433, batch_loss_s: 0.0715, time:34.1180, lr:1e-06\u001b[0m\n",
            "2019-12-19 10:02:28 \u001b[32mINFO     \u001b[0m train.py: [8/300], [470/484], step: 4342, 1.585 samples/sec, batch_loss: 0.3220, batch_loss_c: 0.3035, batch_loss_s: 0.3651, time:25.2423, lr:1e-06\u001b[0m\n",
            "2019-12-19 10:03:08 \u001b[32mINFO     \u001b[0m train.py: [8/300], [480/484], step: 4352, 1.009 samples/sec, batch_loss: 0.0558, batch_loss_c: 0.0516, batch_loss_s: 0.0657, time:39.6493, lr:1e-06\u001b[0m\n",
            "2019-12-19 10:03:20 \u001b[32mINFO     \u001b[0m train.py: [8/300], train_loss: 0.1379, time: 1470.2837, lr: 1e-06\u001b[0m\n",
            "2019-12-19 10:03:23 \u001b[32mINFO     \u001b[0m train.py: [9/300], [0/484], step: 4356, 15.738 samples/sec, batch_loss: 0.0620, batch_loss_c: 0.0454, batch_loss_s: 0.1007, time:2.5417, lr:1e-06\u001b[0m\n",
            "2019-12-19 10:04:07 \u001b[32mINFO     \u001b[0m train.py: [9/300], [10/484], step: 4366, 0.922 samples/sec, batch_loss: 0.0518, batch_loss_c: 0.0408, batch_loss_s: 0.0773, time:43.3653, lr:1e-06\u001b[0m\n",
            "2019-12-19 10:04:39 \u001b[32mINFO     \u001b[0m train.py: [9/300], [20/484], step: 4376, 1.223 samples/sec, batch_loss: 0.0461, batch_loss_c: 0.0398, batch_loss_s: 0.0608, time:32.7066, lr:1e-06\u001b[0m\n",
            "2019-12-19 10:05:14 \u001b[32mINFO     \u001b[0m train.py: [9/300], [30/484], step: 4386, 1.153 samples/sec, batch_loss: 0.3031, batch_loss_c: 0.2946, batch_loss_s: 0.3228, time:34.6822, lr:1e-06\u001b[0m\n",
            "2019-12-19 10:05:38 \u001b[32mINFO     \u001b[0m train.py: [9/300], [40/484], step: 4396, 1.683 samples/sec, batch_loss: 0.0802, batch_loss_c: 0.0654, batch_loss_s: 0.1148, time:23.7614, lr:1e-06\u001b[0m\n",
            "2019-12-19 10:06:01 \u001b[32mINFO     \u001b[0m train.py: [9/300], [50/484], step: 4406, 1.700 samples/sec, batch_loss: 0.1525, batch_loss_c: 0.1712, batch_loss_s: 0.1088, time:23.5238, lr:1e-06\u001b[0m\n",
            "2019-12-19 10:06:44 \u001b[32mINFO     \u001b[0m train.py: [9/300], [60/484], step: 4416, 0.935 samples/sec, batch_loss: 0.0824, batch_loss_c: 0.0733, batch_loss_s: 0.1035, time:42.7860, lr:1e-06\u001b[0m\n",
            "2019-12-19 10:07:15 \u001b[32mINFO     \u001b[0m train.py: [9/300], [70/484], step: 4426, 1.318 samples/sec, batch_loss: 0.6208, batch_loss_c: 0.6421, batch_loss_s: 0.5713, time:30.3377, lr:1e-06\u001b[0m\n",
            "2019-12-19 10:07:44 \u001b[32mINFO     \u001b[0m train.py: [9/300], [80/484], step: 4436, 1.343 samples/sec, batch_loss: 0.0497, batch_loss_c: 0.0389, batch_loss_s: 0.0748, time:29.7874, lr:1e-06\u001b[0m\n",
            "2019-12-19 10:08:22 \u001b[32mINFO     \u001b[0m train.py: [9/300], [90/484], step: 4446, 1.077 samples/sec, batch_loss: 0.0723, batch_loss_c: 0.0640, batch_loss_s: 0.0917, time:37.1432, lr:1e-06\u001b[0m\n",
            "2019-12-19 10:08:48 \u001b[32mINFO     \u001b[0m train.py: [9/300], [100/484], step: 4456, 1.489 samples/sec, batch_loss: 0.3518, batch_loss_c: 0.3458, batch_loss_s: 0.3660, time:26.8642, lr:1e-06\u001b[0m\n",
            "2019-12-19 10:09:20 \u001b[32mINFO     \u001b[0m train.py: [9/300], [110/484], step: 4466, 1.262 samples/sec, batch_loss: 0.0443, batch_loss_c: 0.0397, batch_loss_s: 0.0549, time:31.6909, lr:1e-06\u001b[0m\n",
            "2019-12-19 10:09:49 \u001b[32mINFO     \u001b[0m train.py: [9/300], [120/484], step: 4476, 1.393 samples/sec, batch_loss: 0.0502, batch_loss_c: 0.0446, batch_loss_s: 0.0635, time:28.7175, lr:1e-06\u001b[0m\n",
            "2019-12-19 10:10:29 \u001b[32mINFO     \u001b[0m train.py: [9/300], [130/484], step: 4486, 0.988 samples/sec, batch_loss: 0.1072, batch_loss_c: 0.1125, batch_loss_s: 0.0950, time:40.4990, lr:1e-06\u001b[0m\n",
            "2019-12-19 10:10:57 \u001b[32mINFO     \u001b[0m train.py: [9/300], [140/484], step: 4496, 1.426 samples/sec, batch_loss: 0.3071, batch_loss_c: 0.3008, batch_loss_s: 0.3220, time:28.0556, lr:1e-06\u001b[0m\n",
            "2019-12-19 10:11:20 \u001b[32mINFO     \u001b[0m train.py: [9/300], [150/484], step: 4506, 1.782 samples/sec, batch_loss: 0.0411, batch_loss_c: 0.0306, batch_loss_s: 0.0659, time:22.4447, lr:1e-06\u001b[0m\n",
            "2019-12-19 10:11:50 \u001b[32mINFO     \u001b[0m train.py: [9/300], [160/484], step: 4516, 1.333 samples/sec, batch_loss: 0.0978, batch_loss_c: 0.0842, batch_loss_s: 0.1296, time:30.0082, lr:1e-06\u001b[0m\n",
            "2019-12-19 10:12:29 \u001b[32mINFO     \u001b[0m train.py: [9/300], [170/484], step: 4526, 1.027 samples/sec, batch_loss: 0.0841, batch_loss_c: 0.0765, batch_loss_s: 0.1019, time:38.9653, lr:1e-06\u001b[0m\n",
            "2019-12-19 10:12:54 \u001b[32mINFO     \u001b[0m train.py: [9/300], [180/484], step: 4536, 1.564 samples/sec, batch_loss: 0.0570, batch_loss_c: 0.0552, batch_loss_s: 0.0612, time:25.5828, lr:1e-06\u001b[0m\n",
            "2019-12-19 10:13:17 \u001b[32mINFO     \u001b[0m train.py: [9/300], [190/484], step: 4546, 1.757 samples/sec, batch_loss: 0.0649, batch_loss_c: 0.0547, batch_loss_s: 0.0885, time:22.7657, lr:1e-06\u001b[0m\n",
            "2019-12-19 10:13:44 \u001b[32mINFO     \u001b[0m train.py: [9/300], [200/484], step: 4556, 1.471 samples/sec, batch_loss: 0.3327, batch_loss_c: 0.3190, batch_loss_s: 0.3646, time:27.1977, lr:1e-06\u001b[0m\n",
            "2019-12-19 10:14:08 \u001b[32mINFO     \u001b[0m train.py: [9/300], [210/484], step: 4566, 1.715 samples/sec, batch_loss: 0.0988, batch_loss_c: 0.1016, batch_loss_s: 0.0922, time:23.3183, lr:1e-06\u001b[0m\n",
            "2019-12-19 10:14:46 \u001b[32mINFO     \u001b[0m train.py: [9/300], [220/484], step: 4576, 1.030 samples/sec, batch_loss: 0.0809, batch_loss_c: 0.0599, batch_loss_s: 0.1298, time:38.8288, lr:1e-06\u001b[0m\n",
            "2019-12-19 10:15:09 \u001b[32mINFO     \u001b[0m train.py: [9/300], [230/484], step: 4586, 1.809 samples/sec, batch_loss: 0.0965, batch_loss_c: 0.1109, batch_loss_s: 0.0627, time:22.1077, lr:1e-06\u001b[0m\n",
            "2019-12-19 10:15:36 \u001b[32mINFO     \u001b[0m train.py: [9/300], [240/484], step: 4596, 1.484 samples/sec, batch_loss: 0.0876, batch_loss_c: 0.0812, batch_loss_s: 0.1025, time:26.9456, lr:1e-06\u001b[0m\n",
            "2019-12-19 10:16:09 \u001b[32mINFO     \u001b[0m train.py: [9/300], [250/484], step: 4606, 1.200 samples/sec, batch_loss: 0.0542, batch_loss_c: 0.0426, batch_loss_s: 0.0812, time:33.3342, lr:1e-06\u001b[0m\n",
            "2019-12-19 10:16:54 \u001b[32mINFO     \u001b[0m train.py: [9/300], [260/484], step: 4616, 0.886 samples/sec, batch_loss: 0.0767, batch_loss_c: 0.0662, batch_loss_s: 0.1012, time:45.1511, lr:1e-06\u001b[0m\n",
            "2019-12-19 10:17:24 \u001b[32mINFO     \u001b[0m train.py: [9/300], [270/484], step: 4626, 1.335 samples/sec, batch_loss: 0.0606, batch_loss_c: 0.0579, batch_loss_s: 0.0671, time:29.9642, lr:1e-06\u001b[0m\n",
            "2019-12-19 10:18:03 \u001b[32mINFO     \u001b[0m train.py: [9/300], [280/484], step: 4636, 1.024 samples/sec, batch_loss: 0.1228, batch_loss_c: 0.1156, batch_loss_s: 0.1396, time:39.0454, lr:1e-06\u001b[0m\n",
            "2019-12-19 10:18:27 \u001b[32mINFO     \u001b[0m train.py: [9/300], [290/484], step: 4646, 1.648 samples/sec, batch_loss: 0.0510, batch_loss_c: 0.0394, batch_loss_s: 0.0781, time:24.2709, lr:1e-06\u001b[0m\n",
            "2019-12-19 10:18:56 \u001b[32mINFO     \u001b[0m train.py: [9/300], [300/484], step: 4656, 1.385 samples/sec, batch_loss: 0.0519, batch_loss_c: 0.0399, batch_loss_s: 0.0798, time:28.8843, lr:1e-06\u001b[0m\n",
            "2019-12-19 10:19:29 \u001b[32mINFO     \u001b[0m train.py: [9/300], [310/484], step: 4666, 1.235 samples/sec, batch_loss: 0.3159, batch_loss_c: 0.3190, batch_loss_s: 0.3085, time:32.3960, lr:1e-06\u001b[0m\n",
            "2019-12-19 10:20:08 \u001b[32mINFO     \u001b[0m train.py: [9/300], [320/484], step: 4676, 1.006 samples/sec, batch_loss: 0.1079, batch_loss_c: 0.0935, batch_loss_s: 0.1417, time:39.7700, lr:1e-06\u001b[0m\n",
            "2019-12-19 10:20:36 \u001b[32mINFO     \u001b[0m train.py: [9/300], [330/484], step: 4686, 1.423 samples/sec, batch_loss: 0.2166, batch_loss_c: 0.2157, batch_loss_s: 0.2186, time:28.1128, lr:1e-06\u001b[0m\n",
            "2019-12-19 10:21:08 \u001b[32mINFO     \u001b[0m train.py: [9/300], [340/484], step: 4696, 1.255 samples/sec, batch_loss: 0.0802, batch_loss_c: 0.0723, batch_loss_s: 0.0987, time:31.8691, lr:1e-06\u001b[0m\n",
            "2019-12-19 10:21:38 \u001b[32mINFO     \u001b[0m train.py: [9/300], [350/484], step: 4706, 1.371 samples/sec, batch_loss: 0.1337, batch_loss_c: 0.1393, batch_loss_s: 0.1208, time:29.1828, lr:1e-06\u001b[0m\n",
            "2019-12-19 10:22:11 \u001b[32mINFO     \u001b[0m train.py: [9/300], [360/484], step: 4716, 1.206 samples/sec, batch_loss: 0.1101, batch_loss_c: 0.1114, batch_loss_s: 0.1072, time:33.1786, lr:1e-06\u001b[0m\n",
            "2019-12-19 10:22:45 \u001b[32mINFO     \u001b[0m train.py: [9/300], [370/484], step: 4726, 1.164 samples/sec, batch_loss: 0.1083, batch_loss_c: 0.1110, batch_loss_s: 0.1020, time:34.3726, lr:1e-06\u001b[0m\n",
            "2019-12-19 10:23:15 \u001b[32mINFO     \u001b[0m train.py: [9/300], [380/484], step: 4736, 1.318 samples/sec, batch_loss: 0.0695, batch_loss_c: 0.0546, batch_loss_s: 0.1043, time:30.3481, lr:1e-06\u001b[0m\n",
            "2019-12-19 10:23:39 \u001b[32mINFO     \u001b[0m train.py: [9/300], [390/484], step: 4746, 1.700 samples/sec, batch_loss: 0.1377, batch_loss_c: 0.1199, batch_loss_s: 0.1793, time:23.5352, lr:1e-06\u001b[0m\n",
            "2019-12-19 10:24:19 \u001b[32mINFO     \u001b[0m train.py: [9/300], [400/484], step: 4756, 0.989 samples/sec, batch_loss: 0.0734, batch_loss_c: 0.0705, batch_loss_s: 0.0803, time:40.4404, lr:1e-06\u001b[0m\n",
            "2019-12-19 10:24:47 \u001b[32mINFO     \u001b[0m train.py: [9/300], [410/484], step: 4766, 1.429 samples/sec, batch_loss: 0.1172, batch_loss_c: 0.1337, batch_loss_s: 0.0787, time:27.9923, lr:1e-06\u001b[0m\n",
            "2019-12-19 10:25:11 \u001b[32mINFO     \u001b[0m train.py: [9/300], [420/484], step: 4776, 1.701 samples/sec, batch_loss: 0.0380, batch_loss_c: 0.0327, batch_loss_s: 0.0502, time:23.5208, lr:1e-06\u001b[0m\n",
            "2019-12-19 10:25:49 \u001b[32mINFO     \u001b[0m train.py: [9/300], [430/484], step: 4786, 1.058 samples/sec, batch_loss: 0.1485, batch_loss_c: 0.1624, batch_loss_s: 0.1161, time:37.8093, lr:1e-06\u001b[0m\n",
            "2019-12-19 10:26:16 \u001b[32mINFO     \u001b[0m train.py: [9/300], [440/484], step: 4796, 1.477 samples/sec, batch_loss: 0.3063, batch_loss_c: 0.2980, batch_loss_s: 0.3257, time:27.0732, lr:1e-06\u001b[0m\n",
            "2019-12-19 10:26:52 \u001b[32mINFO     \u001b[0m train.py: [9/300], [450/484], step: 4806, 1.099 samples/sec, batch_loss: 0.0470, batch_loss_c: 0.0393, batch_loss_s: 0.0651, time:36.4051, lr:1e-06\u001b[0m\n",
            "2019-12-19 10:27:23 \u001b[32mINFO     \u001b[0m train.py: [9/300], [460/484], step: 4816, 1.316 samples/sec, batch_loss: 0.3072, batch_loss_c: 0.2929, batch_loss_s: 0.3404, time:30.3970, lr:1e-06\u001b[0m\n",
            "2019-12-19 10:27:48 \u001b[32mINFO     \u001b[0m train.py: [9/300], [470/484], step: 4826, 1.548 samples/sec, batch_loss: 0.0411, batch_loss_c: 0.0344, batch_loss_s: 0.0566, time:25.8361, lr:1e-06\u001b[0m\n",
            "2019-12-19 10:28:18 \u001b[32mINFO     \u001b[0m train.py: [9/300], [480/484], step: 4836, 1.373 samples/sec, batch_loss: 0.1804, batch_loss_c: 0.1724, batch_loss_s: 0.1991, time:29.1303, lr:1e-06\u001b[0m\n",
            "2019-12-19 10:28:24 \u001b[32mINFO     \u001b[0m train.py: [9/300], train_loss: 0.1381, time: 1503.5353, lr: 1e-06\u001b[0m\n",
            "2019-12-19 10:28:41 \u001b[32mINFO     \u001b[0m train.py: [10/300], [0/484], step: 4840, 2.424 samples/sec, batch_loss: 0.1069, batch_loss_c: 0.0949, batch_loss_s: 0.1348, time:16.5043, lr:1e-06\u001b[0m\n",
            "2019-12-19 10:29:19 \u001b[32mINFO     \u001b[0m train.py: [10/300], [10/484], step: 4850, 1.068 samples/sec, batch_loss: 0.0703, batch_loss_c: 0.0627, batch_loss_s: 0.0879, time:37.4514, lr:1e-06\u001b[0m\n",
            "2019-12-19 10:30:08 \u001b[32mINFO     \u001b[0m train.py: [10/300], [20/484], step: 4860, 0.814 samples/sec, batch_loss: 0.3270, batch_loss_c: 0.3036, batch_loss_s: 0.3815, time:49.1673, lr:1e-06\u001b[0m\n",
            "2019-12-19 10:31:03 \u001b[32mINFO     \u001b[0m train.py: [10/300], [30/484], step: 4870, 0.728 samples/sec, batch_loss: 0.2119, batch_loss_c: 0.2100, batch_loss_s: 0.2164, time:54.9158, lr:1e-06\u001b[0m\n",
            "2019-12-19 10:31:29 \u001b[32mINFO     \u001b[0m train.py: [10/300], [40/484], step: 4880, 1.512 samples/sec, batch_loss: 0.0573, batch_loss_c: 0.0514, batch_loss_s: 0.0713, time:26.4534, lr:1e-06\u001b[0m\n",
            "2019-12-19 10:32:07 \u001b[32mINFO     \u001b[0m train.py: [10/300], [50/484], step: 4890, 1.060 samples/sec, batch_loss: 0.3501, batch_loss_c: 0.3363, batch_loss_s: 0.3824, time:37.7236, lr:1e-06\u001b[0m\n",
            "2019-12-19 10:32:45 \u001b[32mINFO     \u001b[0m train.py: [10/300], [60/484], step: 4900, 1.044 samples/sec, batch_loss: 0.2992, batch_loss_c: 0.2960, batch_loss_s: 0.3067, time:38.3247, lr:1e-06\u001b[0m\n",
            "2019-12-19 10:33:09 \u001b[32mINFO     \u001b[0m train.py: [10/300], [70/484], step: 4910, 1.688 samples/sec, batch_loss: 0.0631, batch_loss_c: 0.0567, batch_loss_s: 0.0781, time:23.6906, lr:1e-06\u001b[0m\n",
            "2019-12-19 10:33:44 \u001b[32mINFO     \u001b[0m train.py: [10/300], [80/484], step: 4920, 1.147 samples/sec, batch_loss: 0.2776, batch_loss_c: 0.2727, batch_loss_s: 0.2889, time:34.8811, lr:1e-06\u001b[0m\n",
            "2019-12-19 10:34:08 \u001b[32mINFO     \u001b[0m train.py: [10/300], [90/484], step: 4930, 1.662 samples/sec, batch_loss: 0.3301, batch_loss_c: 0.3256, batch_loss_s: 0.3405, time:24.0602, lr:1e-06\u001b[0m\n",
            "2019-12-19 10:34:37 \u001b[32mINFO     \u001b[0m train.py: [10/300], [100/484], step: 4940, 1.400 samples/sec, batch_loss: 0.0591, batch_loss_c: 0.0515, batch_loss_s: 0.0769, time:28.5657, lr:1e-06\u001b[0m\n",
            "2019-12-19 10:34:59 \u001b[32mINFO     \u001b[0m train.py: [10/300], [110/484], step: 4950, 1.802 samples/sec, batch_loss: 0.0979, batch_loss_c: 0.0884, batch_loss_s: 0.1202, time:22.1958, lr:1e-06\u001b[0m\n",
            "2019-12-19 10:35:23 \u001b[32mINFO     \u001b[0m train.py: [10/300], [120/484], step: 4960, 1.659 samples/sec, batch_loss: 0.0578, batch_loss_c: 0.0432, batch_loss_s: 0.0921, time:24.1136, lr:1e-06\u001b[0m\n",
            "2019-12-19 10:35:49 \u001b[32mINFO     \u001b[0m train.py: [10/300], [130/484], step: 4970, 1.520 samples/sec, batch_loss: 0.0518, batch_loss_c: 0.0388, batch_loss_s: 0.0821, time:26.3213, lr:1e-06\u001b[0m\n",
            "2019-12-19 10:36:18 \u001b[32mINFO     \u001b[0m train.py: [10/300], [140/484], step: 4980, 1.377 samples/sec, batch_loss: 0.1217, batch_loss_c: 0.1234, batch_loss_s: 0.1177, time:29.0518, lr:1e-06\u001b[0m\n",
            "2019-12-19 10:37:06 \u001b[32mINFO     \u001b[0m train.py: [10/300], [150/484], step: 4990, 0.840 samples/sec, batch_loss: 0.2867, batch_loss_c: 0.2843, batch_loss_s: 0.2922, time:47.6234, lr:1e-06\u001b[0m\n",
            "2019-12-19 10:37:34 \u001b[32mINFO     \u001b[0m train.py: [10/300], [160/484], step: 5000, 1.409 samples/sec, batch_loss: 0.1484, batch_loss_c: 0.1495, batch_loss_s: 0.1458, time:28.3808, lr:1e-06\u001b[0m\n",
            "2019-12-19 10:38:01 \u001b[32mINFO     \u001b[0m train.py: [10/300], [170/484], step: 5010, 1.482 samples/sec, batch_loss: 0.1263, batch_loss_c: 0.1317, batch_loss_s: 0.1138, time:26.9967, lr:1e-06\u001b[0m\n",
            "2019-12-19 10:38:29 \u001b[32mINFO     \u001b[0m train.py: [10/300], [180/484], step: 5020, 1.438 samples/sec, batch_loss: 0.0691, batch_loss_c: 0.0720, batch_loss_s: 0.0625, time:27.8068, lr:1e-06\u001b[0m\n",
            "2019-12-19 10:38:56 \u001b[32mINFO     \u001b[0m train.py: [10/300], [190/484], step: 5030, 1.516 samples/sec, batch_loss: 0.3132, batch_loss_c: 0.3129, batch_loss_s: 0.3138, time:26.3879, lr:1e-06\u001b[0m\n",
            "2019-12-19 10:39:19 \u001b[32mINFO     \u001b[0m train.py: [10/300], [200/484], step: 5040, 1.725 samples/sec, batch_loss: 0.0846, batch_loss_c: 0.0836, batch_loss_s: 0.0871, time:23.1864, lr:1e-06\u001b[0m\n",
            "2019-12-19 10:39:53 \u001b[32mINFO     \u001b[0m train.py: [10/300], [210/484], step: 5050, 1.175 samples/sec, batch_loss: 0.0587, batch_loss_c: 0.0507, batch_loss_s: 0.0773, time:34.0417, lr:1e-06\u001b[0m\n",
            "2019-12-19 10:40:30 \u001b[32mINFO     \u001b[0m train.py: [10/300], [220/484], step: 5060, 1.072 samples/sec, batch_loss: 0.0711, batch_loss_c: 0.0599, batch_loss_s: 0.0972, time:37.3247, lr:1e-06\u001b[0m\n",
            "2019-12-19 10:40:57 \u001b[32mINFO     \u001b[0m train.py: [10/300], [230/484], step: 5070, 1.495 samples/sec, batch_loss: 0.0732, batch_loss_c: 0.0637, batch_loss_s: 0.0956, time:26.7506, lr:1e-06\u001b[0m\n",
            "2019-12-19 10:41:20 \u001b[32mINFO     \u001b[0m train.py: [10/300], [240/484], step: 5080, 1.736 samples/sec, batch_loss: 0.0374, batch_loss_c: 0.0302, batch_loss_s: 0.0542, time:23.0431, lr:1e-06\u001b[0m\n",
            "2019-12-19 10:41:52 \u001b[32mINFO     \u001b[0m train.py: [10/300], [250/484], step: 5090, 1.243 samples/sec, batch_loss: 0.0608, batch_loss_c: 0.0563, batch_loss_s: 0.0714, time:32.1697, lr:1e-06\u001b[0m\n",
            "2019-12-19 10:42:16 \u001b[32mINFO     \u001b[0m train.py: [10/300], [260/484], step: 5100, 1.653 samples/sec, batch_loss: 0.1867, batch_loss_c: 0.1782, batch_loss_s: 0.2064, time:24.2024, lr:1e-06\u001b[0m\n",
            "2019-12-19 10:42:42 \u001b[32mINFO     \u001b[0m train.py: [10/300], [270/484], step: 5110, 1.536 samples/sec, batch_loss: 0.0777, batch_loss_c: 0.0701, batch_loss_s: 0.0956, time:26.0363, lr:1e-06\u001b[0m\n",
            "2019-12-19 10:43:17 \u001b[32mINFO     \u001b[0m train.py: [10/300], [280/484], step: 5120, 1.152 samples/sec, batch_loss: 0.1003, batch_loss_c: 0.0940, batch_loss_s: 0.1148, time:34.7322, lr:1e-06\u001b[0m\n",
            "2019-12-19 10:43:42 \u001b[32mINFO     \u001b[0m train.py: [10/300], [290/484], step: 5130, 1.584 samples/sec, batch_loss: 0.0554, batch_loss_c: 0.0484, batch_loss_s: 0.0716, time:25.2545, lr:1e-06\u001b[0m\n",
            "2019-12-19 10:44:37 \u001b[32mINFO     \u001b[0m train.py: [10/300], [300/484], step: 5140, 0.734 samples/sec, batch_loss: 0.0491, batch_loss_c: 0.0376, batch_loss_s: 0.0759, time:54.5139, lr:1e-06\u001b[0m\n",
            "2019-12-19 10:45:08 \u001b[32mINFO     \u001b[0m train.py: [10/300], [310/484], step: 5150, 1.300 samples/sec, batch_loss: 0.0540, batch_loss_c: 0.0437, batch_loss_s: 0.0783, time:30.7596, lr:1e-06\u001b[0m\n",
            "2019-12-19 10:45:33 \u001b[32mINFO     \u001b[0m train.py: [10/300], [320/484], step: 5160, 1.586 samples/sec, batch_loss: 0.0529, batch_loss_c: 0.0465, batch_loss_s: 0.0677, time:25.2238, lr:1e-06\u001b[0m\n",
            "2019-12-19 10:45:57 \u001b[32mINFO     \u001b[0m train.py: [10/300], [330/484], step: 5170, 1.649 samples/sec, batch_loss: 0.1216, batch_loss_c: 0.1173, batch_loss_s: 0.1316, time:24.2615, lr:1e-06\u001b[0m\n",
            "2019-12-19 10:46:34 \u001b[32mINFO     \u001b[0m train.py: [10/300], [340/484], step: 5180, 1.084 samples/sec, batch_loss: 0.1011, batch_loss_c: 0.0737, batch_loss_s: 0.1650, time:36.9065, lr:1e-06\u001b[0m\n",
            "2019-12-19 10:46:56 \u001b[32mINFO     \u001b[0m train.py: [10/300], [350/484], step: 5190, 1.791 samples/sec, batch_loss: 0.0409, batch_loss_c: 0.0327, batch_loss_s: 0.0600, time:22.3360, lr:1e-06\u001b[0m\n",
            "2019-12-19 10:47:20 \u001b[32mINFO     \u001b[0m train.py: [10/300], [360/484], step: 5200, 1.722 samples/sec, batch_loss: 0.0730, batch_loss_c: 0.0692, batch_loss_s: 0.0817, time:23.2330, lr:1e-06\u001b[0m\n",
            "2019-12-19 10:47:46 \u001b[32mINFO     \u001b[0m train.py: [10/300], [370/484], step: 5210, 1.504 samples/sec, batch_loss: 0.0550, batch_loss_c: 0.0492, batch_loss_s: 0.0687, time:26.6024, lr:1e-06\u001b[0m\n",
            "2019-12-19 10:48:21 \u001b[32mINFO     \u001b[0m train.py: [10/300], [380/484], step: 5220, 1.131 samples/sec, batch_loss: 0.0518, batch_loss_c: 0.0396, batch_loss_s: 0.0802, time:35.3616, lr:1e-06\u001b[0m\n",
            "2019-12-19 10:48:48 \u001b[32mINFO     \u001b[0m train.py: [10/300], [390/484], step: 5230, 1.483 samples/sec, batch_loss: 0.0663, batch_loss_c: 0.0651, batch_loss_s: 0.0692, time:26.9771, lr:1e-06\u001b[0m\n",
            "2019-12-19 10:49:13 \u001b[32mINFO     \u001b[0m train.py: [10/300], [400/484], step: 5240, 1.626 samples/sec, batch_loss: 0.0930, batch_loss_c: 0.0956, batch_loss_s: 0.0870, time:24.6012, lr:1e-06\u001b[0m\n",
            "2019-12-19 10:49:40 \u001b[32mINFO     \u001b[0m train.py: [10/300], [410/484], step: 5250, 1.482 samples/sec, batch_loss: 0.0805, batch_loss_c: 0.0732, batch_loss_s: 0.0976, time:26.9856, lr:1e-06\u001b[0m\n",
            "2019-12-19 10:50:03 \u001b[32mINFO     \u001b[0m train.py: [10/300], [420/484], step: 5260, 1.722 samples/sec, batch_loss: 0.1874, batch_loss_c: 0.1741, batch_loss_s: 0.2186, time:23.2275, lr:1e-06\u001b[0m\n",
            "2019-12-19 10:50:34 \u001b[32mINFO     \u001b[0m train.py: [10/300], [430/484], step: 5270, 1.303 samples/sec, batch_loss: 0.4508, batch_loss_c: 0.4875, batch_loss_s: 0.3654, time:30.7012, lr:1e-06\u001b[0m\n",
            "2019-12-19 10:51:22 \u001b[32mINFO     \u001b[0m train.py: [10/300], [440/484], step: 5280, 0.831 samples/sec, batch_loss: 0.3090, batch_loss_c: 0.2923, batch_loss_s: 0.3480, time:48.1322, lr:1e-06\u001b[0m\n",
            "2019-12-19 10:51:55 \u001b[32mINFO     \u001b[0m train.py: [10/300], [450/484], step: 5290, 1.235 samples/sec, batch_loss: 0.1175, batch_loss_c: 0.1301, batch_loss_s: 0.0881, time:32.3969, lr:1e-06\u001b[0m\n",
            "2019-12-19 10:52:32 \u001b[32mINFO     \u001b[0m train.py: [10/300], [460/484], step: 5300, 1.059 samples/sec, batch_loss: 0.5217, batch_loss_c: 0.5189, batch_loss_s: 0.5281, time:37.7834, lr:1e-06\u001b[0m\n",
            "2019-12-19 10:52:55 \u001b[32mINFO     \u001b[0m train.py: [10/300], [470/484], step: 5310, 1.794 samples/sec, batch_loss: 0.0762, batch_loss_c: 0.0629, batch_loss_s: 0.1072, time:22.2975, lr:1e-06\u001b[0m\n",
            "2019-12-19 10:53:21 \u001b[32mINFO     \u001b[0m train.py: [10/300], [480/484], step: 5320, 1.510 samples/sec, batch_loss: 0.1044, batch_loss_c: 0.0785, batch_loss_s: 0.1649, time:26.4951, lr:1e-06\u001b[0m\n",
            "2019-12-19 10:53:32 \u001b[32mINFO     \u001b[0m train.py: [10/300], train_loss: 0.1448, time: 1507.3717, lr: 1e-06\u001b[0m\n",
            "2019-12-19 10:53:35 \u001b[32mINFO     \u001b[0m train.py: [11/300], [0/484], step: 5324, 19.205 samples/sec, batch_loss: 0.0547, batch_loss_c: 0.0499, batch_loss_s: 0.0660, time:2.0827, lr:1e-06\u001b[0m\n",
            "2019-12-19 10:54:08 \u001b[32mINFO     \u001b[0m train.py: [11/300], [10/484], step: 5334, 1.206 samples/sec, batch_loss: 0.0641, batch_loss_c: 0.0515, batch_loss_s: 0.0936, time:33.1660, lr:1e-06\u001b[0m\n",
            "2019-12-19 10:54:44 \u001b[32mINFO     \u001b[0m train.py: [11/300], [20/484], step: 5344, 1.105 samples/sec, batch_loss: 0.0529, batch_loss_c: 0.0434, batch_loss_s: 0.0749, time:36.1958, lr:1e-06\u001b[0m\n",
            "2019-12-19 10:55:10 \u001b[32mINFO     \u001b[0m train.py: [11/300], [30/484], step: 5354, 1.573 samples/sec, batch_loss: 0.0713, batch_loss_c: 0.0628, batch_loss_s: 0.0911, time:25.4273, lr:1e-06\u001b[0m\n",
            "2019-12-19 10:55:49 \u001b[32mINFO     \u001b[0m train.py: [11/300], [40/484], step: 5364, 1.026 samples/sec, batch_loss: 0.0625, batch_loss_c: 0.0619, batch_loss_s: 0.0639, time:39.0049, lr:1e-06\u001b[0m\n",
            "2019-12-19 10:56:15 \u001b[32mINFO     \u001b[0m train.py: [11/300], [50/484], step: 5374, 1.536 samples/sec, batch_loss: 0.0576, batch_loss_c: 0.0518, batch_loss_s: 0.0710, time:26.0374, lr:1e-06\u001b[0m\n",
            "2019-12-19 10:56:43 \u001b[32mINFO     \u001b[0m train.py: [11/300], [60/484], step: 5384, 1.420 samples/sec, batch_loss: 0.0686, batch_loss_c: 0.0651, batch_loss_s: 0.0770, time:28.1706, lr:1e-06\u001b[0m\n",
            "2019-12-19 10:57:09 \u001b[32mINFO     \u001b[0m train.py: [11/300], [70/484], step: 5394, 1.529 samples/sec, batch_loss: 0.1068, batch_loss_c: 0.0971, batch_loss_s: 0.1295, time:26.1556, lr:1e-06\u001b[0m\n",
            "2019-12-19 10:57:43 \u001b[32mINFO     \u001b[0m train.py: [11/300], [80/484], step: 5404, 1.178 samples/sec, batch_loss: 0.1820, batch_loss_c: 0.1829, batch_loss_s: 0.1800, time:33.9611, lr:1e-06\u001b[0m\n",
            "2019-12-19 10:58:25 \u001b[32mINFO     \u001b[0m train.py: [11/300], [90/484], step: 5414, 0.959 samples/sec, batch_loss: 0.0422, batch_loss_c: 0.0354, batch_loss_s: 0.0578, time:41.7115, lr:1e-06\u001b[0m\n",
            "2019-12-19 10:59:00 \u001b[32mINFO     \u001b[0m train.py: [11/300], [100/484], step: 5424, 1.133 samples/sec, batch_loss: 0.1665, batch_loss_c: 0.1655, batch_loss_s: 0.1689, time:35.3168, lr:1e-06\u001b[0m\n",
            "2019-12-19 10:59:30 \u001b[32mINFO     \u001b[0m train.py: [11/300], [110/484], step: 5434, 1.313 samples/sec, batch_loss: 0.2893, batch_loss_c: 0.2824, batch_loss_s: 0.3053, time:30.4611, lr:1e-06\u001b[0m\n",
            "2019-12-19 10:59:59 \u001b[32mINFO     \u001b[0m train.py: [11/300], [120/484], step: 5444, 1.395 samples/sec, batch_loss: 0.3370, batch_loss_c: 0.3122, batch_loss_s: 0.3948, time:28.6682, lr:1e-06\u001b[0m\n",
            "2019-12-19 11:00:23 \u001b[32mINFO     \u001b[0m train.py: [11/300], [130/484], step: 5454, 1.683 samples/sec, batch_loss: 0.0392, batch_loss_c: 0.0315, batch_loss_s: 0.0574, time:23.7684, lr:1e-06\u001b[0m\n",
            "2019-12-19 11:01:04 \u001b[32mINFO     \u001b[0m train.py: [11/300], [140/484], step: 5464, 0.971 samples/sec, batch_loss: 0.1086, batch_loss_c: 0.1041, batch_loss_s: 0.1191, time:41.2045, lr:1e-06\u001b[0m\n",
            "2019-12-19 11:01:31 \u001b[32mINFO     \u001b[0m train.py: [11/300], [150/484], step: 5474, 1.493 samples/sec, batch_loss: 0.1200, batch_loss_c: 0.1218, batch_loss_s: 0.1157, time:26.7962, lr:1e-06\u001b[0m\n",
            "2019-12-19 11:02:07 \u001b[32mINFO     \u001b[0m train.py: [11/300], [160/484], step: 5484, 1.096 samples/sec, batch_loss: 0.0510, batch_loss_c: 0.0463, batch_loss_s: 0.0619, time:36.4925, lr:1e-06\u001b[0m\n",
            "2019-12-19 11:02:34 \u001b[32mINFO     \u001b[0m train.py: [11/300], [170/484], step: 5494, 1.515 samples/sec, batch_loss: 0.1006, batch_loss_c: 0.0954, batch_loss_s: 0.1126, time:26.4045, lr:1e-06\u001b[0m\n",
            "2019-12-19 11:03:14 \u001b[32mINFO     \u001b[0m train.py: [11/300], [180/484], step: 5504, 1.004 samples/sec, batch_loss: 0.1318, batch_loss_c: 0.1511, batch_loss_s: 0.0870, time:39.8337, lr:1e-06\u001b[0m\n",
            "2019-12-19 11:03:44 \u001b[32mINFO     \u001b[0m train.py: [11/300], [190/484], step: 5514, 1.297 samples/sec, batch_loss: 0.1171, batch_loss_c: 0.1279, batch_loss_s: 0.0920, time:30.8314, lr:1e-06\u001b[0m\n",
            "2019-12-19 11:04:08 \u001b[32mINFO     \u001b[0m train.py: [11/300], [200/484], step: 5524, 1.695 samples/sec, batch_loss: 0.0570, batch_loss_c: 0.0547, batch_loss_s: 0.0625, time:23.6046, lr:1e-06\u001b[0m\n",
            "2019-12-19 11:04:37 \u001b[32mINFO     \u001b[0m train.py: [11/300], [210/484], step: 5534, 1.369 samples/sec, batch_loss: 0.5246, batch_loss_c: 0.5199, batch_loss_s: 0.5355, time:29.2116, lr:1e-06\u001b[0m\n",
            "2019-12-19 11:05:32 \u001b[32mINFO     \u001b[0m train.py: [11/300], [220/484], step: 5544, 0.733 samples/sec, batch_loss: 0.0867, batch_loss_c: 0.0743, batch_loss_s: 0.1157, time:54.5405, lr:1e-06\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}