{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PSENet.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kavyajeetbora/PSENet.pytorch/blob/master/PSENet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hhJ-6yyXcnRx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "08956abf-aef1-4a3e-9013-b93e9da52703"
      },
      "source": [
        "import shutil, os\n",
        "os.chdir('/content')\n",
        "directory = '/content/cloned-repo'\n",
        "if os.path.exists(directory):\n",
        "  shutil.rmtree(directory)\n",
        "else:\n",
        "  print('Directory not found, creating new directory...')\n",
        "  !git clone https://github.com/kavyajeetbora/PSENet.pytorch.git /content/cloned-repo\n",
        "  %cd /content/cloned-repo\n",
        "  !ls"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Directory not found, creating new directory...\n",
            "Cloning into '/content/cloned-repo'...\n",
            "remote: Enumerating objects: 65, done.\u001b[K\n",
            "remote: Counting objects: 100% (65/65), done.\u001b[K\n",
            "remote: Compressing objects: 100% (58/58), done.\u001b[K\n",
            "remote: Total 361 (delta 34), reused 15 (delta 7), pack-reused 296\u001b[K\n",
            "Receiving objects: 100% (361/361), 8.12 MiB | 11.50 MiB/s, done.\n",
            "Resolving deltas: 100% (160/160), done.\n",
            "/content/cloned-repo\n",
            "cal_recall  eval.py\t\t     LICENSE\t pse\t    utils\n",
            "config.py   imgs\t\t     models\t README.md\n",
            "dataset     install_dependencies.sh  predict.py  train.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9x221Ya0RBGO",
        "outputId": "98535504-8777-4d1e-8356-c054d8e1a6b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "!chmod +x install_dependencies.sh # make shell script executable\n",
        "!./install_dependencies.sh # run the shell script"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyclipper in /usr/local/lib/python3.6/dist-packages (1.1.0.post1)\n",
            "Requirement already satisfied: Polygon3 in /usr/local/lib/python3.6/dist-packages (3.0.8)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.6/dist-packages (4.0.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eowsKRg-TvLP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1b2daa16-ce41-4533-ab59-73adc9f0683a"
      },
      "source": [
        "!python3 train.py"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "make: Entering directory '/content/cloned-repo/pse'\n",
            "make: 'pse.so' is up to date.\n",
            "make: Leaving directory '/content/cloned-repo/pse'\n",
            "2019-11-20 06:19:57 \u001b[32mINFO     \u001b[0m utils.py: logger init finished\u001b[0m\n",
            "2019-11-20 06:19:57 \u001b[32mINFO     \u001b[0m train.py: {'Lambda': 0.7,\n",
            " 'OHEM_ratio': 3,\n",
            " 'backbone': 'resnet152',\n",
            " 'checkpoint': '',\n",
            " 'data_shape': 640,\n",
            " 'display_input_images': False,\n",
            " 'display_interval': 10,\n",
            " 'display_output_images': False,\n",
            " 'end_lr': 1e-07,\n",
            " 'epochs': 5,\n",
            " 'gpu_id': '0',\n",
            " 'lr': 0.0001,\n",
            " 'lr_decay_step': [200, 400],\n",
            " 'lr_gamma': 0.1,\n",
            " 'm': 0.5,\n",
            " 'n': 6,\n",
            " 'output_dir': 'output/psenet_icd2015_resnet152_4gpu_author_crop_adam_MultiStepLR_authorloss',\n",
            " 'pretrained': True,\n",
            " 'restart_training': True,\n",
            " 'scale': 1,\n",
            " 'seed': 2,\n",
            " 'show_images_interval': 50,\n",
            " 'start_epoch': 0,\n",
            " 'train_batch_size': 4,\n",
            " 'trainroot': '/content/drive/My Drive/Colab Notebooks/padh.ai.notebooks/15. '\n",
            "              'Object Detection/AI4Bharat Dataset/Training Set',\n",
            " 'warm_up_epoch': 6,\n",
            " 'warm_up_lr': 1e-05,\n",
            " 'weight_decay': 0.0005,\n",
            " 'workers': 12}\u001b[0m\n",
            "2019-11-20 06:19:57 \u001b[32mINFO     \u001b[0m train.py: train with gpu 0 and pytorch 1.3.1\u001b[0m\n",
            "Downloading: \"https://download.pytorch.org/models/resnet152-b121ed2d.pth\" to /root/.cache/torch/checkpoints/resnet152-b121ed2d.pth\n",
            "100% 230M/230M [00:03<00:00, 75.3MB/s]\n",
            "2019-11-20 06:54:05 \u001b[32mINFO     \u001b[0m resnet.py: load pretrained models from imagenet\u001b[0m\n",
            "2019-11-20 06:54:11 \u001b[32mINFO     \u001b[0m train.py: train dataset has 5000 samples,1250 in dataloader\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:100: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule.See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "2019-11-20 06:54:42 \u001b[32mINFO     \u001b[0m train.py: [0/5], [0/1250], step: 0, 1.272 samples/sec, batch_loss: 1.0000, batch_loss_c: 1.0000, batch_loss_s: 1.0000, time:31.4363, lr:0.0001\u001b[0m\n",
            "2019-11-20 06:55:07 \u001b[32mINFO     \u001b[0m train.py: [0/5], [10/1250], step: 10, 1.608 samples/sec, batch_loss: 1.0000, batch_loss_c: 1.0000, batch_loss_s: 1.0000, time:24.8829, lr:0.0001\u001b[0m\n",
            "2019-11-20 06:55:32 \u001b[32mINFO     \u001b[0m train.py: [0/5], [20/1250], step: 20, 1.587 samples/sec, batch_loss: 1.0000, batch_loss_c: 1.0000, batch_loss_s: 1.0000, time:25.1995, lr:0.0001\u001b[0m\n",
            "2019-11-20 06:55:58 \u001b[32mINFO     \u001b[0m train.py: [0/5], [30/1250], step: 30, 1.587 samples/sec, batch_loss: 1.0000, batch_loss_c: 1.0000, batch_loss_s: 1.0000, time:25.1986, lr:0.0001\u001b[0m\n",
            "2019-11-20 06:56:23 \u001b[32mINFO     \u001b[0m train.py: [0/5], [40/1250], step: 40, 1.571 samples/sec, batch_loss: 1.0000, batch_loss_c: 1.0000, batch_loss_s: 1.0000, time:25.4582, lr:0.0001\u001b[0m\n",
            "2019-11-20 06:56:48 \u001b[32mINFO     \u001b[0m train.py: [0/5], [50/1250], step: 50, 1.580 samples/sec, batch_loss: 1.0000, batch_loss_c: 1.0000, batch_loss_s: 1.0000, time:25.3204, lr:0.0001\u001b[0m\n",
            "2019-11-20 06:57:14 \u001b[32mINFO     \u001b[0m train.py: [0/5], [60/1250], step: 60, 1.576 samples/sec, batch_loss: 1.0000, batch_loss_c: 1.0000, batch_loss_s: 1.0000, time:25.3737, lr:0.0001\u001b[0m\n",
            "2019-11-20 06:57:39 \u001b[32mINFO     \u001b[0m train.py: [0/5], [70/1250], step: 70, 1.581 samples/sec, batch_loss: 1.0000, batch_loss_c: 1.0000, batch_loss_s: 1.0000, time:25.2964, lr:0.0001\u001b[0m\n",
            "2019-11-20 06:58:04 \u001b[32mINFO     \u001b[0m train.py: [0/5], [80/1250], step: 80, 1.580 samples/sec, batch_loss: 1.0000, batch_loss_c: 1.0000, batch_loss_s: 1.0000, time:25.3163, lr:0.0001\u001b[0m\n",
            "2019-11-20 06:58:30 \u001b[32mINFO     \u001b[0m train.py: [0/5], [90/1250], step: 90, 1.572 samples/sec, batch_loss: 1.0000, batch_loss_c: 1.0000, batch_loss_s: 1.0000, time:25.4450, lr:0.0001\u001b[0m\n",
            "2019-11-20 06:58:55 \u001b[32mINFO     \u001b[0m train.py: [0/5], [100/1250], step: 100, 1.578 samples/sec, batch_loss: 1.0000, batch_loss_c: 1.0000, batch_loss_s: 1.0000, time:25.3532, lr:0.0001\u001b[0m\n",
            "2019-11-20 06:59:21 \u001b[32mINFO     \u001b[0m train.py: [0/5], [110/1250], step: 110, 1.562 samples/sec, batch_loss: 1.0000, batch_loss_c: 1.0000, batch_loss_s: 1.0000, time:25.6099, lr:0.0001\u001b[0m\n",
            "2019-11-20 06:59:46 \u001b[32mINFO     \u001b[0m train.py: [0/5], [120/1250], step: 120, 1.575 samples/sec, batch_loss: 1.0000, batch_loss_c: 1.0000, batch_loss_s: 1.0000, time:25.4003, lr:0.0001\u001b[0m\n",
            "2019-11-20 07:00:12 \u001b[32mINFO     \u001b[0m train.py: [0/5], [130/1250], step: 130, 1.562 samples/sec, batch_loss: 1.0000, batch_loss_c: 1.0000, batch_loss_s: 1.0000, time:25.6104, lr:0.0001\u001b[0m\n",
            "2019-11-20 07:00:37 \u001b[32mINFO     \u001b[0m train.py: [0/5], [140/1250], step: 140, 1.564 samples/sec, batch_loss: 1.0000, batch_loss_c: 1.0000, batch_loss_s: 1.0000, time:25.5779, lr:0.0001\u001b[0m\n",
            "2019-11-20 07:01:03 \u001b[32mINFO     \u001b[0m train.py: [0/5], [150/1250], step: 150, 1.574 samples/sec, batch_loss: 1.0000, batch_loss_c: 1.0000, batch_loss_s: 1.0000, time:25.4097, lr:0.0001\u001b[0m\n",
            "2019-11-20 07:01:28 \u001b[32mINFO     \u001b[0m train.py: [0/5], [160/1250], step: 160, 1.579 samples/sec, batch_loss: 1.0000, batch_loss_c: 1.0000, batch_loss_s: 1.0000, time:25.3330, lr:0.0001\u001b[0m\n",
            "2019-11-20 07:01:53 \u001b[32mINFO     \u001b[0m train.py: [0/5], [170/1250], step: 170, 1.575 samples/sec, batch_loss: 1.0000, batch_loss_c: 1.0000, batch_loss_s: 1.0000, time:25.3956, lr:0.0001\u001b[0m\n",
            "2019-11-20 07:02:19 \u001b[32mINFO     \u001b[0m train.py: [0/5], [180/1250], step: 180, 1.572 samples/sec, batch_loss: 1.0000, batch_loss_c: 1.0000, batch_loss_s: 1.0000, time:25.4488, lr:0.0001\u001b[0m\n",
            "2019-11-20 07:02:44 \u001b[32mINFO     \u001b[0m train.py: [0/5], [190/1250], step: 190, 1.569 samples/sec, batch_loss: 1.0000, batch_loss_c: 1.0000, batch_loss_s: 1.0000, time:25.4956, lr:0.0001\u001b[0m\n",
            "2019-11-20 07:03:10 \u001b[32mINFO     \u001b[0m train.py: [0/5], [200/1250], step: 200, 1.579 samples/sec, batch_loss: 1.0000, batch_loss_c: 1.0000, batch_loss_s: 1.0000, time:25.3319, lr:0.0001\u001b[0m\n",
            "2019-11-20 07:03:35 \u001b[32mINFO     \u001b[0m train.py: [0/5], [210/1250], step: 210, 1.568 samples/sec, batch_loss: 1.0000, batch_loss_c: 1.0000, batch_loss_s: 1.0000, time:25.5057, lr:0.0001\u001b[0m\n",
            "2019-11-20 07:04:01 \u001b[32mINFO     \u001b[0m train.py: [0/5], [220/1250], step: 220, 1.565 samples/sec, batch_loss: 1.0000, batch_loss_c: 1.0000, batch_loss_s: 1.0000, time:25.5534, lr:0.0001\u001b[0m\n",
            "2019-11-20 07:04:26 \u001b[32mINFO     \u001b[0m train.py: [0/5], [230/1250], step: 230, 1.578 samples/sec, batch_loss: 1.0000, batch_loss_c: 1.0000, batch_loss_s: 1.0000, time:25.3548, lr:0.0001\u001b[0m\n",
            "2019-11-20 07:04:52 \u001b[32mINFO     \u001b[0m train.py: [0/5], [240/1250], step: 240, 1.568 samples/sec, batch_loss: 1.0000, batch_loss_c: 1.0000, batch_loss_s: 1.0000, time:25.5028, lr:0.0001\u001b[0m\n",
            "2019-11-20 07:05:17 \u001b[32mINFO     \u001b[0m train.py: [0/5], [250/1250], step: 250, 1.578 samples/sec, batch_loss: 1.0000, batch_loss_c: 1.0000, batch_loss_s: 1.0000, time:25.3561, lr:0.0001\u001b[0m\n",
            "2019-11-20 07:05:42 \u001b[32mINFO     \u001b[0m train.py: [0/5], [260/1250], step: 260, 1.572 samples/sec, batch_loss: 1.0000, batch_loss_c: 1.0000, batch_loss_s: 1.0000, time:25.4406, lr:0.0001\u001b[0m\n",
            "2019-11-20 07:06:08 \u001b[32mINFO     \u001b[0m train.py: [0/5], [270/1250], step: 270, 1.581 samples/sec, batch_loss: 1.0000, batch_loss_c: 1.0000, batch_loss_s: 1.0000, time:25.3021, lr:0.0001\u001b[0m\n",
            "2019-11-20 07:06:33 \u001b[32mINFO     \u001b[0m train.py: [0/5], [280/1250], step: 280, 1.580 samples/sec, batch_loss: 1.0000, batch_loss_c: 1.0000, batch_loss_s: 1.0000, time:25.3091, lr:0.0001\u001b[0m\n",
            "2019-11-20 07:06:59 \u001b[32mINFO     \u001b[0m train.py: [0/5], [290/1250], step: 290, 1.572 samples/sec, batch_loss: 1.0000, batch_loss_c: 1.0000, batch_loss_s: 1.0000, time:25.4448, lr:0.0001\u001b[0m\n",
            "2019-11-20 07:07:24 \u001b[32mINFO     \u001b[0m train.py: [0/5], [300/1250], step: 300, 1.579 samples/sec, batch_loss: 1.0000, batch_loss_c: 1.0000, batch_loss_s: 1.0000, time:25.3250, lr:0.0001\u001b[0m\n",
            "2019-11-20 07:07:49 \u001b[32mINFO     \u001b[0m train.py: [0/5], [310/1250], step: 310, 1.571 samples/sec, batch_loss: 1.0000, batch_loss_c: 1.0000, batch_loss_s: 1.0000, time:25.4680, lr:0.0001\u001b[0m\n",
            "2019-11-20 07:08:15 \u001b[32mINFO     \u001b[0m train.py: [0/5], [320/1250], step: 320, 1.570 samples/sec, batch_loss: 1.0000, batch_loss_c: 1.0000, batch_loss_s: 1.0000, time:25.4816, lr:0.0001\u001b[0m\n",
            "2019-11-20 07:08:40 \u001b[32mINFO     \u001b[0m train.py: [0/5], [330/1250], step: 330, 1.581 samples/sec, batch_loss: 1.0000, batch_loss_c: 1.0000, batch_loss_s: 1.0000, time:25.2994, lr:0.0001\u001b[0m\n",
            "2019-11-20 07:09:05 \u001b[32mINFO     \u001b[0m train.py: [0/5], [340/1250], step: 340, 1.576 samples/sec, batch_loss: 1.0000, batch_loss_c: 1.0000, batch_loss_s: 1.0000, time:25.3849, lr:0.0001\u001b[0m\n",
            "2019-11-20 07:09:31 \u001b[32mINFO     \u001b[0m train.py: [0/5], [350/1250], step: 350, 1.575 samples/sec, batch_loss: 1.0000, batch_loss_c: 1.0000, batch_loss_s: 1.0000, time:25.3931, lr:0.0001\u001b[0m\n",
            "2019-11-20 07:09:56 \u001b[32mINFO     \u001b[0m train.py: [0/5], [360/1250], step: 360, 1.574 samples/sec, batch_loss: 1.0000, batch_loss_c: 1.0000, batch_loss_s: 1.0000, time:25.4090, lr:0.0001\u001b[0m\n",
            "2019-11-20 07:10:22 \u001b[32mINFO     \u001b[0m train.py: [0/5], [370/1250], step: 370, 1.578 samples/sec, batch_loss: 1.0000, batch_loss_c: 1.0000, batch_loss_s: 1.0000, time:25.3540, lr:0.0001\u001b[0m\n",
            "2019-11-20 07:10:47 \u001b[32mINFO     \u001b[0m train.py: [0/5], [380/1250], step: 380, 1.574 samples/sec, batch_loss: 1.0000, batch_loss_c: 1.0000, batch_loss_s: 1.0000, time:25.4071, lr:0.0001\u001b[0m\n",
            "2019-11-20 07:11:13 \u001b[32mINFO     \u001b[0m train.py: [0/5], [390/1250], step: 390, 1.567 samples/sec, batch_loss: 1.0000, batch_loss_c: 1.0000, batch_loss_s: 1.0000, time:25.5335, lr:0.0001\u001b[0m\n",
            "2019-11-20 07:11:38 \u001b[32mINFO     \u001b[0m train.py: [0/5], [400/1250], step: 400, 1.575 samples/sec, batch_loss: 1.0000, batch_loss_c: 1.0000, batch_loss_s: 1.0000, time:25.3983, lr:0.0001\u001b[0m\n",
            "2019-11-20 07:12:03 \u001b[32mINFO     \u001b[0m train.py: [0/5], [410/1250], step: 410, 1.568 samples/sec, batch_loss: 1.0000, batch_loss_c: 1.0000, batch_loss_s: 1.0000, time:25.5062, lr:0.0001\u001b[0m\n",
            "2019-11-20 07:12:29 \u001b[32mINFO     \u001b[0m train.py: [0/5], [420/1250], step: 420, 1.576 samples/sec, batch_loss: 1.0000, batch_loss_c: 1.0000, batch_loss_s: 1.0000, time:25.3744, lr:0.0001\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKd2nqBUgR6A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}